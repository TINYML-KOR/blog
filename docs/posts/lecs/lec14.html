<!DOCTYPE html>
<html xmlns="http://www.w3.org/1999/xhtml" lang="en" xml:lang="en"><head>

<meta charset="utf-8">
<meta name="generator" content="quarto-1.4.554">

<meta name="viewport" content="width=device-width, initial-scale=1.0, user-scalable=yes">

<meta name="author" content="Seunghyun Oh">
<meta name="dcterms.date" content="2024-04-26">
<meta name="description" content="Vision Transformer for TinyML">

<title>TinyML KOR - 🧑‍🏫 Lecture 14</title>
<style>
code{white-space: pre-wrap;}
span.smallcaps{font-variant: small-caps;}
div.columns{display: flex; gap: min(4vw, 1.5em);}
div.column{flex: auto; overflow-x: auto;}
div.hanging-indent{margin-left: 1.5em; text-indent: -1.5em;}
ul.task-list{list-style: none;}
ul.task-list li input[type="checkbox"] {
  width: 0.8em;
  margin: 0 0.8em 0.2em -1em; /* quarto-specific, see https://github.com/quarto-dev/quarto-cli/issues/4556 */ 
  vertical-align: middle;
}
</style>


<script src="../../site_libs/quarto-nav/quarto-nav.js"></script>
<script src="../../site_libs/quarto-nav/headroom.min.js"></script>
<script src="../../site_libs/clipboard/clipboard.min.js"></script>
<script src="../../site_libs/quarto-search/autocomplete.umd.js"></script>
<script src="../../site_libs/quarto-search/fuse.min.js"></script>
<script src="../../site_libs/quarto-search/quarto-search.js"></script>
<meta name="quarto:offset" content="../../">
<script src="../../site_libs/quarto-html/quarto.js"></script>
<script src="../../site_libs/quarto-html/popper.min.js"></script>
<script src="../../site_libs/quarto-html/tippy.umd.min.js"></script>
<script src="../../site_libs/quarto-html/anchor.min.js"></script>
<link href="../../site_libs/quarto-html/tippy.css" rel="stylesheet">
<link href="../../site_libs/quarto-html/quarto-syntax-highlighting.css" rel="stylesheet" id="quarto-text-highlighting-styles">
<script src="../../site_libs/bootstrap/bootstrap.min.js"></script>
<link href="../../site_libs/bootstrap/bootstrap-icons.css" rel="stylesheet">
<link href="../../site_libs/bootstrap/bootstrap.min.css" rel="stylesheet" id="quarto-bootstrap" data-mode="light">
<script id="quarto-search-options" type="application/json">{
  "location": "navbar",
  "copy-button": false,
  "collapse-after": 3,
  "panel-placement": "end",
  "type": "overlay",
  "limit": 50,
  "keyboard-shortcut": [
    "f",
    "/",
    "s"
  ],
  "show-item-context": false,
  "language": {
    "search-no-results-text": "No results",
    "search-matching-documents-text": "matching documents",
    "search-copy-link-title": "Copy link to search",
    "search-hide-matches-text": "Hide additional matches",
    "search-more-match-text": "more match in this document",
    "search-more-matches-text": "more matches in this document",
    "search-clear-button-title": "Clear",
    "search-text-placeholder": "",
    "search-detached-cancel-button-title": "Cancel",
    "search-submit-button-title": "Submit",
    "search-label": "Search"
  }
}</script>

  <script src="https://polyfill.io/v3/polyfill.min.js?features=es6"></script>
  <script src="https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-chtml-full.js" type="text/javascript"></script>

<script type="text/javascript">
const typesetMath = (el) => {
  if (window.MathJax) {
    // MathJax Typeset
    window.MathJax.typeset([el]);
  } else if (window.katex) {
    // KaTeX Render
    var mathElements = el.getElementsByClassName("math");
    var macros = [];
    for (var i = 0; i < mathElements.length; i++) {
      var texText = mathElements[i].firstChild;
      if (mathElements[i].tagName == "SPAN") {
        window.katex.render(texText.data, mathElements[i], {
          displayMode: mathElements[i].classList.contains('display'),
          throwOnError: false,
          macros: macros,
          fleqn: false
        });
      }
    }
  }
}
window.Quarto = {
  typesetMath
};
</script>

<link rel="stylesheet" href="../../styles.css">
</head>

<body class="nav-fixed">

<div id="quarto-search-results"></div>
  <header id="quarto-header" class="headroom fixed-top">
    <nav class="navbar navbar-expand-lg " data-bs-theme="dark">
      <div class="navbar-container container-fluid">
      <div class="navbar-brand-container mx-auto">
    <a class="navbar-brand" href="../../index.html">
    <span class="navbar-title">TinyML KOR</span>
    </a>
  </div>
            <div id="quarto-search" class="" title="Search"></div>
          <button class="navbar-toggler" type="button" data-bs-toggle="collapse" data-bs-target="#navbarCollapse" aria-controls="navbarCollapse" aria-expanded="false" aria-label="Toggle navigation" onclick="if (window.quartoToggleHeadroom) { window.quartoToggleHeadroom(); }">
  <span class="navbar-toggler-icon"></span>
</button>
          <div class="collapse navbar-collapse" id="navbarCollapse">
            <ul class="navbar-nav navbar-nav-scroll ms-auto">
  <li class="nav-item">
    <a class="nav-link" href="../../about.html"> 
<span class="menu-text">TinyML Study Group</span></a>
  </li>  
  <li class="nav-item compact">
    <a class="nav-link" href="https://github.com/"> <i class="bi bi-github" role="img">
</i> 
<span class="menu-text"></span></a>
  </li>  
</ul>
          </div> <!-- /navcollapse -->
          <div class="quarto-navbar-tools">
</div>
      </div> <!-- /container-fluid -->
    </nav>
</header>
<!-- content -->
<div id="quarto-content" class="quarto-container page-columns page-rows-contents page-layout-article page-navbar">
<!-- sidebar -->
<!-- margin-sidebar -->
    <div id="quarto-margin-sidebar" class="sidebar margin-sidebar">
        <nav id="TOC" role="doc-toc" class="toc-active">
    <h2 id="toc-title">On this page</h2>
   
  <ul>
  <li><a href="#vision-transformer-vit-for-tinyml" id="toc-vision-transformer-vit-for-tinyml" class="nav-link active" data-scroll-target="#vision-transformer-vit-for-tinyml">Vision Transformer (ViT) for TinyML</a>
  <ul class="collapse">
  <li><a href="#basics-of-vision-transformer-vit" id="toc-basics-of-vision-transformer-vit" class="nav-link" data-scroll-target="#basics-of-vision-transformer-vit"><strong>1. Basics of Vision Transformer (ViT)</strong></a></li>
  <li><a href="#efficient-vit-acceleration-techniques" id="toc-efficient-vit-acceleration-techniques" class="nav-link" data-scroll-target="#efficient-vit-acceleration-techniques"><strong>2. Efficient ViT &amp; acceleration techniques</strong></a>
  <ul class="collapse">
  <li><a href="#window-attention" id="toc-window-attention" class="nav-link" data-scroll-target="#window-attention">2.1 Window attention</a></li>
  <li><a href="#linear-attention" id="toc-linear-attention" class="nav-link" data-scroll-target="#linear-attention">2.2 Linear attention</a></li>
  <li><a href="#sparse-attention" id="toc-sparse-attention" class="nav-link" data-scroll-target="#sparse-attention">2.3 Sparse attention</a></li>
  </ul></li>
  <li><a href="#self-supervised-learning-for-vit" id="toc-self-supervised-learning-for-vit" class="nav-link" data-scroll-target="#self-supervised-learning-for-vit"><strong>3. Self-supervised learning for ViT</strong></a>
  <ul class="collapse">
  <li><a href="#contrastive-learning" id="toc-contrastive-learning" class="nav-link" data-scroll-target="#contrastive-learning">3.1 Contrastive learning</a></li>
  <li><a href="#masked-image-modeling" id="toc-masked-image-modeling" class="nav-link" data-scroll-target="#masked-image-modeling">3.2 Masked image modeling</a></li>
  </ul></li>
  <li><a href="#multi-modal-llm" id="toc-multi-modal-llm" class="nav-link" data-scroll-target="#multi-modal-llm">4. <strong>Multi-modal LLM</strong></a></li>
  </ul></li>
  </ul>
</nav>
    </div>
<!-- main -->
<main class="content" id="quarto-document-content">

<header id="title-block-header" class="quarto-title-block default">
<div class="quarto-title">
<h1 class="title">🧑‍🏫 Lecture 14</h1>
  <div class="quarto-categories">
    <div class="quarto-category">lecture</div>
    <div class="quarto-category">transformer</div>
    <div class="quarto-category">vision transformer</div>
  </div>
  </div>

<div>
  <div class="description">
    Vision Transformer for TinyML
  </div>
</div>


<div class="quarto-title-meta">

    <div>
    <div class="quarto-title-meta-heading">Author</div>
    <div class="quarto-title-meta-contents">
             <p>Seunghyun Oh </p>
          </div>
  </div>
    
    <div>
    <div class="quarto-title-meta-heading">Published</div>
    <div class="quarto-title-meta-contents">
      <p class="date">April 26, 2024</p>
    </div>
  </div>
  
    
  </div>
  


</header>


<style>
p {
  line-height: 2; /* Adjust the line-height property as needed */
  text-align: justify;
  text-justify: inter-word; /* Adjust the justification mode as needed */
}

.image-container {
  text-align: center; /* Center-align the contents horizontally */
}

.centered-text {
  text-align: center; /* Center-align the text within the container */
}
</style>
<section id="vision-transformer-vit-for-tinyml" class="level1">
<h1>Vision Transformer (ViT) for TinyML</h1>
<p>이번 시간은 Transformer 모델에서도 Vision에 주로 어떻게 사용하는지 알아보려고 합니다. 그리고 이를 경량화하거나 가속화하는 기법, 그리고 제한된 리소스에서 어떻게 잘 활용할 수 있을지 알아봅시다.</p>
<section id="basics-of-vision-transformer-vit" class="level2">
<h2 class="anchored" data-anchor-id="basics-of-vision-transformer-vit"><strong>1. Basics of Vision Transformer (ViT)</strong></h2>
<p>Vision Transformer는 뭘까요? LLM으로 많이 사용하는 Language 모델의 경우, 입력으로 토큰이 들어와 Transformer 모델 구조로 Encoder, Decoder 구조에 따라 BERT(Encoder), GPT(Decoder) 그리고 BART, T5(Encoder - Decoder) 구조로 사용하죠. 그럼 Vision에서는 이 구조를 어떻게 사용할까요?</p>
<div class="quarto-figure quarto-figure-center">
<figure class="figure">
<p><img src="../../images/lec14/a0dce9fc-0196-4819-a392-87cd4135e492.png" class="img-fluid figure-img"></p>
<figcaption>Reference. MIT-TinyML lecture14 Vision Transformer in <a href="https://efficientml.ai/">https://efficientml.ai</a></figcaption>
</figure>
</div>
<p>생각보다 간단해요. 이미지가 만약 96x96이 있다면 이름 32x32 이미지 9개로 나눕니다. 나눈 이미지를 Patch라고 부를게요. 그럼 이 Patch를 Linear Projection을 통해서 토큰처럼 768개의 Vision Transformer(ViT)의 입력으로 들어갑니다. 실제로 아이디어를 구현할 때는 32x32 Convolution 레이어에 stride 32, padding 0, 입력 채널 3, 출력 채널 768로 연산합니다. 그 다음은, 입력이 동일해 졌으니 모델 구조는 흔히 보는 아래 그림의 구조를 사용하죠.</p>
<div class="quarto-figure quarto-figure-center">
<figure class="figure">
<p><img src="../../images/lec14/ca5f52e2-27eb-4bea-81ef-37291e020c53.png" class="img-fluid figure-img"></p>
<figcaption>Reference. MIT-TinyML lecture14 Vision Transformer in <a href="https://efficientml.ai/">https://efficientml.ai</a></figcaption>
</figure>
</div>
<p>여기서 Patch 의 크기 또한 파라미터로 할 수 있습니다. 앞으로 ViT를 말할 때는 Patch도 유심히 보셔야할 겁니다.</p>
<div class="quarto-figure quarto-figure-center">
<figure class="figure">
<p><img src="../../images/lec14/ee3a9190-e81b-4b53-b1a4-147339a36c3f.png" class="img-fluid figure-img"></p>
<figcaption>Reference. MIT-TinyML lecture14 Vision Transformer in <a href="https://efficientml.ai/">https://efficientml.ai</a></figcaption>
</figure>
</div>
<p>그런데 왜 Vision Transformer를 쓸까요? 기존에 CNN 구조에 ResNet이나 MobileNet의 구조도 충분히 성능이 괜찮지 않나요? CNN과 Transformer를 Vision task에서 비교해보면 훈련 데이터수가 적을 때는 확실히 CNN이 강세를 보이죠.</p>
<div class="quarto-figure quarto-figure-center">
<figure class="figure">
<p><img src="../../images/lec14/c1a83fa2-1a94-4a50-a555-336cc2c1fabc.png" class="img-fluid figure-img"></p>
<figcaption>Reference. MIT-TinyML lecture14 Vision Transformer in <a href="https://efficientml.ai/">https://efficientml.ai</a></figcaption>
</figure>
</div>
<p><strong>하.지.만.</strong> 데이터수가 300M인 경우를 살펴보시죠. <strong>ViT는 데이터수가 많으면 많을 수록 CNN에 비해서 훨씬 강세를 보이는 것</strong>을 확인할 수 있죠? 이 때문에, 저희는 ViT에 매료될 수 밖에 없었습니다.</p>
<div class="quarto-figure quarto-figure-center">
<figure class="figure">
<p><img src="../../images/lec14/d2b025c5-739c-42b8-a4da-0f4d6a1ba893.png" class="img-fluid figure-img"></p>
<figcaption>Reference. MIT-TinyML lecture14 Vision Transformer in <a href="https://efficientml.ai/">https://efficientml.ai</a></figcaption>
</figure>
</div>
<p>Vision에 대한 Application으로 Medical Image Segmentation, Super Resolution, Autonomous Driving, Segmentation로써 주로 사용해요.</p>
<div class="quarto-figure quarto-figure-center">
<figure class="figure">
<p><img src="../../images/lec14/Lecture_14-Vision_Transformer_(5).jpg" class="img-fluid figure-img"></p>
<figcaption>Reference. MIT-TinyML lecture14 Vision Transformer in <a href="https://efficientml.ai/">https://efficientml.ai</a></figcaption>
</figure>
</div>
<p>문제는 이 어플리케이션들은 모두 High resolution에 prediction을 요구하지만, ViT는 Input resolution이 높아질 때마다 연산량이 어마어마해집니다. 선형적이라기 보단 지수적으로 증가하는 것 처럼 보이네요. 바로 이 문제 때문에, 우리가 “Efficient and Acceleration”에 대해서 고민할 수 밖에 없게 됩니다.</p>
<p>*mIoU: mean Intersection of Union, GMAC: Giga Multiply and Add Computation</p>
<div class="quarto-figure quarto-figure-center">
<figure class="figure">
<p><img src="../../images/lec14/94cb6dd4-2750-415d-8e15-b8177affe2f5.png" class="img-fluid figure-img"></p>
<figcaption>Reference. MIT-TinyML lecture14 Vision Transformer in <a href="https://efficientml.ai/">https://efficientml.ai</a></figcaption>
</figure>
</div>
</section>
<section id="efficient-vit-acceleration-techniques" class="level2">
<h2 class="anchored" data-anchor-id="efficient-vit-acceleration-techniques"><strong>2. Efficient ViT &amp; acceleration techniques</strong></h2>
<section id="window-attention" class="level3">
<h3 class="anchored" data-anchor-id="window-attention">2.1 Window attention</h3>
<p>처음으로 소개할 기술은 <strong>Window attention</strong> 입니다. 기존에 attention은 레이어마다 patch의 크기가 동일하게 들어가겠죠. 하지만 Window attention은 레이어마다 patch의 크기를 다르게 하는 방법입니다. 그.리.고. 여기서 중요한 건 <strong>Window attention은 그 Patch안에 다시 Patch를 하는 방법입니다. 그리고 이를 통해 병렬연산을 가능하게 만드는 거죠.</strong></p>
<div class="quarto-figure quarto-figure-center">
<figure class="figure">
<p><img src="../../images/lec14/1244eb7d-48a3-4c07-ba3d-519815372d8c.png" class="img-fluid figure-img"></p>
<figcaption>Reference. MIT-TinyML lecture14 Vision Transformer in <a href="https://efficientml.ai/">https://efficientml.ai</a></figcaption>
</figure>
</div>
<p>하지만 저렇게 연산하면 문제점이 Window간 정보교환이 없다는 점인데요. 이는 레이어별로 “Shift Window”를 통해 해결합니다.</p>
<div class="quarto-figure quarto-figure-center">
<figure class="figure">
<p><img src="../../images/lec14/e3509a3f-2578-41c1-a654-1a3dd94d02d1.png" class="img-fluid figure-img"></p>
<figcaption>Reference. MIT-TinyML lecture14 Vision Transformer in <a href="https://efficientml.ai/">https://efficientml.ai</a></figcaption>
</figure>
</div>
</section>
<section id="linear-attention" class="level3">
<h3 class="anchored" data-anchor-id="linear-attention">2.2 Linear attention</h3>
<p>두 번째 소개할 기술은 Linear attention 입니다. 기존에 Attention 연산중에 Softmax가 있었죠? exponential 연산은 직접 구현해보면 연산량이 쪼금 힘듭니다. 그래서 이를 Linear 레이어인 Relu로 대체를 하는데요. 여기서 알고리즘 복잡도가 O(<span class="math inline">\(n^2\)</span>)인 부분까지 행렬의 곱셈에서 결합법칙이 가능하다는 부분을 이용해 O(<span class="math inline">\(n\)</span>)으로 줄여버립니다. 마지막 복잡도가 줄어드는 것이 이해가 안가신 다면, Scale이후에 나오는 레이어가 <span class="math inline">\(n \times d\)</span> 인 점과 n-차원과 d-차원에 대해서 비교해보시면 빠르게 납득이 가실겁니다!</p>
<div class="quarto-figure quarto-figure-center">
<figure class="figure">
<p><img src="../../images/lec14/fab831c8-c65a-4a80-a73b-23f22f8bb524.png" class="img-fluid figure-img"></p>
<figcaption>Reference. MIT-TinyML lecture14 Vision Transformer in <a href="https://efficientml.ai/">https://efficientml.ai</a></figcaption>
</figure>
</div>
<p>하지만 여기서도 문제가 있습니다. attention에서 성능을 보는 방법중에 attention map을 통해 보면 Linear Attention이 사진의 특징을 잘 못잡아 냅니다.</p>
<div class="quarto-figure quarto-figure-center">
<figure class="figure">
<p><img src="../../images/lec14/d373e88a-17e9-4aff-8555-71b9382d01bb.png" class="img-fluid figure-img"></p>
<figcaption>Reference. MIT-TinyML lecture14 Vision Transformer in <a href="https://efficientml.ai/">https://efficientml.ai</a></figcaption>
</figure>
</div>
<p>당연히 Softmax 보다 distribution이 날카롭지 않기 때문에 특징점에서도 두드러지지 않는게 문제죠. 성능도, 나오지 않을 것이구요. “Multi-scale” learning을 하기가 어려울 겁니다.</p>
<div class="quarto-figure quarto-figure-center">
<figure class="figure">
<p><img src="../../images/lec14/347fbc60-7fa7-47ff-b13f-d9b8dbd089e7.png" class="img-fluid figure-img"></p>
<figcaption>Reference. MIT-TinyML lecture14 Vision Transformer in <a href="https://efficientml.ai/">https://efficientml.ai</a></figcaption>
</figure>
</div>
<p>해결해야겠죠? 방법은 레이어를 하나 더 넣으면 됩니다. 성능도 오히려 전보다 훨씬 좋아졌네요.</p>
<div class="quarto-figure quarto-figure-center">
<figure class="figure">
<p><img src="../../images/lec14/ea99a29b-31ee-43d2-8470-67176261ded8.png" class="img-fluid figure-img"></p>
<figcaption>Reference. MIT-TinyML lecture14 Vision Transformer in <a href="https://efficientml.ai/">https://efficientml.ai</a></figcaption>
</figure>
</div>
</section>
<section id="sparse-attention" class="level3">
<h3 class="anchored" data-anchor-id="sparse-attention">2.3 Sparse attention</h3>
<p>세 번째 기술을 소개드리기 전에, Vision Application을 하다 보면 아래와 같은 상황이 많습니다. 이미지의 해상도를 줄이거나, Pruning을 통해 이미지가 특정부분만 들어오게되는 경우가 있죠.</p>
<div class="quarto-figure quarto-figure-center">
<figure class="figure">
<p><img src="../../images/lec14/1676630f-64b9-4308-8891-7f7e0e55891e.png" class="img-fluid figure-img"></p>
<figcaption>Reference. MIT-TinyML lecture14 Vision Transformer in <a href="https://efficientml.ai/">https://efficientml.ai</a></figcaption>
</figure>
</div>
<p>그래서, Sparse attention이라는 기술을 사용합니다. 이 기술을 Pruning 과 비슷하게 Patch마다 Importance를 계산해 줄을 세웁니다.</p>
<div class="quarto-figure quarto-figure-center">
<figure class="figure">
<p><img src="../../images/lec14/Lecture_14-Vision_Transformer_(1).jpg" class="img-fluid figure-img"></p>
<figcaption>Reference. MIT-TinyML lecture14 Vision Transformer in <a href="https://efficientml.ai/">https://efficientml.ai</a></figcaption>
</figure>
</div>
<p>그리고 줄을 세운 Patch에서 N번의 반복하는 fine-tuning을 통해 모델을 재학습시키죠.</p>
<div class="quarto-figure quarto-figure-center">
<figure class="figure">
<p><img src="../../images/lec14/09b90662-c653-4afb-ab8b-bf4548d89e74.png" class="img-fluid figure-img"></p>
<figcaption>Reference. MIT-TinyML lecture14 Vision Transformer in <a href="https://efficientml.ai/">https://efficientml.ai</a></figcaption>
</figure>
</div>
<p>여기서 Constraint에 만족하는 조합을 찾기 위해 Evolutionary Search를 이용합니다(Evolutionary Search는 <a href="https://tinyml-kor.github.io/blog/posts/labs/lab03.html">Lab 3</a>를 참고해주세요).</p>
<div class="quarto-figure quarto-figure-center">
<figure class="figure">
<p><img src="../../images/lec14/c88001b6-412d-4766-8c77-7ba3bf2e6d7e.png" class="img-fluid figure-img"></p>
<figcaption>Reference. MIT-TinyML lecture14 Vision Transformer in <a href="https://efficientml.ai/">https://efficientml.ai</a></figcaption>
</figure>
</div>
<p>성능이 궁금하다면 <a href="https://arxiv.org/pdf/2303.17605">이 논문</a>을 참고해주세요 :)</p>
</section>
</section>
<section id="self-supervised-learning-for-vit" class="level2">
<h2 class="anchored" data-anchor-id="self-supervised-learning-for-vit"><strong>3. Self-supervised learning for ViT</strong></h2>
<p>Efficient + Acceleration에 대한 기술은 여기까지입니다. 그럼 다시 처음으로 돌아가서, 혹시 Vision Transformer 첫 성능 그래프 기억하시나요? 데이터가 어마어마하게 많아야 했던 부분이요(아래에 가져와 봤습니다). 그런데, 이렇게 <strong>데이터를 많이 구하는 건 현실적으로 많이 어려울 수 밖에 없습니다. 대표적인 예시로 의료데이터가 그렇죠.</strong> 그럼 이 데이터가 부족한 건 어떻게 해결할 수 있을까요?</p>
<div class="quarto-figure quarto-figure-center">
<figure class="figure">
<p><img src="../../images/lec14/d2b025c5-739c-42b8-a4da-0f4d6a1ba893.png" class="img-fluid figure-img"></p>
<figcaption>Reference. MIT-TinyML lecture14 Vision Transformer in <a href="https://efficientml.ai/">https://efficientml.ai</a></figcaption>
</figure>
</div>
<section id="contrastive-learning" class="level3">
<h3 class="anchored" data-anchor-id="contrastive-learning">3.1 Contrastive learning</h3>
<p>첫번째 방법은 Contrastive learning 입니다. 이 방법은 TinyML에서 뿐 아니라 많이 쓰이는 방법인데, Positive Sample과 Negative Sample을 가지고 embedding vector를 멀게 하는 방식입니다. 예를 들어, 고양이 사진을 구분하는 테스크에서 Positive Sample은 고양이 사진이 될 것이고, Negative Sample은 여기서 강아지 사진이 될 겁니다.</p>
<div class="quarto-figure quarto-figure-center">
<figure class="figure">
<p><img src="../../images/lec14/d5554c3e-7c99-492f-aa92-4d0c8e0ca3f3.png" class="img-fluid figure-img"></p>
<figcaption>Reference. MIT-TinyML lecture14 Vision Transformer in <a href="https://efficientml.ai/">https://efficientml.ai</a></figcaption>
</figure>
</div>
<p>실험결과를 보면 Supervised 방법으로는 CIFAR-100, Oxford Flowers-102, Oxford-IIIT Pets 데이터 셋에서는 모델 성능이 나오지 않지만, 위 방법을 적용한 모델은 성능이 어느정도 나오는 것을 볼 수 있습니다.</p>
<div class="quarto-figure quarto-figure-center">
<figure class="figure">
<p><img src="../../images/lec14/78f92619-7122-4801-8b17-04e98d351ada.png" class="img-fluid figure-img"></p>
<figcaption>Reference. An Empirical Study of Training Self-Supervised Vision Transformers [Chen et al., 2021]</figcaption>
</figure>
</div>
<p>Contrastive Learning으로 Multi-Modal을 사용할 수도 있습니다. 아래 논문은 텍스트와 이미지를 모두 받는 형태로 디자인돼 있습니다.</p>
<div class="quarto-figure quarto-figure-center">
<figure class="figure">
<p><img src="../../images/lec14/cffe9021-ac76-4827-a7ee-5405c915317c.png" class="img-fluid figure-img"></p>
<figcaption>Reference. Learning Transferable Visual Models From Natural Language Supervision [Radford et al., 2021]</figcaption>
</figure>
</div>
</section>
<section id="masked-image-modeling" class="level3">
<h3 class="anchored" data-anchor-id="masked-image-modeling">3.2 Masked image modeling</h3>
<p>두번째 방법은 Mask 입니다. 아래 모델 구조를 보시죠.</p>
<div class="quarto-figure quarto-figure-center">
<figure class="figure">
<p><img src="../../images/lec14/9a81e7a2-dec8-43bd-a767-1ebec6f3fd76.png" class="img-fluid figure-img"></p>
<figcaption>Reference. MIT-TinyML lecture14 Vision Transformer in <a href="https://efficientml.ai/">https://efficientml.ai</a></figcaption>
</figure>
</div>
<p>BERT 모델입니다. Mask방법은 입력 토큰에 마스크를 씌워 출력에서 이를 맞추는 테스크로 모델을 훈련시키는 방법입니다. 그럼 Vision은 어떻게 할까요?</p>
<div class="quarto-figure quarto-figure-center">
<figure class="figure">
<p><img src="../../images/lec14/2ee77c6b-afd7-4997-9617-bf66244c409b.png" class="img-fluid figure-img"></p>
<figcaption>Reference. Masked Autoencoders Are Scalable Vision Learners [He et al., 2022]</figcaption>
</figure>
</div>
<p>LLM과 훈련하는 방식은 Mask를 씌우고 이를 예측하도록 하는 훈련인 것은 비슷합니다. 다만 ViT의 경우 Encoder와 Decoder가 같이 있으면서 Encoder보다는 Decoder의 모델크기가 더 작다는 걸 강조합니다. 흥미로운 부분은 Masking ratio가 BERT의 경우 15%인 반면, 아래 실험결과에서 ViT의 경우는 무려 75%나 됩니다. 강의노트에서는 “이미지가 언어보다 더 information density가 낮아서 그렇다.”라고 말합니다.</p>
<div class="quarto-figure quarto-figure-center">
<figure class="figure">
<p><img src="../../images/lec14/90ae81a5-5b0a-4179-b4be-51a34dc0540e.png" class="img-fluid figure-img"></p>
<figcaption>Reference. Masked Autoencoders Are Scalable Vision Learners [He et al., 2022]</figcaption>
</figure>
</div>
<div class="quarto-figure quarto-figure-center">
<figure class="figure">
<p><img src="../../images/lec14/Lecture_14-Vision_Transformer_(9).jpg" class="img-fluid figure-img"></p>
<figcaption>Lecture_14-Vision Transformer (9).jpg</figcaption>
</figure>
</div>
</section>
</section>
<section id="multi-modal-llm" class="level2">
<h2 class="anchored" data-anchor-id="multi-modal-llm">4. <strong>Multi-modal LLM</strong></h2>
<ul>
<li><a href="https://arxiv.org/pdf/2204.14198">Cross attention (Flamingo)</a></li>
<li><a href="https://palm-e.github.io">Visual token (PaLM-E)</a></li>
</ul>
<p>마지막으로 Multi-modal LLM에 대해서 언급을 하는데, 자세한 내용은 다루지 않아 궁금하신 분들을 위해 논문은 링크로 달아두고 설명은 넘어가도록 하겠습니다. 여기까지 Vision Transformer 였습니다. LLM과 비슷하면서도 모델크기가 똑같다면 연산량 높은 것과 데이터가 부족한 것을 어떻게 해결하는가가 중점인 강의였습니다. 다음 시간에는 GAN, Video, and Point Cloud로 돌아오겠습니다 :D</p>


</section>
</section>

</main> <!-- /main -->
<script id="quarto-html-after-body" type="application/javascript">
window.document.addEventListener("DOMContentLoaded", function (event) {
  const toggleBodyColorMode = (bsSheetEl) => {
    const mode = bsSheetEl.getAttribute("data-mode");
    const bodyEl = window.document.querySelector("body");
    if (mode === "dark") {
      bodyEl.classList.add("quarto-dark");
      bodyEl.classList.remove("quarto-light");
    } else {
      bodyEl.classList.add("quarto-light");
      bodyEl.classList.remove("quarto-dark");
    }
  }
  const toggleBodyColorPrimary = () => {
    const bsSheetEl = window.document.querySelector("link#quarto-bootstrap");
    if (bsSheetEl) {
      toggleBodyColorMode(bsSheetEl);
    }
  }
  toggleBodyColorPrimary();  
  const icon = "";
  const anchorJS = new window.AnchorJS();
  anchorJS.options = {
    placement: 'right',
    icon: icon
  };
  anchorJS.add('.anchored');
  const isCodeAnnotation = (el) => {
    for (const clz of el.classList) {
      if (clz.startsWith('code-annotation-')) {                     
        return true;
      }
    }
    return false;
  }
  const clipboard = new window.ClipboardJS('.code-copy-button', {
    text: function(trigger) {
      const codeEl = trigger.previousElementSibling.cloneNode(true);
      for (const childEl of codeEl.children) {
        if (isCodeAnnotation(childEl)) {
          childEl.remove();
        }
      }
      return codeEl.innerText;
    }
  });
  clipboard.on('success', function(e) {
    // button target
    const button = e.trigger;
    // don't keep focus
    button.blur();
    // flash "checked"
    button.classList.add('code-copy-button-checked');
    var currentTitle = button.getAttribute("title");
    button.setAttribute("title", "Copied!");
    let tooltip;
    if (window.bootstrap) {
      button.setAttribute("data-bs-toggle", "tooltip");
      button.setAttribute("data-bs-placement", "left");
      button.setAttribute("data-bs-title", "Copied!");
      tooltip = new bootstrap.Tooltip(button, 
        { trigger: "manual", 
          customClass: "code-copy-button-tooltip",
          offset: [0, -8]});
      tooltip.show();    
    }
    setTimeout(function() {
      if (tooltip) {
        tooltip.hide();
        button.removeAttribute("data-bs-title");
        button.removeAttribute("data-bs-toggle");
        button.removeAttribute("data-bs-placement");
      }
      button.setAttribute("title", currentTitle);
      button.classList.remove('code-copy-button-checked');
    }, 1000);
    // clear code selection
    e.clearSelection();
  });
    var localhostRegex = new RegExp(/^(?:http|https):\/\/localhost\:?[0-9]*\//);
    var mailtoRegex = new RegExp(/^mailto:/);
      var filterRegex = new RegExp('/' + window.location.host + '/');
    var isInternal = (href) => {
        return filterRegex.test(href) || localhostRegex.test(href) || mailtoRegex.test(href);
    }
    // Inspect non-navigation links and adorn them if external
 	var links = window.document.querySelectorAll('a[href]:not(.nav-link):not(.navbar-brand):not(.toc-action):not(.sidebar-link):not(.sidebar-item-toggle):not(.pagination-link):not(.no-external):not([aria-hidden]):not(.dropdown-item):not(.quarto-navigation-tool)');
    for (var i=0; i<links.length; i++) {
      const link = links[i];
      if (!isInternal(link.href)) {
        // undo the damage that might have been done by quarto-nav.js in the case of
        // links that we want to consider external
        if (link.dataset.originalHref !== undefined) {
          link.href = link.dataset.originalHref;
        }
      }
    }
  function tippyHover(el, contentFn, onTriggerFn, onUntriggerFn) {
    const config = {
      allowHTML: true,
      maxWidth: 500,
      delay: 100,
      arrow: false,
      appendTo: function(el) {
          return el.parentElement;
      },
      interactive: true,
      interactiveBorder: 10,
      theme: 'quarto',
      placement: 'bottom-start',
    };
    if (contentFn) {
      config.content = contentFn;
    }
    if (onTriggerFn) {
      config.onTrigger = onTriggerFn;
    }
    if (onUntriggerFn) {
      config.onUntrigger = onUntriggerFn;
    }
    window.tippy(el, config); 
  }
  const noterefs = window.document.querySelectorAll('a[role="doc-noteref"]');
  for (var i=0; i<noterefs.length; i++) {
    const ref = noterefs[i];
    tippyHover(ref, function() {
      // use id or data attribute instead here
      let href = ref.getAttribute('data-footnote-href') || ref.getAttribute('href');
      try { href = new URL(href).hash; } catch {}
      const id = href.replace(/^#\/?/, "");
      const note = window.document.getElementById(id);
      if (note) {
        return note.innerHTML;
      } else {
        return "";
      }
    });
  }
  const xrefs = window.document.querySelectorAll('a.quarto-xref');
  const processXRef = (id, note) => {
    // Strip column container classes
    const stripColumnClz = (el) => {
      el.classList.remove("page-full", "page-columns");
      if (el.children) {
        for (const child of el.children) {
          stripColumnClz(child);
        }
      }
    }
    stripColumnClz(note)
    if (id === null || id.startsWith('sec-')) {
      // Special case sections, only their first couple elements
      const container = document.createElement("div");
      if (note.children && note.children.length > 2) {
        container.appendChild(note.children[0].cloneNode(true));
        for (let i = 1; i < note.children.length; i++) {
          const child = note.children[i];
          if (child.tagName === "P" && child.innerText === "") {
            continue;
          } else {
            container.appendChild(child.cloneNode(true));
            break;
          }
        }
        if (window.Quarto?.typesetMath) {
          window.Quarto.typesetMath(container);
        }
        return container.innerHTML
      } else {
        if (window.Quarto?.typesetMath) {
          window.Quarto.typesetMath(note);
        }
        return note.innerHTML;
      }
    } else {
      // Remove any anchor links if they are present
      const anchorLink = note.querySelector('a.anchorjs-link');
      if (anchorLink) {
        anchorLink.remove();
      }
      if (window.Quarto?.typesetMath) {
        window.Quarto.typesetMath(note);
      }
      // TODO in 1.5, we should make sure this works without a callout special case
      if (note.classList.contains("callout")) {
        return note.outerHTML;
      } else {
        return note.innerHTML;
      }
    }
  }
  for (var i=0; i<xrefs.length; i++) {
    const xref = xrefs[i];
    tippyHover(xref, undefined, function(instance) {
      instance.disable();
      let url = xref.getAttribute('href');
      let hash = undefined; 
      if (url.startsWith('#')) {
        hash = url;
      } else {
        try { hash = new URL(url).hash; } catch {}
      }
      if (hash) {
        const id = hash.replace(/^#\/?/, "");
        const note = window.document.getElementById(id);
        if (note !== null) {
          try {
            const html = processXRef(id, note.cloneNode(true));
            instance.setContent(html);
          } finally {
            instance.enable();
            instance.show();
          }
        } else {
          // See if we can fetch this
          fetch(url.split('#')[0])
          .then(res => res.text())
          .then(html => {
            const parser = new DOMParser();
            const htmlDoc = parser.parseFromString(html, "text/html");
            const note = htmlDoc.getElementById(id);
            if (note !== null) {
              const html = processXRef(id, note);
              instance.setContent(html);
            } 
          }).finally(() => {
            instance.enable();
            instance.show();
          });
        }
      } else {
        // See if we can fetch a full url (with no hash to target)
        // This is a special case and we should probably do some content thinning / targeting
        fetch(url)
        .then(res => res.text())
        .then(html => {
          const parser = new DOMParser();
          const htmlDoc = parser.parseFromString(html, "text/html");
          const note = htmlDoc.querySelector('main.content');
          if (note !== null) {
            // This should only happen for chapter cross references
            // (since there is no id in the URL)
            // remove the first header
            if (note.children.length > 0 && note.children[0].tagName === "HEADER") {
              note.children[0].remove();
            }
            const html = processXRef(null, note);
            instance.setContent(html);
          } 
        }).finally(() => {
          instance.enable();
          instance.show();
        });
      }
    }, function(instance) {
    });
  }
      let selectedAnnoteEl;
      const selectorForAnnotation = ( cell, annotation) => {
        let cellAttr = 'data-code-cell="' + cell + '"';
        let lineAttr = 'data-code-annotation="' +  annotation + '"';
        const selector = 'span[' + cellAttr + '][' + lineAttr + ']';
        return selector;
      }
      const selectCodeLines = (annoteEl) => {
        const doc = window.document;
        const targetCell = annoteEl.getAttribute("data-target-cell");
        const targetAnnotation = annoteEl.getAttribute("data-target-annotation");
        const annoteSpan = window.document.querySelector(selectorForAnnotation(targetCell, targetAnnotation));
        const lines = annoteSpan.getAttribute("data-code-lines").split(",");
        const lineIds = lines.map((line) => {
          return targetCell + "-" + line;
        })
        let top = null;
        let height = null;
        let parent = null;
        if (lineIds.length > 0) {
            //compute the position of the single el (top and bottom and make a div)
            const el = window.document.getElementById(lineIds[0]);
            top = el.offsetTop;
            height = el.offsetHeight;
            parent = el.parentElement.parentElement;
          if (lineIds.length > 1) {
            const lastEl = window.document.getElementById(lineIds[lineIds.length - 1]);
            const bottom = lastEl.offsetTop + lastEl.offsetHeight;
            height = bottom - top;
          }
          if (top !== null && height !== null && parent !== null) {
            // cook up a div (if necessary) and position it 
            let div = window.document.getElementById("code-annotation-line-highlight");
            if (div === null) {
              div = window.document.createElement("div");
              div.setAttribute("id", "code-annotation-line-highlight");
              div.style.position = 'absolute';
              parent.appendChild(div);
            }
            div.style.top = top - 2 + "px";
            div.style.height = height + 4 + "px";
            div.style.left = 0;
            let gutterDiv = window.document.getElementById("code-annotation-line-highlight-gutter");
            if (gutterDiv === null) {
              gutterDiv = window.document.createElement("div");
              gutterDiv.setAttribute("id", "code-annotation-line-highlight-gutter");
              gutterDiv.style.position = 'absolute';
              const codeCell = window.document.getElementById(targetCell);
              const gutter = codeCell.querySelector('.code-annotation-gutter');
              gutter.appendChild(gutterDiv);
            }
            gutterDiv.style.top = top - 2 + "px";
            gutterDiv.style.height = height + 4 + "px";
          }
          selectedAnnoteEl = annoteEl;
        }
      };
      const unselectCodeLines = () => {
        const elementsIds = ["code-annotation-line-highlight", "code-annotation-line-highlight-gutter"];
        elementsIds.forEach((elId) => {
          const div = window.document.getElementById(elId);
          if (div) {
            div.remove();
          }
        });
        selectedAnnoteEl = undefined;
      };
        // Handle positioning of the toggle
    window.addEventListener(
      "resize",
      throttle(() => {
        elRect = undefined;
        if (selectedAnnoteEl) {
          selectCodeLines(selectedAnnoteEl);
        }
      }, 10)
    );
    function throttle(fn, ms) {
    let throttle = false;
    let timer;
      return (...args) => {
        if(!throttle) { // first call gets through
            fn.apply(this, args);
            throttle = true;
        } else { // all the others get throttled
            if(timer) clearTimeout(timer); // cancel #2
            timer = setTimeout(() => {
              fn.apply(this, args);
              timer = throttle = false;
            }, ms);
        }
      };
    }
      // Attach click handler to the DT
      const annoteDls = window.document.querySelectorAll('dt[data-target-cell]');
      for (const annoteDlNode of annoteDls) {
        annoteDlNode.addEventListener('click', (event) => {
          const clickedEl = event.target;
          if (clickedEl !== selectedAnnoteEl) {
            unselectCodeLines();
            const activeEl = window.document.querySelector('dt[data-target-cell].code-annotation-active');
            if (activeEl) {
              activeEl.classList.remove('code-annotation-active');
            }
            selectCodeLines(clickedEl);
            clickedEl.classList.add('code-annotation-active');
          } else {
            // Unselect the line
            unselectCodeLines();
            clickedEl.classList.remove('code-annotation-active');
          }
        });
      }
  const findCites = (el) => {
    const parentEl = el.parentElement;
    if (parentEl) {
      const cites = parentEl.dataset.cites;
      if (cites) {
        return {
          el,
          cites: cites.split(' ')
        };
      } else {
        return findCites(el.parentElement)
      }
    } else {
      return undefined;
    }
  };
  var bibliorefs = window.document.querySelectorAll('a[role="doc-biblioref"]');
  for (var i=0; i<bibliorefs.length; i++) {
    const ref = bibliorefs[i];
    const citeInfo = findCites(ref);
    if (citeInfo) {
      tippyHover(citeInfo.el, function() {
        var popup = window.document.createElement('div');
        citeInfo.cites.forEach(function(cite) {
          var citeDiv = window.document.createElement('div');
          citeDiv.classList.add('hanging-indent');
          citeDiv.classList.add('csl-entry');
          var biblioDiv = window.document.getElementById('ref-' + cite);
          if (biblioDiv) {
            citeDiv.innerHTML = biblioDiv.innerHTML;
          }
          popup.appendChild(citeDiv);
        });
        return popup.innerHTML;
      });
    }
  }
});
</script>
<script src="https://giscus.app/client.js" data-repo="TINYML-KOR/blog" data-repo-id="R_kgDOLC9iGA" data-category="General" data-category-id="DIC_kwDOLC9iGM4Cc7eP" data-mapping="title" data-reactions-enabled="1" data-emit-metadata="0" data-input-position="top" data-theme="light" data-lang="en" crossorigin="anonymous" async="">
</script>
<input type="hidden" id="giscus-base-theme" value="light">
<input type="hidden" id="giscus-alt-theme" value="dark">
</div> <!-- /content -->




</body></html>