<!DOCTYPE html>
<html xmlns="http://www.w3.org/1999/xhtml" lang="en" xml:lang="en"><head>

<meta charset="utf-8">
<meta name="generator" content="quarto-1.4.549">

<meta name="viewport" content="width=device-width, initial-scale=1.0, user-scalable=yes">

<meta name="author" content="Seunghyun Oh">
<meta name="dcterms.date" content="2024-03-05">
<meta name="description" content="Quantization">

<title>TinyML KOR - ğŸ§‘â€ğŸ« Lecture 5-6</title>
<style>
code{white-space: pre-wrap;}
span.smallcaps{font-variant: small-caps;}
div.columns{display: flex; gap: min(4vw, 1.5em);}
div.column{flex: auto; overflow-x: auto;}
div.hanging-indent{margin-left: 1.5em; text-indent: -1.5em;}
ul.task-list{list-style: none;}
ul.task-list li input[type="checkbox"] {
  width: 0.8em;
  margin: 0 0.8em 0.2em -1em; /* quarto-specific, see https://github.com/quarto-dev/quarto-cli/issues/4556 */ 
  vertical-align: middle;
}
</style>


<script src="../../site_libs/quarto-nav/quarto-nav.js"></script>
<script src="../../site_libs/quarto-nav/headroom.min.js"></script>
<script src="../../site_libs/clipboard/clipboard.min.js"></script>
<script src="../../site_libs/quarto-search/autocomplete.umd.js"></script>
<script src="../../site_libs/quarto-search/fuse.min.js"></script>
<script src="../../site_libs/quarto-search/quarto-search.js"></script>
<meta name="quarto:offset" content="../../">
<script src="../../site_libs/quarto-html/quarto.js"></script>
<script src="../../site_libs/quarto-html/popper.min.js"></script>
<script src="../../site_libs/quarto-html/tippy.umd.min.js"></script>
<script src="../../site_libs/quarto-html/anchor.min.js"></script>
<link href="../../site_libs/quarto-html/tippy.css" rel="stylesheet">
<link href="../../site_libs/quarto-html/quarto-syntax-highlighting.css" rel="stylesheet" id="quarto-text-highlighting-styles">
<script src="../../site_libs/bootstrap/bootstrap.min.js"></script>
<link href="../../site_libs/bootstrap/bootstrap-icons.css" rel="stylesheet">
<link href="../../site_libs/bootstrap/bootstrap.min.css" rel="stylesheet" id="quarto-bootstrap" data-mode="light">
<script id="quarto-search-options" type="application/json">{
  "location": "navbar",
  "copy-button": false,
  "collapse-after": 3,
  "panel-placement": "end",
  "type": "overlay",
  "limit": 50,
  "keyboard-shortcut": [
    "f",
    "/",
    "s"
  ],
  "show-item-context": false,
  "language": {
    "search-no-results-text": "No results",
    "search-matching-documents-text": "matching documents",
    "search-copy-link-title": "Copy link to search",
    "search-hide-matches-text": "Hide additional matches",
    "search-more-match-text": "more match in this document",
    "search-more-matches-text": "more matches in this document",
    "search-clear-button-title": "Clear",
    "search-text-placeholder": "",
    "search-detached-cancel-button-title": "Cancel",
    "search-submit-button-title": "Submit",
    "search-label": "Search"
  }
}</script>

  <script src="https://polyfill.io/v3/polyfill.min.js?features=es6"></script>
  <script src="https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-chtml-full.js" type="text/javascript"></script>

<script type="text/javascript">
const typesetMath = (el) => {
  if (window.MathJax) {
    // MathJax Typeset
    window.MathJax.typeset([el]);
  } else if (window.katex) {
    // KaTeX Render
    var mathElements = el.getElementsByClassName("math");
    var macros = [];
    for (var i = 0; i < mathElements.length; i++) {
      var texText = mathElements[i].firstChild;
      if (mathElements[i].tagName == "SPAN") {
        window.katex.render(texText.data, mathElements[i], {
          displayMode: mathElements[i].classList.contains('display'),
          throwOnError: false,
          macros: macros,
          fleqn: false
        });
      }
    }
  }
}
window.Quarto = {
  typesetMath
};
</script>

<link rel="stylesheet" href="../../styles.css">
</head>

<body class="nav-fixed">

<div id="quarto-search-results"></div>
  <header id="quarto-header" class="headroom fixed-top">
    <nav class="navbar navbar-expand-lg " data-bs-theme="dark">
      <div class="navbar-container container-fluid">
      <div class="navbar-brand-container mx-auto">
    <a class="navbar-brand" href="../../index.html">
    <span class="navbar-title">TinyML KOR</span>
    </a>
  </div>
            <div id="quarto-search" class="" title="Search"></div>
          <button class="navbar-toggler" type="button" data-bs-toggle="collapse" data-bs-target="#navbarCollapse" aria-controls="navbarCollapse" aria-expanded="false" aria-label="Toggle navigation" onclick="if (window.quartoToggleHeadroom) { window.quartoToggleHeadroom(); }">
  <span class="navbar-toggler-icon"></span>
</button>
          <div class="collapse navbar-collapse" id="navbarCollapse">
            <ul class="navbar-nav navbar-nav-scroll ms-auto">
  <li class="nav-item">
    <a class="nav-link" href="../../about.html"> 
<span class="menu-text">TinyML Study Group</span></a>
  </li>  
  <li class="nav-item compact">
    <a class="nav-link" href="https://github.com/"> <i class="bi bi-github" role="img">
</i> 
<span class="menu-text"></span></a>
  </li>  
</ul>
          </div> <!-- /navcollapse -->
          <div class="quarto-navbar-tools">
</div>
      </div> <!-- /container-fluid -->
    </nav>
</header>
<!-- content -->
<div id="quarto-content" class="quarto-container page-columns page-rows-contents page-layout-article page-navbar">
<!-- sidebar -->
<!-- margin-sidebar -->
    <div id="quarto-margin-sidebar" class="sidebar margin-sidebar">
        <nav id="TOC" role="doc-toc" class="toc-active">
    <h2 id="toc-title">On this page</h2>
   
  <ul>
  <li><a href="#common-network-quantization" id="toc-common-network-quantization" class="nav-link active" data-scroll-target="#common-network-quantization">1. Common Network Quantization</a>
  <ul class="collapse">
  <li><a href="#k-means-based-quantization" id="toc-k-means-based-quantization" class="nav-link" data-scroll-target="#k-means-based-quantization">1.1 K-Means-based Quantization</a></li>
  <li><a href="#linear-quantization" id="toc-linear-quantization" class="nav-link" data-scroll-target="#linear-quantization">1.2 Linear Quantization</a></li>
  <li><a href="#scale-and-zero-point" id="toc-scale-and-zero-point" class="nav-link" data-scroll-target="#scale-and-zero-point">1.3 Scale and Zero point</a></li>
  <li><a href="#quantized-matrix-multiplication" id="toc-quantized-matrix-multiplication" class="nav-link" data-scroll-target="#quantized-matrix-multiplication">1.4 Quantized Matrix Multiplication</a></li>
  <li><a href="#symmetric-linear-quantization" id="toc-symmetric-linear-quantization" class="nav-link" data-scroll-target="#symmetric-linear-quantization">1.5 Symmetric Linear Quantization</a></li>
  <li><a href="#linear-quantization-examples" id="toc-linear-quantization-examples" class="nav-link" data-scroll-target="#linear-quantization-examples">1.6 Linear Quantization examples</a></li>
  </ul></li>
  <li><a href="#post-training-quantization-ptq" id="toc-post-training-quantization-ptq" class="nav-link" data-scroll-target="#post-training-quantization-ptq">2. Post-training Quantization (PTQ)</a>
  <ul class="collapse">
  <li><a href="#weight-quantization" id="toc-weight-quantization" class="nav-link" data-scroll-target="#weight-quantization">2.1 Weight quantization</a></li>
  <li><a href="#post-training-int8-linear-quantization-result" id="toc-post-training-int8-linear-quantization-result" class="nav-link" data-scroll-target="#post-training-int8-linear-quantization-result">2.4 Post-Training INT8 Linear Quantization Result</a></li>
  </ul></li>
  <li><a href="#quantization-aware-trainingqat" id="toc-quantization-aware-trainingqat" class="nav-link" data-scroll-target="#quantization-aware-trainingqat">3. Quantization-Aware Training(QAT)</a>
  <ul class="collapse">
  <li><a href="#quantization-aware-training" id="toc-quantization-aware-training" class="nav-link" data-scroll-target="#quantization-aware-training">3.1 Quantization-Aware Training</a></li>
  <li><a href="#straight-through-estimatorste" id="toc-straight-through-estimatorste" class="nav-link" data-scroll-target="#straight-through-estimatorste">3.2 Straight-Through Estimator(STE)</a></li>
  </ul></li>
  <li><a href="#binary-and-ternary-quantization" id="toc-binary-and-ternary-quantization" class="nav-link" data-scroll-target="#binary-and-ternary-quantization">4. Binary and Ternary Quantization</a>
  <ul class="collapse">
  <li><a href="#binarization-deterministic-binarization" id="toc-binarization-deterministic-binarization" class="nav-link" data-scroll-target="#binarization-deterministic-binarization">4.1 Binarization: <strong>Deterministic Binarization</strong></a></li>
  <li><a href="#binarization-stochastic-binarization" id="toc-binarization-stochastic-binarization" class="nav-link" data-scroll-target="#binarization-stochastic-binarization">4.2 Binarization: <strong>Stochastic Binarization</strong></a></li>
  <li><a href="#binarization-use-scale" id="toc-binarization-use-scale" class="nav-link" data-scroll-target="#binarization-use-scale">4.3 Binarization: Use Scale</a></li>
  <li><a href="#binarization-activation" id="toc-binarization-activation" class="nav-link" data-scroll-target="#binarization-activation">4.4 Binarization: Activation</a></li>
  <li><a href="#ternary-weight-networkstwn" id="toc-ternary-weight-networkstwn" class="nav-link" data-scroll-target="#ternary-weight-networkstwn">4.5 <strong>Ternary Weight Networks(TWN)</strong></a></li>
  <li><a href="#accuracy-degradation" id="toc-accuracy-degradation" class="nav-link" data-scroll-target="#accuracy-degradation">4.7 Accuracy Degradation</a></li>
  </ul></li>
  <li><a href="#low-bit-width-quantization" id="toc-low-bit-width-quantization" class="nav-link" data-scroll-target="#low-bit-width-quantization">5. Low Bit-Width Quantization</a>
  <ul class="collapse">
  <li><a href="#train-binarized-neural-networks-from-scratch" id="toc-train-binarized-neural-networks-from-scratch" class="nav-link" data-scroll-target="#train-binarized-neural-networks-from-scratch">5.1 Train Binarized Neural Networks From Scratch</a></li>
  <li><a href="#quantization-aware-training-dorefa-net-with-low-bit-width-gradients" id="toc-quantization-aware-training-dorefa-net-with-low-bit-width-gradients" class="nav-link" data-scroll-target="#quantization-aware-training-dorefa-net-with-low-bit-width-gradients">5.2 Quantization-Aware Training: DoReFa-Net With Low Bit-Width Gradients</a></li>
  <li><a href="#replace-the-activation-function-parameterized-clipping-activation-function" id="toc-replace-the-activation-function-parameterized-clipping-activation-function" class="nav-link" data-scroll-target="#replace-the-activation-function-parameterized-clipping-activation-function">5.3 Replace the Activation Function: Parameterized Clipping Activation Function</a></li>
  <li><a href="#modify-the-neural-network-architecture" id="toc-modify-the-neural-network-architecture" class="nav-link" data-scroll-target="#modify-the-neural-network-architecture">5.4 Modify the Neural Network Architecture</a></li>
  <li><a href="#no-quantization-on-first-and-last-layer" id="toc-no-quantization-on-first-and-last-layer" class="nav-link" data-scroll-target="#no-quantization-on-first-and-last-layer">5.5 No Quantization on First and Last Layer</a></li>
  <li><a href="#iterative-quantization-incremental-network-quantization" id="toc-iterative-quantization-incremental-network-quantization" class="nav-link" data-scroll-target="#iterative-quantization-incremental-network-quantization">5.6 Iterative Quantization: Incremental Network Quantization</a></li>
  </ul></li>
  <li><a href="#mixed-precision-quantization" id="toc-mixed-precision-quantization" class="nav-link" data-scroll-target="#mixed-precision-quantization">6. Mixed-precision quantization</a>
  <ul class="collapse">
  <li><a href="#uniform-quantization" id="toc-uniform-quantization" class="nav-link" data-scroll-target="#uniform-quantization">6.1 Uniform Quantization</a></li>
  <li><a href="#mixed-precision-quantization-1" id="toc-mixed-precision-quantization-1" class="nav-link" data-scroll-target="#mixed-precision-quantization-1">6.2 Mixed-precision Quantization</a></li>
  <li><a href="#huge-design-space-and-solution-design-automation" id="toc-huge-design-space-and-solution-design-automation" class="nav-link" data-scroll-target="#huge-design-space-and-solution-design-automation">6.3 Huge Design Space and Solution: Design Automation</a></li>
  </ul></li>
  <li><a href="#reference" id="toc-reference" class="nav-link" data-scroll-target="#reference">7. Reference</a></li>
  </ul>
</nav>
    </div>
<!-- main -->
<main class="content" id="quarto-document-content">

<header id="title-block-header" class="quarto-title-block default">
<div class="quarto-title">
<h1 class="title">ğŸ§‘â€ğŸ« Lecture 5-6</h1>
  <div class="quarto-categories">
    <div class="quarto-category">lecture</div>
    <div class="quarto-category">quantization</div>
  </div>
  </div>

<div>
  <div class="description">
    Quantization
  </div>
</div>


<div class="quarto-title-meta">

    <div>
    <div class="quarto-title-meta-heading">Author</div>
    <div class="quarto-title-meta-contents">
             <p>Seunghyun Oh </p>
          </div>
  </div>
    
    <div>
    <div class="quarto-title-meta-heading">Published</div>
    <div class="quarto-title-meta-contents">
      <p class="date">March 5, 2024</p>
    </div>
  </div>
  
    
  </div>
  


</header>


<style>
p {
  line-height: 2; /* Adjust the line-height property as needed */
  text-align: justify;
  text-justify: inter-word; /* Adjust the justification mode as needed */
}

.image-container {
  text-align: center; /* Center-align the contents horizontally */
}

.centered-text {
  text-align: center; /* Center-align the text within the container */
}
</style>
<p>ì´ë²ˆ ê¸€ì—ì„œëŠ” MIT HAN LABì—ì„œ ê°•ì˜í•˜ëŠ” <a href="https://www.youtube.com/playlist?list=PL80kAHvQbh-ocildRaxjjBy6MR1ZsNCU7">TinyML and Efficient Deep Learning Computing</a>ì— ë‚˜ì˜¤ëŠ” Quantization ë°©ë²•ì„ ì†Œê°œí•˜ë ¤ í•œë‹¤. Quantization(ì–‘ìí™”) ì‹ í˜¸ì™€ ì´ë¯¸ì§€ì—ì„œ ì•„ë‚ ë¡œê·¸ë¥¼ ë””ì§€í„¸ë¡œ ë³€í™˜í•˜ëŠ” ê³¼ì •ì—ì„œ ì‚¬ìš©í•˜ëŠ” ê°œë…ì´ë‹¤. ì•„ë˜ ê·¸ë¦¼ê³¼ ê°™ì´ ì—°ì†ì ì¸ ì„¼ì„œë¡œ ë¶€í„° ë“¤ì–´ì˜¤ëŠ” ì•„ë‚ ë¡œê·¸ ë°ì´í„° ë‚˜ ì´ë¯¸ì§€ë¥¼ í‘œí˜„í•˜ê¸° ìœ„í•´ ë‹¨ìœ„ ì‹œê°„ì— ëŒ€í•´ì„œ ë°ì´í„°ë¥¼ ìƒ˜í”Œë§í•˜ì—¬ ë°ì´í„°ë¥¼ ìˆ˜ì§‘í•œë‹¤.</p>
<div class="quarto-figure quarto-figure-center">
<figure class="figure">
<p><img src="../../images/lec05/1/intro.png" class="img-fluid figure-img"></p>
<figcaption>Reference. MIT-TinyML-lecture5-Quantization-1</figcaption>
</figure>
</div>
<p>ë””ì§€í„¸ë¡œ ë°ì´í„°ë¥¼ ë³€í™˜í•˜ê¸° ìœ„í•´ ë°ì´í„° íƒ€ì…ì„ ì •í•˜ë©´ì„œ ì´ë¥¼ í•˜ë‚˜ì”© ì–‘ìí™”í•œë‹¤. ì–‘ìˆ˜ì™€ ìŒìˆ˜ë¥¼ í‘œí˜„í•˜ê¸° ìœ„í•´ Unsigned Integer ì—ì„œ Signed Integer, Signedì—ì„œë„ Sign-Magnitude ë°©ì‹ê³¼ Twoâ€™s Complementë°©ì‹ìœ¼ë¡œ, ê·¸ë¦¬ê³  ë” ë§ì€ ì†Œìˆ«ì  ìë¦¬ë¥¼ í‘œí˜„í•˜ê¸° ìœ„í•´ Fixed-pointì—ì„œ Floating pointë¡œ ë°ì´í„° íƒ€ì…ì—ì„œ ìˆ˜ì˜ ë²”ì£¼ë¥¼ í™•ì¥ì‹œí‚¨ë‹¤. ì°¸ê³ ë¡œ Deviceì˜ Computationalityì™€ ML ëª¨ë¸ì˜ ì„±ëŠ¥ì§€í‘œì¤‘ í•˜ë‚˜ì¸ FLOPì´ ë°”ë¡œ floating point operations per secondì´ë‹¤.</p>
<p><img src="../../images/lec05/1/comp-bitwidth-fix-float.png" class="img-fluid"></p>
<div class="quarto-figure quarto-figure-center">
<figure class="figure">
<p><img src="../../images/lec05/1/comp-memory-fix-float.png" class="img-fluid figure-img"></p>
<figcaption>Reference. MIT-TinyML-lecture5-Quantization-1</figcaption>
</figure>
</div>
<p><a href="https://ooshyun.github.io/2023/02/24/Fixed-point-vs-Floating-point.html">ì´ ê¸€</a>ì—ì„œ floating pointë¥¼ ì´í•´í•˜ë©´, fixed pointë¥¼ ì‚¬ìš©í•˜ëŠ” ê²ƒì´ ë§¤ëª¨ë¦¬ì—ì„œ, ê·¸ë¦¬ê³  ì—°ì‚°ì—ì„œ ë” íš¨ìœ¨ì ì¼ ê²ƒì´ë¼ê³  ì˜ˆìƒí•´ë³¼ ìˆ ìˆ˜ ìˆë‹¤. MLëª¨ë¸ì„ í´ë¼ìš°ë“œ ì„œë²„ì—ì„œ ëŒë¦´ ë•ŒëŠ” í¬ê²Œ ë¬¸ì œë˜ì§€ ì•Šì•˜ì§€ë§Œ ì•„ë˜ ë‘ ê°€ì§€ í‘œë¥¼ ë³´ë©´ ì—ë„ˆì§€ì†Œëª¨, ì¦‰ ë°°í„°ë¦¬ íš¨ìœ¨ì—ì„œ í¬ê²Œ ì°¨ì´ê°€ ë³´ì¸ë‹¤. ê·¸ë ‡ê¸° ë•Œë¬¸ì— ëª¨ë¸ì—ì„œ Floating pointë¥¼ fixed pointë¡œ ë” ë§ì´ ë°”ê¾¸ë ¤ê³  í•˜ëŠ”ë° ì´ ë°©ë²•ìœ¼ë¡œ ë‚˜ì˜¨ ê²ƒì´ ë°”ë¡œ Quatizationì´ë‹¤.</p>
<p>ì´ë²ˆ ê¸€ì—ì„œëŠ” Quntization ì¤‘ì—ì„œ Quantization ë°©ë²•ê³¼ ê·¸ ì¤‘ Linearí•œ ë°©ë²•ì— ëŒ€í•´ ë” ìì„¸í•˜ê²Œ, ê·¸ë¦¬ê³  Post-training Quantizationê¹Œì§€ ë‹¤ë£¨ê³ , ë‹¤ìŒ ê¸€ì—ì„œëŠ” Quantization-Aware Training, Binary/Tenary Quantization, Mixed Precision Quantizationê¹Œì§€ ë‹¤ë£¨ë ¤ê³  í•œë‹¤.</p>
<section id="common-network-quantization" class="level2">
<h2 class="anchored" data-anchor-id="common-network-quantization">1. Common Network Quantization</h2>
<p>ì•ì„œì„œ ì†Œê°œí•œ ê²ƒì²˜ëŸ¼ Neural Netoworkë¥¼ ìœ„í•œ Quantizationì€ ë‹¤ìŒê³¼ ê°™ì´ ë‚˜ëˆŒ ìˆ˜ ìˆë‹¤. Quantization ë°©ë²•ì„ í•˜ë‚˜ì”© ì•Œì•„ë³´ì.</p>
<div class="quarto-figure quarto-figure-center">
<figure class="figure">
<p><img src="../../images/lec05/1/intro.png" class="img-fluid figure-img"></p>
<figcaption>Reference. MIT-TinyML-lecture5-Quantization-1</figcaption>
</figure>
</div>
<p>
<img src="../../images/lec05/1/quantization-method.png" width="500" height="300" class="projects__article__img__center">
</p><p align="center">
<em class="projects__img__caption"> Reference. MIT-TinyML-lecture5-Quantization-1 in https://efficientml.ai </em>
</p>
<p></p>
<section id="k-means-based-quantization" class="level3">
<h3 class="anchored" data-anchor-id="k-means-based-quantization">1.1 K-Means-based Quantization</h3>
<p>ê·¸ ì¤‘ ì²« ë²ˆì§¸ë¡œ K-means-based Quantizationì´ ìˆë‹¤. <a href="https://arxiv.org/abs/1510.00149">Deep Compression [Han&nbsp;et al., ICLR 2016]</a> ë…¼ë¬¸ì— ì†Œê°œí–ˆë‹¤ëŠ” ì´ ë°©ë²•ì€ ì¤‘ì‹¬ê°’ì„ ê¸°ì¤€ìœ¼ë¡œ clusteringì„ í•˜ëŠ” ë°©ë²•ì´ë‹¤. ì˜ˆì œë¥¼ ë´ë³´ì.</p>
<div class="quarto-figure quarto-figure-center">
<figure class="figure">
<p><img src="../../images/lec05/1/k-mean-quantization.png" class="img-fluid figure-img"></p>
<figcaption>Reference. MIT-TinyML-lecture5-Quantization-1</figcaption>
</figure>
</div>
<p>ìœ„ ì˜ˆì œëŠ” weightë¥¼ codebookì—ì„œ -1, 0, 1.5, 2ë¡œ ë‚˜ëˆ  ê°ê°ì— ë§ëŠ” ì¸ë±ìŠ¤ë¡œ í‘œê¸°í•œë‹¤. ì´ë ‡ê²Œ ì—°ì‚°ì„ í•˜ë©´ ê¸°ì¡´ì— 64bytesë¥¼ ì‚¬ìš©í–ˆë˜ weightê°€ 20bytesë¡œ ì¤„ì–´ë“ ë‹¤. codebookìœ¼ë¡œ ì˜ˆì œëŠ” 2bitë¡œ ë‚˜ëˆ´ì§€ë§Œ, ì´ë¥¼ N-bitë§Œí¼ ì¤„ì¸ë‹¤ë©´ ìš°ë¦¬ëŠ” ì´ 32/Në°°ì˜ ë©”ëª¨ë¦¬ë¥¼ ì¤„ì¼ ìˆ˜ ìˆë‹¤. í•˜ì§€ë§Œ ì´ ê³¼ì •ì—ì„œ quantizatio error, ì¦‰ quantizationì„ í•˜ê¸° ì „ê³¼ í•œ í›„ì— ì˜¤ì°¨ê°€ ìƒê¸°ëŠ” ê²ƒì„ ìœ„ ì˜ˆì œì—ì„œ ë³¼ ìˆ˜ ìˆë‹¤. ë©”ëª¨ë¦¬ ì‚¬ìš©ëŸ‰ì„ ì¤„ì´ëŠ” ê²ƒë„ ì¢‹ì§€ë§Œ, ì´ ë•Œë¬¸ì— ì„±ëŠ¥ì— ì˜¤ì°¨ê°€ ìƒê¸°ì§€ ì•Šê²Œ í•˜ê¸°ìœ„í•´ ì´ ì˜¤ì°¨ë¥¼ ì¤„ì´ëŠ” ê²ƒ ë˜í•œ ì¤‘ìš”í•˜ë‹¤.</p>
<div class="quarto-figure quarto-figure-center">
<figure class="figure">
<p><img src="../../images/lec05/1/k-mean-quantization-finetuning.png" class="img-fluid figure-img"></p>
<figcaption>Reference. MIT-TinyML-lecture5-Quantization-1</figcaption>
</figure>
</div>
<p>ì´ë¥¼ ë³´ì™„í•˜ê¸° ìœ„í•´ Quantizedí•œ Weightë¥¼ ìœ„ì— ê·¸ë¦¼ì²˜ëŸ¼ Fine-tuningí•˜ê¸°ë„ í•œë‹¤. centroidë¥¼ fine-tuningí•œë‹¤ê³  ìƒê°í•˜ë©´ ë˜ëŠ”ë°, ê° centroidì—ì„œ ìƒê¸°ëŠ” ì˜¤ì°¨ë¥¼ í‰ê· ë‚´ tuningí•˜ëŠ” ë°©ë²•ì´ë‹¤. ì´ ë°©ë²•ì„ ì œì•ˆí•œ <a href="https://arxiv.org/abs/1510.00149">ë…¼ë¬¸</a> ì—ì„œëŠ” Convolution ë ˆì´ì–´ì—ì„œëŠ” 4bitê¹Œì§€ centroidë¥¼ ê°€ì¡Œì„ ë•Œ, Full-Connected layerì—ì„œëŠ” 2 bitê¹Œì§€ centroidë¥¼ ê°€ì¡Œì„ ë•Œ ì„±ëŠ¥ì— í•˜ë½ì´ ì—†ë‹¤ê³  ë§í•˜ê³  ìˆì—ˆë‹¤.</p>
<div class="quarto-figure quarto-figure-center">
<figure class="figure">
<p><img src="../../images/lec05/1/continuous-data.png" class="img-fluid figure-img"></p>
<figcaption>Reference. Deep Compression [Han et al., ICLR 2016]</figcaption>
</figure>
</div>
<p>ì´ë ‡ê²Œ Quantization ëœ WeightëŠ” ìœ„ì²˜ëŸ¼ ì—°ì†ì ì¸ ê°’ì—ì„œ ì•„ë˜ì²˜ëŸ¼ Discreteí•œ ê°’ìœ¼ë¡œ ë°”ë€ë‹¤.</p>
<div class="quarto-figure quarto-figure-center">
<figure class="figure">
<p><img src="../../images/lec05/1/discrete-data.png" class="img-fluid figure-img"></p>
<figcaption>Reference. Deep Compression [Han et al., ICLR 2016]</figcaption>
</figure>
</div>
<p>ë…¼ë¬¸ì€ ì´ë ‡ê²Œ Quantizationí•œ weightë¥¼ í•œ ë²ˆ ë” Huffman codingë¥¼ ì´ìš©í•´ ìµœì í™”ì‹œí‚¨ë‹¤. ì§§ê²Œ ì„¤ëª…í•˜ìë©´, ë¹ˆë„ìˆ˜ê°€ ë†’ì€ ë¬¸ìëŠ” ì§§ì€ ì´ì§„ì½”ë“œë¥¼, ë¹ˆë„ ìˆ˜ê°€ ë‚®ì€ ë¬¸ìì—ëŠ” ê¸´ ì´ì§„ì½”ë“œë¥¼ ì“°ëŠ” ë°©ë²•ì´ë‹¤. ì••ì¶• ê²°ê³¼ë¡œ Generalí•œ ëª¨ë¸ê³¼ ì••ì¶• ë¹„ìœ¨ì´ ê½¤ í° SqueezeNetì„ ì˜ˆë¡œ ë“ ë‹¤. ìì„¸í•œ ë‚´ìš©ì€ ë…¼ë¬¸ì„ ì°¸ê³ í•˜ëŠ” ê±¸ë¡œ.</p>
<div class="quarto-figure quarto-figure-center">
<figure class="figure">
<p><img src="../../images/lec05/1/deep-compression.png" class="img-fluid figure-img"></p>
<figcaption>Reference. Deep Compression [Han et al., ICLR 2016]</figcaption>
</figure>
</div>
<div class="quarto-figure quarto-figure-center">
<figure class="figure">
<p><img src="../../images/lec05/1/deep-compression-result.png" class="img-fluid figure-img"></p>
<figcaption>Reference. Deep Compression [Han et al., ICLR 2016]</figcaption>
</figure>
</div>
<p>inferenceë¥¼ ìœ„í•´ weightë¥¼ Decodingí•˜ëŠ” ê³¼ì •ì€ inferenceê³¼ì •ì—ì„œ ì €ì¥í•œ clusterì˜ ì¸ë±ìŠ¤ë¥¼ ì´ìš©í•´ codebookì—ì„œ í•´ë‹¹í•˜ëŠ” ê°’ì„ ì°¾ì•„ë‚´ëŠ” ê²ƒì´ë‹¤. ì´ ë°©ë²•ì€ ì €ì¥ ê³µê°„ì„ ì¤„ì¼ ìˆ˜ëŠ” ìˆì§€ë§Œ, floating point Computationì´ë‚˜ ë©”ëª¨ë¦¬ ì ‘ê·¼í•˜ëŠ” ë°©ì‹ìœ¼ë¡œ centroidë¥¼ ì“°ëŠ” í•œê³„ê°€ ìˆì„ ìˆ˜ ë°–ì— ì—†ë‹¤.</p>
<div class="quarto-figure quarto-figure-center">
<figure class="figure">
<p><img src="../../images/lec05/1/intro.png" class="img-fluid figure-img"></p>
<figcaption>Reference. MIT-TinyML-lecture5-Quantization-1</figcaption>
</figure>
</div>
<p>
<img src="../../images/lec05/1/decoding-deep-compression.png" width="500" height="200" class="projects__article__img__center">
</p><p align="center">
<em class="projects__img__caption"> Reference. Deep Compression [Han et al., ICLR 2016] </em>
</p>
<p></p>
</section>
<section id="linear-quantization" class="level3">
<h3 class="anchored" data-anchor-id="linear-quantization">1.2 Linear Quantization</h3>
<p>ë‘ ë²ˆì§¸ ë°©ë²•ì€ Linear Quatizationì´ë‹¤. floating-pointì¸ weightë¥¼ N-bitì˜ ì •ìˆ˜ë¡œ affine mappingì„ ì‹œí‚¤ëŠ” ë°©ë²•ì´ë‹¤. ê°„ë‹¨í•˜ê²Œ ì‹ìœ¼ë¡œ ë³´ëŠ” ê²Œ ë” ì´í•´ê°€ ì‰½ë‹¤.</p>
<div class="quarto-figure quarto-figure-center">
<figure class="figure">
<p><img src="../../images/lec05/1/linear-quantization-eq.png" class="img-fluid figure-img"></p>
<figcaption>Reference. MIT-TinyML-lecture5-Quantization-1</figcaption>
</figure>
</div>
<p>ì—¬ê¸°ì„œ S(Scale of Linear Quantization)ì™€ Z(Zero point of Linear Quantization)ê°€ ìˆëŠ”ë° ì´ ë‘˜ì´ quantization parameter ë¡œì¨ tuningì„ í•  ìˆ˜ ìˆëŠ” ê°’ì¸ ê²ƒì´ë‹¤.</p>
<div class="quarto-figure quarto-figure-center">
<figure class="figure">
<p><img src="../../images/lec05/1/linear-quantization-img.png" class="img-fluid figure-img"></p>
<figcaption>Reference. MIT-TinyML-lecture5-Quantization-1</figcaption>
</figure>
</div>
</section>
<section id="scale-and-zero-point" class="level3">
<h3 class="anchored" data-anchor-id="scale-and-zero-point">1.3 Scale and Zero point</h3>
<div class="quarto-figure quarto-figure-center">
<figure class="figure">
<p><img src="../../images/lec05/1/scale-zero-point.png" class="img-fluid figure-img"></p>
<figcaption>Reference. MIT-TinyML-lecture5-Quantization-1</figcaption>
</figure>
</div>
<p>ì´ Scaleê³¼ Zero point ë‘ íŒŒë¼ë¯¸í„°ë¥¼ ì´ìš©í•´ì„œ affine mappingì€ ìœ„ ê·¸ë¦¼ê³¼ ê°™ë‹¤. Bit ìˆ˜(Bit Width)ê°€ ë‚®ì•„ì§€ë©´ ë‚®ì•„ì§ˆ ìˆ˜ë¡, floating pointì—ì„œ í‘œí˜„í•  ìˆëŠ” ìˆ˜ ë˜í•œ ì¤„ì–´ë“¤ ê²ƒì´ë‹¤. ê·¸ë ‡ë‹¤ë©´ Scaleì™€ Zero pointëŠ” ê°ê° ì–´ë–»ê²Œ ê³„ì‚°í• ê¹Œ?</p>
<p>ìš°ì„  floating-point ì¸ ìˆ«ìì˜ ë²”ìœ„ ì¤‘ ìµœëŒ€ê°’ê³¼ ìµœì†Ÿê°’ì— ë§ê²Œ ë‘ ì‹ì„ ì„¸ìš°ê³  ì´ë¥¼ ì—°ë¦½ë°©ì •ì‹ìœ¼ë¡œ Scaleê³¼ Zero pointì„ êµ¬í•  ìˆ˜ ìˆë‹¤.</p>
<ul>
<li><p>Scale point <span class="math display">\[
  r_{max} = S(q_{max}-Z)
  \]</span> <span class="math display">\[
  r_{min} = S(q_{min}-Z)
  \]</span></p>
<p><span class="math display">\[
  r_{max} - r_{min} = S(q_{max} - q_{min})
  \]</span></p>
<p><span class="math display">\[
  S = \dfrac{r_{max}-r_{min}}{q_{max}-q_{min}}
  \]</span></p></li>
<li><p>Zero point <span class="math display">\[
  r_{min} = S(q_{min}-Z)
  \]</span></p>
<p><span class="math display">\[
  Z=q_{min}-\dfrac{r_{min}}{S}
  \]</span></p>
<p><span class="math display">\[
  Z = round\Big(q_{min}-\dfrac{r_{min}}{S}\Big)
  \]</span></p></li>
</ul>
<p>ì˜ˆë¥¼ ë“¤ì–´, ì•„ë˜ì™€ ê°™ì€ ì˜ˆì œì—ì„œ <span class="math inline">\(r_{max}\)</span> ëŠ”<span class="math inline">\(2.12\)</span> ì´ê³  <span class="math inline">\(r_{min}\)</span> ì€ <span class="math inline">\(-1.08\)</span> ë¡œ Scaleì„ ê³„ì‚°í•˜ë©´ ì•„ë˜ ê·¸ë¦¼ì²˜ëŸ¼ ëœë‹¤. Zero pointëŠ” <span class="math inline">\(-1\)</span> ë¡œ ê³„ì‚°í•  ìˆ˜ ìˆë‹¤.</p>
<p><img src="../../images/lec05/1/scale-zero-point-ex1.png" class="img-fluid"></p>
<div class="quarto-figure quarto-figure-center">
<figure class="figure">
<p><img src="../../images/lec05/1/scale-zero-point-ex2.png" class="img-fluid figure-img"></p>
<figcaption>Reference. MIT-TinyML-lecture5-Quantization-1</figcaption>
</figure>
</div>
<p>ê·¸ëŸ¼ Symmetricí•˜ê²Œ rì˜ ë²”ìœ„ë¥¼ ì œí•œí•˜ëŠ” ê²ƒê³¼ ê°™ì€ ë‹¤ë¥¸ Linear Quantizationì€ ì—†ì„ê¹Œ? ì´ë¥¼ ì•ì„œ, Quatizedëœ ê°’ë“¤ì´ Matrix Multiplicationì„ í•˜ë©´ì„œ ë¯¸ë¦¬ ê³„ì‚°ë  ìˆ˜ ìˆëŠ” ìˆ˜ (Quantized Weight, Scale, Zero point)ê°€ ìˆìœ¼ë‹ˆ inferenceì‹œ ì—°ì‚°ëŸ‰ì„ ì¤„ì´ê¸° ìœ„í•´ ë¯¸ë¦¬ ê³„ì‚°í•  ìˆ˜ ìˆëŠ” íŒŒë¼ë¯¸í„°ëŠ” ì—†ì„ê¹Œ?</p>
</section>
<section id="quantized-matrix-multiplication" class="level3">
<h3 class="anchored" data-anchor-id="quantized-matrix-multiplication">1.4 Quantized Matrix Multiplication</h3>
<p>ì…ë ¥ X, Weight W, ê²°ê³¼ Yê°€ Matrix Multiplicationì„ í–ˆë‹¤ê³  í•  ë•Œ ì‹ì„ ê³„ì‚°í•´ë³´ì.</p>
<p><span class="math display">\[
Y=WX
\]</span></p>
<p><span class="math display">\[
S_Y(q_Y-Z_Y) = S_W(q_W-Z_W) \cdot S_X(q_X-Z_X
\]</span></p>
<p><span class="math display">\[
\vdots
\]</span></p>
<div class="quarto-figure quarto-figure-center">
<figure class="figure">
<p><img src="../../images/lec05/1/quantized-matrix-multi.png" class="img-fluid figure-img"></p>
<figcaption>Reference. MIT-TinyML-lecture5-Quantization-1</figcaption>
</figure>
</div>
<p>ì—¬ê¸°ì„œ ë§ˆì§€ë§‰ ì •ë¦¬í•œ ì‹ì„ ì‚´í´ë³´ë©´,</p>
<p><span class="math inline">\(Z_x\)</span> ì™€ <span class="math inline">\(q_w, Z_w, Z_X\)</span> ì˜ ê²½ìš°ëŠ” ë¯¸ë¦¬ ì—°ì‚°ì´ ê°€ëŠ¥í•˜ë‹¤. ë˜ <span class="math inline">\(S_wS_X/S_Y\)</span> ì˜ ê²½ìš° í•­ìƒ ìˆ˜ì˜ ë²”ìœ„ê°€ <span class="math inline">\((0, 1)\)</span> ë¡œ <span class="math inline">\(2^{-n}M_0\)</span> , <span class="math inline">\(M_0 \in [0.5, 1)\)</span> ë¡œ ë³€í˜•í•˜ë©´ N-bit Integerë¡œ Fixed-point í˜•íƒœë¡œ í‘œí˜„ ê°€ëŠ¥í•˜ë‹¤. ì—¬ê¸°ì— <span class="math inline">\(Z_w\)</span>ê°€ 0ì´ë©´ ì–´ë–¨ê¹Œ? ë˜ ë¯¸ë¦¬ ê³„ì‚°í•  ìˆ˜ ìˆëŠ” í•­ì´ ë³´ì¸ë‹¤.</p>
</section>
<section id="symmetric-linear-quantization" class="level3">
<h3 class="anchored" data-anchor-id="symmetric-linear-quantization">1.5 Symmetric Linear Quantization</h3>
<div class="quarto-figure quarto-figure-center">
<figure class="figure">
<p><img src="../../images/lec05/1/sym-linear-quant.png" class="img-fluid figure-img"></p>
<figcaption>Reference. MIT-TinyML-lecture5-Quantization-1</figcaption>
</figure>
</div>
<p><span class="math inline">\(Z_w = 0\)</span> ì´ë¼ê³  í•¨ì€ ë°”ë¡œ ìœ„ì™€ ê°™ì€ Weight ë¶„í¬ì¸ë°, ë°”ë¡œ Symmetricí•œ Linear Quantizationìœ¼ë¡œ <span class="math inline">\(Z_w\)</span>ë¥¼ 0ìœ¼ë¡œ ë§Œë“¤ì–´ <span class="math inline">\(Z_w q_x\)</span>í•­ì„ 0ìœ¼ë¡œ ë‘˜ ìˆ˜ ìˆì–´ ì—°ì‚°ì„ ë˜ ì¤„ì¼ ìˆ˜ ìˆì„ ê²ƒì´ë‹¤.</p>
<p>Symmetric Linear Quantizationì€ ì£¼ì–´ì§„ ë°ì´í„°ì—ì„œ Full range modeì™€ Restrict range modeë¡œ ë‚˜ë‰œë‹¤.</p>
<p>ì²« ë²ˆì§¸ Full range mode ëŠ” Scaleì„ real number(ë°ì´í„°, weight)ì—ì„œ ë²”ìœ„ê°€ ë„“ì€ ìª½ì— ë§ì¶”ëŠ” ê²ƒì´ë‹¤. ì˜ˆë¥¼ ë“¤ì–´ ì•„ë˜ì˜ ê²½ìš°, r_minì´ r_maxë³´ë‹¤ ì ˆëŒ“ê°’ì´ ë” í¬ê¸° ë•Œë¬¸ì— r_minì— ë§ì¶° q_minì„ ê°€ì§€ê³  Scaleì„ êµ¬í•œë‹¤. ì´ ë°©ë²•ì€ Pytorch native quantizationê³¼ ONNXì—ì„œ ì‚¬ìš©ëœë‹¤ê³  ê°•ì˜ì—ì„œ ì†Œê°œí•œë‹¤.</p>
<div class="quarto-figure quarto-figure-center">
<figure class="figure">
<p><img src="../../images/lec05/1/sym-full-range.png" class="img-fluid figure-img"></p>
<figcaption>Reference. MIT-TinyML-lecture5-Quantization-1</figcaption>
</figure>
</div>
<p>ë‘ ë²ˆì§¸ Restrict range modeëŠ” Scaleì„ real number(ë°ì´í„°, weight)ì—ì„œ ë²”ìœ„ê°€ ì¢ì€ ìª½ì— ë§ì¶”ëŠ” ê²ƒì´ë‹¤. ì˜ˆë¥¼ ë“¤ì–´ ì•„ë˜ì˜ ê²½ìš°, r_minê°€ r_maxë³´ë‹¤ ì ˆëŒ“ê°’ì´ ë” í¬ê¸° ë•Œë¬¸ì— r_minì— ë§ì¶”ë©´ì„œ q_maxì— ë§ë„ë¡ Scaleì„ êµ¬í•œë‹¤. ì´ ë°©ë²•ì€ <a href="https://www.tensorflow.org/lite/performance/quantization_spec">TensorFlow</a>, NVIDIA TensorRT, Intel DNNLì—ì„œ ì‚¬ìš©ëœë‹¤ê³  ê°•ì˜ì—ì„œ ì†Œê°œí•œë‹¤.</p>
<div class="quarto-figure quarto-figure-center">
<figure class="figure">
<p><img src="../../images/lec05/1/sym-restrict-range.png" class="img-fluid figure-img"></p>
<figcaption>Reference. MIT-TinyML-lecture5-Quantization-1</figcaption>
</figure>
</div>
<p>ê·¸ë ‡ë‹¤ë©´ ì™œ Symmetric ì¨ì•¼í• ê¹Œ? Asymmetric ë°©ë²•ê³¼ Symmetric ë°©ë²•ì˜ ì°¨ì´ëŠ” ë­˜ê¹Œ? (feat. Neural Network Distiller) ì•„ë˜ ê·¸ë¦¼ì„ ì°¸ê³ í•˜ë©´ ë˜ì§€ë§Œ, ê°€ì¥ í° ì°¨ì´ë¡œ ë³´ì´ëŠ” ê²ƒì€ Computation vs Compactful quantized rangeë¡œ ì´í•´ê°„ë‹¤.</p>
<div class="quarto-figure quarto-figure-center">
<figure class="figure">
<p><img src="../../images/lec05/1/sym-range-comp.png" class="img-fluid figure-img"></p>
<figcaption>Reference. MIT-TinyML-lecture5-Quantization-1</figcaption>
</figure>
</div>
</section>
<section id="linear-quantization-examples" class="level3">
<h3 class="anchored" data-anchor-id="linear-quantization-examples">1.6 Linear Quantization examples</h3>
<p>ê·¸ëŸ¼ Quatization ë°©ë²•ì— ëŒ€í•´ ì•Œì•„ë´¤ìœ¼ë‹ˆ ì´ë¥¼ Full-Connected Layer, Convolution Layerì— ì ìš©í•´ë³´ê³  ì–´ë–¤ íš¨ê³¼ê°€ ìˆëŠ”ì§€ ì•Œì•„ë³´ì.</p>
<section id="full-connected-layer" class="level4">
<h4 class="anchored" data-anchor-id="full-connected-layer">1.6.1 Full-Connected Layer</h4>
<p>ì•„ë˜ì²˜ëŸ¼ ì‹ì„ ì „ê°œí•´ë³´ë©´ ë¯¸ë¦¬ ì—°ì‚°í•  ê³„ì‚°í•  ìˆ˜ ìˆëŠ” í•­ê³¼ N-bit integerë¡œ í‘œí˜„í•  ìˆëŠ” í•­ìœ¼ë¡œ ë‚˜ëˆŒ ìˆ˜ ìˆë‹¤(ì „ê°œí•˜ëŠ” ì´ìœ ëŠ” ì•„ë§ˆ ë¯¸ë¦¬ ê³„ì‚°í•  ìˆ˜ ìˆëŠ” í•­ì„ ì•Œì•„ë³´ê¸° ìœ„í•¨ì´ ì•„ë‹ê¹Œ ì‹¶ë‹¤).</p>
<p><span class="math display">\[
Y=WX+b
\]</span></p>
<p><span class="math display">\[
\downarrow
\]</span></p>
<p><span class="math display">\[
S_Y(q_Y - Z_Y) = S_W(q_W - Z_W) \cdot S_X(q_X - Z_X) + S_b(q_b - Z_b)
\]</span></p>
<p><span class="math display">\[
\downarrow \ Z_w=0
\]</span></p>
<p><span class="math display">\[
S_Y(q_Y - Z_Y) = S_WS_X(q_Wq_X - Z_xq_W) + S_b(q_b - Z_b)
\]</span></p>
<p><span class="math display">\[
\downarrow \ Z_b=0, S_b=S_WS_X
\]</span></p>
<p><span class="math display">\[
S_Y(q_Y - Z_Y) = S_WS_X(q_Wq_X - Z_xq_W+q_b)
\]</span></p>
<p><span class="math display">\[
\downarrow
\]</span></p>
<p><span class="math display">\[
q_Y = \dfrac{S_WS_X}{S_Y}(q_Wq_X+q_b - Z_Xq_W) + Z_Y
\]</span></p>
<p><span class="math display">\[
\downarrow \ q_{bias}=q_b-Z_xq_W\\
\]</span></p>
<p><span class="math display">\[
q_Y = \dfrac{S_WS_X}{S_Y}(q_Wq_X+ q_{bias}) + Z_Y\\
\]</span></p>
<p>ê°„ë‹¨íˆ í‘œê¸°í•˜ê¸° ìœ„í•´ <span class="math inline">\(Z_W=0, Z_b=0, S_b = S_W S_X\)</span> ì´ë¼ê³  ê°€ì •í•œë‹¤.</p>
<div class="quarto-figure quarto-figure-center">
<figure class="figure">
<p><img src="../../images/lec05/1/full-connected-layer.png" class="img-fluid figure-img"></p>
<figcaption>Reference. MIT-TinyML-lecture5-Quantization-1</figcaption>
</figure>
</div>
</section>
<section id="convolutional-layer" class="level4">
<h4 class="anchored" data-anchor-id="convolutional-layer">1.6.2 Convolutional Layer</h4>
<p>Convolution Layerì˜ ê²½ìš°ëŠ” Weightì™€ Xì˜ ê³±ì˜ ê²½ìš°ë¥¼ Convolutionìœ¼ë¡œ ë°”ê¿”ì„œ ìƒê°í•´ë³´ë©´ ëœë‹¤. ê·¸ë„ ê·¸ëŸ´ ê²ƒì´ Convolutionì€ Kernelê³¼ Inputì˜ ê³±ì˜ í•©ìœ¼ë¡œ ì´ë£¨ì–´ì ¸ ìˆê¸° ë•Œë¬¸ì— Full-Connectedì™€ ê±°ì˜ ìœ ì‚¬í•˜ê²Œ ì „ê°œë  ìˆ˜ ìˆì„ ê²ƒì´ë‹¤.</p>
<div class="quarto-figure quarto-figure-center">
<figure class="figure">
<p><img src="../../images/lec05/1/conv-layer.png" class="img-fluid figure-img"></p>
<figcaption>Reference. MIT-TinyML-lecture5-Quantization-1</figcaption>
</figure>
</div>
</section>
</section>
</section>
<section id="post-training-quantization-ptq" class="level2">
<h2 class="anchored" data-anchor-id="post-training-quantization-ptq">2. Post-training Quantization (PTQ)</h2>
<p>ê·¸ëŸ¼ ì•ì„œì„œ Quantizaedí•œ Layerë¥¼ Fine tuningí•  ì—†ì„ê¹Œ? <strong>â€œHow should we get the optimal linear quantization parameters (S, Z)?â€</strong> ì´ ì§ˆë¬¸ì— ëŒ€í•´ì„œ Weight, Activation, Bias ì„¸ ê°€ì§€ì™€ ê·¸ì— ëŒ€í•˜ì—¬ ë…¼ë¬¸ì—ì„œ ë³´ì—¬ì£¼ëŠ” ê²°ê³¼ê¹Œì§€ ì•Œì•„ë³´ì.</p>
<section id="weight-quantization" class="level3">
<h3 class="anchored" data-anchor-id="weight-quantization">2.1 Weight quantization</h3>
<p><strong>TL;DR.</strong> ì´ ê°•ì˜ì—ì„œ ì†Œê°œí•˜ëŠ” Weight quantizationì€ Grandularityì— ë”°ë¼ Whole(Per-Tensor), Channel, ê·¸ë¦¬ê³  Layerë¡œ ë“¤ì–´ê°„ë‹¤.</p>
<section id="granularity" class="level4">
<h4 class="anchored" data-anchor-id="granularity">2.1.1 Granularity</h4>
<p>Weight quantizationì—ì„œ Granularityì— ë”°ë¼ì„œ Per-Tensor, Per-Channel, Group, ê·¸ë¦¬ê³  Generalized í•˜ëŠ” ë°©ë²•ìœ¼ë¡œ í™•ì¥ì‹œì¼œ Shared Micro-exponent(MX) data typeì„ ì°¨ë¡€ë¡œ ë³´ì—¬ì¤€ë‹¤. Scaleì„ ëª‡ ê°œë‚˜ ë‘˜ ê²ƒì´ëƒ, ê·¸ Scaleì„ ì ìš©í•˜ëŠ” ë²”ìœ„ë¥¼ ì–´ë–»ê²Œ ë‘˜ ê²ƒì´ëƒ, ê·¸ë¦¬ê³  Scaleì„ ì–¼ë§ˆë‚˜ ë””í…Œì¼í•˜ê²Œ(e.g.&nbsp;floating-point)í•  ê²ƒì´ëƒì— ì´ˆì ì„ ë‘”ë‹¤.</p>
<div class="quarto-figure quarto-figure-center">
<figure class="figure">
<p><img src="../../images/lec05/1/granularity.png" class="img-fluid figure-img"></p>
<figcaption>Reference. MIT-TinyML-lecture5-Quantization-2</figcaption>
</figure>
</div>
<p>ì²« ë²ˆì§¸ëŠ” <strong>Per-Tensor Quantization</strong> íŠ¹ë³„í•˜ê²Œ ì„¤ëª…í•  ê²ƒ ì—†ì´ ì´ì „ê¹Œì§€ ì„¤ëª…í–ˆë˜ í•˜ë‚˜ì˜ Scaleì„ ì‚¬ìš©í•˜ëŠ” Linear Quantizationì´ë¼ê³  ìƒê°í•˜ë©´ ë˜ê² ë‹¤. íŠ¹ì§•ìœ¼ë¡œëŠ” Large modelì— ëŒ€í•´ì„œëŠ” ì„±ëŠ¥ì´ ê´œì°®ì§€ë§Œ ì‘ì€ ëª¨ë¸ë¡œ ë–¨ì–´ì§€ë©´ ì„±ëŠ¥ì´ ê¸‰ê²©í•˜ê²Œ ë–¨ì–´ì§„ë‹¤ê³  ì„¤ëª…í•œë‹¤. Channelë³„ë¡œ weight ë²”ì£¼ê°€ ë„“ì€ ê²½ìš°ë‚˜ outlier weightê°€ ìˆëŠ” ê²½ìš° quantization ì´í›„ì— ì„±ëŠ¥ì´ í•˜ë½í–ˆë‹¤ê³  ë§í•œë‹¤.</p>
<div class="quarto-figure quarto-figure-center">
<figure class="figure">
<p><img src="../../images/lec05/1/per-channel-quant.png" class="img-fluid figure-img"></p>
<figcaption>Reference. MIT-TinyML-lecture5-Quantization-2</figcaption>
</figure>
</div>
<p>ê·¸ë˜ì„œ ê·¸ í•´ê²°ë°©ì•ˆìœ¼ë¡œ ë‚˜ì˜¤ëŠ” ê²ƒì´ ë‘ ë²ˆì§¸ ë°©ë²•ì¸ <strong>Per-Channel Quantization</strong>ì´ë‹¤. ìœ„ ì˜ˆì œì—ì„œ ë³´ë©´ Channel ë§ˆë‹¤ ìµœëŒ€ê°’ê³¼ ê°ê°ì— ë§ëŠ” Scaleì„ ë”°ë¡œ ê°€ì§€ëŠ” ê²ƒì„ ë³¼ ìˆ˜ ìˆë‹¤. ê·¸ë¦¬ê³  ì ìš©í•œ ê²°ê³¼ì¸ ì•„ë˜ ê·¸ë¦¼ì„ ë³´ë©´ Per-Channelê³¼ Per-Tensorë¥¼ ë¹„êµí•´ë³´ë©´ Per-Channelì´ ê¸°ì¡´ì— floating point weightì™€ì˜ ì°¨ì´ê°€ ë” ì ë‹¤. í•˜ì§€ë§Œ, ë§Œì•½ í•˜ë“œì›¨ì–´ì—ì„œ Per-Channel Quantizationì„ ì§€ì›í•˜ì§€ ì•ŠëŠ”ë‹¤ë©´ ë¶ˆí•„ìš”í•œ ì—°ì‚°ì„ ì¶”ê°€ë¡œ í•´ì•¼í•˜ê¸° ë•Œë¬¸ì— ì´ëŠ” ì í•©í•œ ë°©ë²•ì´ ë  ìˆ˜ ì—†ë‹¤ëŠ” ì ë„ ê³ ë ¤í•´ì•¼í•  ê²ƒì´ë‹¤(ì´ëŠ” ì´ì „ <a href="https://ooshyun.github.io/2023/12/04/Optimization-for-tiny-engine-1.html">Tiny Engineì— ëŒ€í•œ ê¸€</a>ì—ì„œ Channelë‚´ì— ìºì‹±ì„ ì´ìš©í•œ ìµœì í™”ì™€ ì—°ê´€ì´ ìˆë‹¤). ê·¸ëŸ¼ ë˜ ë‹¤ë¥¸ ë°©ë²•ì€ ì—†ì„ê¹Œ?</p>
<div class="quarto-figure quarto-figure-center">
<figure class="figure">
<p><img src="../../images/lec05/1/per-channel-vs-per-tensor.png" class="img-fluid figure-img"></p>
<figcaption>Reference. MIT-TinyML-lecture5-Quantization-2</figcaption>
</figure>
</div>
<p>ì„¸ ë²ˆì§¸ ë°©ë²•ì€ <strong>Group Quantization</strong>ìœ¼ë¡œ ì†Œê°œí•˜ëŠ” <strong>Per-vector Scaled Quantizationì™€ Shared Micro-exponent(MX) data type</strong> ì´ë‹¤. Per-vector Scaled Quantizationì€ 2023ë…„ë„ ê°•ì˜ë¶€í„° ì†Œê°œí•˜ëŠ”ë°, ì´ ë°©ë²•ì€ Scale factorë¥¼ ê·¸ë£¹ë³„ë¡œ í•˜ë‚˜, Per-Tensorë¡œ í•˜ë‚˜ë¡œ ë‘ê°œë¥¼ ë‘ëŠ” ë°©ë²•ì´ë‹¤. ì•„ë˜ì˜ ê·¸ë¦¼ì„ ë³´ë©´,</p>
<div class="quarto-figure quarto-figure-center">
<figure class="figure">
<p><img src="../../images/lec05/1/group-quantization.png" class="img-fluid figure-img"></p>
<figcaption>Reference. MIT-TinyML-lecture5-Quantization-2</figcaption>
</figure>
</div>
<p><span class="math display">\[
r=S(q-Z) \rightarrow r=\gamma \cdot S_{q}(q-Z)
\]</span></p>
<p><span class="math inline">\(S_q\)</span> ë¡œ vectorë³„ ìŠ¤ì¼€ì¼ë§ì„ í•˜ë‚˜, <span class="math inline">\(\gamma\)</span> ë¡œ Tensorì— ìŠ¤ì¼€ì¼ë§ì„ í•˜ë©° ê°ë§ˆëŠ” floating pointë¡œ í•˜ëŠ” ê²ƒì„ ë³¼ ìˆ˜ìˆë‹¤. ì•„ë¬´ë˜ë„ vectorë‹¨ìœ„ë¡œ ìŠ¤ì¼€ì¼ë§ì„ í•˜ê²Œë˜ë©´ channelê³¼ ë¹„êµí•´ì„œ í•˜ë“œì›¨ì–´ í”Œë«í¼ì— ë§ê²Œ accuracyì˜ trade-offë¥¼ ì¡°ì ˆí•˜ê¸° ë” ìˆ˜ì›”í•  ê²ƒìœ¼ë¡œ ë³´ì¸ë‹¤.</p>
<p>ì—¬ê¸°ì„œ ê°•ì˜ëŠ” ì§€í‘œì¸ Memory Overheadë¡œ <strong>â€œEffective Bit Widthâ€</strong>ë¥¼ ì†Œê°œí•œë‹¤. ì´ëŠ” Microsoftì—ì„œ ì œê³µí•˜ëŠ” Quantization Approach MX4, MX6, MX9ê³¼ ì—°ê²°ë¼ ìˆëŠ”ë°, ì´ ë°ì´í„°íƒ€ì…ì€ ì¡°ê¸ˆ ì´í›„ì— ë” ìì„¸íˆ ì„¤ëª…í•  ê²ƒì´ë‹¤. Effective Bit Width? ì˜ˆì‹œ í•˜ë‚˜ë¥¼ ë“¤ì–´ ì´í•´í•´ë³´ì. ë§Œì•½ 4-bit Quatizationì„ 4-bit per-vector scaleì„ 16 elements(4ê°œì˜ weightê°€ ê°ê° 4bitë¥¼ ê°€ì§„ë‹¤ê³  ìƒê°í•˜ë©´ 16 elementë¡œ ê³„ì‚°ëœë‹¤ ìœ ì¶”í•  ìˆë‹¤) ë¼ë©´, Effective Bit WidthëŠ” 4(Scale bit) + 4(Vector Scale bit) / 16(Vector Size) = 4.25ê°€ ëœë‹¤. Elementë‹¹ Scale bitë¼ê³  ê°„ë‹¨í•˜ê²Œ ìƒê°í•  ìˆ˜ë„ ìˆì„ ë“¯ ì‹¶ë‹¤.</p>
<p>ë§ˆì§€ë§‰ Per-vector Scaled Quantizationì„ ì´í•´í•˜ë‹¤ë³´ë©´ ì´ì „ì— Per-Tensor, Per-Channelë„ ê·¸ë£¹ìœ¼ë¡œ ì–¼ë§ˆë§Œí¼ ë¬¶ëŠ” ì°¨ì´ê°€ ìˆê³ , ì´ëŠ” ì´ë“¤ì„ ì¼ë°˜í™”í•  ìˆ˜ ìˆì–´ ë³´ì¸ë‹¤. ê°•ì˜ì—ì„œ ë°”ë¡œ ë‹¤ìŒì— ì†Œê°œí•˜ëŠ” ë°©ë²•ì´ ë°”ë¡œ <strong>Multi-level scaling scheme</strong>ì´ë‹¤. Per-Channel Quantizationì™€ Per-Vector Quantization(VSQ, Vector-Scale Quantization)ë¶€í„° ë´ë³´ì.</p>
<div class="quarto-figure quarto-figure-center">
<figure class="figure">
<p><img src="../../images/lec05/1/multi-level-scaling-scheme-1.png" class="img-fluid figure-img"></p>
<figcaption>Reference. With Shared Microexponents, A Little Shifting Goes a Long Way [Bita Rouhani et al.]</figcaption>
</figure>
</div>
<p>Per-Channel QuantizationëŠ” Scale factorê°€ í•˜ë‚˜ë¡œ Effective Bit WidthëŠ” 4ê°€ ëœë‹¤. ê·¸ë¦¬ê³  VSQëŠ” ì´ì „ì— ê³„ì‚°í–ˆ ë“¯ 4.25ê°€ ë  ê²ƒì´ë‹¤(ì°¸ê³ ë¡œ Per Channelë¡œ ì ìš©ë˜ëŠ” Scaleì˜ ê²½ìš° elementì˜ ìˆ˜ê°€ ë§ì•„ì„œ ê·¸ëŸ°ì§€ ë”°ë¡œ Effective Bit Widthë¡œ ê³„ì‚°í•˜ì§€ëŠ” ì•ŠëŠ”ë‹¤). VSQê¹Œì§€ ë³´ë©´ì„œ Effective Bit WidthëŠ”,</p>
<pre><code>Effective Bit Width = Scale bit + Group 0 Scale bit / Group 0 Size +...
e.g. VSQ Data type int4 = Scale bit (4) + Group 0 Scale bit(4) / Group 0 Size(16) = 4.25</code></pre>
<p>ì´ë ‡ê²Œ ê³„ì‚°í•  ìˆ˜ ìˆë‹¤. ê·¸ë¦¬ê³ , MX4, MX6, MX9ê°€ ë‚˜ì˜¨ë‹¤. ì°¸ê³ ë¡œ SëŠ” Sign bit, Mì€ Mantissa bit, EëŠ” Exponent bitë¥¼ ì˜ë¯¸í•œë‹¤(Mantissaë‚˜ Exponentì— ëŒ€í•œ ìì„¸í•œ ë‚´ìš©ì€ <a href="https://ooshyun.github.io/2023/02/24/Fixed-point-vs-Floating-point.html">floating point vs fixed point ê¸€</a>ì„ ì°¸ê³ í•˜ì). ì•„ë˜ëŠ” Microsoftì—ì„œ ì œê³µí•˜ëŠ” Quantization Approach MX4, MX6, MX9ì— ëŒ€í•œ í‘œì´ë‹¤.</p>
<div class="quarto-figure quarto-figure-center">
<figure class="figure">
<p><img src="../../images/lec05/1/multi-level-scaling-scheme-2.png" class="img-fluid figure-img"></p>
<figcaption>Reference. MIT-TinyML-lecture5-Quantization-1</figcaption>
</figure>
</div>
</section>
<section id="weight-equalization" class="level4">
<h4 class="anchored" data-anchor-id="weight-equalization">2.1.2 Weight Equalization</h4>
<p>ì—¬ê¸°ê¹Œì§€ Weight Quatizationì—ì„œ ê·¸ë£¹ìœ¼ë¡œ ì–¼ë§ˆë§Œí¼ ë¬¶ëŠ”ì§€ì— ë”°ë¼(ê°•ì˜ì—ì„œëŠ” Granularity) Quatizationì„ í•˜ëŠ” ì—¬ëŸ¬ ë°©ë²•ì„ ì†Œê°œí–ˆë‹¤. ë‹¤ìŒìœ¼ë¡œ ì†Œê°œ í•  ë°©ë²•ì€ Weight Equalizationì´ë‹¤. 2022ë…„ì— ì†Œê°œí•´ì¤€ ë‚´ìš©ì¸ë°, ì´ëŠ” ië²ˆì§¸ layerì˜ output channelë¥¼ scaling down í•˜ë©´ì„œ i+1ë²ˆì§¸ layerì˜ input channelì„ scaling up í•´ì„œ Scaleë¡œ ì¸í•´ Quantization ì „í›„ë¡œ ìƒê¸°ëŠ” Layerê°„ ì°¨ì´ë¥¼ ì¤„ì´ëŠ” ë°©ë²•ì´ë‹¤.</p>
<div class="quarto-figure quarto-figure-center">
<figure class="figure">
<p><img src="../../images/lec05/1/weight-equalization.png" class="img-fluid figure-img"></p>
<figcaption>Reference. Data-Free Quantization Through Weight Equalization and Bias Correction [Markus et al., ICCV 2019]</figcaption>
</figure>
</div>
<p>ì˜ˆë¥¼ ë“¤ì–´ ìœ„ì— ê·¸ë¦¼ì²˜ëŸ¼ Layer iì˜ output channelê³¼ Layer i+1ì˜ input channelì´ ìˆë‹¤. ì—¬ê¸°ì„œ ì‹ì„ ì „ê°œí•˜ë©´ ì•„ë˜ì™€ ê°™ì€ë°,</p>
<p><span class="math display">\[
\begin{aligned}
y^{(i+1)}&amp;=f(W^{(i+1)}x^{(i+1)}+b^{(i+1)}) \\
         &amp;=f(W^{(i+1)} \cdot f(W^{(i)}x^{(i)}+b^{(i)}) +b^{(i+1)}) \\
         &amp;=f(W^{(i+1)}S \cdot f(S^{-1}(W^{(i)}x^{(i)}+S^{-1}b^{(i)})) +b^{(i+1)})
\end{aligned}
\]</span></p>
<p>where <span class="math inline">\(S = diag(s)\)</span> , <span class="math inline">\(s_j\)</span> is the weight equalization scale factor of output channel <span class="math inline">\(j\)</span></p>
<p>ì—¬ê¸°ì„œ Scale(S)ê°€ i+1ë²ˆì§¸ layerì˜ weightì—, ië²ˆì§¸ weightì— 1/S ë¡œ Scaleë  ë–„ ê¸°ì¡´ì— Scale í•˜ì§€ ì•Šì€ ì‹ê³¼ ìœ ì‚¬í•˜ê²Œ ìœ ì§€í•  ìˆëŠ” ê²ƒì„ ë³¼ ìˆ˜ ìˆë‹¤. ì¦‰,</p>
<p><span class="math display">\[
r^{(i)}_{oc_j} / s = r^{(i+1)}_{ic_j} \cdot s
\]</span></p>
<p><span class="math display">\[
s_j = \dfrac{1}{r^{(i+1)}_{ic=j}}\sqrt{r_{oc=j}^{(i)}\cdot r_{ic=j}^{(i+1)}}
\]</span></p>
<p><span class="math display">\[
r^{(i)}_{oc_j} = r^{(i)}_{oc_j} / s = \sqrt{r_{oc=j}^{(i)}\cdot r_{ic=j}^{(i+1)}}
\]</span></p>
<p><span class="math display">\[
r^{(i)}_{ic_j} =r^{(i)}_{ic_j} \cdot s = \sqrt{r_{oc=j}^{(i)}\cdot r_{ic=j}^{(i+1)}}
\]</span></p>
<p>ì´ë ‡ê²Œ í•˜ë©´ ië²ˆì§¸ layerì˜ output channelê³¼ i+1ë²ˆì§¸ layerì˜ input channelì˜ Scaleì„ ê°ê° <span class="math inline">\(S\)</span> ì™€ <span class="math inline">\(1/S\)</span> ë¡œí•˜ë©° weightê°„ì˜ ê²©ì°¨ë¥¼ ì¤„ì¼ ìˆ˜ ìˆë‹¤.</p>
</section>
<section id="adaptive-rounding" class="level4">
<h4 class="anchored" data-anchor-id="adaptive-rounding">2.1.3 Adaptive rounding</h4>
<div class="quarto-figure quarto-figure-center">
<figure class="figure">
<p><img src="../../images/lec05/1/adaptive-rounding.png" class="img-fluid figure-img"></p>
<figcaption>Reference. MIT-TinyML-lecture5-Quantization-1</figcaption>
</figure>
</div>
ë§ˆì§€ë§‰ ì†Œê°œí•˜ëŠ” ë°©ë²•ì€ Adaptive rounding ì´ë‹¤. ë°˜ì˜¬ë¦¼ì€ Round-to-nearestìœ¼ë¡œ ë¶ˆë¦¬ëŠ” ì¼ë°˜ì ì¸ ë°˜ì˜¬ë¦¼ì„ ìƒê°í•  ìˆ˜ ìˆê³ , í•˜ë‚˜ì˜ ê¸°ì¤€ì„ ê°€ì§€ê³  ë°˜ì˜¬ë¦¼ì„ í•˜ëŠ” Adaptive Roundë¥¼ ìƒê°í•  í•  ìˆ˜ ìˆë‹¤. ê°•ì˜ì—ì„œëŠ” Round-to-nearestê°€ ìµœì ì˜ ë°©ë²•ì´ ë˜ì§€ ì•ŠëŠ”ë‹¤ê³  ë§í•˜ë©°, Adaptive roundë¡œ weightì— 0ë¶€í„° 1 ì‚¬ì´ì˜ ê°’ì„ ë”í•´ ìˆ˜ì‹ì²˜ëŸ¼ <span class="math inline">\(\tilde{w} = \lfloor\lfloor  w\rfloor + \delta\rceil, \delta \in [0, 1]\)</span> ìµœì ì˜ Optimalí•œ ë°˜ì˜¬ë¦¼ ê°’ì„ êµ¬í•œë‹¤. $$
<span class="math display">\[\begin{aligned}
&amp;argmin_V\lvert\lvert Wx-\tilde Wx\lvert\lvert ^2_F + \lambda f_{reg}(V) \\

\rightarrow &amp; argmin_V\lvert\lvert Wx-\lfloor\lfloor W \rfloor + h(V) \rceil x\lvert\lvert ^2_F + \lambda f_{reg}(V)
\end{aligned}\]</span>
<p>$$ ### 2.2 Activation quantization ë‘ ë²ˆì§¸ë¡œ Activation quantizationì´ ìˆë‹¤. ëª¨ë¸ê²°ê³¼ë¡œ ë‚˜ì˜¤ëŠ” ê²°ê³¼ë¥¼ ì§ì ‘ì ìœ¼ë¡œ ê²°ì •í•˜ëŠ” Activation Quatizationì—ì„œëŠ” ë‘ ê°€ì§€ë¥¼ ê³ ë ¤í•œ ë°©ë²•ì„ ì†Œê°œí•œë‹¤. í•˜ë‚˜ëŠ” Activation ë ˆì´ì–´ì—ì„œ ê²°ê³¼ê°’ì„ Smoothingí•œ ë¶„í¬ë¥¼ ê°€ì§€ê²Œ í•˜ê¸° ìœ„í•´ Exponential Moving Average(EMA)ë¥¼ ì‚¬ìš©í•˜ëŠ” ë°©ë²•ì´ê³ , ë‹¤ë¥¸ í•˜ë‚˜ëŠ” ë‹¤ì–‘í•œ ì…ë ¥ê°’ì„ ê³ ë ¤í•´ batch samplesì„ FP32 ëª¨ë¸ê³¼ calibrationí•˜ëŠ” ë°©ë²•ì´ë‹¤.</p>
<p>Exponential Moving Average (EMA)ì€ ì•„ë˜ ì‹ì—ì„œ <span class="math inline">\(\alpha\)</span> ë¥¼ êµ¬í•˜ëŠ” ë°©ë²•ì´ë‹¤. <span class="math display">\[
\hat r^{(t)}_{max, min} = \alpha r^{(t)}_{max, min} + (1-\alpha) \hat r^{(t)}_{max, min}  
\]</span> Calibrationì˜ ì»¨ì…‰ì€ ë§ì€ inputì˜ min/max í‰ê· ì„ ì´ìš©í•˜ìëŠ” ê²ƒì´ë‹¤. ê·¸ë˜ì„œ trained FP32 modelê³¼ sample batchë¥¼ ê°€ì§€ê³  quantizedí•œ ëª¨ë¸ì˜ ê²°ê³¼ì™€ calibrationì„ ëŒë¦¬ë©´ì„œ ê·¸ ì°¨ì´ë¥¼ ìµœì†Œí™” ì‹œí‚¤ëŠ”ë°, ì—¬ê¸°ì— ì´ìš©í•˜ëŠ” ì§€í‘œëŠ” loss of informationì™€ Newton-Raphson methodë¥¼ ì‚¬ìš©í•œ Mean Square Error(MSE)ê°€ ìˆë‹¤. <span class="math display">\[
MSE = \underset{\lvert r \lvert_{max}}{min}\ \mathbb{E}[(X-Q(X))^2]
\]</span> <span class="math display">\[
KL\ divergence=D_{KL}(P\lvert\lvert Q) = \sum_i^N P(x_i)log\dfrac{P(x_i)}{Q(x_i)}
\]</span> ### 2.3 Quanization Bias Correction</p>
<p>ë§ˆì§€ë§‰ìœ¼ë¡œ Quatizationìœ¼ë¡œ biased errorë¥¼ ì¡ëŠ”ë‹¤ëŠ” ê²ƒì„ ì†Œê°œí•œë‹¤. <span class="math inline">\(\epsilon = Q(W)-W\)</span> ì´ë¼ê³  ë‘ê³  ì•„ë˜ì²˜ëŸ¼ ì‹ì´ ì „ê°œì‹œí‚¤ë©´ ë§ˆì§€ë§‰ í•­ì—ì„œ ë³´ì´ëŠ” <span class="math inline">\(-\epsilon\mathbb{E}[x]\)</span> ë¶€ë¶„ì´ biasë¥¼ quatizationì„ í•  ë•Œ ì œê±° ëœë‹¤ê³  í•œë‹¤(ì´ ë¶€ë¶„ì€ 2023ë…„ì—ëŠ” ì†Œê°œí•˜ì§„ ì•ŠëŠ”ë°, ë‹¹ì—°í•œ ê²ƒì´ì–´ì„œ ì•ˆí•˜ëŠ”ì§€, í˜¹ì€ ì˜í–¥ì´ í¬ì§€ ì•Šì•„ì„œ ê·¸ëŸ°ì§€ëŠ” ëª¨ë¥´ê² ë‹¤. Bias Quatizationì´í›„ì— MobileNetV2ì—ì„œ í•œ ë ˆì´ì–´ì˜ outputì„ ë³´ë©´ ì–´ëŠì •ë„ ì œê±°ë˜ëŠ” ê²ƒì²˜ëŸ¼ ë³´ì¸ë‹¤). <span class="math display">\[
\begin{aligned}
\mathbb{E}[y] &amp;= \mathbb{E}[Wx] + \mathbb{E}[\epsilon x] - \mathbb{E}[\epsilon x],\ \mathbb{E}[Q(W)x] = \mathbb{E}[Wx] + \mathbb{E}[\epsilon x] \\
\mathbb{E}[y] &amp;= \mathbb{E}[Q(W)x] - \epsilon\mathbb{E}[x]
\end{aligned}
\]</span></p>
<div class="quarto-figure quarto-figure-center">
<figure class="figure">
<p><img src="../../images/lec05/1/quantization-bias-correction.png" class="img-fluid figure-img"></p>
<figcaption>Reference. MIT-TinyML-lecture5-Quantization-2</figcaption>
</figure>
</div>
</section>
</section>
<section id="post-training-int8-linear-quantization-result" class="level3">
<h3 class="anchored" data-anchor-id="post-training-int8-linear-quantization-result">2.4 Post-Training INT8 Linear Quantization Result</h3>
<p>ì•ì„  Post-Training Quantizationì„ ì ìš©í•œ ê²°ê³¼ë¥¼ ë³´ì—¬ì¤€ë‹¤. ì´ë¯¸ì§€ê³„ì—´ ëª¨ë¸ì„ ëª¨ë‘ ì‚¬ìš©í–ˆìœ¼ë©°, ì„±ëŠ¥í•˜ë½í­ì€ ì§€í‘œë¡œ ë³´ì—¬ì¤€ë‹¤. ë¹„êµì  í° ëª¨ë¸ë“¤ì˜ ê²½ìš° ì¤€ìˆ˜í•œ ì„±ëŠ¥ì„ ë³´ì—¬ì£¼ì§€ë§Œ MobileNetV1, V2ì™€ ê°™ì€ ì‘ì€ ëª¨ë¸ì€ ìƒê°ë³´ë‹¤ Quantizationìœ¼ë¡œ ë–¨ì–´ì§€ëŠ” ì„±ëŠ¥í­(-11.8%, -2.1%) ì´ í° ê²ƒì„ ë³¼ ìˆ˜ ìˆë‹¤. ê·¸ëŸ¼ ì‘ì€ í¬ê¸°ì˜ ëª¨ë¸ë“¤ì€ ì–´ë–»ê²Œ Training í•´ì•¼í• ê¹Œ?</p>
<div class="quarto-figure quarto-figure-center">
<figure class="figure">
<p><img src="../../images/lec05/1/post-training-result.png" class="img-fluid figure-img"></p>
<figcaption>Reference. MIT-TinyML-lecture5-Quantization-2</figcaption>
</figure>
</div>
</section>
</section>
<section id="quantization-aware-trainingqat" class="level2">
<h2 class="anchored" data-anchor-id="quantization-aware-trainingqat">3. Quantization-Aware Training(QAT)</h2>
<section id="quantization-aware-training" class="level3">
<h3 class="anchored" data-anchor-id="quantization-aware-training">3.1 Quantization-Aware Training</h3>
<div class="quarto-figure quarto-figure-center">
<figure class="figure">
<p><img src="../../images/lec05/2/Untitled.png" class="img-fluid figure-img"></p>
<figcaption>Reference. MIT-TinyML-lecture06-Quantization-2</figcaption>
</figure>
</div>
<ul>
<li>Usually, fine-tuning a pre-trained floating point model provides better accuracy than training from scratch.</li>
</ul>
<p>ì´ì „ì— K-mean Quantizationì—ì„œ Fine-tuningë•Œ Centroidì— gradientë¥¼ ë°˜ì˜í–ˆì—ˆë‹¤. Quantization-Aware Trainingì€ ì´ì™€ ìœ ì‚¬í•˜ê²Œ Quantization - Reconstructionì„ í†µí•´ ë§Œë“¤ì–´ì§„ Weightë¡œ Trainingì„ í•˜ëŠ” ë°©ë²•ì„ ë§í•œë‹¤. ì˜ˆì‹œë¥¼ ë“¤ì–´ì„œ ìì„¸íˆ ì‚´í´ë³´ì.</p>
<div class="quarto-figure quarto-figure-center">
<figure class="figure">
<p><img src="../../images/lec05/2/Untitled 1.png" class="img-fluid figure-img"></p>
<figcaption>Reference. MIT-TinyML-lecture06-Quantization-2</figcaption>
</figure>
</div>
<ul>
<li>A full precision copy of the weights W is maintained throughout the training.</li>
<li><strong>The small gradients are accumulated without loss of precision</strong></li>
<li>Once the model is trained, only the quantized weights are used for inference</li>
</ul>
<p>ìœ„ ê·¸ë¦¼ì—ì„œ Layer Nì´ ë³´ì¸ë‹¤. ì´ Layer Nì€ weightsë¥¼ íŒŒë¼ë¯¸í„°ë¡œ ê°€ì§€ì§€ë§Œ, ì‹¤ì œë¡œ Training ê³¼ì •ì—ì„œ ì“°ì´ëŠ” weightëŠ” â€œweight quantizationâ€ì„ í†µí•´ Quantization - Reconstructionì„ í†µí•´ ë§Œë“¤ì–´ì§„ Weightë¥¼ ê°€ì§€ê³  í›ˆë ¨ì„ í•  ê²ƒì´ë‹¤.</p>
</section>
<section id="straight-through-estimatorste" class="level3">
<h3 class="anchored" data-anchor-id="straight-through-estimatorste">3.2 Straight-Through Estimator(STE)</h3>
<div class="quarto-figure quarto-figure-center">
<figure class="figure">
<p><img src="../../images/lec05/2/Untitled 2.png" class="img-fluid figure-img"></p>
<figcaption>Reference. MIT-TinyML-lecture06-Quantization-2</figcaption>
</figure>
</div>
<p>ê·¸ëŸ¼ í›ˆë ¨ì—ì„œ gradientëŠ” ì–´ë–»ê²Œ ì „ë‹¬í•  ìˆ˜ ìˆì„ê¹Œ? Quantizationì˜ ê°œë…ìƒ, weight quantizationì—ì„œ weightë¡œ ë„˜ì–´ê°€ëŠ” gradientëŠ” ì—†ì„ ìˆ˜ ë°–ì— ì—†ë‹¤. ê·¸ë ‡ê²Œ ë˜ë©´ ì‚¬ì‹¤ìƒ weightë¡œ back propagationì´ ë  ìˆ˜ ì—†ê²Œ ë˜ê³ , ê·¸ë˜ì„œ ì†Œê°œí•˜ëŠ” ê°œë…ì´ <strong>Straight-Through Estimator(STE)</strong> ì…ë‹ˆë‹¤. ë§ì´ ê±°ì°½í•´ì„œ ê·¸ë ‡ì§€, Q(W)ì—ì„œ ë°›ì€ gradientë¥¼ ê·¸ëŒ€ë¡œ weights ë¡œ ë„˜ê²¨ì£¼ëŠ” ë°©ì‹ì´ë‹¤.</p>
<ul>
<li><p>Quantization is discrete-valued, and thus the derivative is 0 almost everywhere â†’ NN will learn nothing!</p></li>
<li><p><strong>Straight-Through Estimator(STE) simply passes the gradients through the quantization as if it had been the identity function.</strong></p>
<p><span class="math display">\[
  g_W = \dfrac{\partial L}{\partial  W} = \dfrac{\partial L}{\partial  Q(W)}
  \]</span></p></li>
</ul>
<div class="quarto-figure quarto-figure-center">
<figure class="figure">
<p><img src="../../images/lec05/2/Untitled 3.png" class="img-fluid figure-img"></p>
<figcaption>Reference. MIT-TinyML-lecture06-Quantization-2</figcaption>
</figure>
</div>
<ul>
<li>Reference
<ul>
<li>Neural Networks for Machine Learning [Hinton&nbsp;<em>et al.</em>, Coursera Video Lecture, 2012]</li>
<li>Estimating or Propagating Gradients Through Stochastic Neurons for Conditional Computation [Bengio, arXiv 2013]</li>
</ul></li>
</ul>
<p>ì´ í›ˆë ¨ì˜ ê²°ê³¼ê°€ ê¶ê¸ˆí•˜ì‹œë‹¤ë©´ <a href="https://arxiv.org/pdf/1806.08342.pdf">ì´ ë…¼ë¬¸</a>ì„ ì°¸ê³ í•˜ì. ì°¸ê³ ë¡œ ë…¼ë¬¸ì—ì„œëŠ” MobileNetV1, V2 ê·¸ë¦¬ê³  NASNet-Mobileì„ ì´ìš©í•´ Post-Training Quantizationê³¼ Quantization-Aware Trainingì„ ë¹„êµí•˜ê³  ìˆë‹¤.</p>
</section>
</section>
<section id="binary-and-ternary-quantization" class="level2">
<h2 class="anchored" data-anchor-id="binary-and-ternary-quantization">4. Binary and Ternary Quantization</h2>
<p>ì, ê·¸ëŸ¼ Quantizationì„ ê¶ê·¹ì ìœ¼ë¡œ 2bitë¡œ í•  ìˆ˜ëŠ” ì—†ì„ê¹Œ? ë°”ë¡œ Binary(1, -1)ê³¼ Tenary(1, 0, -1) ì´ë‹¤.</p>
<ul>
<li><p><strong>Can we push the quantization precision to 1 bit?</strong></p>
<div class="quarto-figure quarto-figure-center">
<figure class="figure">
<p><img src="../../images/lec05/2/Untitled 4.png" class="img-fluid figure-img"></p>
<figcaption>Reference. MIT-TinyML-lecture06-Quantization-2</figcaption>
</figure>
</div></li>
<li><p>Reference</p>
<ul>
<li>BinaryConnect: Training Deep Neural Networks with Binary Weights during Propagations [Courbariaux&nbsp;<em>et al.</em>, NeurIPS 2015]</li>
<li>XNOR-Net: ImageNet Classification using Binary Convolutional Neural Networks [Rastegari&nbsp;<em>et al.</em>, ECCV 2016]</li>
</ul></li>
</ul>
<p>ë¨¼ì € Weightë¥¼ 2bitë¡œ Quantizationì„ í•˜ê²Œ ë˜ë©´, ë©”ëª¨ë¦¬ì—ì„œëŠ” 32bitë¥¼ 1bitë¡œ ì¤„ì´ë‹ˆ 32ë°°ë‚˜ ì¤„ì¼ ìˆ˜ ìˆê³ , Computationë„ (8x5)+(-3x2)+(5x0)+(-1x1)ì—ì„œ 5-2+0-1 ë¡œ ì ˆë°˜ì„ ì¤„ì¼ ìˆ˜ ìˆë‹¤.</p>
<section id="binarization-deterministic-binarization" class="level3">
<h3 class="anchored" data-anchor-id="binarization-deterministic-binarization">4.1 Binarization: <strong>Deterministic Binarization</strong></h3>
<p>ê·¸ëŸ¼ Binarizationì—ì„œ +1ê³¼ -1ì„ ì–´ë–¤ ê¸°ì¤€ìœ¼ë¡œ í•´ì•¼í• ê¹Œ? ê°€ì¥ ì‰¬ìš´ ë°©ë²•ì€ threholdë¥¼ ê¸°ì¤€ìœ¼ë¡œ +-1ë¡œ ë‚˜ëˆ„ëŠ” ê²ƒì´ë‹¤.</p>
<p>Directly computes the bit value base on a threshold, usually 0 resulting in a sign function.</p>
<p><span class="math display">\[
q = sign(r) = \begin{dcases}
+1, &amp;r \geq 0 \\
-1, &amp;r &lt; 0
\end{dcases}
\]</span></p>
</section>
<section id="binarization-stochastic-binarization" class="level3">
<h3 class="anchored" data-anchor-id="binarization-stochastic-binarization">4.2 Binarization: <strong>Stochastic Binarization</strong></h3>
<p>ë‹¤ë¥¸ ë°©ë²•ìœ¼ë¡œëŠ” outputì—ì„œ hard-sigmoid functionì„ ê±°ì³ì„œ ë‚˜ì˜¨ ê°’ë§Œí¼ í™•ë¥ ì ìœ¼ë¡œ +-1ì´ ë‚˜ì˜¤ë„ë¡ í•˜ëŠ” ê²ƒì´ë‹¤. í•˜ì§€ë§Œ ì´ ë°©ë²•ì€ ë¬´ì‘ìœ„ë¡œ ë¹„íŠ¸ë¥¼ ìƒì„±í•˜ëŠ” í•˜ë“œì›¨ì–´ë¥¼ í•˜ëŠ” ê²ƒì´ ì–´ë µê¸° ë•Œë¬¸ì— ì‚¬ìš©í•˜ì§„ ì•ŠëŠ”ë‹¤ê³  ì–¸ê¸‰í•œë‹¤.</p>
<ul>
<li><p>Use global statistics or the value of input data to determine the probability of being -1 or +1</p></li>
<li><p>In Binary Connect(BC), probability is determined by hard sigmoid function <span class="math inline">\(\sigma(r)\)</span></p>
<p><span class="math display">\[
  q=\begin{dcases}
  +1, &amp;\text{with probability } p=\sigma(r)\\
  -1, &amp; 1-p
  \end{dcases}
  \\
  where\ \sigma(r)=min(max(\dfrac{r+1}{2}, 0), 1)
  \]</span></p>
<div class="quarto-figure quarto-figure-center">
<figure class="figure">
<p><img src="../../images/lec05/2/Untitled 5.png" class="img-fluid figure-img"></p>
<figcaption>Reference. MIT-TinyML-lecture06-Quantization-2</figcaption>
</figure>
</div></li>
<li><p><strong>Harder to implement</strong> as it requires the hardware to generate random bits when quantizing.</p></li>
</ul>
</section>
<section id="binarization-use-scale" class="level3">
<h3 class="anchored" data-anchor-id="binarization-use-scale">4.3 Binarization: Use Scale</h3>
<p>ì•ì„  ë°©ë²•ì„ ì´ìš©í•´ì„œ ImageNet Top-1 ì„ í‰ê°€í•´ë³´ë©´ Quantizationì´í›„ -21.2%ë‚˜ ì„±ëŠ¥ì´ í•˜ë½í•˜ëŠ” ê±¸ ë³¼ ìˆ˜ ìˆë‹¤. â€œì–´ë–»ê²Œ ë³´ì™„í•  ìˆ˜ ìˆì„ê¹Œ?â€ í•œ ê²ƒì´ linear qunatizationì—ì„œ ì‚¬ìš©í–ˆë˜ Scale ê°œë…ì´ë‹¤.</p>
<ul>
<li><p>Using <strong>Scale</strong>, Minimizing Quantization Error in Binarization</p>
<div class="quarto-figure quarto-figure-center">
<figure class="figure">
<p><img src="../../images/lec05/2/Untitled 6.png" class="img-fluid figure-img"></p>
<figcaption>Reference. MIT-TinyML-lecture06-Quantization-2</figcaption>
</figure>
</div></li>
</ul>
<p>ì—¬ê¸°ì„œ Scaleì€ <span class="math inline">\(\dfrac{1}{n}\lvert\lvert W \lvert\lvert_1\)</span> ë¡œ ê³„ì‚°í•  ìˆ˜ ìˆê³ , ì„±ëŠ¥ì€ í•˜ë½ì´ ê±°ì˜ ì—†ëŠ” ê²ƒë„ í™•ì¸í•  ìˆ˜ ìˆë‹¤. ì™œ <span class="math inline">\(\dfrac{1}{n}\lvert\lvert W \lvert\lvert_1\)</span>ì¸ì§€ëŠ” ì•„ë˜ ì¦ëª…ê³¼ì •ì„ ì°¸ê³ í•˜ì!</p>
<ul>
<li><p>Why <span class="math inline">\(\alpha\)</span> is <span class="math inline">\(\dfrac{1}{n}\lvert\lvert W \lvert\lvert_1\)</span>?</p>
<p><span class="math display">\[
  \begin{aligned}
  &amp;J(B, \alpha)=\lvert\lvert W-\alpha B\lvert\lvert^2 \\
  &amp;\alpha^*, B^*= \underset{\alpha, B}{argmin}\ J(B, \alpha) \\
  &amp;J(B,\alpha) = \alpha^2B^TB-2\alpha W^T B + W^TW\ since\ B \in \{+1, -1\}^n \\
  &amp;B^TB=n(constant), W^TW= constant(a \ known\ variable) \\
  &amp;J(B,\alpha) = \alpha^2n-2\alpha W^T B + C \\
  &amp;B^* = \underset{B}{argmax} \{W^T B\}\ s.t.\ B\in \{+1,-1 \}^n \\
  &amp;\alpha^*=\dfrac{W^TB^*}{n} \\
  &amp;\alpha^*=\dfrac{W^Tsign(W)}{n} = \dfrac{\lvert W_i \lvert}{n} = \dfrac{1}{n}\lvert\lvert W\lvert\lvert_{l1}
  \end{aligned}
  \]</span></p>
<ul>
<li>Reference. XNOR-Net: ImageNet Classification using Binary Convolutional Neural Networks [Rastegari&nbsp;<em>et al.</em>, ECCV 2016]</li>
<li>B*ëŠ” J(B,<span class="math inline">\(\alpha\)</span>)ì—ì„œ ìµœì†Ÿê°’ì„ êµ¬í•´ì•¼í•˜ë¯€ë¡œ <span class="math inline">\(W^T\)</span>B ê°€ ìµœëŒ€ì—¬ì•¼í•˜ê³  ê·¸ëŸ¬ê¸° ìœ„í•´ì„œëŠ” Wê°€ ì–‘ìˆ˜ì¼ë•ŒëŠ” Bë„ ì–‘ìˆ˜, Wê°€ ìŒìˆ˜ì¼ ë•ŒëŠ” Bë„ ìŒìˆ˜ì—¬ì•¼ <span class="math inline">\(W^TB=\sum\lvert W \lvert\)</span> ì´ ë˜ë©´ì„œ ìµœëŒ“ê°’ì´ ë  ìˆ˜ ìˆë‹¤.</li>
</ul></li>
</ul>
</section>
<section id="binarization-activation" class="level3">
<h3 class="anchored" data-anchor-id="binarization-activation">4.4 Binarization: Activation</h3>
<p>ê·¸ëŸ¼ Activationê¹Œì§€ Quantizationì„ í•´ë´…ì‹œë‹¤.</p>
<p><strong>4.4.1 Activation</strong></p>
<div class="quarto-figure quarto-figure-center">
<figure class="figure">
<p><img src="../../images/lec05/2/Untitled 7.png" class="img-fluid figure-img"></p>
<figcaption>Untitled</figcaption>
</figure>
</div>
<p>ì—¬ê¸°ì„œ ì¡°ê¸ˆ ë” ì—°ì‚°ì„ ìµœì í™” í•  ìˆ˜ ìˆì–´ë³´ì´ëŠ” ê²ƒì´ Matrix Muliplicationì´ XOR ì—°ì‚°ê³¼ ë¹„ìŠ·í•˜ê²Œ ë³´ì¸ë‹¤.</p>
<p><strong>4.4.2 XNOR bit count</strong></p>
<div class="quarto-figure quarto-figure-center">
<figure class="figure">
<p><img src="../../images/lec05/2/Untitled 8.png" class="img-fluid figure-img"></p>
<figcaption>Reference. MIT-TinyML-lecture06-Quantization-2</figcaption>
</figure>
</div>
<ul>
<li><span class="math inline">\(y_i=-n+ popcount(W_i\ xnor\ x) &lt;&lt; 1\)</span> â†’ <strong>popcount returns the number of 1</strong></li>
</ul>
<p>ê·¸ë˜ì„œ <strong>popcount</strong>ê³¼ <strong>XNOR</strong>ì„ ì´ìš©í•´ì„œ Computationì—ì„œ ì¢€ ë” ìµœì í™”ë¥¼ ì§„í–‰í•©ë‹ˆë‹¤. ì´ë ‡ê²Œ ìµœì í™”ë¥¼ ì§„í–‰í•˜ê²Œ ë˜ë©´, ë©”ëª¨ë¦¬ëŠ” 32ë°°, Computationì€ 58ë°°ê°€ëŸ‰ ì¤„ì–´ë“¤ ìˆ˜ ìˆë‹¤ê³  ë§í•œë‹¤.</p>
<div class="quarto-figure quarto-figure-center">
<figure class="figure">
<p><img src="../../images/lec05/2/Untitled 9.png" class="img-fluid figure-img"></p>
<figcaption>Reference. MIT-TinyML-lecture06-Quantization-2</figcaption>
</figure>
</div>
<p>ì´ë ‡ê²Œ Weight, Scale factor, Activation, ê·¸ë¦¬ê³  XNOR-Bitcout ê¹Œì§€. ì´ ë„¤ ê°€ì§€ ë‹¨ê³„ë¡œ Binary Quantizationì„ ë‚˜ëˆˆë‹¤. ë‹¤ìŒìœ¼ë¡œëŠ” Ternary Quantizationì€ ì•Œì•„ë³´ì.</p>
<div class="quarto-figure quarto-figure-center">
<figure class="figure">
<p><img src="../../images/lec05/2/Untitled 10.png" class="img-fluid figure-img"></p>
<figcaption>Reference. XNOR-Net: ImageNet Classification using Binary Convolutional Neural Networks [Rastegari&nbsp;<em>et al.</em>, ECCV 2016]</figcaption>
</figure>
</div>
<ul>
<li><ol start="2" type="1">
<li>Binarizing Input ì˜ ê²½ìš°ëŠ” averageë¥¼ ëª¨ë“  channelì— ê°™ì´ ì ìš©í•  ê²ƒì´ê¸° ë•Œë¬¸ì— ê·¸ cë§Œí¼ì„ average filterë¡œ í•œ ë²ˆì— ì ìš©í•œë‹¤ëŠ” ë§ì´ë‹¤.</li>
</ol></li>
</ul>
</section>
<section id="ternary-weight-networkstwn" class="level3">
<h3 class="anchored" data-anchor-id="ternary-weight-networkstwn">4.5 <strong>Ternary Weight Networks(TWN)</strong></h3>
<p>TernaryëŠ” Binary Quantizationê³¼ ë‹¨ê³„ëŠ” ëª¨ë‘ ê°™ì§€ë§Œ, ê°€ì§ˆ ìˆ˜ ìˆëŠ” ê°’ìœ¼ë¡œ <strong>0</strong> ì„ ì¶”ê°€í•œë‹¤. ì•„ë˜ ê·¸ë¦¼ì€ Scaleì„ ì´ìš©í•´ì„œ Quantization Errorë¥¼ ì¤„ì´ëŠ” ë°©ë²•ì„ ë§í•˜ê³  ìˆë‹¤. <span class="math display">\[
q = \begin{dcases}
r_t, &amp;r &gt; \Delta \\
0, &amp;\lvert r\lvert \leq \Delta \\
-r_t, &amp;r &lt; -\Delta
\end{dcases} \\
where\ \Delta = 0.7\times \mathbb{E}(\lvert r \lvert), r_t = \mathbb{E}_{\lvert r \lvert &gt; \Delta}(\lvert r \lvert )
\]</span> <img src="../../images/lec05/2/Untitled 11.png" class="img-fluid" alt="Reference. Trained Ternary Quantization [Zhu&nbsp;et al., ICLR 2017]"> ### 4.6 Trained Ternary Quantization(TTQ)</p>
<p>Tenary Quantizationì—ì„œ ë˜ í•œê°€ì§€ ë‹¤ë¥´ê²Œ ì„¤ëª…í•˜ëŠ” ê²ƒì€ 1ê³¼ -1ë¡œë§Œ ì •í•´ì ¸ ìˆë˜ Binary Quantizationê³¼ ë‹¤ë¥´ê²Œ TenaryëŠ” 1, 0, -1ë¡œ Quantizationì„ í•œ í›„, ì¶”ê°€ì ì¸ í›ˆë ¨ì„ í†µí•´ <span class="math inline">\(w_t\)</span>ì™€ <span class="math inline">\(-w_t\)</span>ë¡œ fine-tuningì„ í•˜ëŠ” ë°©ë²•ë„ ì œì•ˆí•œë‹¤(í•´ë‹¹ <a href="https://arxiv.org/pdf/1612.01064.pdf">ë…¼ë¬¸</a>ì—ì„œëŠ” ì´ëŸ¬í•œ ê¸°ë²•ì„ ì´ìš©í•´ì„œ í•œ ê²°ê³¼ë¥¼ CIFAR-10 ì´ë¯¸ì§€ ë°ì´í„°ë¥¼ ê°€ì§€ê³  ResNets, AlexNet, ImageNetì—ì„œ ë³´ì—¬ì¤€ë‹¤). <span class="math display">\[
q = \begin{dcases}
w_t, &amp;r &gt; \Delta \\
0, &amp;\lvert r\lvert \leq \Delta \\
-w_t, &amp;r &lt; -\Delta
\end{dcases}
\]</span> <img src="../../images/lec05/2/Untitled 12.png" class="img-fluid" alt="Reference. Trained Ternary Quantization [Zhu&nbsp;et al., ICLR 2017]"></p>
</section>
<section id="accuracy-degradation" class="level3">
<h3 class="anchored" data-anchor-id="accuracy-degradation">4.7 Accuracy Degradation</h3>
<p>Binary, Ternary Quantizationì„ ì‚¬ìš©í•œ ê²°ê³¼ë¥¼ ë³´ì—¬ì¤€ë‹¤(Resnet-18 ê²½ìš°ì—ëŠ” Ternary ê°€ ì˜¤íˆë ¤ Binaryë³´ë‹¤ ì„±ëŠ¥ì´ ë” ë–¨ì–´ì§„ë‹¤!)</p>
<ul>
<li><p><strong>Binarization</strong></p>
<div class="quarto-figure quarto-figure-center">
<figure class="figure">
<p><img src="../../images/lec05/2/Untitled 13.png" class="img-fluid figure-img"></p>
<figcaption>Reference. Binarized Neural Networks: Training Deep Neural Networks with Weights and Activations Constrained to +1 or&nbsp;âˆ’1. [Courbariaux&nbsp;<em>et al.</em>, Arxiv 2016], XNOR-Net: ImageNet Classification using Binary Convolutional Neural Networks [Rastegari&nbsp;<em>et al.</em>, ECCV 2016]</figcaption>
</figure>
</div></li>
<li><p><strong>Ternary Weight Networks (TWN)</strong></p>
<div class="quarto-figure quarto-figure-center">
<figure class="figure">
<p><img src="../../images/lec05/2/Untitled 14.png" class="img-fluid figure-img"></p>
<figcaption>Reference. Ternary Weight Networks [Li&nbsp;<em>et al.</em>, Arxiv 2016]</figcaption>
</figure>
</div></li>
<li><p><strong>Trained Ternary Quantization (TTQ)</strong></p>
<div class="quarto-figure quarto-figure-center">
<figure class="figure">
<p><img src="../../images/lec05/2/Untitled 15.png" class="img-fluid figure-img"></p>
<figcaption>Reference. Trained Ternary Quantization [Zhu&nbsp;<em>et al.</em>, ICLR 2017]</figcaption>
</figure>
</div></li>
</ul>
</section>
</section>
<section id="low-bit-width-quantization" class="level2">
<h2 class="anchored" data-anchor-id="low-bit-width-quantization">5. Low Bit-Width Quantization</h2>
<p>ë‚¨ì€ ë¶€ë¶„ë“¤ì€ ì—¬ëŸ¬ê°€ì§€ ì‹¤í—˜ / ì—°êµ¬ë“¤ì„ ì†Œê°œí•˜ê³  ìˆë‹¤.</p>
<ul>
<li>Binary Quantizationì€ Quantization Aware Trainingì„ í•  ìˆ˜ ìˆì„ê¹Œ?</li>
<li>2,3 bitê³¼ 8bit ê·¸ ì¤‘ê°„ìœ¼ë¡œëŠ” Quantizationì„ í•  ìˆ˜ ì—†ì„ê¹Œ?</li>
<li>ë ˆì´ì–´ì—ì„œ Quantizationì„ í•˜ì§€ ì•ŠëŠ” ë ˆì´ì–´, ì˜ˆë¥¼ ë“¤ì–´ ê²°ê³¼ì— ì˜í–¥ì„ ì˜ˆë¯¼í•˜ê²Œ ë¯¸ì¹˜ëŠ” ì²« ë²ˆì§¸ ë ˆì´ì–´ê°€ ê°™ì€ ê²½ìš° Quantizationì„ í•˜ì§€ ì•Šìœ¼ë©´ ì–´ë–»ê²Œ ë ê¹Œ?</li>
<li>Activation í•¨ìˆ˜ë¥¼ ë°”ê¾¸ë©´ ì–´ë–¨ê¹Œ?</li>
<li>ì˜ˆë¥¼ ë“¤ì–´ ì²«ë²ˆì§¸ ë ˆì´ì–´ì˜ Në°° ë„“ê²Œ í•˜ëŠ” ê²ƒê³¼ ê°™ì´ ëª¨ë¸ êµ¬ì¡°ë¥¼ ë°”ê¾¸ë©´ ì–´ë–»ê²Œ ë ê¹Œ?</li>
<li>ì¡°ê¸ˆì”© Quantizationì„ í•  ìˆ˜ ì—†ì„ê¹Œ? (20% â†’ 40% â†’ â€¦ â†’ 100%)</li>
</ul>
<p>ê°•ì˜ì—ì„œëŠ” í¬ê²Œ ì–¸ê¸‰í•˜ì§€ ì•Šê³  ê°„ ë‚´ìš©ë“¤ì´ë¼ ì„¤ëª…ì„ í•˜ì§€ëŠ” ì•Šê² ë‹¤. í•´ë‹¹ ë‚´ìš©ë“¤ì€ ìì„¸í•œ ë‚´ìš©ì„ ì•Œê³ ì‹¶ìœ¼ë©´ ê° íŒŒíŠ¸ì— ì–¸ê¸‰ëœ ë…¼ë¬¸ì„ ì°¸ì¡°í•˜ê¸¸!</p>
<section id="train-binarized-neural-networks-from-scratch" class="level3">
<h3 class="anchored" data-anchor-id="train-binarized-neural-networks-from-scratch">5.1 Train Binarized Neural Networks From Scratch</h3>
<ul>
<li>Straight-Through Estimator(STE)</li>
</ul>
<div class="quarto-figure quarto-figure-center">
<figure class="figure">
<p><img src="../../images/lec05/2/Untitled 16.png" class="img-fluid figure-img"></p>
<figcaption>Reference. MIT-TinyML-lecture06-Quantization-2</figcaption>
</figure>
</div>
<ul>
<li>Gradient pass straight to floating-point weights</li>
<li>Floating-point weight with in [-1, 1]</li>
<li>Reference. Binarized Neural Networks: Training Deep Neural Networks with Weights and Activations Constrained to +1 or&nbsp;âˆ’1. [Courbariaux et al., Arxiv 2016]</li>
</ul>
</section>
<section id="quantization-aware-training-dorefa-net-with-low-bit-width-gradients" class="level3">
<h3 class="anchored" data-anchor-id="quantization-aware-training-dorefa-net-with-low-bit-width-gradients">5.2 Quantization-Aware Training: DoReFa-Net With Low Bit-Width Gradients</h3>
<div class="quarto-figure quarto-figure-center">
<figure class="figure">
<p><img src="../../images/lec05/2/Untitled 17.png" class="img-fluid figure-img"></p>
<figcaption>Reference. MIT-TinyML-lecture06-Quantization-2</figcaption>
</figure>
</div>
<ul>
<li><p>Gradient Quantization</p>
<p><span class="math display">\[
  Q(g) = 2 \cdot max(\lvert G \lvert) \cdot \Large[ \small quantize_k \Large( \small \dfrac{g}{2\cdot max(\lvert G \lvert)} + \dfrac{1}{2} + N(k) \Large ) \small -\dfrac{1}{2} \Large]\small
  \]</span> <span class="math display">\[
  where\ N(k)=\dfrac{\sigma}{2^k-1} and\ \sigma \thicksim Uniform(-0.5, 0.5)
  \]</span></p>
<ul>
<li>Noise function <span class="math inline">\(N(k)\)</span> is added to compensate the potential bias introduced by gradient quantization.</li>
</ul></li>
<li><p>Result</p>
<div class="quarto-figure quarto-figure-center">
<figure class="figure">
<p><img src="../../images/lec05/2/Untitled 18.png" class="img-fluid figure-img"></p>
<figcaption>Reference. DoReFa-Net: Training Low Bitwidth Convolutional Neural Networks with Low Bitwidth Gradients [Zhou&nbsp;<em>et al.</em>, arXiv 2016]</figcaption>
</figure>
</div></li>
</ul>
</section>
<section id="replace-the-activation-function-parameterized-clipping-activation-function" class="level3">
<h3 class="anchored" data-anchor-id="replace-the-activation-function-parameterized-clipping-activation-function">5.3 Replace the Activation Function: Parameterized Clipping Activation Function</h3>
<ul>
<li><p>The most common activation function ReLU is unbounded. The dynamic range of inputs becomes problematic for low bit-width quantization due to very limited range and resolution.</p></li>
<li><p>ReLU is replaced with hard-coded bounded activation functions: ReLU6, ReLU1, etc</p></li>
<li><p>The clipping value per layer can be learned as well: PACT(Parametrized Clipping Activation Function)</p>
<p><img src="../../images/lec05/2/Untitled 19.png" class="img-fluid" alt="Reference. PACT: Parameterized Clipping Activation for Quantized Neural Networks [Choi&nbsp;et al., arXiv 2018]"> <span class="math display">\[
  y=PACT(x;\alpha) = 0.5(\lvert x \lvert - \lvert x -\alpha \lvert + \alpha ) = \begin{dcases}
  0, &amp; x \in [-\infty, 0) \\
  x, &amp; x \in [0, \alpha) \\
  \alpha, &amp; x \in [\alpha, +\infty)
  \end{dcases}
  \]</span></p>
<p>The upper clipping value of the activation function is a trainable. With STE, the gradient is computed as</p>
<p><span class="math display">\[
  \dfrac{\partial Q(y)}{\partial \alpha} = \dfrac{\partial Q(y)}{\partial y} \cdot \dfrac{\partial y}{\partial \alpha} = \begin{dcases}
  0 &amp; x \in (-\infty, \alpha)\\
  1 &amp; x \in [\alpha, +\infty)\\
  \end{dcases}
  \]</span></p>
<p><span class="math display">\[
  \rightarrow
  \dfrac{\partial L}{\partial \alpha} = \dfrac{\partial L}{\partial Q(y)} \cdot \dfrac{\partial Q(y)}{\partial \alpha} = \begin{dcases}
  0 &amp; x \in (-\infty, \alpha)\\
  \frac{\partial L}{\partial Q(y)} &amp; x \in [\alpha, +\infty)\\
  \end{dcases}
  \]</span></p>
<p>The larger <span class="math inline">\(\alpha\)</span>, the more the parameterized clipping function resembles a ReLU function</p>
<ul>
<li><strong>To avoid large quantization errors due to a wide dynamic range <span class="math inline">\([0, \alpha]\)</span>, L2-regularizer for <span class="math inline">\(\alpha\)</span></strong> is included in the training loss function.</li>
</ul></li>
<li><p>Result</p>
<div class="quarto-figure quarto-figure-center">
<figure class="figure">
<p><img src="../../images/lec05/2/Untitled 20.png" class="img-fluid figure-img"></p>
<figcaption>Reference. PACT: Parameterized Clipping Activation for Quantized Neural Networks [Choi&nbsp;<em>et al.</em>, arXiv 2018]</figcaption>
</figure>
</div></li>
</ul>
</section>
<section id="modify-the-neural-network-architecture" class="level3">
<h3 class="anchored" data-anchor-id="modify-the-neural-network-architecture">5.4 Modify the Neural Network Architecture</h3>
<ol type="1">
<li><p>Widen the neural network to compensate for the loss of information due to quantization</p>
<p>ex. Double the channels, reduce the quantization precision</p>
<div class="quarto-figure quarto-figure-center">
<figure class="figure">
<p><img src="../../images/lec05/2/Untitled 21.png" class="img-fluid figure-img"></p>
<figcaption>Reference. WRPN: Wide Reduced-Precision Networks [Mishra&nbsp;<em>et al.</em>, ICLR 2018]</figcaption>
</figure>
</div></li>
<li><p>Replace a single floating-point convolution with multiple binary convolutions.</p>
<ul>
<li>Towards Accurate Binary Convolutional Neural Network [Lin&nbsp;<em>et al.</em>, NeurIPS 2017]</li>
<li>Quantization [Neural Network Distiller]</li>
</ul></li>
</ol>
</section>
<section id="no-quantization-on-first-and-last-layer" class="level3">
<h3 class="anchored" data-anchor-id="no-quantization-on-first-and-last-layer">5.5 No Quantization on First and Last Layer</h3>
<ul>
<li>Because it is more <strong>sensitive</strong> to quantization and <strong>small portion</strong> of the overall computation</li>
<li>Quantizing these layers to 8-bit integer does not reduce accuracy</li>
</ul>
</section>
<section id="iterative-quantization-incremental-network-quantization" class="level3">
<h3 class="anchored" data-anchor-id="iterative-quantization-incremental-network-quantization">5.6 Iterative Quantization: Incremental Network Quantization</h3>
<ul>
<li>Reference. Incremental Network Quantization: Towards Lossless CNNs with Low-precision Weights [Zhou&nbsp;<em>et al.</em>, ICLR 2017]</li>
</ul>
<div class="quarto-figure quarto-figure-center">
<figure class="figure">
<p><img src="../../images/lec05/2/Untitled 22.png" class="img-fluid figure-img"></p>
<figcaption>Reference. Incremental Network Quantization: Towards Lossless CNNs with Low-precision Weights [Zhou&nbsp;<em>et al.</em>, ICLR 2017]</figcaption>
</figure>
</div>
<ul>
<li>Setting
<ul>
<li>Weight quantization only</li>
<li>Quantize weights to <span class="math inline">\(2^n\)</span> for faster computation (<strong>bit shift</strong> instead of multiply)</li>
</ul></li>
<li>Algorithm
<ul>
<li>Start from a pre-trained fp32 model</li>
<li>For the remaining fp32 weights
<ul>
<li>Partition into two disjoint groups(e.g., according to magnitude)</li>
<li>Quantize the first group (higher magnitude), and re-train the other group to recover accuracy</li>
</ul></li>
<li>Repeat until all the weights are quantized (a popular stride is {50%, 75%, 87.5%, 100%})</li>
</ul>
<div class="quarto-figure quarto-figure-center">
<figure class="figure">
<p><img src="../../images/lec05/2/Iterative-Quantization.gif" class="img-fluid figure-img"></p>
<figcaption>Reference. MIT-TinyML-lecture06-Quantization-2</figcaption>
</figure>
</div></li>
</ul>
</section>
</section>
<section id="mixed-precision-quantization" class="level2">
<h2 class="anchored" data-anchor-id="mixed-precision-quantization">6. Mixed-precision quantization</h2>
<p>ë§ˆì§€ë§‰ìœ¼ë¡œ ë ˆì´ì–´ë§ˆë‹¤ Quantization bitë¥¼ ë‹¤ë¥´ê²Œ ê°€ì ¸ê°€ë©´ ì–´ë–¨ì§€ì— ëŒ€í•´ì„œ ì´ì•¼ê¸°í•œë‹¤. í•˜ì§€ë§Œ ê²½ìš°ì˜ ìˆ˜ê°€ 8bit ë³´ë‹¤ ì‘ê±°ë‚˜ ê°™ê²Œ Quantizationì„ í•  ì‹œ, weightì™€ activationë¡œ ê²½ìš°ì˜ ìˆ˜ë¥¼ ê³ ë ¤ë¥¼ í•œë‹¤ë©´ Nê°œ ë ˆì´ì–´ì— ëŒ€í•´ì„œ <span class="math inline">\((8 \times 8)^N\)</span>ë¼ëŠ” ì–´ë§ˆì–´ë§ˆí•œ ê²½ìš°ì˜ ìˆ˜ê°€ ë‚˜ì˜¨ë‹¤. ê·¸ë¦¬ê³  ì´ì— ëŒ€í•´ì„œëŠ” ë‹¤ìŒ íŒŒíŠ¸ì— ë‚˜ê°ˆ Neural Architecture Search(NAS) ì—ì„œ ë‹¤ë£° ë“¯ ì‹¶ë‹¤.</p>
<section id="uniform-quantization" class="level3">
<h3 class="anchored" data-anchor-id="uniform-quantization">6.1 Uniform Quantization</h3>
<p><img src="../../images/lec05/2/Untitled 23.png" class="img-fluid"></p>
</section>
<section id="mixed-precision-quantization-1" class="level3">
<h3 class="anchored" data-anchor-id="mixed-precision-quantization-1">6.2 Mixed-precision Quantization</h3>
<p><img src="../../images/lec05/2/Untitled 24.png" class="img-fluid"></p>
</section>
<section id="huge-design-space-and-solution-design-automation" class="level3">
<h3 class="anchored" data-anchor-id="huge-design-space-and-solution-design-automation">6.3 Huge Design Space and Solution: Design Automation</h3>
<p><img src="../../images/lec05/2/Untitled 25.png" class="img-fluid"></p>
<ul>
<li><p>Design Space: Each of Choices(8x8=64) â†’ <span class="math inline">\(64^n\)</span></p>
<div class="quarto-figure quarto-figure-center">
<figure class="figure">
<p><img src="../../images/lec05/2/Untitled 26.png" class="img-fluid figure-img"></p>
<figcaption>Reference. HAQ: Hardware-Aware Automated Quantization with Mixed Precision [Wang&nbsp;<em>et al.</em>, CVPR 2019]</figcaption>
</figure>
</div></li>
<li><p>Result in Mixed-Precision Quantized MobileNetV1</p>
<div class="quarto-figure quarto-figure-center">
<figure class="figure">
<p><img src="../../images/lec05/2/Untitled 27.png" class="img-fluid figure-img"></p>
<figcaption>Reference. HAQ: Hardware-Aware Automated Quantization with Mixed Precision [Wang&nbsp;<em>et al.</em>, CVPR 2019]</figcaption>
</figure>
</div>
<ul>
<li>This paper compares with Model size, Latency and Energy</li>
</ul></li>
</ul>
<p>ê°€ì¥ ë§ˆì§€ë§‰ì— ì–¸ê¸‰í•˜ëŠ” Edgeì™€ í´ë¼ìš°ë“œì—ì„œëŠ” Convolution ë ˆì´ì–´ì˜ ì¢…ë¥˜ ì¤‘ ë”í•˜ê³  ëœ Quantizationí•˜ëŠ” ë ˆì´ì–´ê°€ ê°ê° depthwiseì™€ pointwiseë¡œ ë‹¤ë¥´ë‹¤ê³  ì´ì•¼ê¸°í•œë‹¤. ì´ ë‚´ìš©ì— ëŒ€í•´ì„œ ë” ìì„¸íˆ ì´í•´í•˜ê¸° ìœ„í•´ì„œëŠ” ì•„ë§ˆë„ NASë¡œ ë„˜ì–´ê°€ë´ì•¼ ì•Œ ìˆ˜ ìˆì§€ ì•Šì„ê¹Œ ì‹¶ë‹¤.</p>
<ul>
<li><p>Quantization Policy for Edge and Cloud</p>
<div class="quarto-figure quarto-figure-center">
<figure class="figure">
<p><img src="../../images/lec05/2/Untitled 28.png" class="img-fluid figure-img"></p>
<figcaption>Reference. HAQ: Hardware-Aware Automated Quantization with Mixed Precision [Wang&nbsp;<em>et al.</em>, CVPR 2019]</figcaption>
</figure>
</div></li>
</ul>
</section>
</section>
<section id="reference" class="level2">
<h2 class="anchored" data-anchor-id="reference">7. Reference</h2>
<ul>
<li><a href="https://efficientml.ai/">TinyML and Efficient Deep Learning Computing on MIT HAN LAB</a></li>
<li><a href="https://www.youtube.com/playlist?list=PL80kAHvQbh-ocildRaxjjBy6MR1ZsNCU7">Youtube for TinyML and Efficient Deep Learning Computing on MIT HAN LAB</a></li>
<li><a href="https://arxiv.org/abs/1510.00149">Deep Compression [Han&nbsp;et al., ICLR 2016]</a></li>
<li><a href="https://arxiv.org/pdf/1712.05877.pdf">Quantization and Training of Neural Networks for Efficient Integer-Arithmetic-Only Inference [Jacob&nbsp;et al., CVPR 2018]</a></li>
<li><a href="https://arxiv.org/pdf/2302.08007.pdf">With Shared Microexponents, A Little Shifting Goes a Long Way [Bita Rouhani et al.]</a></li>
<li><a href="https://arxiv.org/pdf/1906.04721.pdf">Data-Free Quantization Through Weight Equalization and Bias Correction [Markus et al., ICCV 2019]</a></li>
<li><a href="https://arxiv.org/pdf/1308.3432.pdf">Estimating or Propagating Gradients Through Stochastic Neurons for Conditional Computation [Bengio, arXiv 2013]</a></li>
<li><a href="https://arxiv.org/pdf/1602.02830.pdf">Binarized Neural Networks: Training Deep Neural Networks with Weights and Activations Constrained to +1 or&nbsp;âˆ’1. [Courbariaux&nbsp;et al., Arxiv 2016]</a></li>
<li><a href="https://arxiv.org/pdf/1603.05279.pdf">XNOR-Net: ImageNet Classification using Binary Convolutional Neural Networks [Rastegari&nbsp;et al., ECCV 2016]</a></li>
<li><a href="https://arxiv.org/pdf/1605.04711.pdf">Ternary Weight Networks [Li&nbsp;et al., Arxiv 2016]</a></li>
<li><a href="https://arxiv.org/pdf/1612.01064.pdf">Trained Ternary Quantization [Zhu&nbsp;et al., ICLR 2017]</a></li>
<li><a href="https://arxiv.org/pdf/1606.06160.pdf">DoReFa-Net: Training Low Bitwidth Convolutional Neural Networks with Low Bitwidth Gradients [Zhou&nbsp;et al., arXiv 2016]</a></li>
<li><a href="https://arxiv.org/pdf/1709.01134.pdf">WRPN: Wide Reduced-Precision Networks [Mishra&nbsp;et al., ICLR 2018]</a></li>
<li><a href="https://arxiv.org/pdf/1805.06085.pdf">PACT: Parameterized Clipping Activation for Quantized Neural Networks [Choi&nbsp;et al., arXiv 2018]</a></li>
<li><a href="https://arxiv.org/pdf/1811.08886.pdf">HAQ: Hardware-Aware Automated Quantization with Mixed Precision [Wang&nbsp;et al., CVPR 2019]</a></li>
</ul>


</section>

</main> <!-- /main -->
<script id="quarto-html-after-body" type="application/javascript">
window.document.addEventListener("DOMContentLoaded", function (event) {
  const toggleBodyColorMode = (bsSheetEl) => {
    const mode = bsSheetEl.getAttribute("data-mode");
    const bodyEl = window.document.querySelector("body");
    if (mode === "dark") {
      bodyEl.classList.add("quarto-dark");
      bodyEl.classList.remove("quarto-light");
    } else {
      bodyEl.classList.add("quarto-light");
      bodyEl.classList.remove("quarto-dark");
    }
  }
  const toggleBodyColorPrimary = () => {
    const bsSheetEl = window.document.querySelector("link#quarto-bootstrap");
    if (bsSheetEl) {
      toggleBodyColorMode(bsSheetEl);
    }
  }
  toggleBodyColorPrimary();  
  const icon = "î§‹";
  const anchorJS = new window.AnchorJS();
  anchorJS.options = {
    placement: 'right',
    icon: icon
  };
  anchorJS.add('.anchored');
  const isCodeAnnotation = (el) => {
    for (const clz of el.classList) {
      if (clz.startsWith('code-annotation-')) {                     
        return true;
      }
    }
    return false;
  }
  const clipboard = new window.ClipboardJS('.code-copy-button', {
    text: function(trigger) {
      const codeEl = trigger.previousElementSibling.cloneNode(true);
      for (const childEl of codeEl.children) {
        if (isCodeAnnotation(childEl)) {
          childEl.remove();
        }
      }
      return codeEl.innerText;
    }
  });
  clipboard.on('success', function(e) {
    // button target
    const button = e.trigger;
    // don't keep focus
    button.blur();
    // flash "checked"
    button.classList.add('code-copy-button-checked');
    var currentTitle = button.getAttribute("title");
    button.setAttribute("title", "Copied!");
    let tooltip;
    if (window.bootstrap) {
      button.setAttribute("data-bs-toggle", "tooltip");
      button.setAttribute("data-bs-placement", "left");
      button.setAttribute("data-bs-title", "Copied!");
      tooltip = new bootstrap.Tooltip(button, 
        { trigger: "manual", 
          customClass: "code-copy-button-tooltip",
          offset: [0, -8]});
      tooltip.show();    
    }
    setTimeout(function() {
      if (tooltip) {
        tooltip.hide();
        button.removeAttribute("data-bs-title");
        button.removeAttribute("data-bs-toggle");
        button.removeAttribute("data-bs-placement");
      }
      button.setAttribute("title", currentTitle);
      button.classList.remove('code-copy-button-checked');
    }, 1000);
    // clear code selection
    e.clearSelection();
  });
  function tippyHover(el, contentFn, onTriggerFn, onUntriggerFn) {
    const config = {
      allowHTML: true,
      maxWidth: 500,
      delay: 100,
      arrow: false,
      appendTo: function(el) {
          return el.parentElement;
      },
      interactive: true,
      interactiveBorder: 10,
      theme: 'quarto',
      placement: 'bottom-start',
    };
    if (contentFn) {
      config.content = contentFn;
    }
    if (onTriggerFn) {
      config.onTrigger = onTriggerFn;
    }
    if (onUntriggerFn) {
      config.onUntrigger = onUntriggerFn;
    }
    window.tippy(el, config); 
  }
  const noterefs = window.document.querySelectorAll('a[role="doc-noteref"]');
  for (var i=0; i<noterefs.length; i++) {
    const ref = noterefs[i];
    tippyHover(ref, function() {
      // use id or data attribute instead here
      let href = ref.getAttribute('data-footnote-href') || ref.getAttribute('href');
      try { href = new URL(href).hash; } catch {}
      const id = href.replace(/^#\/?/, "");
      const note = window.document.getElementById(id);
      return note.innerHTML;
    });
  }
  const xrefs = window.document.querySelectorAll('a.quarto-xref');
  const processXRef = (id, note) => {
    // Strip column container classes
    const stripColumnClz = (el) => {
      el.classList.remove("page-full", "page-columns");
      if (el.children) {
        for (const child of el.children) {
          stripColumnClz(child);
        }
      }
    }
    stripColumnClz(note)
    if (id === null || id.startsWith('sec-')) {
      // Special case sections, only their first couple elements
      const container = document.createElement("div");
      if (note.children && note.children.length > 2) {
        container.appendChild(note.children[0].cloneNode(true));
        for (let i = 1; i < note.children.length; i++) {
          const child = note.children[i];
          if (child.tagName === "P" && child.innerText === "") {
            continue;
          } else {
            container.appendChild(child.cloneNode(true));
            break;
          }
        }
        if (window.Quarto?.typesetMath) {
          window.Quarto.typesetMath(container);
        }
        return container.innerHTML
      } else {
        if (window.Quarto?.typesetMath) {
          window.Quarto.typesetMath(note);
        }
        return note.innerHTML;
      }
    } else {
      // Remove any anchor links if they are present
      const anchorLink = note.querySelector('a.anchorjs-link');
      if (anchorLink) {
        anchorLink.remove();
      }
      if (window.Quarto?.typesetMath) {
        window.Quarto.typesetMath(note);
      }
      // TODO in 1.5, we should make sure this works without a callout special case
      if (note.classList.contains("callout")) {
        return note.outerHTML;
      } else {
        return note.innerHTML;
      }
    }
  }
  for (var i=0; i<xrefs.length; i++) {
    const xref = xrefs[i];
    tippyHover(xref, undefined, function(instance) {
      instance.disable();
      let url = xref.getAttribute('href');
      let hash = undefined; 
      if (url.startsWith('#')) {
        hash = url;
      } else {
        try { hash = new URL(url).hash; } catch {}
      }
      if (hash) {
        const id = hash.replace(/^#\/?/, "");
        const note = window.document.getElementById(id);
        if (note !== null) {
          try {
            const html = processXRef(id, note.cloneNode(true));
            instance.setContent(html);
          } finally {
            instance.enable();
            instance.show();
          }
        } else {
          // See if we can fetch this
          fetch(url.split('#')[0])
          .then(res => res.text())
          .then(html => {
            const parser = new DOMParser();
            const htmlDoc = parser.parseFromString(html, "text/html");
            const note = htmlDoc.getElementById(id);
            if (note !== null) {
              const html = processXRef(id, note);
              instance.setContent(html);
            } 
          }).finally(() => {
            instance.enable();
            instance.show();
          });
        }
      } else {
        // See if we can fetch a full url (with no hash to target)
        // This is a special case and we should probably do some content thinning / targeting
        fetch(url)
        .then(res => res.text())
        .then(html => {
          const parser = new DOMParser();
          const htmlDoc = parser.parseFromString(html, "text/html");
          const note = htmlDoc.querySelector('main.content');
          if (note !== null) {
            // This should only happen for chapter cross references
            // (since there is no id in the URL)
            // remove the first header
            if (note.children.length > 0 && note.children[0].tagName === "HEADER") {
              note.children[0].remove();
            }
            const html = processXRef(null, note);
            instance.setContent(html);
          } 
        }).finally(() => {
          instance.enable();
          instance.show();
        });
      }
    }, function(instance) {
    });
  }
      let selectedAnnoteEl;
      const selectorForAnnotation = ( cell, annotation) => {
        let cellAttr = 'data-code-cell="' + cell + '"';
        let lineAttr = 'data-code-annotation="' +  annotation + '"';
        const selector = 'span[' + cellAttr + '][' + lineAttr + ']';
        return selector;
      }
      const selectCodeLines = (annoteEl) => {
        const doc = window.document;
        const targetCell = annoteEl.getAttribute("data-target-cell");
        const targetAnnotation = annoteEl.getAttribute("data-target-annotation");
        const annoteSpan = window.document.querySelector(selectorForAnnotation(targetCell, targetAnnotation));
        const lines = annoteSpan.getAttribute("data-code-lines").split(",");
        const lineIds = lines.map((line) => {
          return targetCell + "-" + line;
        })
        let top = null;
        let height = null;
        let parent = null;
        if (lineIds.length > 0) {
            //compute the position of the single el (top and bottom and make a div)
            const el = window.document.getElementById(lineIds[0]);
            top = el.offsetTop;
            height = el.offsetHeight;
            parent = el.parentElement.parentElement;
          if (lineIds.length > 1) {
            const lastEl = window.document.getElementById(lineIds[lineIds.length - 1]);
            const bottom = lastEl.offsetTop + lastEl.offsetHeight;
            height = bottom - top;
          }
          if (top !== null && height !== null && parent !== null) {
            // cook up a div (if necessary) and position it 
            let div = window.document.getElementById("code-annotation-line-highlight");
            if (div === null) {
              div = window.document.createElement("div");
              div.setAttribute("id", "code-annotation-line-highlight");
              div.style.position = 'absolute';
              parent.appendChild(div);
            }
            div.style.top = top - 2 + "px";
            div.style.height = height + 4 + "px";
            div.style.left = 0;
            let gutterDiv = window.document.getElementById("code-annotation-line-highlight-gutter");
            if (gutterDiv === null) {
              gutterDiv = window.document.createElement("div");
              gutterDiv.setAttribute("id", "code-annotation-line-highlight-gutter");
              gutterDiv.style.position = 'absolute';
              const codeCell = window.document.getElementById(targetCell);
              const gutter = codeCell.querySelector('.code-annotation-gutter');
              gutter.appendChild(gutterDiv);
            }
            gutterDiv.style.top = top - 2 + "px";
            gutterDiv.style.height = height + 4 + "px";
          }
          selectedAnnoteEl = annoteEl;
        }
      };
      const unselectCodeLines = () => {
        const elementsIds = ["code-annotation-line-highlight", "code-annotation-line-highlight-gutter"];
        elementsIds.forEach((elId) => {
          const div = window.document.getElementById(elId);
          if (div) {
            div.remove();
          }
        });
        selectedAnnoteEl = undefined;
      };
        // Handle positioning of the toggle
    window.addEventListener(
      "resize",
      throttle(() => {
        elRect = undefined;
        if (selectedAnnoteEl) {
          selectCodeLines(selectedAnnoteEl);
        }
      }, 10)
    );
    function throttle(fn, ms) {
    let throttle = false;
    let timer;
      return (...args) => {
        if(!throttle) { // first call gets through
            fn.apply(this, args);
            throttle = true;
        } else { // all the others get throttled
            if(timer) clearTimeout(timer); // cancel #2
            timer = setTimeout(() => {
              fn.apply(this, args);
              timer = throttle = false;
            }, ms);
        }
      };
    }
      // Attach click handler to the DT
      const annoteDls = window.document.querySelectorAll('dt[data-target-cell]');
      for (const annoteDlNode of annoteDls) {
        annoteDlNode.addEventListener('click', (event) => {
          const clickedEl = event.target;
          if (clickedEl !== selectedAnnoteEl) {
            unselectCodeLines();
            const activeEl = window.document.querySelector('dt[data-target-cell].code-annotation-active');
            if (activeEl) {
              activeEl.classList.remove('code-annotation-active');
            }
            selectCodeLines(clickedEl);
            clickedEl.classList.add('code-annotation-active');
          } else {
            // Unselect the line
            unselectCodeLines();
            clickedEl.classList.remove('code-annotation-active');
          }
        });
      }
  const findCites = (el) => {
    const parentEl = el.parentElement;
    if (parentEl) {
      const cites = parentEl.dataset.cites;
      if (cites) {
        return {
          el,
          cites: cites.split(' ')
        };
      } else {
        return findCites(el.parentElement)
      }
    } else {
      return undefined;
    }
  };
  var bibliorefs = window.document.querySelectorAll('a[role="doc-biblioref"]');
  for (var i=0; i<bibliorefs.length; i++) {
    const ref = bibliorefs[i];
    const citeInfo = findCites(ref);
    if (citeInfo) {
      tippyHover(citeInfo.el, function() {
        var popup = window.document.createElement('div');
        citeInfo.cites.forEach(function(cite) {
          var citeDiv = window.document.createElement('div');
          citeDiv.classList.add('hanging-indent');
          citeDiv.classList.add('csl-entry');
          var biblioDiv = window.document.getElementById('ref-' + cite);
          if (biblioDiv) {
            citeDiv.innerHTML = biblioDiv.innerHTML;
          }
          popup.appendChild(citeDiv);
        });
        return popup.innerHTML;
      });
    }
  }
});
</script>
<script src="https://giscus.app/client.js" data-repo="TINYML-KOR/blog" data-repo-id="R_kgDOLC9iGA" data-category="General" data-category-id="DIC_kwDOLC9iGM4Cc7eP" data-mapping="title" data-reactions-enabled="1" data-emit-metadata="0" data-input-position="top" data-theme="light" data-lang="en" crossorigin="anonymous" async="">
</script>
<input type="hidden" id="giscus-base-theme" value="light">
<input type="hidden" id="giscus-alt-theme" value="dark">
</div> <!-- /content -->




</body></html>