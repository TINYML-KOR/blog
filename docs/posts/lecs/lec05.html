<!DOCTYPE html>
<html xmlns="http://www.w3.org/1999/xhtml" lang="en" xml:lang="en"><head>

<meta charset="utf-8">
<meta name="generator" content="quarto-1.4.549">

<meta name="viewport" content="width=device-width, initial-scale=1.0, user-scalable=yes">

<meta name="author" content="Seunghyun Oh">
<meta name="dcterms.date" content="2024-03-05">
<meta name="description" content="Quantization">

<title>TinyML KOR - üßë‚Äçüè´ Lecture 5-6</title>
<style>
code{white-space: pre-wrap;}
span.smallcaps{font-variant: small-caps;}
div.columns{display: flex; gap: min(4vw, 1.5em);}
div.column{flex: auto; overflow-x: auto;}
div.hanging-indent{margin-left: 1.5em; text-indent: -1.5em;}
ul.task-list{list-style: none;}
ul.task-list li input[type="checkbox"] {
  width: 0.8em;
  margin: 0 0.8em 0.2em -1em; /* quarto-specific, see https://github.com/quarto-dev/quarto-cli/issues/4556 */ 
  vertical-align: middle;
}
</style>


<script src="../../site_libs/quarto-nav/quarto-nav.js"></script>
<script src="../../site_libs/quarto-nav/headroom.min.js"></script>
<script src="../../site_libs/clipboard/clipboard.min.js"></script>
<script src="../../site_libs/quarto-search/autocomplete.umd.js"></script>
<script src="../../site_libs/quarto-search/fuse.min.js"></script>
<script src="../../site_libs/quarto-search/quarto-search.js"></script>
<meta name="quarto:offset" content="../../">
<script src="../../site_libs/quarto-html/quarto.js"></script>
<script src="../../site_libs/quarto-html/popper.min.js"></script>
<script src="../../site_libs/quarto-html/tippy.umd.min.js"></script>
<script src="../../site_libs/quarto-html/anchor.min.js"></script>
<link href="../../site_libs/quarto-html/tippy.css" rel="stylesheet">
<link href="../../site_libs/quarto-html/quarto-syntax-highlighting.css" rel="stylesheet" id="quarto-text-highlighting-styles">
<script src="../../site_libs/bootstrap/bootstrap.min.js"></script>
<link href="../../site_libs/bootstrap/bootstrap-icons.css" rel="stylesheet">
<link href="../../site_libs/bootstrap/bootstrap.min.css" rel="stylesheet" id="quarto-bootstrap" data-mode="light">
<script id="quarto-search-options" type="application/json">{
  "location": "navbar",
  "copy-button": false,
  "collapse-after": 3,
  "panel-placement": "end",
  "type": "overlay",
  "limit": 50,
  "keyboard-shortcut": [
    "f",
    "/",
    "s"
  ],
  "show-item-context": false,
  "language": {
    "search-no-results-text": "No results",
    "search-matching-documents-text": "matching documents",
    "search-copy-link-title": "Copy link to search",
    "search-hide-matches-text": "Hide additional matches",
    "search-more-match-text": "more match in this document",
    "search-more-matches-text": "more matches in this document",
    "search-clear-button-title": "Clear",
    "search-text-placeholder": "",
    "search-detached-cancel-button-title": "Cancel",
    "search-submit-button-title": "Submit",
    "search-label": "Search"
  }
}</script>

  <script src="https://polyfill.io/v3/polyfill.min.js?features=es6"></script>
  <script src="https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-chtml-full.js" type="text/javascript"></script>

<script type="text/javascript">
const typesetMath = (el) => {
  if (window.MathJax) {
    // MathJax Typeset
    window.MathJax.typeset([el]);
  } else if (window.katex) {
    // KaTeX Render
    var mathElements = el.getElementsByClassName("math");
    var macros = [];
    for (var i = 0; i < mathElements.length; i++) {
      var texText = mathElements[i].firstChild;
      if (mathElements[i].tagName == "SPAN") {
        window.katex.render(texText.data, mathElements[i], {
          displayMode: mathElements[i].classList.contains('display'),
          throwOnError: false,
          macros: macros,
          fleqn: false
        });
      }
    }
  }
}
window.Quarto = {
  typesetMath
};
</script>

<link rel="stylesheet" href="../../styles.css">
</head>

<body class="nav-fixed">

<div id="quarto-search-results"></div>
  <header id="quarto-header" class="headroom fixed-top">
    <nav class="navbar navbar-expand-lg " data-bs-theme="dark">
      <div class="navbar-container container-fluid">
      <div class="navbar-brand-container mx-auto">
    <a class="navbar-brand" href="../../index.html">
    <span class="navbar-title">TinyML KOR</span>
    </a>
  </div>
            <div id="quarto-search" class="" title="Search"></div>
          <button class="navbar-toggler" type="button" data-bs-toggle="collapse" data-bs-target="#navbarCollapse" aria-controls="navbarCollapse" aria-expanded="false" aria-label="Toggle navigation" onclick="if (window.quartoToggleHeadroom) { window.quartoToggleHeadroom(); }">
  <span class="navbar-toggler-icon"></span>
</button>
          <div class="collapse navbar-collapse" id="navbarCollapse">
            <ul class="navbar-nav navbar-nav-scroll ms-auto">
  <li class="nav-item">
    <a class="nav-link" href="../../about.html"> 
<span class="menu-text">TinyML Study Group</span></a>
  </li>  
  <li class="nav-item compact">
    <a class="nav-link" href="https://github.com/"> <i class="bi bi-github" role="img">
</i> 
<span class="menu-text"></span></a>
  </li>  
</ul>
          </div> <!-- /navcollapse -->
          <div class="quarto-navbar-tools">
</div>
      </div> <!-- /container-fluid -->
    </nav>
</header>
<!-- content -->
<div id="quarto-content" class="quarto-container page-columns page-rows-contents page-layout-article page-navbar">
<!-- sidebar -->
<!-- margin-sidebar -->
    <div id="quarto-margin-sidebar" class="sidebar margin-sidebar">
        <nav id="TOC" role="doc-toc" class="toc-active">
    <h2 id="toc-title">On this page</h2>
   
  <ul>
  <li><a href="#common-network-quantization" id="toc-common-network-quantization" class="nav-link active" data-scroll-target="#common-network-quantization">1. Common Network Quantization</a>
  <ul class="collapse">
  <li><a href="#k-means-based-quantization" id="toc-k-means-based-quantization" class="nav-link" data-scroll-target="#k-means-based-quantization">1.1 K-Means-based Quantization</a></li>
  <li><a href="#linear-quantization" id="toc-linear-quantization" class="nav-link" data-scroll-target="#linear-quantization">1.2 Linear Quantization</a></li>
  <li><a href="#scale-and-zero-point" id="toc-scale-and-zero-point" class="nav-link" data-scroll-target="#scale-and-zero-point">1.3 Scale and Zero point</a></li>
  <li><a href="#quantized-matrix-multiplication" id="toc-quantized-matrix-multiplication" class="nav-link" data-scroll-target="#quantized-matrix-multiplication">1.4 Quantized Matrix Multiplication</a></li>
  <li><a href="#symmetric-linear-quantization" id="toc-symmetric-linear-quantization" class="nav-link" data-scroll-target="#symmetric-linear-quantization">1.5 Symmetric Linear Quantization</a></li>
  <li><a href="#linear-quantization-examples" id="toc-linear-quantization-examples" class="nav-link" data-scroll-target="#linear-quantization-examples">1.6 Linear Quantization examples</a></li>
  </ul></li>
  <li><a href="#post-training-quantization-ptq" id="toc-post-training-quantization-ptq" class="nav-link" data-scroll-target="#post-training-quantization-ptq">2. Post-training Quantization (PTQ)</a>
  <ul class="collapse">
  <li><a href="#weight-quantization" id="toc-weight-quantization" class="nav-link" data-scroll-target="#weight-quantization">2.1 Weight quantization</a></li>
  <li><a href="#post-training-int8-linear-quantization-result" id="toc-post-training-int8-linear-quantization-result" class="nav-link" data-scroll-target="#post-training-int8-linear-quantization-result">2.4 Post-Training INT8 Linear Quantization Result</a></li>
  </ul></li>
  <li><a href="#quantization-aware-trainingqat" id="toc-quantization-aware-trainingqat" class="nav-link" data-scroll-target="#quantization-aware-trainingqat">3. Quantization-Aware Training(QAT)</a>
  <ul class="collapse">
  <li><a href="#quantization-aware-training" id="toc-quantization-aware-training" class="nav-link" data-scroll-target="#quantization-aware-training">3.1 Quantization-Aware Training</a></li>
  <li><a href="#straight-through-estimatorste" id="toc-straight-through-estimatorste" class="nav-link" data-scroll-target="#straight-through-estimatorste">3.2 Straight-Through Estimator(STE)</a></li>
  </ul></li>
  <li><a href="#binary-and-ternary-quantization" id="toc-binary-and-ternary-quantization" class="nav-link" data-scroll-target="#binary-and-ternary-quantization">4. Binary and Ternary Quantization</a>
  <ul class="collapse">
  <li><a href="#binarization-deterministic-binarization" id="toc-binarization-deterministic-binarization" class="nav-link" data-scroll-target="#binarization-deterministic-binarization">4.1 Binarization: <strong>Deterministic Binarization</strong></a></li>
  <li><a href="#binarization-stochastic-binarization" id="toc-binarization-stochastic-binarization" class="nav-link" data-scroll-target="#binarization-stochastic-binarization">4.2 Binarization: <strong>Stochastic Binarization</strong></a></li>
  <li><a href="#binarization-use-scale" id="toc-binarization-use-scale" class="nav-link" data-scroll-target="#binarization-use-scale">4.3 Binarization: Use Scale</a></li>
  <li><a href="#binarization-activation" id="toc-binarization-activation" class="nav-link" data-scroll-target="#binarization-activation">4.4 Binarization: Activation</a></li>
  <li><a href="#ternary-weight-networkstwn" id="toc-ternary-weight-networkstwn" class="nav-link" data-scroll-target="#ternary-weight-networkstwn">4.5 <strong>Ternary Weight Networks(TWN)</strong></a></li>
  <li><a href="#accuracy-degradation" id="toc-accuracy-degradation" class="nav-link" data-scroll-target="#accuracy-degradation">4.7 Accuracy Degradation</a></li>
  </ul></li>
  <li><a href="#low-bit-width-quantization" id="toc-low-bit-width-quantization" class="nav-link" data-scroll-target="#low-bit-width-quantization">5. Low Bit-Width Quantization</a>
  <ul class="collapse">
  <li><a href="#train-binarized-neural-networks-from-scratch" id="toc-train-binarized-neural-networks-from-scratch" class="nav-link" data-scroll-target="#train-binarized-neural-networks-from-scratch">5.1 Train Binarized Neural Networks From Scratch</a></li>
  <li><a href="#quantization-aware-training-dorefa-net-with-low-bit-width-gradients" id="toc-quantization-aware-training-dorefa-net-with-low-bit-width-gradients" class="nav-link" data-scroll-target="#quantization-aware-training-dorefa-net-with-low-bit-width-gradients">5.2 Quantization-Aware Training: DoReFa-Net With Low Bit-Width Gradients</a></li>
  <li><a href="#replace-the-activation-function-parameterized-clipping-activation-function" id="toc-replace-the-activation-function-parameterized-clipping-activation-function" class="nav-link" data-scroll-target="#replace-the-activation-function-parameterized-clipping-activation-function">5.3 Replace the Activation Function: Parameterized Clipping Activation Function</a></li>
  <li><a href="#modify-the-neural-network-architecture" id="toc-modify-the-neural-network-architecture" class="nav-link" data-scroll-target="#modify-the-neural-network-architecture">5.4 Modify the Neural Network Architecture</a></li>
  <li><a href="#no-quantization-on-first-and-last-layer" id="toc-no-quantization-on-first-and-last-layer" class="nav-link" data-scroll-target="#no-quantization-on-first-and-last-layer">5.5 No Quantization on First and Last Layer</a></li>
  <li><a href="#iterative-quantization-incremental-network-quantization" id="toc-iterative-quantization-incremental-network-quantization" class="nav-link" data-scroll-target="#iterative-quantization-incremental-network-quantization">5.6 Iterative Quantization: Incremental Network Quantization</a></li>
  </ul></li>
  <li><a href="#mixed-precision-quantization" id="toc-mixed-precision-quantization" class="nav-link" data-scroll-target="#mixed-precision-quantization">6. Mixed-precision quantization</a>
  <ul class="collapse">
  <li><a href="#uniform-quantization" id="toc-uniform-quantization" class="nav-link" data-scroll-target="#uniform-quantization">6.1 Uniform Quantization</a></li>
  <li><a href="#mixed-precision-quantization-1" id="toc-mixed-precision-quantization-1" class="nav-link" data-scroll-target="#mixed-precision-quantization-1">6.2 Mixed-precision Quantization</a></li>
  <li><a href="#huge-design-space-and-solution-design-automation" id="toc-huge-design-space-and-solution-design-automation" class="nav-link" data-scroll-target="#huge-design-space-and-solution-design-automation">6.3 Huge Design Space and Solution: Design Automation</a></li>
  </ul></li>
  <li><a href="#reference" id="toc-reference" class="nav-link" data-scroll-target="#reference">7. Reference</a></li>
  </ul>
</nav>
    </div>
<!-- main -->
<main class="content" id="quarto-document-content">

<header id="title-block-header" class="quarto-title-block default">
<div class="quarto-title">
<h1 class="title">üßë‚Äçüè´ Lecture 5-6</h1>
  <div class="quarto-categories">
    <div class="quarto-category">lecture</div>
    <div class="quarto-category">quantization</div>
  </div>
  </div>

<div>
  <div class="description">
    Quantization
  </div>
</div>


<div class="quarto-title-meta">

    <div>
    <div class="quarto-title-meta-heading">Author</div>
    <div class="quarto-title-meta-contents">
             <p>Seunghyun Oh </p>
          </div>
  </div>
    
    <div>
    <div class="quarto-title-meta-heading">Published</div>
    <div class="quarto-title-meta-contents">
      <p class="date">March 5, 2024</p>
    </div>
  </div>
  
    
  </div>
  


</header>


<style>
p {
  line-height: 2; /* Adjust the line-height property as needed */
  text-align: justify;
  text-justify: inter-word; /* Adjust the justification mode as needed */
}

.image-container {
  text-align: center; /* Center-align the contents horizontally */
}

.centered-text {
  text-align: center; /* Center-align the text within the container */
}
</style>
<p>Ïù¥Î≤à Í∏ÄÏóêÏÑúÎäî MIT HAN LABÏóêÏÑú Í∞ïÏùòÌïòÎäî <a href="https://www.youtube.com/playlist?list=PL80kAHvQbh-ocildRaxjjBy6MR1ZsNCU7">TinyML and Efficient Deep Learning Computing</a>Ïóê ÎÇòÏò§Îäî Quantization Î∞©Î≤ïÏùÑ ÏÜåÍ∞úÌïòÎ†§ ÌïúÎã§. Quantization(ÏñëÏûêÌôî) Ïã†Ìò∏ÏôÄ Ïù¥ÎØ∏ÏßÄÏóêÏÑú ÏïÑÎÇ†Î°úÍ∑∏Î•º ÎîîÏßÄÌÑ∏Î°ú Î≥ÄÌôòÌïòÎäî Í≥ºÏ†ïÏóêÏÑú ÏÇ¨Ïö©ÌïòÎäî Í∞úÎÖêÏù¥Îã§. ÏïÑÎûò Í∑∏Î¶ºÍ≥º Í∞ôÏù¥ Ïó∞ÏÜçÏ†ÅÏù∏ ÏÑºÏÑúÎ°ú Î∂ÄÌÑ∞ Îì§Ïñ¥Ïò§Îäî ÏïÑÎÇ†Î°úÍ∑∏ Îç∞Ïù¥ÌÑ∞ ÎÇò Ïù¥ÎØ∏ÏßÄÎ•º ÌëúÌòÑÌïòÍ∏∞ ÏúÑÌï¥ Îã®ÏúÑ ÏãúÍ∞ÑÏóê ÎåÄÌï¥ÏÑú Îç∞Ïù¥ÌÑ∞Î•º ÏÉòÌîåÎßÅÌïòÏó¨ Îç∞Ïù¥ÌÑ∞Î•º ÏàòÏßëÌïúÎã§.</p>
<div class="quarto-figure quarto-figure-center">
<figure class="figure">
<p><img src="../../images/lec05/1/intro.png" class="img-fluid figure-img"></p>
<figcaption>Reference. MIT-TinyML-lecture5-Quantization-1</figcaption>
</figure>
</div>
<p>ÎîîÏßÄÌÑ∏Î°ú Îç∞Ïù¥ÌÑ∞Î•º Î≥ÄÌôòÌïòÍ∏∞ ÏúÑÌï¥ Îç∞Ïù¥ÌÑ∞ ÌÉÄÏûÖÏùÑ Ï†ïÌïòÎ©¥ÏÑú Ïù¥Î•º ÌïòÎÇòÏî© ÏñëÏûêÌôîÌïúÎã§. ÏñëÏàòÏôÄ ÏùåÏàòÎ•º ÌëúÌòÑÌïòÍ∏∞ ÏúÑÌï¥ Unsigned Integer ÏóêÏÑú Signed Integer, SignedÏóêÏÑúÎèÑ Sign-Magnitude Î∞©ÏãùÍ≥º Two‚Äôs ComplementÎ∞©ÏãùÏúºÎ°ú, Í∑∏Î¶¨Í≥† Îçî ÎßéÏùÄ ÏÜåÏà´Ï†ê ÏûêÎ¶¨Î•º ÌëúÌòÑÌïòÍ∏∞ ÏúÑÌï¥ Fixed-pointÏóêÏÑú Floating pointÎ°ú Îç∞Ïù¥ÌÑ∞ ÌÉÄÏûÖÏóêÏÑú ÏàòÏùò Î≤îÏ£ºÎ•º ÌôïÏû•ÏãúÌÇ®Îã§. Ï∞∏Í≥†Î°ú DeviceÏùò ComputationalityÏôÄ ML Î™®Îç∏Ïùò ÏÑ±Îä•ÏßÄÌëúÏ§ë ÌïòÎÇòÏù∏ FLOPÏù¥ Î∞îÎ°ú floating point operations per secondÏù¥Îã§.</p>
<p><img src="../../images/lec05/1/comp-bitwidth-fix-float.png" class="img-fluid"></p>
<div class="quarto-figure quarto-figure-center">
<figure class="figure">
<p><img src="../../images/lec05/1/comp-memory-fix-float.png" class="img-fluid figure-img"></p>
<figcaption>Reference. MIT-TinyML-lecture5-Quantization-1</figcaption>
</figure>
</div>
<p><a href="https://ooshyun.github.io/2023/02/24/Fixed-point-vs-Floating-point.html">Ïù¥ Í∏Ä</a>ÏóêÏÑú floating pointÎ•º Ïù¥Ìï¥ÌïòÎ©¥, fixed pointÎ•º ÏÇ¨Ïö©ÌïòÎäî Í≤ÉÏù¥ Îß§Î™®Î¶¨ÏóêÏÑú, Í∑∏Î¶¨Í≥† Ïó∞ÏÇ∞ÏóêÏÑú Îçî Ìö®Ïú®Ï†ÅÏùº Í≤ÉÏù¥ÎùºÍ≥† ÏòàÏÉÅÌï¥Î≥º Ïûà Ïàò ÏûàÎã§. MLÎ™®Îç∏ÏùÑ ÌÅ¥ÎùºÏö∞Îìú ÏÑúÎ≤ÑÏóêÏÑú ÎèåÎ¶¥ ÎïåÎäî ÌÅ¨Í≤å Î¨∏Ï†úÎêòÏßÄ ÏïäÏïòÏßÄÎßå ÏïÑÎûò Îëê Í∞ÄÏßÄ ÌëúÎ•º Î≥¥Î©¥ ÏóêÎÑàÏßÄÏÜåÎ™®, Ï¶â Î∞∞ÌÑ∞Î¶¨ Ìö®Ïú®ÏóêÏÑú ÌÅ¨Í≤å Ï∞®Ïù¥Í∞Ä Î≥¥Ïù∏Îã§. Í∑∏Î†áÍ∏∞ ÎïåÎ¨∏Ïóê Î™®Îç∏ÏóêÏÑú Floating pointÎ•º fixed pointÎ°ú Îçî ÎßéÏù¥ Î∞îÍæ∏Î†§Í≥† ÌïòÎäîÎç∞ Ïù¥ Î∞©Î≤ïÏúºÎ°ú ÎÇòÏò® Í≤ÉÏù¥ Î∞îÎ°ú QuatizationÏù¥Îã§.</p>
<p>Ïù¥Î≤à Í∏ÄÏóêÏÑúÎäî Quntization Ï§ëÏóêÏÑú Quantization Î∞©Î≤ïÍ≥º Í∑∏ Ï§ë LinearÌïú Î∞©Î≤ïÏóê ÎåÄÌï¥ Îçî ÏûêÏÑ∏ÌïòÍ≤å, Í∑∏Î¶¨Í≥† Post-training QuantizationÍπåÏßÄ Îã§Î£®Í≥†, Îã§Ïùå Í∏ÄÏóêÏÑúÎäî Quantization-Aware Training, Binary/Tenary Quantization, Mixed Precision QuantizationÍπåÏßÄ Îã§Î£®Î†§Í≥† ÌïúÎã§.</p>
<section id="common-network-quantization" class="level2">
<h2 class="anchored" data-anchor-id="common-network-quantization">1. Common Network Quantization</h2>
<p>ÏïûÏÑúÏÑú ÏÜåÍ∞úÌïú Í≤ÉÏ≤òÎüº Neural NetoworkÎ•º ÏúÑÌïú QuantizationÏùÄ Îã§ÏùåÍ≥º Í∞ôÏù¥ ÎÇòÎàå Ïàò ÏûàÎã§. Quantization Î∞©Î≤ïÏùÑ ÌïòÎÇòÏî© ÏïåÏïÑÎ≥¥Ïûê.</p>
<div class="quarto-figure quarto-figure-center">
<figure class="figure">
<p><img src="../../images/lec05/1/intro.png" class="img-fluid figure-img"></p>
<figcaption>Reference. MIT-TinyML-lecture5-Quantization-1</figcaption>
</figure>
</div>
<p>
<img src="../../images/lec05/1/quantization-method.png" width="500" height="300" class="projects__article__img__center">
</p><p align="center">
<em class="projects__img__caption"> Reference. MIT-TinyML-lecture5-Quantization-1 in https://efficientml.ai </em>
</p>
<p></p>
<section id="k-means-based-quantization" class="level3">
<h3 class="anchored" data-anchor-id="k-means-based-quantization">1.1 K-Means-based Quantization</h3>
<p>Í∑∏ Ï§ë Ï≤´ Î≤àÏß∏Î°ú K-means-based QuantizationÏù¥ ÏûàÎã§. <a href="https://arxiv.org/abs/1510.00149">Deep Compression [Han&nbsp;et al., ICLR 2016]</a> ÎÖºÎ¨∏Ïóê ÏÜåÍ∞úÌñàÎã§Îäî Ïù¥ Î∞©Î≤ïÏùÄ Ï§ëÏã¨Í∞íÏùÑ Í∏∞Ï§ÄÏúºÎ°ú clusteringÏùÑ ÌïòÎäî Î∞©Î≤ïÏù¥Îã§. ÏòàÏ†úÎ•º Î¥êÎ≥¥Ïûê.</p>
<div class="quarto-figure quarto-figure-center">
<figure class="figure">
<p><img src="../../images/lec05/1/k-mean-quantization.png" class="img-fluid figure-img"></p>
<figcaption>Reference. MIT-TinyML-lecture5-Quantization-1</figcaption>
</figure>
</div>
<p>ÏúÑ ÏòàÏ†úÎäî weightÎ•º codebookÏóêÏÑú -1, 0, 1.5, 2Î°ú ÎÇòÎà† Í∞ÅÍ∞ÅÏóê ÎßûÎäî Ïù∏Îç±Ïä§Î°ú ÌëúÍ∏∞ÌïúÎã§. Ïù¥Î†áÍ≤å Ïó∞ÏÇ∞ÏùÑ ÌïòÎ©¥ Í∏∞Ï°¥Ïóê 64bytesÎ•º ÏÇ¨Ïö©ÌñàÎçò weightÍ∞Ä 20bytesÎ°ú Ï§ÑÏñ¥Îì†Îã§. codebookÏúºÎ°ú ÏòàÏ†úÎäî 2bitÎ°ú ÎÇòÎà¥ÏßÄÎßå, Ïù¥Î•º N-bitÎßåÌÅº Ï§ÑÏù∏Îã§Î©¥ Ïö∞Î¶¨Îäî Ï¥ù 32/NÎ∞∞Ïùò Î©îÎ™®Î¶¨Î•º Ï§ÑÏùº Ïàò ÏûàÎã§. ÌïòÏßÄÎßå Ïù¥ Í≥ºÏ†ïÏóêÏÑú quantizatio error, Ï¶â quantizationÏùÑ ÌïòÍ∏∞ Ï†ÑÍ≥º Ìïú ÌõÑÏóê Ïò§Ï∞®Í∞Ä ÏÉùÍ∏∞Îäî Í≤ÉÏùÑ ÏúÑ ÏòàÏ†úÏóêÏÑú Î≥º Ïàò ÏûàÎã§. Î©îÎ™®Î¶¨ ÏÇ¨Ïö©ÎüâÏùÑ Ï§ÑÏù¥Îäî Í≤ÉÎèÑ Ï¢ãÏßÄÎßå, Ïù¥ ÎïåÎ¨∏Ïóê ÏÑ±Îä•Ïóê Ïò§Ï∞®Í∞Ä ÏÉùÍ∏∞ÏßÄ ÏïäÍ≤å ÌïòÍ∏∞ÏúÑÌï¥ Ïù¥ Ïò§Ï∞®Î•º Ï§ÑÏù¥Îäî Í≤É ÎòêÌïú Ï§ëÏöîÌïòÎã§.</p>
<div class="quarto-figure quarto-figure-center">
<figure class="figure">
<p><img src="../../images/lec05/1/k-mean-quantization-finetuning.png" class="img-fluid figure-img"></p>
<figcaption>Reference. MIT-TinyML-lecture5-Quantization-1</figcaption>
</figure>
</div>
<p>Ïù¥Î•º Î≥¥ÏôÑÌïòÍ∏∞ ÏúÑÌï¥ QuantizedÌïú WeightÎ•º ÏúÑÏóê Í∑∏Î¶ºÏ≤òÎüº Fine-tuningÌïòÍ∏∞ÎèÑ ÌïúÎã§. centroidÎ•º fine-tuningÌïúÎã§Í≥† ÏÉùÍ∞ÅÌïòÎ©¥ ÎêòÎäîÎç∞, Í∞Å centroidÏóêÏÑú ÏÉùÍ∏∞Îäî Ïò§Ï∞®Î•º ÌèâÍ∑†ÎÇ¥ tuningÌïòÎäî Î∞©Î≤ïÏù¥Îã§. Ïù¥ Î∞©Î≤ïÏùÑ Ï†úÏïàÌïú <a href="https://arxiv.org/abs/1510.00149">ÎÖºÎ¨∏</a> ÏóêÏÑúÎäî Convolution Î†àÏù¥Ïñ¥ÏóêÏÑúÎäî 4bitÍπåÏßÄ centroidÎ•º Í∞ÄÏ°åÏùÑ Îïå, Full-Connected layerÏóêÏÑúÎäî 2 bitÍπåÏßÄ centroidÎ•º Í∞ÄÏ°åÏùÑ Îïå ÏÑ±Îä•Ïóê ÌïòÎùΩÏù¥ ÏóÜÎã§Í≥† ÎßêÌïòÍ≥† ÏûàÏóàÎã§.</p>
<div class="quarto-figure quarto-figure-center">
<figure class="figure">
<p><img src="../../images/lec05/1/continuous-data.png" class="img-fluid figure-img"></p>
<figcaption>Reference. Deep Compression [Han et al., ICLR 2016]</figcaption>
</figure>
</div>
<p>Ïù¥Î†áÍ≤å Quantization Îêú WeightÎäî ÏúÑÏ≤òÎüº Ïó∞ÏÜçÏ†ÅÏù∏ Í∞íÏóêÏÑú ÏïÑÎûòÏ≤òÎüº DiscreteÌïú Í∞íÏúºÎ°ú Î∞îÎÄêÎã§.</p>
<div class="quarto-figure quarto-figure-center">
<figure class="figure">
<p><img src="../../images/lec05/1/discrete-data.png" class="img-fluid figure-img"></p>
<figcaption>Reference. Deep Compression [Han et al., ICLR 2016]</figcaption>
</figure>
</div>
<p>ÎÖºÎ¨∏ÏùÄ Ïù¥Î†áÍ≤å QuantizationÌïú weightÎ•º Ìïú Î≤à Îçî Huffman codingÎ•º Ïù¥Ïö©Ìï¥ ÏµúÏ†ÅÌôîÏãúÌÇ®Îã§. ÏßßÍ≤å ÏÑ§Î™ÖÌïòÏûêÎ©¥, ÎπàÎèÑÏàòÍ∞Ä ÎÜíÏùÄ Î¨∏ÏûêÎäî ÏßßÏùÄ Ïù¥ÏßÑÏΩîÎìúÎ•º, ÎπàÎèÑ ÏàòÍ∞Ä ÎÇÆÏùÄ Î¨∏ÏûêÏóêÎäî Í∏¥ Ïù¥ÏßÑÏΩîÎìúÎ•º Ïì∞Îäî Î∞©Î≤ïÏù¥Îã§. ÏïïÏ∂ï Í≤∞Í≥ºÎ°ú GeneralÌïú Î™®Îç∏Í≥º ÏïïÏ∂ï ÎπÑÏú®Ïù¥ ÍΩ§ ÌÅ∞ SqueezeNetÏùÑ ÏòàÎ°ú Îì†Îã§. ÏûêÏÑ∏Ìïú ÎÇ¥Ïö©ÏùÄ ÎÖºÎ¨∏ÏùÑ Ï∞∏Í≥†ÌïòÎäî Í±∏Î°ú.</p>
<div class="quarto-figure quarto-figure-center">
<figure class="figure">
<p><img src="../../images/lec05/1/deep-compression.png" class="img-fluid figure-img"></p>
<figcaption>Reference. Deep Compression [Han et al., ICLR 2016]</figcaption>
</figure>
</div>
<div class="quarto-figure quarto-figure-center">
<figure class="figure">
<p><img src="../../images/lec05/1/deep-compression-result.png" class="img-fluid figure-img"></p>
<figcaption>Reference. Deep Compression [Han et al., ICLR 2016]</figcaption>
</figure>
</div>
<p>inferenceÎ•º ÏúÑÌï¥ weightÎ•º DecodingÌïòÎäî Í≥ºÏ†ïÏùÄ inferenceÍ≥ºÏ†ïÏóêÏÑú Ï†ÄÏû•Ìïú clusterÏùò Ïù∏Îç±Ïä§Î•º Ïù¥Ïö©Ìï¥ codebookÏóêÏÑú Ìï¥ÎãπÌïòÎäî Í∞íÏùÑ Ï∞æÏïÑÎÇ¥Îäî Í≤ÉÏù¥Îã§. Ïù¥ Î∞©Î≤ïÏùÄ Ï†ÄÏû• Í≥µÍ∞ÑÏùÑ Ï§ÑÏùº ÏàòÎäî ÏûàÏßÄÎßå, floating point ComputationÏù¥ÎÇò Î©îÎ™®Î¶¨ Ï†ëÍ∑ºÌïòÎäî Î∞©ÏãùÏúºÎ°ú centroidÎ•º Ïì∞Îäî ÌïúÍ≥ÑÍ∞Ä ÏûàÏùÑ Ïàò Î∞ñÏóê ÏóÜÎã§.</p>
<div class="quarto-figure quarto-figure-center">
<figure class="figure">
<p><img src="../../images/lec05/1/intro.png" class="img-fluid figure-img"></p>
<figcaption>Reference. MIT-TinyML-lecture5-Quantization-1</figcaption>
</figure>
</div>
<p>
<img src="../../images/lec05/1/decoding-deep-compression.png" width="500" height="200" class="projects__article__img__center">
</p><p align="center">
<em class="projects__img__caption"> Reference. Deep Compression [Han et al., ICLR 2016] </em>
</p>
<p></p>
</section>
<section id="linear-quantization" class="level3">
<h3 class="anchored" data-anchor-id="linear-quantization">1.2 Linear Quantization</h3>
<p>Îëê Î≤àÏß∏ Î∞©Î≤ïÏùÄ Linear QuatizationÏù¥Îã§. floating-pointÏù∏ weightÎ•º N-bitÏùò Ï†ïÏàòÎ°ú affine mappingÏùÑ ÏãúÌÇ§Îäî Î∞©Î≤ïÏù¥Îã§. Í∞ÑÎã®ÌïòÍ≤å ÏãùÏúºÎ°ú Î≥¥Îäî Í≤å Îçî Ïù¥Ìï¥Í∞Ä ÏâΩÎã§.</p>
<div class="quarto-figure quarto-figure-center">
<figure class="figure">
<p><img src="../../images/lec05/1/linear-quantization-eq.png" class="img-fluid figure-img"></p>
<figcaption>Reference. MIT-TinyML-lecture5-Quantization-1</figcaption>
</figure>
</div>
<p>Ïó¨Í∏∞ÏÑú S(Scale of Linear Quantization)ÏôÄ Z(Zero point of Linear Quantization)Í∞Ä ÏûàÎäîÎç∞ Ïù¥ ÎëòÏù¥ quantization parameter Î°úÏç® tuningÏùÑ Ìï† Ïàò ÏûàÎäî Í∞íÏù∏ Í≤ÉÏù¥Îã§.</p>
<div class="quarto-figure quarto-figure-center">
<figure class="figure">
<p><img src="../../images/lec05/1/linear-quantization-img.png" class="img-fluid figure-img"></p>
<figcaption>Reference. MIT-TinyML-lecture5-Quantization-1</figcaption>
</figure>
</div>
</section>
<section id="scale-and-zero-point" class="level3">
<h3 class="anchored" data-anchor-id="scale-and-zero-point">1.3 Scale and Zero point</h3>
<div class="quarto-figure quarto-figure-center">
<figure class="figure">
<p><img src="../../images/lec05/1/scale-zero-point.png" class="img-fluid figure-img"></p>
<figcaption>Reference. MIT-TinyML-lecture5-Quantization-1</figcaption>
</figure>
</div>
<p>Ïù¥ ScaleÍ≥º Zero point Îëê ÌååÎùºÎØ∏ÌÑ∞Î•º Ïù¥Ïö©Ìï¥ÏÑú affine mappingÏùÄ ÏúÑ Í∑∏Î¶ºÍ≥º Í∞ôÎã§. Bit Ïàò(Bit Width)Í∞Ä ÎÇÆÏïÑÏßÄÎ©¥ ÎÇÆÏïÑÏßà ÏàòÎ°ù, floating pointÏóêÏÑú ÌëúÌòÑÌï† ÏûàÎäî Ïàò ÎòêÌïú Ï§ÑÏñ¥Îì§ Í≤ÉÏù¥Îã§. Í∑∏Î†áÎã§Î©¥ ScaleÏôÄ Zero pointÎäî Í∞ÅÍ∞Å Ïñ¥ÎñªÍ≤å Í≥ÑÏÇ∞Ìï†Íπå?</p>
<p>Ïö∞ÏÑ† floating-point Ïù∏ Ïà´ÏûêÏùò Î≤îÏúÑ Ï§ë ÏµúÎåÄÍ∞íÍ≥º ÏµúÏÜüÍ∞íÏóê ÎßûÍ≤å Îëê ÏãùÏùÑ ÏÑ∏Ïö∞Í≥† Ïù¥Î•º Ïó∞Î¶ΩÎ∞©Ï†ïÏãùÏúºÎ°ú ScaleÍ≥º Zero pointÏùÑ Íµ¨Ìï† Ïàò ÏûàÎã§.</p>
<ul>
<li><p>Scale point <span class="math display">\[
  r_{max} = S(q_{max}-Z)
  \]</span> <span class="math display">\[
  r_{min} = S(q_{min}-Z)
  \]</span></p>
<p><span class="math display">\[
  r_{max} - r_{min} = S(q_{max} - q_{min})
  \]</span></p>
<p><span class="math display">\[
  S = \dfrac{r_{max}-r_{min}}{q_{max}-q_{min}}
  \]</span></p></li>
<li><p>Zero point <span class="math display">\[
  r_{min} = S(q_{min}-Z)
  \]</span></p>
<p><span class="math display">\[
  Z=q_{min}-\dfrac{r_{min}}{S}
  \]</span></p>
<p><span class="math display">\[
  Z = round\Big(q_{min}-\dfrac{r_{min}}{S}\Big)
  \]</span></p></li>
</ul>
<p>ÏòàÎ•º Îì§Ïñ¥, ÏïÑÎûòÏôÄ Í∞ôÏùÄ ÏòàÏ†úÏóêÏÑú <span class="math inline">\(r_{max}\)</span> Îäî<span class="math inline">\(2.12\)</span> Ïù¥Í≥† <span class="math inline">\(r_{min}\)</span> ÏùÄ <span class="math inline">\(-1.08\)</span> Î°ú ScaleÏùÑ Í≥ÑÏÇ∞ÌïòÎ©¥ ÏïÑÎûò Í∑∏Î¶ºÏ≤òÎüº ÎêúÎã§. Zero pointÎäî <span class="math inline">\(-1\)</span> Î°ú Í≥ÑÏÇ∞Ìï† Ïàò ÏûàÎã§.</p>
<p><img src="../../images/lec05/1/scale-zero-point-ex1.png" class="img-fluid"></p>
<div class="quarto-figure quarto-figure-center">
<figure class="figure">
<p><img src="../../images/lec05/1/scale-zero-point-ex2.png" class="img-fluid figure-img"></p>
<figcaption>Reference. MIT-TinyML-lecture5-Quantization-1</figcaption>
</figure>
</div>
<p>Í∑∏Îüº SymmetricÌïòÍ≤å rÏùò Î≤îÏúÑÎ•º Ï†úÌïúÌïòÎäî Í≤ÉÍ≥º Í∞ôÏùÄ Îã§Î•∏ Linear QuantizationÏùÄ ÏóÜÏùÑÍπå? Ïù¥Î•º ÏïûÏÑú, QuatizedÎêú Í∞íÎì§Ïù¥ Matrix MultiplicationÏùÑ ÌïòÎ©¥ÏÑú ÎØ∏Î¶¨ Í≥ÑÏÇ∞Îê† Ïàò ÏûàÎäî Ïàò (Quantized Weight, Scale, Zero point)Í∞Ä ÏûàÏúºÎãà inferenceÏãú Ïó∞ÏÇ∞ÎüâÏùÑ Ï§ÑÏù¥Í∏∞ ÏúÑÌï¥ ÎØ∏Î¶¨ Í≥ÑÏÇ∞Ìï† Ïàò ÏûàÎäî ÌååÎùºÎØ∏ÌÑ∞Îäî ÏóÜÏùÑÍπå?</p>
</section>
<section id="quantized-matrix-multiplication" class="level3">
<h3 class="anchored" data-anchor-id="quantized-matrix-multiplication">1.4 Quantized Matrix Multiplication</h3>
<p>ÏûÖÎ†• X, Weight W, Í≤∞Í≥º YÍ∞Ä Matrix MultiplicationÏùÑ ÌñàÎã§Í≥† Ìï† Îïå ÏãùÏùÑ Í≥ÑÏÇ∞Ìï¥Î≥¥Ïûê.</p>
<p><span class="math display">\[
Y=WX
\]</span></p>
<p><span class="math display">\[
S_Y(q_Y-Z_Y) = S_W(q_W-Z_W) \cdot S_X(q_X-Z_X
\]</span></p>
<p><span class="math display">\[
\vdots
\]</span></p>
<div class="quarto-figure quarto-figure-center">
<figure class="figure">
<p><img src="../../images/lec05/1/quantized-matrix-multi.png" class="img-fluid figure-img"></p>
<figcaption>Reference. MIT-TinyML-lecture5-Quantization-1</figcaption>
</figure>
</div>
<p>Ïó¨Í∏∞ÏÑú ÎßàÏßÄÎßâ Ï†ïÎ¶¨Ìïú ÏãùÏùÑ ÏÇ¥Ìé¥Î≥¥Î©¥,</p>
<p><span class="math inline">\(Z_x\)</span> ÏôÄ <span class="math inline">\(q_w, Z_w, Z_X\)</span> Ïùò Í≤ΩÏö∞Îäî ÎØ∏Î¶¨ Ïó∞ÏÇ∞Ïù¥ Í∞ÄÎä•ÌïòÎã§. Îòê <span class="math inline">\(S_wS_X/S_Y\)</span> Ïùò Í≤ΩÏö∞ Ìï≠ÏÉÅ ÏàòÏùò Î≤îÏúÑÍ∞Ä <span class="math inline">\((0, 1)\)</span> Î°ú <span class="math inline">\(2^{-n}M_0\)</span> , <span class="math inline">\(M_0 \in [0.5, 1)\)</span> Î°ú Î≥ÄÌòïÌïòÎ©¥ N-bit IntegerÎ°ú Fixed-point ÌòïÌÉúÎ°ú ÌëúÌòÑ Í∞ÄÎä•ÌïòÎã§. Ïó¨Í∏∞Ïóê <span class="math inline">\(Z_w\)</span>Í∞Ä 0Ïù¥Î©¥ Ïñ¥Îñ®Íπå? Îòê ÎØ∏Î¶¨ Í≥ÑÏÇ∞Ìï† Ïàò ÏûàÎäî Ìï≠Ïù¥ Î≥¥Ïù∏Îã§.</p>
</section>
<section id="symmetric-linear-quantization" class="level3">
<h3 class="anchored" data-anchor-id="symmetric-linear-quantization">1.5 Symmetric Linear Quantization</h3>
<div class="quarto-figure quarto-figure-center">
<figure class="figure">
<p><img src="../../images/lec05/1/sym-linear-quant.png" class="img-fluid figure-img"></p>
<figcaption>Reference. MIT-TinyML-lecture5-Quantization-1</figcaption>
</figure>
</div>
<p><span class="math inline">\(Z_w = 0\)</span> Ïù¥ÎùºÍ≥† Ìï®ÏùÄ Î∞îÎ°ú ÏúÑÏôÄ Í∞ôÏùÄ Weight Î∂ÑÌè¨Ïù∏Îç∞, Î∞îÎ°ú SymmetricÌïú Linear QuantizationÏúºÎ°ú <span class="math inline">\(Z_w\)</span>Î•º 0ÏúºÎ°ú ÎßåÎì§Ïñ¥ <span class="math inline">\(Z_w q_x\)</span>Ìï≠ÏùÑ 0ÏúºÎ°ú Îëò Ïàò ÏûàÏñ¥ Ïó∞ÏÇ∞ÏùÑ Îòê Ï§ÑÏùº Ïàò ÏûàÏùÑ Í≤ÉÏù¥Îã§.</p>
<p>Symmetric Linear QuantizationÏùÄ Ï£ºÏñ¥ÏßÑ Îç∞Ïù¥ÌÑ∞ÏóêÏÑú Full range modeÏôÄ Restrict range modeÎ°ú ÎÇòÎâúÎã§.</p>
<p>Ï≤´ Î≤àÏß∏ Full range mode Îäî ScaleÏùÑ real number(Îç∞Ïù¥ÌÑ∞, weight)ÏóêÏÑú Î≤îÏúÑÍ∞Ä ÎÑìÏùÄ Ï™ΩÏóê ÎßûÏ∂îÎäî Í≤ÉÏù¥Îã§. ÏòàÎ•º Îì§Ïñ¥ ÏïÑÎûòÏùò Í≤ΩÏö∞, r_minÏù¥ r_maxÎ≥¥Îã§ Ï†àÎåìÍ∞íÏù¥ Îçî ÌÅ¨Í∏∞ ÎïåÎ¨∏Ïóê r_minÏóê ÎßûÏ∂∞ q_minÏùÑ Í∞ÄÏßÄÍ≥† ScaleÏùÑ Íµ¨ÌïúÎã§. Ïù¥ Î∞©Î≤ïÏùÄ Pytorch native quantizationÍ≥º ONNXÏóêÏÑú ÏÇ¨Ïö©ÎêúÎã§Í≥† Í∞ïÏùòÏóêÏÑú ÏÜåÍ∞úÌïúÎã§.</p>
<div class="quarto-figure quarto-figure-center">
<figure class="figure">
<p><img src="../../images/lec05/1/sym-full-range.png" class="img-fluid figure-img"></p>
<figcaption>Reference. MIT-TinyML-lecture5-Quantization-1</figcaption>
</figure>
</div>
<p>Îëê Î≤àÏß∏ Restrict range modeÎäî ScaleÏùÑ real number(Îç∞Ïù¥ÌÑ∞, weight)ÏóêÏÑú Î≤îÏúÑÍ∞Ä Ï¢ÅÏùÄ Ï™ΩÏóê ÎßûÏ∂îÎäî Í≤ÉÏù¥Îã§. ÏòàÎ•º Îì§Ïñ¥ ÏïÑÎûòÏùò Í≤ΩÏö∞, r_minÍ∞Ä r_maxÎ≥¥Îã§ Ï†àÎåìÍ∞íÏù¥ Îçî ÌÅ¨Í∏∞ ÎïåÎ¨∏Ïóê r_minÏóê ÎßûÏ∂îÎ©¥ÏÑú q_maxÏóê ÎßûÎèÑÎ°ù ScaleÏùÑ Íµ¨ÌïúÎã§. Ïù¥ Î∞©Î≤ïÏùÄ <a href="https://www.tensorflow.org/lite/performance/quantization_spec">TensorFlow</a>, NVIDIA TensorRT, Intel DNNLÏóêÏÑú ÏÇ¨Ïö©ÎêúÎã§Í≥† Í∞ïÏùòÏóêÏÑú ÏÜåÍ∞úÌïúÎã§.</p>
<div class="quarto-figure quarto-figure-center">
<figure class="figure">
<p><img src="../../images/lec05/1/sym-restrict-range.png" class="img-fluid figure-img"></p>
<figcaption>Reference. MIT-TinyML-lecture5-Quantization-1</figcaption>
</figure>
</div>
<p>Í∑∏Î†áÎã§Î©¥ Ïôú Symmetric Ïç®ÏïºÌï†Íπå? Asymmetric Î∞©Î≤ïÍ≥º Symmetric Î∞©Î≤ïÏùò Ï∞®Ïù¥Îäî Î≠òÍπå? (feat. Neural Network Distiller) ÏïÑÎûò Í∑∏Î¶ºÏùÑ Ï∞∏Í≥†ÌïòÎ©¥ ÎêòÏßÄÎßå, Í∞ÄÏû• ÌÅ∞ Ï∞®Ïù¥Î°ú Î≥¥Ïù¥Îäî Í≤ÉÏùÄ Computation vs Compactful quantized rangeÎ°ú Ïù¥Ìï¥Í∞ÑÎã§.</p>
<div class="quarto-figure quarto-figure-center">
<figure class="figure">
<p><img src="../../images/lec05/1/sym-range-comp.png" class="img-fluid figure-img"></p>
<figcaption>Reference. MIT-TinyML-lecture5-Quantization-1</figcaption>
</figure>
</div>
</section>
<section id="linear-quantization-examples" class="level3">
<h3 class="anchored" data-anchor-id="linear-quantization-examples">1.6 Linear Quantization examples</h3>
<p>Í∑∏Îüº Quatization Î∞©Î≤ïÏóê ÎåÄÌï¥ ÏïåÏïÑÎ¥§ÏúºÎãà Ïù¥Î•º Full-Connected Layer, Convolution LayerÏóê Ï†ÅÏö©Ìï¥Î≥¥Í≥† Ïñ¥Îñ§ Ìö®Í≥ºÍ∞Ä ÏûàÎäîÏßÄ ÏïåÏïÑÎ≥¥Ïûê.</p>
<section id="full-connected-layer" class="level4">
<h4 class="anchored" data-anchor-id="full-connected-layer">1.6.1 Full-Connected Layer</h4>
<p>ÏïÑÎûòÏ≤òÎüº ÏãùÏùÑ Ï†ÑÍ∞úÌï¥Î≥¥Î©¥ ÎØ∏Î¶¨ Ïó∞ÏÇ∞Ìï† Í≥ÑÏÇ∞Ìï† Ïàò ÏûàÎäî Ìï≠Í≥º N-bit integerÎ°ú ÌëúÌòÑÌï† ÏûàÎäî Ìï≠ÏúºÎ°ú ÎÇòÎàå Ïàò ÏûàÎã§(Ï†ÑÍ∞úÌïòÎäî Ïù¥Ïú†Îäî ÏïÑÎßà ÎØ∏Î¶¨ Í≥ÑÏÇ∞Ìï† Ïàò ÏûàÎäî Ìï≠ÏùÑ ÏïåÏïÑÎ≥¥Í∏∞ ÏúÑÌï®Ïù¥ ÏïÑÎãêÍπå Ïã∂Îã§).</p>
<p><span class="math display">\[
Y=WX+b
\]</span></p>
<p><span class="math display">\[
\downarrow
\]</span></p>
<p><span class="math display">\[
S_Y(q_Y - Z_Y) = S_W(q_W - Z_W) \cdot S_X(q_X - Z_X) + S_b(q_b - Z_b)
\]</span></p>
<p><span class="math display">\[
\downarrow \ Z_w=0
\]</span></p>
<p><span class="math display">\[
S_Y(q_Y - Z_Y) = S_WS_X(q_Wq_X - Z_xq_W) + S_b(q_b - Z_b)
\]</span></p>
<p><span class="math display">\[
\downarrow \ Z_b=0, S_b=S_WS_X
\]</span></p>
<p><span class="math display">\[
S_Y(q_Y - Z_Y) = S_WS_X(q_Wq_X - Z_xq_W+q_b)
\]</span></p>
<p><span class="math display">\[
\downarrow
\]</span></p>
<p><span class="math display">\[
q_Y = \dfrac{S_WS_X}{S_Y}(q_Wq_X+q_b - Z_Xq_W) + Z_Y
\]</span></p>
<p><span class="math display">\[
\downarrow \ q_{bias}=q_b-Z_xq_W\\
\]</span></p>
<p><span class="math display">\[
q_Y = \dfrac{S_WS_X}{S_Y}(q_Wq_X+ q_{bias}) + Z_Y\\
\]</span></p>
<p>Í∞ÑÎã®Ìûà ÌëúÍ∏∞ÌïòÍ∏∞ ÏúÑÌï¥ <span class="math inline">\(Z_W=0, Z_b=0, S_b = S_W S_X\)</span> Ïù¥ÎùºÍ≥† Í∞ÄÏ†ïÌïúÎã§.</p>
<div class="quarto-figure quarto-figure-center">
<figure class="figure">
<p><img src="../../images/lec05/1/full-connected-layer.png" class="img-fluid figure-img"></p>
<figcaption>Reference. MIT-TinyML-lecture5-Quantization-1</figcaption>
</figure>
</div>
</section>
<section id="convolutional-layer" class="level4">
<h4 class="anchored" data-anchor-id="convolutional-layer">1.6.2 Convolutional Layer</h4>
<p>Convolution LayerÏùò Í≤ΩÏö∞Îäî WeightÏôÄ XÏùò Í≥±Ïùò Í≤ΩÏö∞Î•º ConvolutionÏúºÎ°ú Î∞îÍøîÏÑú ÏÉùÍ∞ÅÌï¥Î≥¥Î©¥ ÎêúÎã§. Í∑∏ÎèÑ Í∑∏Îü¥ Í≤ÉÏù¥ ConvolutionÏùÄ KernelÍ≥º InputÏùò Í≥±Ïùò Ìï©ÏúºÎ°ú Ïù¥Î£®Ïñ¥Ï†∏ ÏûàÍ∏∞ ÎïåÎ¨∏Ïóê Full-ConnectedÏôÄ Í±∞Ïùò Ïú†ÏÇ¨ÌïòÍ≤å Ï†ÑÍ∞úÎê† Ïàò ÏûàÏùÑ Í≤ÉÏù¥Îã§.</p>
<div class="quarto-figure quarto-figure-center">
<figure class="figure">
<p><img src="../../images/lec05/1/conv-layer.png" class="img-fluid figure-img"></p>
<figcaption>Reference. MIT-TinyML-lecture5-Quantization-1</figcaption>
</figure>
</div>
</section>
</section>
</section>
<section id="post-training-quantization-ptq" class="level2">
<h2 class="anchored" data-anchor-id="post-training-quantization-ptq">2. Post-training Quantization (PTQ)</h2>
<p>Í∑∏Îüº ÏïûÏÑúÏÑú QuantizaedÌïú LayerÎ•º Fine tuningÌï† ÏóÜÏùÑÍπå? <strong>‚ÄúHow should we get the optimal linear quantization parameters (S, Z)?‚Äù</strong> Ïù¥ ÏßàÎ¨∏Ïóê ÎåÄÌï¥ÏÑú Weight, Activation, Bias ÏÑ∏ Í∞ÄÏßÄÏôÄ Í∑∏Ïóê ÎåÄÌïòÏó¨ ÎÖºÎ¨∏ÏóêÏÑú Î≥¥Ïó¨Ï£ºÎäî Í≤∞Í≥ºÍπåÏßÄ ÏïåÏïÑÎ≥¥Ïûê.</p>
<section id="weight-quantization" class="level3">
<h3 class="anchored" data-anchor-id="weight-quantization">2.1 Weight quantization</h3>
<p><strong>TL;DR.</strong> Ïù¥ Í∞ïÏùòÏóêÏÑú ÏÜåÍ∞úÌïòÎäî Weight quantizationÏùÄ GrandularityÏóê Îî∞Îùº Whole(Per-Tensor), Channel, Í∑∏Î¶¨Í≥† LayerÎ°ú Îì§Ïñ¥Í∞ÑÎã§.</p>
<section id="granularity" class="level4">
<h4 class="anchored" data-anchor-id="granularity">2.1.1 Granularity</h4>
<p>Weight quantizationÏóêÏÑú GranularityÏóê Îî∞ÎùºÏÑú Per-Tensor, Per-Channel, Group, Í∑∏Î¶¨Í≥† Generalized ÌïòÎäî Î∞©Î≤ïÏúºÎ°ú ÌôïÏû•ÏãúÏºú Shared Micro-exponent(MX) data typeÏùÑ Ï∞®Î°ÄÎ°ú Î≥¥Ïó¨Ï§ÄÎã§. ScaleÏùÑ Î™á Í∞úÎÇò Îëò Í≤ÉÏù¥ÎÉê, Í∑∏ ScaleÏùÑ Ï†ÅÏö©ÌïòÎäî Î≤îÏúÑÎ•º Ïñ¥ÎñªÍ≤å Îëò Í≤ÉÏù¥ÎÉê, Í∑∏Î¶¨Í≥† ScaleÏùÑ ÏñºÎßàÎÇò ÎîîÌÖåÏùºÌïòÍ≤å(e.g.&nbsp;floating-point)Ìï† Í≤ÉÏù¥ÎÉêÏóê Ï¥àÏ†êÏùÑ ÎëîÎã§.</p>
<div class="quarto-figure quarto-figure-center">
<figure class="figure">
<p><img src="../../images/lec05/1/granularity.png" class="img-fluid figure-img"></p>
<figcaption>Reference. MIT-TinyML-lecture5-Quantization-2</figcaption>
</figure>
</div>
<p>Ï≤´ Î≤àÏß∏Îäî <strong>Per-Tensor Quantization</strong> ÌäπÎ≥ÑÌïòÍ≤å ÏÑ§Î™ÖÌï† Í≤É ÏóÜÏù¥ Ïù¥Ï†ÑÍπåÏßÄ ÏÑ§Î™ÖÌñàÎçò ÌïòÎÇòÏùò ScaleÏùÑ ÏÇ¨Ïö©ÌïòÎäî Linear QuantizationÏù¥ÎùºÍ≥† ÏÉùÍ∞ÅÌïòÎ©¥ ÎêòÍ≤†Îã§. ÌäπÏßïÏúºÎ°úÎäî Large modelÏóê ÎåÄÌï¥ÏÑúÎäî ÏÑ±Îä•Ïù¥ Í¥úÏ∞ÆÏßÄÎßå ÏûëÏùÄ Î™®Îç∏Î°ú Îñ®Ïñ¥ÏßÄÎ©¥ ÏÑ±Îä•Ïù¥ Í∏âÍ≤©ÌïòÍ≤å Îñ®Ïñ¥ÏßÑÎã§Í≥† ÏÑ§Î™ÖÌïúÎã§. ChannelÎ≥ÑÎ°ú weight Î≤îÏ£ºÍ∞Ä ÎÑìÏùÄ Í≤ΩÏö∞ÎÇò outlier weightÍ∞Ä ÏûàÎäî Í≤ΩÏö∞ quantization Ïù¥ÌõÑÏóê ÏÑ±Îä•Ïù¥ ÌïòÎùΩÌñàÎã§Í≥† ÎßêÌïúÎã§.</p>
<div class="quarto-figure quarto-figure-center">
<figure class="figure">
<p><img src="../../images/lec05/1/per-channel-quant.png" class="img-fluid figure-img"></p>
<figcaption>Reference. MIT-TinyML-lecture5-Quantization-2</figcaption>
</figure>
</div>
<p>Í∑∏ÎûòÏÑú Í∑∏ Ìï¥Í≤∞Î∞©ÏïàÏúºÎ°ú ÎÇòÏò§Îäî Í≤ÉÏù¥ Îëê Î≤àÏß∏ Î∞©Î≤ïÏù∏ <strong>Per-Channel Quantization</strong>Ïù¥Îã§. ÏúÑ ÏòàÏ†úÏóêÏÑú Î≥¥Î©¥ Channel ÎßàÎã§ ÏµúÎåÄÍ∞íÍ≥º Í∞ÅÍ∞ÅÏóê ÎßûÎäî ScaleÏùÑ Îî∞Î°ú Í∞ÄÏßÄÎäî Í≤ÉÏùÑ Î≥º Ïàò ÏûàÎã§. Í∑∏Î¶¨Í≥† Ï†ÅÏö©Ìïú Í≤∞Í≥ºÏù∏ ÏïÑÎûò Í∑∏Î¶ºÏùÑ Î≥¥Î©¥ Per-ChannelÍ≥º Per-TensorÎ•º ÎπÑÍµêÌï¥Î≥¥Î©¥ Per-ChannelÏù¥ Í∏∞Ï°¥Ïóê floating point weightÏôÄÏùò Ï∞®Ïù¥Í∞Ä Îçî Ï†ÅÎã§. ÌïòÏßÄÎßå, ÎßåÏïΩ ÌïòÎìúÏõ®Ïñ¥ÏóêÏÑú Per-Channel QuantizationÏùÑ ÏßÄÏõêÌïòÏßÄ ÏïäÎäîÎã§Î©¥ Î∂àÌïÑÏöîÌïú Ïó∞ÏÇ∞ÏùÑ Ï∂îÍ∞ÄÎ°ú Ìï¥ÏïºÌïòÍ∏∞ ÎïåÎ¨∏Ïóê Ïù¥Îäî Ï†ÅÌï©Ìïú Î∞©Î≤ïÏù¥ Îê† Ïàò ÏóÜÎã§Îäî Ï†êÎèÑ Í≥†Î†§Ìï¥ÏïºÌï† Í≤ÉÏù¥Îã§(Ïù¥Îäî Ïù¥Ï†Ñ <a href="https://ooshyun.github.io/2023/12/04/Optimization-for-tiny-engine-1.html">Tiny EngineÏóê ÎåÄÌïú Í∏Ä</a>ÏóêÏÑú ChannelÎÇ¥Ïóê Ï∫êÏã±ÏùÑ Ïù¥Ïö©Ìïú ÏµúÏ†ÅÌôîÏôÄ Ïó∞Í¥ÄÏù¥ ÏûàÎã§). Í∑∏Îüº Îòê Îã§Î•∏ Î∞©Î≤ïÏùÄ ÏóÜÏùÑÍπå?</p>
<div class="quarto-figure quarto-figure-center">
<figure class="figure">
<p><img src="../../images/lec05/1/per-channel-vs-per-tensor.png" class="img-fluid figure-img"></p>
<figcaption>Reference. MIT-TinyML-lecture5-Quantization-2</figcaption>
</figure>
</div>
<p>ÏÑ∏ Î≤àÏß∏ Î∞©Î≤ïÏùÄ <strong>Group Quantization</strong>ÏúºÎ°ú ÏÜåÍ∞úÌïòÎäî <strong>Per-vector Scaled QuantizationÏôÄ Shared Micro-exponent(MX) data type</strong> Ïù¥Îã§. Per-vector Scaled QuantizationÏùÄ 2023ÎÖÑÎèÑ Í∞ïÏùòÎ∂ÄÌÑ∞ ÏÜåÍ∞úÌïòÎäîÎç∞, Ïù¥ Î∞©Î≤ïÏùÄ Scale factorÎ•º Í∑∏Î£πÎ≥ÑÎ°ú ÌïòÎÇò, Per-TensorÎ°ú ÌïòÎÇòÎ°ú ÎëêÍ∞úÎ•º ÎëêÎäî Î∞©Î≤ïÏù¥Îã§. ÏïÑÎûòÏùò Í∑∏Î¶ºÏùÑ Î≥¥Î©¥,</p>
<div class="quarto-figure quarto-figure-center">
<figure class="figure">
<p><img src="../../images/lec05/1/group-quantization.png" class="img-fluid figure-img"></p>
<figcaption>Reference. MIT-TinyML-lecture5-Quantization-2</figcaption>
</figure>
</div>
<p><span class="math display">\[
r=S(q-Z) \rightarrow r=\gamma \cdot S_{q}(q-Z)
\]</span></p>
<p><span class="math inline">\(S_q\)</span> Î°ú vectorÎ≥Ñ Ïä§ÏºÄÏùºÎßÅÏùÑ ÌïòÎÇò, <span class="math inline">\(\gamma\)</span> Î°ú TensorÏóê Ïä§ÏºÄÏùºÎßÅÏùÑ ÌïòÎ©∞ Í∞êÎßàÎäî floating pointÎ°ú ÌïòÎäî Í≤ÉÏùÑ Î≥º ÏàòÏûàÎã§. ÏïÑÎ¨¥ÎûòÎèÑ vectorÎã®ÏúÑÎ°ú Ïä§ÏºÄÏùºÎßÅÏùÑ ÌïòÍ≤åÎêòÎ©¥ channelÍ≥º ÎπÑÍµêÌï¥ÏÑú ÌïòÎìúÏõ®Ïñ¥ ÌîåÎû´ÌèºÏóê ÎßûÍ≤å accuracyÏùò trade-offÎ•º Ï°∞Ï†àÌïòÍ∏∞ Îçî ÏàòÏõîÌï† Í≤ÉÏúºÎ°ú Î≥¥Ïù∏Îã§.</p>
<p>Ïó¨Í∏∞ÏÑú Í∞ïÏùòÎäî ÏßÄÌëúÏù∏ Memory OverheadÎ°ú <strong>‚ÄúEffective Bit Width‚Äù</strong>Î•º ÏÜåÍ∞úÌïúÎã§. Ïù¥Îäî MicrosoftÏóêÏÑú Ï†úÍ≥µÌïòÎäî Quantization Approach MX4, MX6, MX9Í≥º Ïó∞Í≤∞Îèº ÏûàÎäîÎç∞, Ïù¥ Îç∞Ïù¥ÌÑ∞ÌÉÄÏûÖÏùÄ Ï°∞Í∏à Ïù¥ÌõÑÏóê Îçî ÏûêÏÑ∏Ìûà ÏÑ§Î™ÖÌï† Í≤ÉÏù¥Îã§. Effective Bit Width? ÏòàÏãú ÌïòÎÇòÎ•º Îì§Ïñ¥ Ïù¥Ìï¥Ìï¥Î≥¥Ïûê. ÎßåÏïΩ 4-bit QuatizationÏùÑ 4-bit per-vector scaleÏùÑ 16 elements(4Í∞úÏùò weightÍ∞Ä Í∞ÅÍ∞Å 4bitÎ•º Í∞ÄÏßÑÎã§Í≥† ÏÉùÍ∞ÅÌïòÎ©¥ 16 elementÎ°ú Í≥ÑÏÇ∞ÎêúÎã§ Ïú†Ï∂îÌï† ÏûàÎã§) ÎùºÎ©¥, Effective Bit WidthÎäî 4(Scale bit) + 4(Vector Scale bit) / 16(Vector Size) = 4.25Í∞Ä ÎêúÎã§. ElementÎãπ Scale bitÎùºÍ≥† Í∞ÑÎã®ÌïòÍ≤å ÏÉùÍ∞ÅÌï† ÏàòÎèÑ ÏûàÏùÑ ÎìØ Ïã∂Îã§.</p>
<p>ÎßàÏßÄÎßâ Per-vector Scaled QuantizationÏùÑ Ïù¥Ìï¥ÌïòÎã§Î≥¥Î©¥ Ïù¥Ï†ÑÏóê Per-Tensor, Per-ChannelÎèÑ Í∑∏Î£πÏúºÎ°ú ÏñºÎßàÎßåÌÅº Î¨∂Îäî Ï∞®Ïù¥Í∞Ä ÏûàÍ≥†, Ïù¥Îäî Ïù¥Îì§ÏùÑ ÏùºÎ∞òÌôîÌï† Ïàò ÏûàÏñ¥ Î≥¥Ïù∏Îã§. Í∞ïÏùòÏóêÏÑú Î∞îÎ°ú Îã§ÏùåÏóê ÏÜåÍ∞úÌïòÎäî Î∞©Î≤ïÏù¥ Î∞îÎ°ú <strong>Multi-level scaling scheme</strong>Ïù¥Îã§. Per-Channel QuantizationÏôÄ Per-Vector Quantization(VSQ, Vector-Scale Quantization)Î∂ÄÌÑ∞ Î¥êÎ≥¥Ïûê.</p>
<div class="quarto-figure quarto-figure-center">
<figure class="figure">
<p><img src="../../images/lec05/1/multi-level-scaling-scheme-1.png" class="img-fluid figure-img"></p>
<figcaption>Reference. With Shared Microexponents, A Little Shifting Goes a Long Way [Bita Rouhani et al.]</figcaption>
</figure>
</div>
<p>Per-Channel QuantizationÎäî Scale factorÍ∞Ä ÌïòÎÇòÎ°ú Effective Bit WidthÎäî 4Í∞Ä ÎêúÎã§. Í∑∏Î¶¨Í≥† VSQÎäî Ïù¥Ï†ÑÏóê Í≥ÑÏÇ∞Ìñà ÎìØ 4.25Í∞Ä Îê† Í≤ÉÏù¥Îã§(Ï∞∏Í≥†Î°ú Per ChannelÎ°ú Ï†ÅÏö©ÎêòÎäî ScaleÏùò Í≤ΩÏö∞ elementÏùò ÏàòÍ∞Ä ÎßéÏïÑÏÑú Í∑∏Îü∞ÏßÄ Îî∞Î°ú Effective Bit WidthÎ°ú Í≥ÑÏÇ∞ÌïòÏßÄÎäî ÏïäÎäîÎã§). VSQÍπåÏßÄ Î≥¥Î©¥ÏÑú Effective Bit WidthÎäî,</p>
<pre><code>Effective Bit Width = Scale bit + Group 0 Scale bit / Group 0 Size +...
e.g. VSQ Data type int4 = Scale bit (4) + Group 0 Scale bit(4) / Group 0 Size(16) = 4.25</code></pre>
<p>Ïù¥Î†áÍ≤å Í≥ÑÏÇ∞Ìï† Ïàò ÏûàÎã§. Í∑∏Î¶¨Í≥†, MX4, MX6, MX9Í∞Ä ÎÇòÏò®Îã§. Ï∞∏Í≥†Î°ú SÎäî Sign bit, MÏùÄ Mantissa bit, EÎäî Exponent bitÎ•º ÏùòÎØ∏ÌïúÎã§(MantissaÎÇò ExponentÏóê ÎåÄÌïú ÏûêÏÑ∏Ìïú ÎÇ¥Ïö©ÏùÄ <a href="https://ooshyun.github.io/2023/02/24/Fixed-point-vs-Floating-point.html">floating point vs fixed point Í∏Ä</a>ÏùÑ Ï∞∏Í≥†ÌïòÏûê). ÏïÑÎûòÎäî MicrosoftÏóêÏÑú Ï†úÍ≥µÌïòÎäî Quantization Approach MX4, MX6, MX9Ïóê ÎåÄÌïú ÌëúÏù¥Îã§.</p>
<div class="quarto-figure quarto-figure-center">
<figure class="figure">
<p><img src="../../images/lec05/1/multi-level-scaling-scheme-2.png" class="img-fluid figure-img"></p>
<figcaption>Reference. MIT-TinyML-lecture5-Quantization-1</figcaption>
</figure>
</div>
</section>
<section id="weight-equalization" class="level4">
<h4 class="anchored" data-anchor-id="weight-equalization">2.1.2 Weight Equalization</h4>
<p>Ïó¨Í∏∞ÍπåÏßÄ Weight QuatizationÏóêÏÑú Í∑∏Î£πÏúºÎ°ú ÏñºÎßàÎßåÌÅº Î¨∂ÎäîÏßÄÏóê Îî∞Îùº(Í∞ïÏùòÏóêÏÑúÎäî Granularity) QuatizationÏùÑ ÌïòÎäî Ïó¨Îü¨ Î∞©Î≤ïÏùÑ ÏÜåÍ∞úÌñàÎã§. Îã§ÏùåÏúºÎ°ú ÏÜåÍ∞ú Ìï† Î∞©Î≤ïÏùÄ Weight EqualizationÏù¥Îã§. 2022ÎÖÑÏóê ÏÜåÍ∞úÌï¥Ï§Ä ÎÇ¥Ïö©Ïù∏Îç∞, Ïù¥Îäî iÎ≤àÏß∏ layerÏùò output channelÎ•º scaling down ÌïòÎ©¥ÏÑú i+1Î≤àÏß∏ layerÏùò input channelÏùÑ scaling up Ìï¥ÏÑú ScaleÎ°ú Ïù∏Ìï¥ Quantization Ï†ÑÌõÑÎ°ú ÏÉùÍ∏∞Îäî LayerÍ∞Ñ Ï∞®Ïù¥Î•º Ï§ÑÏù¥Îäî Î∞©Î≤ïÏù¥Îã§.</p>
<div class="quarto-figure quarto-figure-center">
<figure class="figure">
<p><img src="../../images/lec05/1/weight-equalization.png" class="img-fluid figure-img"></p>
<figcaption>Reference. Data-Free Quantization Through Weight Equalization and Bias Correction [Markus et al., ICCV 2019]</figcaption>
</figure>
</div>
<p>ÏòàÎ•º Îì§Ïñ¥ ÏúÑÏóê Í∑∏Î¶ºÏ≤òÎüº Layer iÏùò output channelÍ≥º Layer i+1Ïùò input channelÏù¥ ÏûàÎã§. Ïó¨Í∏∞ÏÑú ÏãùÏùÑ Ï†ÑÍ∞úÌïòÎ©¥ ÏïÑÎûòÏôÄ Í∞ôÏùÄÎç∞,</p>
<p><span class="math display">\[
\begin{aligned}
y^{(i+1)}&amp;=f(W^{(i+1)}x^{(i+1)}+b^{(i+1)}) \\
         &amp;=f(W^{(i+1)} \cdot f(W^{(i)}x^{(i)}+b^{(i)}) +b^{(i+1)}) \\
         &amp;=f(W^{(i+1)}S \cdot f(S^{-1}(W^{(i)}x^{(i)}+S^{-1}b^{(i)})) +b^{(i+1)})
\end{aligned}
\]</span></p>
<p>where <span class="math inline">\(S = diag(s)\)</span> , <span class="math inline">\(s_j\)</span> is the weight equalization scale factor of output channel <span class="math inline">\(j\)</span></p>
<p>Ïó¨Í∏∞ÏÑú Scale(S)Í∞Ä i+1Î≤àÏß∏ layerÏùò weightÏóê, iÎ≤àÏß∏ weightÏóê 1/S Î°ú ScaleÎê† ÎñÑ Í∏∞Ï°¥Ïóê Scale ÌïòÏßÄ ÏïäÏùÄ ÏãùÍ≥º Ïú†ÏÇ¨ÌïòÍ≤å Ïú†ÏßÄÌï† ÏûàÎäî Í≤ÉÏùÑ Î≥º Ïàò ÏûàÎã§. Ï¶â,</p>
<p><span class="math display">\[
r^{(i)}_{oc_j} / s = r^{(i+1)}_{ic_j} \cdot s
\]</span></p>
<p><span class="math display">\[
s_j = \dfrac{1}{r^{(i+1)}_{ic=j}}\sqrt{r_{oc=j}^{(i)}\cdot r_{ic=j}^{(i+1)}}
\]</span></p>
<p><span class="math display">\[
r^{(i)}_{oc_j} = r^{(i)}_{oc_j} / s = \sqrt{r_{oc=j}^{(i)}\cdot r_{ic=j}^{(i+1)}}
\]</span></p>
<p><span class="math display">\[
r^{(i)}_{ic_j} =r^{(i)}_{ic_j} \cdot s = \sqrt{r_{oc=j}^{(i)}\cdot r_{ic=j}^{(i+1)}}
\]</span></p>
<p>Ïù¥Î†áÍ≤å ÌïòÎ©¥ iÎ≤àÏß∏ layerÏùò output channelÍ≥º i+1Î≤àÏß∏ layerÏùò input channelÏùò ScaleÏùÑ Í∞ÅÍ∞Å <span class="math inline">\(S\)</span> ÏôÄ <span class="math inline">\(1/S\)</span> Î°úÌïòÎ©∞ weightÍ∞ÑÏùò Í≤©Ï∞®Î•º Ï§ÑÏùº Ïàò ÏûàÎã§.</p>
</section>
<section id="adaptive-rounding" class="level4">
<h4 class="anchored" data-anchor-id="adaptive-rounding">2.1.3 Adaptive rounding</h4>
<div class="quarto-figure quarto-figure-center">
<figure class="figure">
<p><img src="../../images/lec05/1/adaptive-rounding.png" class="img-fluid figure-img"></p>
<figcaption>Reference. MIT-TinyML-lecture5-Quantization-1</figcaption>
</figure>
</div>
ÎßàÏßÄÎßâ ÏÜåÍ∞úÌïòÎäî Î∞©Î≤ïÏùÄ Adaptive rounding Ïù¥Îã§. Î∞òÏò¨Î¶ºÏùÄ Round-to-nearestÏúºÎ°ú Î∂àÎ¶¨Îäî ÏùºÎ∞òÏ†ÅÏù∏ Î∞òÏò¨Î¶ºÏùÑ ÏÉùÍ∞ÅÌï† Ïàò ÏûàÍ≥†, ÌïòÎÇòÏùò Í∏∞Ï§ÄÏùÑ Í∞ÄÏßÄÍ≥† Î∞òÏò¨Î¶ºÏùÑ ÌïòÎäî Adaptive RoundÎ•º ÏÉùÍ∞ÅÌï† Ìï† Ïàò ÏûàÎã§. Í∞ïÏùòÏóêÏÑúÎäî Round-to-nearestÍ∞Ä ÏµúÏ†ÅÏùò Î∞©Î≤ïÏù¥ ÎêòÏßÄ ÏïäÎäîÎã§Í≥† ÎßêÌïòÎ©∞, Adaptive roundÎ°ú weightÏóê 0Î∂ÄÌÑ∞ 1 ÏÇ¨Ïù¥Ïùò Í∞íÏùÑ ÎçîÌï¥ ÏàòÏãùÏ≤òÎüº <span class="math inline">\(\tilde{w} = \lfloor\lfloor  w\rfloor + \delta\rceil, \delta \in [0, 1]\)</span> ÏµúÏ†ÅÏùò OptimalÌïú Î∞òÏò¨Î¶º Í∞íÏùÑ Íµ¨ÌïúÎã§. $$
<span class="math display">\[\begin{aligned}
&amp;argmin_V\lvert\lvert Wx-\tilde Wx\lvert\lvert ^2_F + \lambda f_{reg}(V) \\

\rightarrow &amp; argmin_V\lvert\lvert Wx-\lfloor\lfloor W \rfloor + h(V) \rceil x\lvert\lvert ^2_F + \lambda f_{reg}(V)
\end{aligned}\]</span>
<p>$$ ### 2.2 Activation quantization Îëê Î≤àÏß∏Î°ú Activation quantizationÏù¥ ÏûàÎã§. Î™®Îç∏Í≤∞Í≥ºÎ°ú ÎÇòÏò§Îäî Í≤∞Í≥ºÎ•º ÏßÅÏ†ëÏ†ÅÏúºÎ°ú Í≤∞Ï†ïÌïòÎäî Activation QuatizationÏóêÏÑúÎäî Îëê Í∞ÄÏßÄÎ•º Í≥†Î†§Ìïú Î∞©Î≤ïÏùÑ ÏÜåÍ∞úÌïúÎã§. ÌïòÎÇòÎäî Activation Î†àÏù¥Ïñ¥ÏóêÏÑú Í≤∞Í≥ºÍ∞íÏùÑ SmoothingÌïú Î∂ÑÌè¨Î•º Í∞ÄÏßÄÍ≤å ÌïòÍ∏∞ ÏúÑÌï¥ Exponential Moving Average(EMA)Î•º ÏÇ¨Ïö©ÌïòÎäî Î∞©Î≤ïÏù¥Í≥†, Îã§Î•∏ ÌïòÎÇòÎäî Îã§ÏñëÌïú ÏûÖÎ†•Í∞íÏùÑ Í≥†Î†§Ìï¥ batch samplesÏùÑ FP32 Î™®Îç∏Í≥º calibrationÌïòÎäî Î∞©Î≤ïÏù¥Îã§.</p>
<p>Exponential Moving Average (EMA)ÏùÄ ÏïÑÎûò ÏãùÏóêÏÑú <span class="math inline">\(\alpha\)</span> Î•º Íµ¨ÌïòÎäî Î∞©Î≤ïÏù¥Îã§. <span class="math display">\[
\hat r^{(t)}_{max, min} = \alpha r^{(t)}_{max, min} + (1-\alpha) \hat r^{(t)}_{max, min}  
\]</span> CalibrationÏùò Ïª®ÏÖâÏùÄ ÎßéÏùÄ inputÏùò min/max ÌèâÍ∑†ÏùÑ Ïù¥Ïö©ÌïòÏûêÎäî Í≤ÉÏù¥Îã§. Í∑∏ÎûòÏÑú trained FP32 modelÍ≥º sample batchÎ•º Í∞ÄÏßÄÍ≥† quantizedÌïú Î™®Îç∏Ïùò Í≤∞Í≥ºÏôÄ calibrationÏùÑ ÎèåÎ¶¨Î©¥ÏÑú Í∑∏ Ï∞®Ïù¥Î•º ÏµúÏÜåÌôî ÏãúÌÇ§ÎäîÎç∞, Ïó¨Í∏∞Ïóê Ïù¥Ïö©ÌïòÎäî ÏßÄÌëúÎäî loss of informationÏôÄ Newton-Raphson methodÎ•º ÏÇ¨Ïö©Ìïú Mean Square Error(MSE)Í∞Ä ÏûàÎã§. <span class="math display">\[
MSE = \underset{\lvert r \lvert_{max}}{min}\ \mathbb{E}[(X-Q(X))^2]
\]</span> <span class="math display">\[
KL\ divergence=D_{KL}(P\lvert\lvert Q) = \sum_i^N P(x_i)log\dfrac{P(x_i)}{Q(x_i)}
\]</span> ### 2.3 Quanization Bias Correction</p>
<p>ÎßàÏßÄÎßâÏúºÎ°ú QuatizationÏúºÎ°ú biased errorÎ•º Ïû°ÎäîÎã§Îäî Í≤ÉÏùÑ ÏÜåÍ∞úÌïúÎã§. <span class="math inline">\(\epsilon = Q(W)-W\)</span> Ïù¥ÎùºÍ≥† ÎëêÍ≥† ÏïÑÎûòÏ≤òÎüº ÏãùÏù¥ Ï†ÑÍ∞úÏãúÌÇ§Î©¥ ÎßàÏßÄÎßâ Ìï≠ÏóêÏÑú Î≥¥Ïù¥Îäî <span class="math inline">\(-\epsilon\mathbb{E}[x]\)</span> Î∂ÄÎ∂ÑÏù¥ biasÎ•º quatizationÏùÑ Ìï† Îïå Ï†úÍ±∞ ÎêúÎã§Í≥† ÌïúÎã§(Ïù¥ Î∂ÄÎ∂ÑÏùÄ 2023ÎÖÑÏóêÎäî ÏÜåÍ∞úÌïòÏßÑ ÏïäÎäîÎç∞, ÎãπÏó∞Ìïú Í≤ÉÏù¥Ïñ¥ÏÑú ÏïàÌïòÎäîÏßÄ, ÌòπÏùÄ ÏòÅÌñ•Ïù¥ ÌÅ¨ÏßÄ ÏïäÏïÑÏÑú Í∑∏Îü∞ÏßÄÎäî Î™®Î•¥Í≤†Îã§. Bias QuatizationÏù¥ÌõÑÏóê MobileNetV2ÏóêÏÑú Ìïú Î†àÏù¥Ïñ¥Ïùò outputÏùÑ Î≥¥Î©¥ Ïñ¥ÎäêÏ†ïÎèÑ Ï†úÍ±∞ÎêòÎäî Í≤ÉÏ≤òÎüº Î≥¥Ïù∏Îã§). <span class="math display">\[
\begin{aligned}
\mathbb{E}[y] &amp;= \mathbb{E}[Wx] + \mathbb{E}[\epsilon x] - \mathbb{E}[\epsilon x],\ \mathbb{E}[Q(W)x] = \mathbb{E}[Wx] + \mathbb{E}[\epsilon x] \\
\mathbb{E}[y] &amp;= \mathbb{E}[Q(W)x] - \epsilon\mathbb{E}[x]
\end{aligned}
\]</span></p>
<div class="quarto-figure quarto-figure-center">
<figure class="figure">
<p><img src="../../images/lec05/1/quantization-bias-correction.png" class="img-fluid figure-img"></p>
<figcaption>Reference. MIT-TinyML-lecture5-Quantization-2</figcaption>
</figure>
</div>
</section>
</section>
<section id="post-training-int8-linear-quantization-result" class="level3">
<h3 class="anchored" data-anchor-id="post-training-int8-linear-quantization-result">2.4 Post-Training INT8 Linear Quantization Result</h3>
<p>ÏïûÏÑ† Post-Training QuantizationÏùÑ Ï†ÅÏö©Ìïú Í≤∞Í≥ºÎ•º Î≥¥Ïó¨Ï§ÄÎã§. Ïù¥ÎØ∏ÏßÄÍ≥ÑÏó¥ Î™®Îç∏ÏùÑ Î™®Îëê ÏÇ¨Ïö©ÌñàÏúºÎ©∞, ÏÑ±Îä•ÌïòÎùΩÌè≠ÏùÄ ÏßÄÌëúÎ°ú Î≥¥Ïó¨Ï§ÄÎã§. ÎπÑÍµêÏ†Å ÌÅ∞ Î™®Îç∏Îì§Ïùò Í≤ΩÏö∞ Ï§ÄÏàòÌïú ÏÑ±Îä•ÏùÑ Î≥¥Ïó¨Ï£ºÏßÄÎßå MobileNetV1, V2ÏôÄ Í∞ôÏùÄ ÏûëÏùÄ Î™®Îç∏ÏùÄ ÏÉùÍ∞ÅÎ≥¥Îã§ QuantizationÏúºÎ°ú Îñ®Ïñ¥ÏßÄÎäî ÏÑ±Îä•Ìè≠(-11.8%, -2.1%) Ïù¥ ÌÅ∞ Í≤ÉÏùÑ Î≥º Ïàò ÏûàÎã§. Í∑∏Îüº ÏûëÏùÄ ÌÅ¨Í∏∞Ïùò Î™®Îç∏Îì§ÏùÄ Ïñ¥ÎñªÍ≤å Training Ìï¥ÏïºÌï†Íπå?</p>
<div class="quarto-figure quarto-figure-center">
<figure class="figure">
<p><img src="../../images/lec05/1/post-training-result.png" class="img-fluid figure-img"></p>
<figcaption>Reference. MIT-TinyML-lecture5-Quantization-2</figcaption>
</figure>
</div>
</section>
</section>
<section id="quantization-aware-trainingqat" class="level2">
<h2 class="anchored" data-anchor-id="quantization-aware-trainingqat">3. Quantization-Aware Training(QAT)</h2>
<section id="quantization-aware-training" class="level3">
<h3 class="anchored" data-anchor-id="quantization-aware-training">3.1 Quantization-Aware Training</h3>
<div class="quarto-figure quarto-figure-center">
<figure class="figure">
<p><img src="../../images/lec05/2/Untitled.png" class="img-fluid figure-img"></p>
<figcaption>Reference. MIT-TinyML-lecture06-Quantization-2</figcaption>
</figure>
</div>
<ul>
<li>Usually, fine-tuning a pre-trained floating point model provides better accuracy than training from scratch.</li>
</ul>
<p>Ïù¥Ï†ÑÏóê K-mean QuantizationÏóêÏÑú Fine-tuningÎïå CentroidÏóê gradientÎ•º Î∞òÏòÅÌñàÏóàÎã§. Quantization-Aware TrainingÏùÄ Ïù¥ÏôÄ Ïú†ÏÇ¨ÌïòÍ≤å Quantization - ReconstructionÏùÑ ÌÜµÌï¥ ÎßåÎì§Ïñ¥ÏßÑ WeightÎ°ú TrainingÏùÑ ÌïòÎäî Î∞©Î≤ïÏùÑ ÎßêÌïúÎã§. ÏòàÏãúÎ•º Îì§Ïñ¥ÏÑú ÏûêÏÑ∏Ìûà ÏÇ¥Ìé¥Î≥¥Ïûê.</p>
<div class="quarto-figure quarto-figure-center">
<figure class="figure">
<p><img src="../../images/lec05/2/Untitled 1.png" class="img-fluid figure-img"></p>
<figcaption>Reference. MIT-TinyML-lecture06-Quantization-2</figcaption>
</figure>
</div>
<ul>
<li>A full precision copy of the weights W is maintained throughout the training.</li>
<li><strong>The small gradients are accumulated without loss of precision</strong></li>
<li>Once the model is trained, only the quantized weights are used for inference</li>
</ul>
<p>ÏúÑ Í∑∏Î¶ºÏóêÏÑú Layer NÏù¥ Î≥¥Ïù∏Îã§. Ïù¥ Layer NÏùÄ weightsÎ•º ÌååÎùºÎØ∏ÌÑ∞Î°ú Í∞ÄÏßÄÏßÄÎßå, Ïã§Ï†úÎ°ú Training Í≥ºÏ†ïÏóêÏÑú Ïì∞Ïù¥Îäî weightÎäî ‚Äúweight quantization‚ÄùÏùÑ ÌÜµÌï¥ Quantization - ReconstructionÏùÑ ÌÜµÌï¥ ÎßåÎì§Ïñ¥ÏßÑ WeightÎ•º Í∞ÄÏßÄÍ≥† ÌõàÎ†®ÏùÑ Ìï† Í≤ÉÏù¥Îã§.</p>
</section>
<section id="straight-through-estimatorste" class="level3">
<h3 class="anchored" data-anchor-id="straight-through-estimatorste">3.2 Straight-Through Estimator(STE)</h3>
<div class="quarto-figure quarto-figure-center">
<figure class="figure">
<p><img src="../../images/lec05/2/Untitled 2.png" class="img-fluid figure-img"></p>
<figcaption>Reference. MIT-TinyML-lecture06-Quantization-2</figcaption>
</figure>
</div>
<p>Í∑∏Îüº ÌõàÎ†®ÏóêÏÑú gradientÎäî Ïñ¥ÎñªÍ≤å Ï†ÑÎã¨Ìï† Ïàò ÏûàÏùÑÍπå? QuantizationÏùò Í∞úÎÖêÏÉÅ, weight quantizationÏóêÏÑú weightÎ°ú ÎÑòÏñ¥Í∞ÄÎäî gradientÎäî ÏóÜÏùÑ Ïàò Î∞ñÏóê ÏóÜÎã§. Í∑∏Î†áÍ≤å ÎêòÎ©¥ ÏÇ¨Ïã§ÏÉÅ weightÎ°ú back propagationÏù¥ Îê† Ïàò ÏóÜÍ≤å ÎêòÍ≥†, Í∑∏ÎûòÏÑú ÏÜåÍ∞úÌïòÎäî Í∞úÎÖêÏù¥ <strong>Straight-Through Estimator(STE)</strong> ÏûÖÎãàÎã§. ÎßêÏù¥ Í±∞Ï∞ΩÌï¥ÏÑú Í∑∏Î†áÏßÄ, Q(W)ÏóêÏÑú Î∞õÏùÄ gradientÎ•º Í∑∏ÎåÄÎ°ú weights Î°ú ÎÑòÍ≤®Ï£ºÎäî Î∞©ÏãùÏù¥Îã§.</p>
<ul>
<li><p>Quantization is discrete-valued, and thus the derivative is 0 almost everywhere ‚Üí NN will learn nothing!</p></li>
<li><p><strong>Straight-Through Estimator(STE) simply passes the gradients through the quantization as if it had been the identity function.</strong></p>
<p><span class="math display">\[
  g_W = \dfrac{\partial L}{\partial  W} = \dfrac{\partial L}{\partial  Q(W)}
  \]</span></p></li>
</ul>
<div class="quarto-figure quarto-figure-center">
<figure class="figure">
<p><img src="../../images/lec05/2/Untitled 3.png" class="img-fluid figure-img"></p>
<figcaption>Reference. MIT-TinyML-lecture06-Quantization-2</figcaption>
</figure>
</div>
<ul>
<li>Reference
<ul>
<li>Neural Networks for Machine Learning [Hinton&nbsp;<em>et al.</em>, Coursera Video Lecture, 2012]</li>
<li>Estimating or Propagating Gradients Through Stochastic Neurons for Conditional Computation [Bengio, arXiv 2013]</li>
</ul></li>
</ul>
<p>Ïù¥ ÌõàÎ†®Ïùò Í≤∞Í≥ºÍ∞Ä Í∂ÅÍ∏àÌïòÏãúÎã§Î©¥ <a href="https://arxiv.org/pdf/1806.08342.pdf">Ïù¥ ÎÖºÎ¨∏</a>ÏùÑ Ï∞∏Í≥†ÌïòÏûê. Ï∞∏Í≥†Î°ú ÎÖºÎ¨∏ÏóêÏÑúÎäî MobileNetV1, V2 Í∑∏Î¶¨Í≥† NASNet-MobileÏùÑ Ïù¥Ïö©Ìï¥ Post-Training QuantizationÍ≥º Quantization-Aware TrainingÏùÑ ÎπÑÍµêÌïòÍ≥† ÏûàÎã§.</p>
</section>
</section>
<section id="binary-and-ternary-quantization" class="level2">
<h2 class="anchored" data-anchor-id="binary-and-ternary-quantization">4. Binary and Ternary Quantization</h2>
<p>Ïûê, Í∑∏Îüº QuantizationÏùÑ Í∂ÅÍ∑πÏ†ÅÏúºÎ°ú 2bitÎ°ú Ìï† ÏàòÎäî ÏóÜÏùÑÍπå? Î∞îÎ°ú Binary(1, -1)Í≥º Tenary(1, 0, -1) Ïù¥Îã§.</p>
<ul>
<li><p><strong>Can we push the quantization precision to 1 bit?</strong></p>
<div class="quarto-figure quarto-figure-center">
<figure class="figure">
<p><img src="../../images/lec05/2/Untitled 4.png" class="img-fluid figure-img"></p>
<figcaption>Reference. MIT-TinyML-lecture06-Quantization-2</figcaption>
</figure>
</div></li>
<li><p>Reference</p>
<ul>
<li>BinaryConnect: Training Deep Neural Networks with Binary Weights during Propagations [Courbariaux&nbsp;<em>et al.</em>, NeurIPS 2015]</li>
<li>XNOR-Net: ImageNet Classification using Binary Convolutional Neural Networks [Rastegari&nbsp;<em>et al.</em>, ECCV 2016]</li>
</ul></li>
</ul>
<p>Î®ºÏ†Ä WeightÎ•º 2bitÎ°ú QuantizationÏùÑ ÌïòÍ≤å ÎêòÎ©¥, Î©îÎ™®Î¶¨ÏóêÏÑúÎäî 32bitÎ•º 1bitÎ°ú Ï§ÑÏù¥Îãà 32Î∞∞ÎÇò Ï§ÑÏùº Ïàò ÏûàÍ≥†, ComputationÎèÑ (8x5)+(-3x2)+(5x0)+(-1x1)ÏóêÏÑú 5-2+0-1 Î°ú Ï†àÎ∞òÏùÑ Ï§ÑÏùº Ïàò ÏûàÎã§.</p>
<section id="binarization-deterministic-binarization" class="level3">
<h3 class="anchored" data-anchor-id="binarization-deterministic-binarization">4.1 Binarization: <strong>Deterministic Binarization</strong></h3>
<p>Í∑∏Îüº BinarizationÏóêÏÑú +1Í≥º -1ÏùÑ Ïñ¥Îñ§ Í∏∞Ï§ÄÏúºÎ°ú Ìï¥ÏïºÌï†Íπå? Í∞ÄÏû• Ïâ¨Ïö¥ Î∞©Î≤ïÏùÄ threholdÎ•º Í∏∞Ï§ÄÏúºÎ°ú +-1Î°ú ÎÇòÎàÑÎäî Í≤ÉÏù¥Îã§.</p>
<p>Directly computes the bit value base on a threshold, usually 0 resulting in a sign function.</p>
<p><span class="math display">\[
q = sign(r) = \begin{dcases}
+1, &amp;r \geq 0 \\
-1, &amp;r &lt; 0
\end{dcases}
\]</span></p>
</section>
<section id="binarization-stochastic-binarization" class="level3">
<h3 class="anchored" data-anchor-id="binarization-stochastic-binarization">4.2 Binarization: <strong>Stochastic Binarization</strong></h3>
<p>Îã§Î•∏ Î∞©Î≤ïÏúºÎ°úÎäî outputÏóêÏÑú hard-sigmoid functionÏùÑ Í±∞Ï≥êÏÑú ÎÇòÏò® Í∞íÎßåÌÅº ÌôïÎ•†Ï†ÅÏúºÎ°ú +-1Ïù¥ ÎÇòÏò§ÎèÑÎ°ù ÌïòÎäî Í≤ÉÏù¥Îã§. ÌïòÏßÄÎßå Ïù¥ Î∞©Î≤ïÏùÄ Î¨¥ÏûëÏúÑÎ°ú ÎπÑÌä∏Î•º ÏÉùÏÑ±ÌïòÎäî ÌïòÎìúÏõ®Ïñ¥Î•º ÌïòÎäî Í≤ÉÏù¥ Ïñ¥Î†µÍ∏∞ ÎïåÎ¨∏Ïóê ÏÇ¨Ïö©ÌïòÏßÑ ÏïäÎäîÎã§Í≥† Ïñ∏Í∏âÌïúÎã§.</p>
<ul>
<li><p>Use global statistics or the value of input data to determine the probability of being -1 or +1</p></li>
<li><p>In Binary Connect(BC), probability is determined by hard sigmoid function <span class="math inline">\(\sigma(r)\)</span></p>
<p><span class="math display">\[
  q=\begin{dcases}
  +1, &amp;\text{with probability } p=\sigma(r)\\
  -1, &amp; 1-p
  \end{dcases}
  \\
  where\ \sigma(r)=min(max(\dfrac{r+1}{2}, 0), 1)
  \]</span></p>
<div class="quarto-figure quarto-figure-center">
<figure class="figure">
<p><img src="../../images/lec05/2/Untitled 5.png" class="img-fluid figure-img"></p>
<figcaption>Reference. MIT-TinyML-lecture06-Quantization-2</figcaption>
</figure>
</div></li>
<li><p><strong>Harder to implement</strong> as it requires the hardware to generate random bits when quantizing.</p></li>
</ul>
</section>
<section id="binarization-use-scale" class="level3">
<h3 class="anchored" data-anchor-id="binarization-use-scale">4.3 Binarization: Use Scale</h3>
<p>ÏïûÏÑ† Î∞©Î≤ïÏùÑ Ïù¥Ïö©Ìï¥ÏÑú ImageNet Top-1 ÏùÑ ÌèâÍ∞ÄÌï¥Î≥¥Î©¥ QuantizationÏù¥ÌõÑ -21.2%ÎÇò ÏÑ±Îä•Ïù¥ ÌïòÎùΩÌïòÎäî Í±∏ Î≥º Ïàò ÏûàÎã§. ‚ÄúÏñ¥ÎñªÍ≤å Î≥¥ÏôÑÌï† Ïàò ÏûàÏùÑÍπå?‚Äù Ìïú Í≤ÉÏù¥ linear qunatizationÏóêÏÑú ÏÇ¨Ïö©ÌñàÎçò Scale Í∞úÎÖêÏù¥Îã§.</p>
<ul>
<li><p>Using <strong>Scale</strong>, Minimizing Quantization Error in Binarization</p>
<div class="quarto-figure quarto-figure-center">
<figure class="figure">
<p><img src="../../images/lec05/2/Untitled 6.png" class="img-fluid figure-img"></p>
<figcaption>Reference. MIT-TinyML-lecture06-Quantization-2</figcaption>
</figure>
</div></li>
</ul>
<p>Ïó¨Í∏∞ÏÑú ScaleÏùÄ <span class="math inline">\(\dfrac{1}{n}\lvert\lvert W \lvert\lvert_1\)</span> Î°ú Í≥ÑÏÇ∞Ìï† Ïàò ÏûàÍ≥†, ÏÑ±Îä•ÏùÄ ÌïòÎùΩÏù¥ Í±∞Ïùò ÏóÜÎäî Í≤ÉÎèÑ ÌôïÏù∏Ìï† Ïàò ÏûàÎã§. Ïôú <span class="math inline">\(\dfrac{1}{n}\lvert\lvert W \lvert\lvert_1\)</span>Ïù∏ÏßÄÎäî ÏïÑÎûò Ï¶ùÎ™ÖÍ≥ºÏ†ïÏùÑ Ï∞∏Í≥†ÌïòÏûê!</p>
<ul>
<li><p>Why <span class="math inline">\(\alpha\)</span> is <span class="math inline">\(\dfrac{1}{n}\lvert\lvert W \lvert\lvert_1\)</span>?</p>
<p><span class="math display">\[
  \begin{aligned}
  &amp;J(B, \alpha)=\lvert\lvert W-\alpha B\lvert\lvert^2 \\
  &amp;\alpha^*, B^*= \underset{\alpha, B}{argmin}\ J(B, \alpha) \\
  &amp;J(B,\alpha) = \alpha^2B^TB-2\alpha W^T B + W^TW\ since\ B \in \{+1, -1\}^n \\
  &amp;B^TB=n(constant), W^TW= constant(a \ known\ variable) \\
  &amp;J(B,\alpha) = \alpha^2n-2\alpha W^T B + C \\
  &amp;B^* = \underset{B}{argmax} \{W^T B\}\ s.t.\ B\in \{+1,-1 \}^n \\
  &amp;\alpha^*=\dfrac{W^TB^*}{n} \\
  &amp;\alpha^*=\dfrac{W^Tsign(W)}{n} = \dfrac{\lvert W_i \lvert}{n} = \dfrac{1}{n}\lvert\lvert W\lvert\lvert_{l1}
  \end{aligned}
  \]</span></p>
<ul>
<li>Reference. XNOR-Net: ImageNet Classification using Binary Convolutional Neural Networks [Rastegari&nbsp;<em>et al.</em>, ECCV 2016]</li>
<li>B*Îäî J(B,<span class="math inline">\(\alpha\)</span>)ÏóêÏÑú ÏµúÏÜüÍ∞íÏùÑ Íµ¨Ìï¥ÏïºÌïòÎØÄÎ°ú <span class="math inline">\(W^T\)</span>B Í∞Ä ÏµúÎåÄÏó¨ÏïºÌïòÍ≥† Í∑∏Îü¨Í∏∞ ÏúÑÌï¥ÏÑúÎäî WÍ∞Ä ÏñëÏàòÏùºÎïåÎäî BÎèÑ ÏñëÏàò, WÍ∞Ä ÏùåÏàòÏùº ÎïåÎäî BÎèÑ ÏùåÏàòÏó¨Ïïº <span class="math inline">\(W^TB=\sum\lvert W \lvert\)</span> Ïù¥ ÎêòÎ©¥ÏÑú ÏµúÎåìÍ∞íÏù¥ Îê† Ïàò ÏûàÎã§.</li>
</ul></li>
</ul>
</section>
<section id="binarization-activation" class="level3">
<h3 class="anchored" data-anchor-id="binarization-activation">4.4 Binarization: Activation</h3>
<p>Í∑∏Îüº ActivationÍπåÏßÄ QuantizationÏùÑ Ìï¥Î¥ÖÏãúÎã§.</p>
<p><strong>4.4.1 Activation</strong></p>
<div class="quarto-figure quarto-figure-center">
<figure class="figure">
<p><img src="../../images/lec05/2/Untitled 7.png" class="img-fluid figure-img"></p>
<figcaption>Untitled</figcaption>
</figure>
</div>
<p>Ïó¨Í∏∞ÏÑú Ï°∞Í∏à Îçî Ïó∞ÏÇ∞ÏùÑ ÏµúÏ†ÅÌôî Ìï† Ïàò ÏûàÏñ¥Î≥¥Ïù¥Îäî Í≤ÉÏù¥ Matrix MuliplicationÏù¥ XOR Ïó∞ÏÇ∞Í≥º ÎπÑÏä∑ÌïòÍ≤å Î≥¥Ïù∏Îã§.</p>
<p><strong>4.4.2 XNOR bit count</strong></p>
<div class="quarto-figure quarto-figure-center">
<figure class="figure">
<p><img src="../../images/lec05/2/Untitled 8.png" class="img-fluid figure-img"></p>
<figcaption>Reference. MIT-TinyML-lecture06-Quantization-2</figcaption>
</figure>
</div>
<ul>
<li><span class="math inline">\(y_i=-n+ popcount(W_i\ xnor\ x) &lt;&lt; 1\)</span> ‚Üí <strong>popcount returns the number of 1</strong></li>
</ul>
<p>Í∑∏ÎûòÏÑú <strong>popcount</strong>Í≥º <strong>XNOR</strong>ÏùÑ Ïù¥Ïö©Ìï¥ÏÑú ComputationÏóêÏÑú Ï¢Ä Îçî ÏµúÏ†ÅÌôîÎ•º ÏßÑÌñâÌï©ÎãàÎã§. Ïù¥Î†áÍ≤å ÏµúÏ†ÅÌôîÎ•º ÏßÑÌñâÌïòÍ≤å ÎêòÎ©¥, Î©îÎ™®Î¶¨Îäî 32Î∞∞, ComputationÏùÄ 58Î∞∞Í∞ÄÎüâ Ï§ÑÏñ¥Îì§ Ïàò ÏûàÎã§Í≥† ÎßêÌïúÎã§.</p>
<div class="quarto-figure quarto-figure-center">
<figure class="figure">
<p><img src="../../images/lec05/2/Untitled 9.png" class="img-fluid figure-img"></p>
<figcaption>Reference. MIT-TinyML-lecture06-Quantization-2</figcaption>
</figure>
</div>
<p>Ïù¥Î†áÍ≤å Weight, Scale factor, Activation, Í∑∏Î¶¨Í≥† XNOR-Bitcout ÍπåÏßÄ. Ï¥ù ÎÑ§ Í∞ÄÏßÄ Îã®Í≥ÑÎ°ú Binary QuantizationÏùÑ ÎÇòÎààÎã§. Îã§ÏùåÏúºÎ°úÎäî Ternary QuantizationÏùÄ ÏïåÏïÑÎ≥¥Ïûê.</p>
<div class="quarto-figure quarto-figure-center">
<figure class="figure">
<p><img src="../../images/lec05/2/Untitled 10.png" class="img-fluid figure-img"></p>
<figcaption>Reference. XNOR-Net: ImageNet Classification using Binary Convolutional Neural Networks [Rastegari&nbsp;<em>et al.</em>, ECCV 2016]</figcaption>
</figure>
</div>
<ul>
<li><ol start="2" type="1">
<li>Binarizing Input Ïùò Í≤ΩÏö∞Îäî averageÎ•º Î™®Îì† channelÏóê Í∞ôÏù¥ Ï†ÅÏö©Ìï† Í≤ÉÏù¥Í∏∞ ÎïåÎ¨∏Ïóê Í∑∏ cÎßåÌÅºÏùÑ average filterÎ°ú Ìïú Î≤àÏóê Ï†ÅÏö©ÌïúÎã§Îäî ÎßêÏù¥Îã§.</li>
</ol></li>
</ul>
</section>
<section id="ternary-weight-networkstwn" class="level3">
<h3 class="anchored" data-anchor-id="ternary-weight-networkstwn">4.5 <strong>Ternary Weight Networks(TWN)</strong></h3>
<p>TernaryÎäî Binary QuantizationÍ≥º Îã®Í≥ÑÎäî Î™®Îëê Í∞ôÏßÄÎßå, Í∞ÄÏßà Ïàò ÏûàÎäî Í∞íÏúºÎ°ú <strong>0</strong> ÏùÑ Ï∂îÍ∞ÄÌïúÎã§. ÏïÑÎûò Í∑∏Î¶ºÏùÄ ScaleÏùÑ Ïù¥Ïö©Ìï¥ÏÑú Quantization ErrorÎ•º Ï§ÑÏù¥Îäî Î∞©Î≤ïÏùÑ ÎßêÌïòÍ≥† ÏûàÎã§. <span class="math display">\[
q = \begin{dcases}
r_t, &amp;r &gt; \Delta \\
0, &amp;\lvert r\lvert \leq \Delta \\
-r_t, &amp;r &lt; -\Delta
\end{dcases} \\
where\ \Delta = 0.7\times \mathbb{E}(\lvert r \lvert), r_t = \mathbb{E}_{\lvert r \lvert &gt; \Delta}(\lvert r \lvert )
\]</span> <img src="../../images/lec05/2/Untitled 11.png" class="img-fluid" alt="Reference. Trained Ternary Quantization [Zhu&nbsp;et al., ICLR 2017]"> ### 4.6 Trained Ternary Quantization(TTQ)</p>
<p>Tenary QuantizationÏóêÏÑú Îòê ÌïúÍ∞ÄÏßÄ Îã§Î•¥Í≤å ÏÑ§Î™ÖÌïòÎäî Í≤ÉÏùÄ 1Í≥º -1Î°úÎßå Ï†ïÌï¥Ï†∏ ÏûàÎçò Binary QuantizationÍ≥º Îã§Î•¥Í≤å TenaryÎäî 1, 0, -1Î°ú QuantizationÏùÑ Ìïú ÌõÑ, Ï∂îÍ∞ÄÏ†ÅÏù∏ ÌõàÎ†®ÏùÑ ÌÜµÌï¥ <span class="math inline">\(w_t\)</span>ÏôÄ <span class="math inline">\(-w_t\)</span>Î°ú fine-tuningÏùÑ ÌïòÎäî Î∞©Î≤ïÎèÑ Ï†úÏïàÌïúÎã§(Ìï¥Îãπ <a href="https://arxiv.org/pdf/1612.01064.pdf">ÎÖºÎ¨∏</a>ÏóêÏÑúÎäî Ïù¥Îü¨Ìïú Í∏∞Î≤ïÏùÑ Ïù¥Ïö©Ìï¥ÏÑú Ìïú Í≤∞Í≥ºÎ•º CIFAR-10 Ïù¥ÎØ∏ÏßÄ Îç∞Ïù¥ÌÑ∞Î•º Í∞ÄÏßÄÍ≥† ResNets, AlexNet, ImageNetÏóêÏÑú Î≥¥Ïó¨Ï§ÄÎã§). <span class="math display">\[
q = \begin{dcases}
w_t, &amp;r &gt; \Delta \\
0, &amp;\lvert r\lvert \leq \Delta \\
-w_t, &amp;r &lt; -\Delta
\end{dcases}
\]</span> <img src="../../images/lec05/2/Untitled 12.png" class="img-fluid" alt="Reference. Trained Ternary Quantization [Zhu&nbsp;et al., ICLR 2017]"></p>
</section>
<section id="accuracy-degradation" class="level3">
<h3 class="anchored" data-anchor-id="accuracy-degradation">4.7 Accuracy Degradation</h3>
<p>Binary, Ternary QuantizationÏùÑ ÏÇ¨Ïö©Ìïú Í≤∞Í≥ºÎ•º Î≥¥Ïó¨Ï§ÄÎã§(Resnet-18 Í≤ΩÏö∞ÏóêÎäî Ternary Í∞Ä Ïò§ÌûàÎ†§ BinaryÎ≥¥Îã§ ÏÑ±Îä•Ïù¥ Îçî Îñ®Ïñ¥ÏßÑÎã§!)</p>
<ul>
<li><p><strong>Binarization</strong></p>
<div class="quarto-figure quarto-figure-center">
<figure class="figure">
<p><img src="../../images/lec05/2/Untitled 13.png" class="img-fluid figure-img"></p>
<figcaption>Reference. Binarized Neural Networks: Training Deep Neural Networks with Weights and Activations Constrained to +1 or&nbsp;‚àí1. [Courbariaux&nbsp;<em>et al.</em>, Arxiv 2016], XNOR-Net: ImageNet Classification using Binary Convolutional Neural Networks [Rastegari&nbsp;<em>et al.</em>, ECCV 2016]</figcaption>
</figure>
</div></li>
<li><p><strong>Ternary Weight Networks (TWN)</strong></p>
<div class="quarto-figure quarto-figure-center">
<figure class="figure">
<p><img src="../../images/lec05/2/Untitled 14.png" class="img-fluid figure-img"></p>
<figcaption>Reference. Ternary Weight Networks [Li&nbsp;<em>et al.</em>, Arxiv 2016]</figcaption>
</figure>
</div></li>
<li><p><strong>Trained Ternary Quantization (TTQ)</strong></p>
<div class="quarto-figure quarto-figure-center">
<figure class="figure">
<p><img src="../../images/lec05/2/Untitled 15.png" class="img-fluid figure-img"></p>
<figcaption>Reference. Trained Ternary Quantization [Zhu&nbsp;<em>et al.</em>, ICLR 2017]</figcaption>
</figure>
</div></li>
</ul>
</section>
</section>
<section id="low-bit-width-quantization" class="level2">
<h2 class="anchored" data-anchor-id="low-bit-width-quantization">5. Low Bit-Width Quantization</h2>
<p>ÎÇ®ÏùÄ Î∂ÄÎ∂ÑÎì§ÏùÄ Ïó¨Îü¨Í∞ÄÏßÄ Ïã§Ìóò / Ïó∞Íµ¨Îì§ÏùÑ ÏÜåÍ∞úÌïòÍ≥† ÏûàÎã§.</p>
<ul>
<li>Binary QuantizationÏùÄ Quantization Aware TrainingÏùÑ Ìï† Ïàò ÏûàÏùÑÍπå?</li>
<li>2,3 bitÍ≥º 8bit Í∑∏ Ï§ëÍ∞ÑÏúºÎ°úÎäî QuantizationÏùÑ Ìï† Ïàò ÏóÜÏùÑÍπå?</li>
<li>Î†àÏù¥Ïñ¥ÏóêÏÑú QuantizationÏùÑ ÌïòÏßÄ ÏïäÎäî Î†àÏù¥Ïñ¥, ÏòàÎ•º Îì§Ïñ¥ Í≤∞Í≥ºÏóê ÏòÅÌñ•ÏùÑ ÏòàÎØºÌïòÍ≤å ÎØ∏ÏπòÎäî Ï≤´ Î≤àÏß∏ Î†àÏù¥Ïñ¥Í∞Ä Í∞ôÏùÄ Í≤ΩÏö∞ QuantizationÏùÑ ÌïòÏßÄ ÏïäÏúºÎ©¥ Ïñ¥ÎñªÍ≤å Îê†Íπå?</li>
<li>Activation Ìï®ÏàòÎ•º Î∞îÍæ∏Î©¥ Ïñ¥Îñ®Íπå?</li>
<li>ÏòàÎ•º Îì§Ïñ¥ Ï≤´Î≤àÏß∏ Î†àÏù¥Ïñ¥Ïùò NÎ∞∞ ÎÑìÍ≤å ÌïòÎäî Í≤ÉÍ≥º Í∞ôÏù¥ Î™®Îç∏ Íµ¨Ï°∞Î•º Î∞îÍæ∏Î©¥ Ïñ¥ÎñªÍ≤å Îê†Íπå?</li>
<li>Ï°∞Í∏àÏî© QuantizationÏùÑ Ìï† Ïàò ÏóÜÏùÑÍπå? (20% ‚Üí 40% ‚Üí ‚Ä¶ ‚Üí 100%)</li>
</ul>
<p>Í∞ïÏùòÏóêÏÑúÎäî ÌÅ¨Í≤å Ïñ∏Í∏âÌïòÏßÄ ÏïäÍ≥† Í∞Ñ ÎÇ¥Ïö©Îì§Ïù¥Îùº ÏÑ§Î™ÖÏùÑ ÌïòÏßÄÎäî ÏïäÍ≤†Îã§. Ìï¥Îãπ ÎÇ¥Ïö©Îì§ÏùÄ ÏûêÏÑ∏Ìïú ÎÇ¥Ïö©ÏùÑ ÏïåÍ≥†Ïã∂ÏúºÎ©¥ Í∞Å ÌååÌä∏Ïóê Ïñ∏Í∏âÎêú ÎÖºÎ¨∏ÏùÑ Ï∞∏Ï°∞ÌïòÍ∏∏!</p>
<section id="train-binarized-neural-networks-from-scratch" class="level3">
<h3 class="anchored" data-anchor-id="train-binarized-neural-networks-from-scratch">5.1 Train Binarized Neural Networks From Scratch</h3>
<ul>
<li>Straight-Through Estimator(STE)</li>
</ul>
<div class="quarto-figure quarto-figure-center">
<figure class="figure">
<p><img src="../../images/lec05/2/Untitled 16.png" class="img-fluid figure-img"></p>
<figcaption>Reference. MIT-TinyML-lecture06-Quantization-2</figcaption>
</figure>
</div>
<ul>
<li>Gradient pass straight to floating-point weights</li>
<li>Floating-point weight with in [-1, 1]</li>
<li>Reference. Binarized Neural Networks: Training Deep Neural Networks with Weights and Activations Constrained to +1 or&nbsp;‚àí1. [Courbariaux et al., Arxiv 2016]</li>
</ul>
</section>
<section id="quantization-aware-training-dorefa-net-with-low-bit-width-gradients" class="level3">
<h3 class="anchored" data-anchor-id="quantization-aware-training-dorefa-net-with-low-bit-width-gradients">5.2 Quantization-Aware Training: DoReFa-Net With Low Bit-Width Gradients</h3>
<div class="quarto-figure quarto-figure-center">
<figure class="figure">
<p><img src="../../images/lec05/2/Untitled 17.png" class="img-fluid figure-img"></p>
<figcaption>Reference. MIT-TinyML-lecture06-Quantization-2</figcaption>
</figure>
</div>
<ul>
<li><p>Gradient Quantization</p>
<p><span class="math display">\[
  Q(g) = 2 \cdot max(\lvert G \lvert) \cdot \Large[ \small quantize_k \Large( \small \dfrac{g}{2\cdot max(\lvert G \lvert)} + \dfrac{1}{2} + N(k) \Large ) \small -\dfrac{1}{2} \Large]\small
  \]</span> <span class="math display">\[
  where\ N(k)=\dfrac{\sigma}{2^k-1} and\ \sigma \thicksim Uniform(-0.5, 0.5)
  \]</span></p>
<ul>
<li>Noise function <span class="math inline">\(N(k)\)</span> is added to compensate the potential bias introduced by gradient quantization.</li>
</ul></li>
<li><p>Result</p>
<div class="quarto-figure quarto-figure-center">
<figure class="figure">
<p><img src="../../images/lec05/2/Untitled 18.png" class="img-fluid figure-img"></p>
<figcaption>Reference. DoReFa-Net: Training Low Bitwidth Convolutional Neural Networks with Low Bitwidth Gradients [Zhou&nbsp;<em>et al.</em>, arXiv 2016]</figcaption>
</figure>
</div></li>
</ul>
</section>
<section id="replace-the-activation-function-parameterized-clipping-activation-function" class="level3">
<h3 class="anchored" data-anchor-id="replace-the-activation-function-parameterized-clipping-activation-function">5.3 Replace the Activation Function: Parameterized Clipping Activation Function</h3>
<ul>
<li><p>The most common activation function ReLU is unbounded. The dynamic range of inputs becomes problematic for low bit-width quantization due to very limited range and resolution.</p></li>
<li><p>ReLU is replaced with hard-coded bounded activation functions: ReLU6, ReLU1, etc</p></li>
<li><p>The clipping value per layer can be learned as well: PACT(Parametrized Clipping Activation Function)</p>
<p><img src="../../images/lec05/2/Untitled 19.png" class="img-fluid" alt="Reference. PACT: Parameterized Clipping Activation for Quantized Neural Networks [Choi&nbsp;et al., arXiv 2018]"> <span class="math display">\[
  y=PACT(x;\alpha) = 0.5(\lvert x \lvert - \lvert x -\alpha \lvert + \alpha ) = \begin{dcases}
  0, &amp; x \in [-\infty, 0) \\
  x, &amp; x \in [0, \alpha) \\
  \alpha, &amp; x \in [\alpha, +\infty)
  \end{dcases}
  \]</span></p>
<p>The upper clipping value of the activation function is a trainable. With STE, the gradient is computed as</p>
<p><span class="math display">\[
  \dfrac{\partial Q(y)}{\partial \alpha} = \dfrac{\partial Q(y)}{\partial y} \cdot \dfrac{\partial y}{\partial \alpha} = \begin{dcases}
  0 &amp; x \in (-\infty, \alpha)\\
  1 &amp; x \in [\alpha, +\infty)\\
  \end{dcases}
  \]</span></p>
<p><span class="math display">\[
  \rightarrow
  \dfrac{\partial L}{\partial \alpha} = \dfrac{\partial L}{\partial Q(y)} \cdot \dfrac{\partial Q(y)}{\partial \alpha} = \begin{dcases}
  0 &amp; x \in (-\infty, \alpha)\\
  \frac{\partial L}{\partial Q(y)} &amp; x \in [\alpha, +\infty)\\
  \end{dcases}
  \]</span></p>
<p>The larger <span class="math inline">\(\alpha\)</span>, the more the parameterized clipping function resembles a ReLU function</p>
<ul>
<li><strong>To avoid large quantization errors due to a wide dynamic range <span class="math inline">\([0, \alpha]\)</span>, L2-regularizer for <span class="math inline">\(\alpha\)</span></strong> is included in the training loss function.</li>
</ul></li>
<li><p>Result</p>
<div class="quarto-figure quarto-figure-center">
<figure class="figure">
<p><img src="../../images/lec05/2/Untitled 20.png" class="img-fluid figure-img"></p>
<figcaption>Reference. PACT: Parameterized Clipping Activation for Quantized Neural Networks [Choi&nbsp;<em>et al.</em>, arXiv 2018]</figcaption>
</figure>
</div></li>
</ul>
</section>
<section id="modify-the-neural-network-architecture" class="level3">
<h3 class="anchored" data-anchor-id="modify-the-neural-network-architecture">5.4 Modify the Neural Network Architecture</h3>
<ol type="1">
<li><p>Widen the neural network to compensate for the loss of information due to quantization</p>
<p>ex. Double the channels, reduce the quantization precision</p>
<div class="quarto-figure quarto-figure-center">
<figure class="figure">
<p><img src="../../images/lec05/2/Untitled 21.png" class="img-fluid figure-img"></p>
<figcaption>Reference. WRPN: Wide Reduced-Precision Networks [Mishra&nbsp;<em>et al.</em>, ICLR 2018]</figcaption>
</figure>
</div></li>
<li><p>Replace a single floating-point convolution with multiple binary convolutions.</p>
<ul>
<li>Towards Accurate Binary Convolutional Neural Network [Lin&nbsp;<em>et al.</em>, NeurIPS 2017]</li>
<li>Quantization [Neural Network Distiller]</li>
</ul></li>
</ol>
</section>
<section id="no-quantization-on-first-and-last-layer" class="level3">
<h3 class="anchored" data-anchor-id="no-quantization-on-first-and-last-layer">5.5 No Quantization on First and Last Layer</h3>
<ul>
<li>Because it is more <strong>sensitive</strong> to quantization and <strong>small portion</strong> of the overall computation</li>
<li>Quantizing these layers to 8-bit integer does not reduce accuracy</li>
</ul>
</section>
<section id="iterative-quantization-incremental-network-quantization" class="level3">
<h3 class="anchored" data-anchor-id="iterative-quantization-incremental-network-quantization">5.6 Iterative Quantization: Incremental Network Quantization</h3>
<ul>
<li>Reference. Incremental Network Quantization: Towards Lossless CNNs with Low-precision Weights [Zhou&nbsp;<em>et al.</em>, ICLR 2017]</li>
</ul>
<div class="quarto-figure quarto-figure-center">
<figure class="figure">
<p><img src="../../images/lec05/2/Untitled 22.png" class="img-fluid figure-img"></p>
<figcaption>Reference. Incremental Network Quantization: Towards Lossless CNNs with Low-precision Weights [Zhou&nbsp;<em>et al.</em>, ICLR 2017]</figcaption>
</figure>
</div>
<ul>
<li>Setting
<ul>
<li>Weight quantization only</li>
<li>Quantize weights to <span class="math inline">\(2^n\)</span> for faster computation (<strong>bit shift</strong> instead of multiply)</li>
</ul></li>
<li>Algorithm
<ul>
<li>Start from a pre-trained fp32 model</li>
<li>For the remaining fp32 weights
<ul>
<li>Partition into two disjoint groups(e.g., according to magnitude)</li>
<li>Quantize the first group (higher magnitude), and re-train the other group to recover accuracy</li>
</ul></li>
<li>Repeat until all the weights are quantized (a popular stride is {50%, 75%, 87.5%, 100%})</li>
</ul>
<div class="quarto-figure quarto-figure-center">
<figure class="figure">
<p><img src="../../images/lec05/2/Iterative-Quantization.gif" class="img-fluid figure-img"></p>
<figcaption>Reference. MIT-TinyML-lecture06-Quantization-2</figcaption>
</figure>
</div></li>
</ul>
</section>
</section>
<section id="mixed-precision-quantization" class="level2">
<h2 class="anchored" data-anchor-id="mixed-precision-quantization">6. Mixed-precision quantization</h2>
<p>ÎßàÏßÄÎßâÏúºÎ°ú Î†àÏù¥Ïñ¥ÎßàÎã§ Quantization bitÎ•º Îã§Î•¥Í≤å Í∞ÄÏ†∏Í∞ÄÎ©¥ Ïñ¥Îñ®ÏßÄÏóê ÎåÄÌï¥ÏÑú Ïù¥ÏïºÍ∏∞ÌïúÎã§. ÌïòÏßÄÎßå Í≤ΩÏö∞Ïùò ÏàòÍ∞Ä 8bit Î≥¥Îã§ ÏûëÍ±∞ÎÇò Í∞ôÍ≤å QuantizationÏùÑ Ìï† Ïãú, weightÏôÄ activationÎ°ú Í≤ΩÏö∞Ïùò ÏàòÎ•º Í≥†Î†§Î•º ÌïúÎã§Î©¥ NÍ∞ú Î†àÏù¥Ïñ¥Ïóê ÎåÄÌï¥ÏÑú <span class="math inline">\((8 \times 8)^N\)</span>ÎùºÎäî Ïñ¥ÎßàÏñ¥ÎßàÌïú Í≤ΩÏö∞Ïùò ÏàòÍ∞Ä ÎÇòÏò®Îã§. Í∑∏Î¶¨Í≥† Ïù¥Ïóê ÎåÄÌï¥ÏÑúÎäî Îã§Ïùå ÌååÌä∏Ïóê ÎÇòÍ∞à Neural Architecture Search(NAS) ÏóêÏÑú Îã§Î£∞ ÎìØ Ïã∂Îã§.</p>
<section id="uniform-quantization" class="level3">
<h3 class="anchored" data-anchor-id="uniform-quantization">6.1 Uniform Quantization</h3>
<p><img src="../../images/lec05/2/Untitled 23.png" class="img-fluid"></p>
</section>
<section id="mixed-precision-quantization-1" class="level3">
<h3 class="anchored" data-anchor-id="mixed-precision-quantization-1">6.2 Mixed-precision Quantization</h3>
<p><img src="../../images/lec05/2/Untitled 24.png" class="img-fluid"></p>
</section>
<section id="huge-design-space-and-solution-design-automation" class="level3">
<h3 class="anchored" data-anchor-id="huge-design-space-and-solution-design-automation">6.3 Huge Design Space and Solution: Design Automation</h3>
<p><img src="../../images/lec05/2/Untitled 25.png" class="img-fluid"></p>
<ul>
<li><p>Design Space: Each of Choices(8x8=64) ‚Üí <span class="math inline">\(64^n\)</span></p>
<div class="quarto-figure quarto-figure-center">
<figure class="figure">
<p><img src="../../images/lec05/2/Untitled 26.png" class="img-fluid figure-img"></p>
<figcaption>Reference. HAQ: Hardware-Aware Automated Quantization with Mixed Precision [Wang&nbsp;<em>et al.</em>, CVPR 2019]</figcaption>
</figure>
</div></li>
<li><p>Result in Mixed-Precision Quantized MobileNetV1</p>
<div class="quarto-figure quarto-figure-center">
<figure class="figure">
<p><img src="../../images/lec05/2/Untitled 27.png" class="img-fluid figure-img"></p>
<figcaption>Reference. HAQ: Hardware-Aware Automated Quantization with Mixed Precision [Wang&nbsp;<em>et al.</em>, CVPR 2019]</figcaption>
</figure>
</div>
<ul>
<li>This paper compares with Model size, Latency and Energy</li>
</ul></li>
</ul>
<p>Í∞ÄÏû• ÎßàÏßÄÎßâÏóê Ïñ∏Í∏âÌïòÎäî EdgeÏôÄ ÌÅ¥ÎùºÏö∞ÎìúÏóêÏÑúÎäî Convolution Î†àÏù¥Ïñ¥Ïùò Ï¢ÖÎ•ò Ï§ë ÎçîÌïòÍ≥† Îçú QuantizationÌïòÎäî Î†àÏù¥Ïñ¥Í∞Ä Í∞ÅÍ∞Å depthwiseÏôÄ pointwiseÎ°ú Îã§Î•¥Îã§Í≥† Ïù¥ÏïºÍ∏∞ÌïúÎã§. Ïù¥ ÎÇ¥Ïö©Ïóê ÎåÄÌï¥ÏÑú Îçî ÏûêÏÑ∏Ìûà Ïù¥Ìï¥ÌïòÍ∏∞ ÏúÑÌï¥ÏÑúÎäî ÏïÑÎßàÎèÑ NASÎ°ú ÎÑòÏñ¥Í∞ÄÎ¥êÏïº Ïïå Ïàò ÏûàÏßÄ ÏïäÏùÑÍπå Ïã∂Îã§.</p>
<ul>
<li><p>Quantization Policy for Edge and Cloud</p>
<div class="quarto-figure quarto-figure-center">
<figure class="figure">
<p><img src="../../images/lec05/2/Untitled 28.png" class="img-fluid figure-img"></p>
<figcaption>Reference. HAQ: Hardware-Aware Automated Quantization with Mixed Precision [Wang&nbsp;<em>et al.</em>, CVPR 2019]</figcaption>
</figure>
</div></li>
</ul>
</section>
</section>
<section id="reference" class="level2">
<h2 class="anchored" data-anchor-id="reference">7. Reference</h2>
<ul>
<li><a href="https://efficientml.ai/">TinyML and Efficient Deep Learning Computing on MIT HAN LAB</a></li>
<li><a href="https://www.youtube.com/playlist?list=PL80kAHvQbh-ocildRaxjjBy6MR1ZsNCU7">Youtube for TinyML and Efficient Deep Learning Computing on MIT HAN LAB</a></li>
<li><a href="https://arxiv.org/abs/1510.00149">Deep Compression [Han&nbsp;et al., ICLR 2016]</a></li>
<li><a href="https://arxiv.org/pdf/1712.05877.pdf">Quantization and Training of Neural Networks for Efficient Integer-Arithmetic-Only Inference [Jacob&nbsp;et al., CVPR 2018]</a></li>
<li><a href="https://arxiv.org/pdf/2302.08007.pdf">With Shared Microexponents, A Little Shifting Goes a Long Way [Bita Rouhani et al.]</a></li>
<li><a href="https://arxiv.org/pdf/1906.04721.pdf">Data-Free Quantization Through Weight Equalization and Bias Correction [Markus et al., ICCV 2019]</a></li>
<li><a href="https://arxiv.org/pdf/1308.3432.pdf">Estimating or Propagating Gradients Through Stochastic Neurons for Conditional Computation [Bengio, arXiv 2013]</a></li>
<li><a href="https://arxiv.org/pdf/1602.02830.pdf">Binarized Neural Networks: Training Deep Neural Networks with Weights and Activations Constrained to +1 or&nbsp;‚àí1. [Courbariaux&nbsp;et al., Arxiv 2016]</a></li>
<li><a href="https://arxiv.org/pdf/1603.05279.pdf">XNOR-Net: ImageNet Classification using Binary Convolutional Neural Networks [Rastegari&nbsp;et al., ECCV 2016]</a></li>
<li><a href="https://arxiv.org/pdf/1605.04711.pdf">Ternary Weight Networks [Li&nbsp;et al., Arxiv 2016]</a></li>
<li><a href="https://arxiv.org/pdf/1612.01064.pdf">Trained Ternary Quantization [Zhu&nbsp;et al., ICLR 2017]</a></li>
<li><a href="https://arxiv.org/pdf/1606.06160.pdf">DoReFa-Net: Training Low Bitwidth Convolutional Neural Networks with Low Bitwidth Gradients [Zhou&nbsp;et al., arXiv 2016]</a></li>
<li><a href="https://arxiv.org/pdf/1709.01134.pdf">WRPN: Wide Reduced-Precision Networks [Mishra&nbsp;et al., ICLR 2018]</a></li>
<li><a href="https://arxiv.org/pdf/1805.06085.pdf">PACT: Parameterized Clipping Activation for Quantized Neural Networks [Choi&nbsp;et al., arXiv 2018]</a></li>
<li><a href="https://arxiv.org/pdf/1811.08886.pdf">HAQ: Hardware-Aware Automated Quantization with Mixed Precision [Wang&nbsp;et al., CVPR 2019]</a></li>
</ul>


</section>

</main> <!-- /main -->
<script id="quarto-html-after-body" type="application/javascript">
window.document.addEventListener("DOMContentLoaded", function (event) {
  const toggleBodyColorMode = (bsSheetEl) => {
    const mode = bsSheetEl.getAttribute("data-mode");
    const bodyEl = window.document.querySelector("body");
    if (mode === "dark") {
      bodyEl.classList.add("quarto-dark");
      bodyEl.classList.remove("quarto-light");
    } else {
      bodyEl.classList.add("quarto-light");
      bodyEl.classList.remove("quarto-dark");
    }
  }
  const toggleBodyColorPrimary = () => {
    const bsSheetEl = window.document.querySelector("link#quarto-bootstrap");
    if (bsSheetEl) {
      toggleBodyColorMode(bsSheetEl);
    }
  }
  toggleBodyColorPrimary();  
  const icon = "Óßã";
  const anchorJS = new window.AnchorJS();
  anchorJS.options = {
    placement: 'right',
    icon: icon
  };
  anchorJS.add('.anchored');
  const isCodeAnnotation = (el) => {
    for (const clz of el.classList) {
      if (clz.startsWith('code-annotation-')) {                     
        return true;
      }
    }
    return false;
  }
  const clipboard = new window.ClipboardJS('.code-copy-button', {
    text: function(trigger) {
      const codeEl = trigger.previousElementSibling.cloneNode(true);
      for (const childEl of codeEl.children) {
        if (isCodeAnnotation(childEl)) {
          childEl.remove();
        }
      }
      return codeEl.innerText;
    }
  });
  clipboard.on('success', function(e) {
    // button target
    const button = e.trigger;
    // don't keep focus
    button.blur();
    // flash "checked"
    button.classList.add('code-copy-button-checked');
    var currentTitle = button.getAttribute("title");
    button.setAttribute("title", "Copied!");
    let tooltip;
    if (window.bootstrap) {
      button.setAttribute("data-bs-toggle", "tooltip");
      button.setAttribute("data-bs-placement", "left");
      button.setAttribute("data-bs-title", "Copied!");
      tooltip = new bootstrap.Tooltip(button, 
        { trigger: "manual", 
          customClass: "code-copy-button-tooltip",
          offset: [0, -8]});
      tooltip.show();    
    }
    setTimeout(function() {
      if (tooltip) {
        tooltip.hide();
        button.removeAttribute("data-bs-title");
        button.removeAttribute("data-bs-toggle");
        button.removeAttribute("data-bs-placement");
      }
      button.setAttribute("title", currentTitle);
      button.classList.remove('code-copy-button-checked');
    }, 1000);
    // clear code selection
    e.clearSelection();
  });
  function tippyHover(el, contentFn, onTriggerFn, onUntriggerFn) {
    const config = {
      allowHTML: true,
      maxWidth: 500,
      delay: 100,
      arrow: false,
      appendTo: function(el) {
          return el.parentElement;
      },
      interactive: true,
      interactiveBorder: 10,
      theme: 'quarto',
      placement: 'bottom-start',
    };
    if (contentFn) {
      config.content = contentFn;
    }
    if (onTriggerFn) {
      config.onTrigger = onTriggerFn;
    }
    if (onUntriggerFn) {
      config.onUntrigger = onUntriggerFn;
    }
    window.tippy(el, config); 
  }
  const noterefs = window.document.querySelectorAll('a[role="doc-noteref"]');
  for (var i=0; i<noterefs.length; i++) {
    const ref = noterefs[i];
    tippyHover(ref, function() {
      // use id or data attribute instead here
      let href = ref.getAttribute('data-footnote-href') || ref.getAttribute('href');
      try { href = new URL(href).hash; } catch {}
      const id = href.replace(/^#\/?/, "");
      const note = window.document.getElementById(id);
      return note.innerHTML;
    });
  }
  const xrefs = window.document.querySelectorAll('a.quarto-xref');
  const processXRef = (id, note) => {
    // Strip column container classes
    const stripColumnClz = (el) => {
      el.classList.remove("page-full", "page-columns");
      if (el.children) {
        for (const child of el.children) {
          stripColumnClz(child);
        }
      }
    }
    stripColumnClz(note)
    if (id === null || id.startsWith('sec-')) {
      // Special case sections, only their first couple elements
      const container = document.createElement("div");
      if (note.children && note.children.length > 2) {
        container.appendChild(note.children[0].cloneNode(true));
        for (let i = 1; i < note.children.length; i++) {
          const child = note.children[i];
          if (child.tagName === "P" && child.innerText === "") {
            continue;
          } else {
            container.appendChild(child.cloneNode(true));
            break;
          }
        }
        if (window.Quarto?.typesetMath) {
          window.Quarto.typesetMath(container);
        }
        return container.innerHTML
      } else {
        if (window.Quarto?.typesetMath) {
          window.Quarto.typesetMath(note);
        }
        return note.innerHTML;
      }
    } else {
      // Remove any anchor links if they are present
      const anchorLink = note.querySelector('a.anchorjs-link');
      if (anchorLink) {
        anchorLink.remove();
      }
      if (window.Quarto?.typesetMath) {
        window.Quarto.typesetMath(note);
      }
      // TODO in 1.5, we should make sure this works without a callout special case
      if (note.classList.contains("callout")) {
        return note.outerHTML;
      } else {
        return note.innerHTML;
      }
    }
  }
  for (var i=0; i<xrefs.length; i++) {
    const xref = xrefs[i];
    tippyHover(xref, undefined, function(instance) {
      instance.disable();
      let url = xref.getAttribute('href');
      let hash = undefined; 
      if (url.startsWith('#')) {
        hash = url;
      } else {
        try { hash = new URL(url).hash; } catch {}
      }
      if (hash) {
        const id = hash.replace(/^#\/?/, "");
        const note = window.document.getElementById(id);
        if (note !== null) {
          try {
            const html = processXRef(id, note.cloneNode(true));
            instance.setContent(html);
          } finally {
            instance.enable();
            instance.show();
          }
        } else {
          // See if we can fetch this
          fetch(url.split('#')[0])
          .then(res => res.text())
          .then(html => {
            const parser = new DOMParser();
            const htmlDoc = parser.parseFromString(html, "text/html");
            const note = htmlDoc.getElementById(id);
            if (note !== null) {
              const html = processXRef(id, note);
              instance.setContent(html);
            } 
          }).finally(() => {
            instance.enable();
            instance.show();
          });
        }
      } else {
        // See if we can fetch a full url (with no hash to target)
        // This is a special case and we should probably do some content thinning / targeting
        fetch(url)
        .then(res => res.text())
        .then(html => {
          const parser = new DOMParser();
          const htmlDoc = parser.parseFromString(html, "text/html");
          const note = htmlDoc.querySelector('main.content');
          if (note !== null) {
            // This should only happen for chapter cross references
            // (since there is no id in the URL)
            // remove the first header
            if (note.children.length > 0 && note.children[0].tagName === "HEADER") {
              note.children[0].remove();
            }
            const html = processXRef(null, note);
            instance.setContent(html);
          } 
        }).finally(() => {
          instance.enable();
          instance.show();
        });
      }
    }, function(instance) {
    });
  }
      let selectedAnnoteEl;
      const selectorForAnnotation = ( cell, annotation) => {
        let cellAttr = 'data-code-cell="' + cell + '"';
        let lineAttr = 'data-code-annotation="' +  annotation + '"';
        const selector = 'span[' + cellAttr + '][' + lineAttr + ']';
        return selector;
      }
      const selectCodeLines = (annoteEl) => {
        const doc = window.document;
        const targetCell = annoteEl.getAttribute("data-target-cell");
        const targetAnnotation = annoteEl.getAttribute("data-target-annotation");
        const annoteSpan = window.document.querySelector(selectorForAnnotation(targetCell, targetAnnotation));
        const lines = annoteSpan.getAttribute("data-code-lines").split(",");
        const lineIds = lines.map((line) => {
          return targetCell + "-" + line;
        })
        let top = null;
        let height = null;
        let parent = null;
        if (lineIds.length > 0) {
            //compute the position of the single el (top and bottom and make a div)
            const el = window.document.getElementById(lineIds[0]);
            top = el.offsetTop;
            height = el.offsetHeight;
            parent = el.parentElement.parentElement;
          if (lineIds.length > 1) {
            const lastEl = window.document.getElementById(lineIds[lineIds.length - 1]);
            const bottom = lastEl.offsetTop + lastEl.offsetHeight;
            height = bottom - top;
          }
          if (top !== null && height !== null && parent !== null) {
            // cook up a div (if necessary) and position it 
            let div = window.document.getElementById("code-annotation-line-highlight");
            if (div === null) {
              div = window.document.createElement("div");
              div.setAttribute("id", "code-annotation-line-highlight");
              div.style.position = 'absolute';
              parent.appendChild(div);
            }
            div.style.top = top - 2 + "px";
            div.style.height = height + 4 + "px";
            div.style.left = 0;
            let gutterDiv = window.document.getElementById("code-annotation-line-highlight-gutter");
            if (gutterDiv === null) {
              gutterDiv = window.document.createElement("div");
              gutterDiv.setAttribute("id", "code-annotation-line-highlight-gutter");
              gutterDiv.style.position = 'absolute';
              const codeCell = window.document.getElementById(targetCell);
              const gutter = codeCell.querySelector('.code-annotation-gutter');
              gutter.appendChild(gutterDiv);
            }
            gutterDiv.style.top = top - 2 + "px";
            gutterDiv.style.height = height + 4 + "px";
          }
          selectedAnnoteEl = annoteEl;
        }
      };
      const unselectCodeLines = () => {
        const elementsIds = ["code-annotation-line-highlight", "code-annotation-line-highlight-gutter"];
        elementsIds.forEach((elId) => {
          const div = window.document.getElementById(elId);
          if (div) {
            div.remove();
          }
        });
        selectedAnnoteEl = undefined;
      };
        // Handle positioning of the toggle
    window.addEventListener(
      "resize",
      throttle(() => {
        elRect = undefined;
        if (selectedAnnoteEl) {
          selectCodeLines(selectedAnnoteEl);
        }
      }, 10)
    );
    function throttle(fn, ms) {
    let throttle = false;
    let timer;
      return (...args) => {
        if(!throttle) { // first call gets through
            fn.apply(this, args);
            throttle = true;
        } else { // all the others get throttled
            if(timer) clearTimeout(timer); // cancel #2
            timer = setTimeout(() => {
              fn.apply(this, args);
              timer = throttle = false;
            }, ms);
        }
      };
    }
      // Attach click handler to the DT
      const annoteDls = window.document.querySelectorAll('dt[data-target-cell]');
      for (const annoteDlNode of annoteDls) {
        annoteDlNode.addEventListener('click', (event) => {
          const clickedEl = event.target;
          if (clickedEl !== selectedAnnoteEl) {
            unselectCodeLines();
            const activeEl = window.document.querySelector('dt[data-target-cell].code-annotation-active');
            if (activeEl) {
              activeEl.classList.remove('code-annotation-active');
            }
            selectCodeLines(clickedEl);
            clickedEl.classList.add('code-annotation-active');
          } else {
            // Unselect the line
            unselectCodeLines();
            clickedEl.classList.remove('code-annotation-active');
          }
        });
      }
  const findCites = (el) => {
    const parentEl = el.parentElement;
    if (parentEl) {
      const cites = parentEl.dataset.cites;
      if (cites) {
        return {
          el,
          cites: cites.split(' ')
        };
      } else {
        return findCites(el.parentElement)
      }
    } else {
      return undefined;
    }
  };
  var bibliorefs = window.document.querySelectorAll('a[role="doc-biblioref"]');
  for (var i=0; i<bibliorefs.length; i++) {
    const ref = bibliorefs[i];
    const citeInfo = findCites(ref);
    if (citeInfo) {
      tippyHover(citeInfo.el, function() {
        var popup = window.document.createElement('div');
        citeInfo.cites.forEach(function(cite) {
          var citeDiv = window.document.createElement('div');
          citeDiv.classList.add('hanging-indent');
          citeDiv.classList.add('csl-entry');
          var biblioDiv = window.document.getElementById('ref-' + cite);
          if (biblioDiv) {
            citeDiv.innerHTML = biblioDiv.innerHTML;
          }
          popup.appendChild(citeDiv);
        });
        return popup.innerHTML;
      });
    }
  }
});
</script>
<script src="https://giscus.app/client.js" data-repo="TINYML-KOR/blog" data-repo-id="R_kgDOLC9iGA" data-category="General" data-category-id="DIC_kwDOLC9iGM4Cc7eP" data-mapping="title" data-reactions-enabled="1" data-emit-metadata="0" data-input-position="top" data-theme="light" data-lang="en" crossorigin="anonymous" async="">
</script>
<input type="hidden" id="giscus-base-theme" value="light">
<input type="hidden" id="giscus-alt-theme" value="dark">
</div> <!-- /content -->




</body></html>