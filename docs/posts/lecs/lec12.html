<!DOCTYPE html>
<html xmlns="http://www.w3.org/1999/xhtml" lang="en" xml:lang="en"><head>

<meta charset="utf-8">
<meta name="generator" content="quarto-1.4.554">

<meta name="viewport" content="width=device-width, initial-scale=1.0, user-scalable=yes">

<meta name="author" content="Gijeong Seong">
<meta name="dcterms.date" content="2024-04-19">
<meta name="description" content="Transformer and LLM">

<title>TinyML KOR - üßë‚Äçüè´ Lecture 12-13</title>
<style>
code{white-space: pre-wrap;}
span.smallcaps{font-variant: small-caps;}
div.columns{display: flex; gap: min(4vw, 1.5em);}
div.column{flex: auto; overflow-x: auto;}
div.hanging-indent{margin-left: 1.5em; text-indent: -1.5em;}
ul.task-list{list-style: none;}
ul.task-list li input[type="checkbox"] {
  width: 0.8em;
  margin: 0 0.8em 0.2em -1em; /* quarto-specific, see https://github.com/quarto-dev/quarto-cli/issues/4556 */ 
  vertical-align: middle;
}
</style>


<script src="../../site_libs/quarto-nav/quarto-nav.js"></script>
<script src="../../site_libs/quarto-nav/headroom.min.js"></script>
<script src="../../site_libs/clipboard/clipboard.min.js"></script>
<script src="../../site_libs/quarto-search/autocomplete.umd.js"></script>
<script src="../../site_libs/quarto-search/fuse.min.js"></script>
<script src="../../site_libs/quarto-search/quarto-search.js"></script>
<meta name="quarto:offset" content="../../">
<script src="../../site_libs/quarto-html/quarto.js"></script>
<script src="../../site_libs/quarto-html/popper.min.js"></script>
<script src="../../site_libs/quarto-html/tippy.umd.min.js"></script>
<script src="../../site_libs/quarto-html/anchor.min.js"></script>
<link href="../../site_libs/quarto-html/tippy.css" rel="stylesheet">
<link href="../../site_libs/quarto-html/quarto-syntax-highlighting.css" rel="stylesheet" id="quarto-text-highlighting-styles">
<script src="../../site_libs/bootstrap/bootstrap.min.js"></script>
<link href="../../site_libs/bootstrap/bootstrap-icons.css" rel="stylesheet">
<link href="../../site_libs/bootstrap/bootstrap.min.css" rel="stylesheet" id="quarto-bootstrap" data-mode="light">
<script id="quarto-search-options" type="application/json">{
  "location": "navbar",
  "copy-button": false,
  "collapse-after": 3,
  "panel-placement": "end",
  "type": "overlay",
  "limit": 50,
  "keyboard-shortcut": [
    "f",
    "/",
    "s"
  ],
  "show-item-context": false,
  "language": {
    "search-no-results-text": "No results",
    "search-matching-documents-text": "matching documents",
    "search-copy-link-title": "Copy link to search",
    "search-hide-matches-text": "Hide additional matches",
    "search-more-match-text": "more match in this document",
    "search-more-matches-text": "more matches in this document",
    "search-clear-button-title": "Clear",
    "search-text-placeholder": "",
    "search-detached-cancel-button-title": "Cancel",
    "search-submit-button-title": "Submit",
    "search-label": "Search"
  }
}</script>


<link rel="stylesheet" href="../../styles.css">
</head>

<body class="nav-fixed">

<div id="quarto-search-results"></div>
  <header id="quarto-header" class="headroom fixed-top">
    <nav class="navbar navbar-expand-lg " data-bs-theme="dark">
      <div class="navbar-container container-fluid">
      <div class="navbar-brand-container mx-auto">
    <a class="navbar-brand" href="../../index.html">
    <span class="navbar-title">TinyML KOR</span>
    </a>
  </div>
            <div id="quarto-search" class="" title="Search"></div>
          <button class="navbar-toggler" type="button" data-bs-toggle="collapse" data-bs-target="#navbarCollapse" aria-controls="navbarCollapse" aria-expanded="false" aria-label="Toggle navigation" onclick="if (window.quartoToggleHeadroom) { window.quartoToggleHeadroom(); }">
  <span class="navbar-toggler-icon"></span>
</button>
          <div class="collapse navbar-collapse" id="navbarCollapse">
            <ul class="navbar-nav navbar-nav-scroll ms-auto">
  <li class="nav-item">
    <a class="nav-link" href="../../about.html"> 
<span class="menu-text">TinyML Study Group</span></a>
  </li>  
  <li class="nav-item compact">
    <a class="nav-link" href="https://github.com/"> <i class="bi bi-github" role="img">
</i> 
<span class="menu-text"></span></a>
  </li>  
</ul>
          </div> <!-- /navcollapse -->
          <div class="quarto-navbar-tools">
</div>
      </div> <!-- /container-fluid -->
    </nav>
</header>
<!-- content -->
<div id="quarto-content" class="quarto-container page-columns page-rows-contents page-layout-article page-navbar">
<!-- sidebar -->
<!-- margin-sidebar -->
    <div id="quarto-margin-sidebar" class="sidebar margin-sidebar">
        <nav id="TOC" role="doc-toc" class="toc-active">
    <h2 id="toc-title">On this page</h2>
   
  <ul>
  <li><a href="#transformer-basics" id="toc-transformer-basics" class="nav-link active" data-scroll-target="#transformer-basics">1. Transformer basics</a></li>
  <li><a href="#transformer-design-variants" id="toc-transformer-design-variants" class="nav-link" data-scroll-target="#transformer-design-variants">2. Transformer design variants</a>
  <ul class="collapse">
  <li><a href="#absolute-positional-encoding---relative-positional-encoding" id="toc-absolute-positional-encoding---relative-positional-encoding" class="nav-link" data-scroll-target="#absolute-positional-encoding---relative-positional-encoding">Absolute positional encoding -&gt; Relative positional encoding</a>
  <ul class="collapse">
  <li><a href="#attention-with-linear-biases-alibi" id="toc-attention-with-linear-biases-alibi" class="nav-link" data-scroll-target="#attention-with-linear-biases-alibi">Attention with Linear Biases (ALiBi)</a></li>
  <li><a href="#rotary-positional-embedding-rope" id="toc-rotary-positional-embedding-rope" class="nav-link" data-scroll-target="#rotary-positional-embedding-rope">Rotary Positional Embedding (RoPE)</a></li>
  </ul></li>
  <li><a href="#kv-cache-optimizationsmulti-head-attention-mha---multi-query-attention-mqa---grouped-query-attentiongqa" id="toc-kv-cache-optimizationsmulti-head-attention-mha---multi-query-attention-mqa---grouped-query-attentiongqa" class="nav-link" data-scroll-target="#kv-cache-optimizationsmulti-head-attention-mha---multi-query-attention-mqa---grouped-query-attentiongqa">KV cache optimizations(Multi-Head Attention (MHA) -&gt; Multi-Query Attention (MQA) -&gt; Grouped-Query Attention(GQA)</a></li>
  <li><a href="#ffn-glu" id="toc-ffn-glu" class="nav-link" data-scroll-target="#ffn-glu">FFN-&gt;GLU</a></li>
  </ul></li>
  <li><a href="#large-language-modelsllms" id="toc-large-language-modelsllms" class="nav-link" data-scroll-target="#large-language-modelsllms">3. Large language models(LLMs)</a></li>
  <li><a href="#advanced-topics-multi-modal-llm" id="toc-advanced-topics-multi-modal-llm" class="nav-link" data-scroll-target="#advanced-topics-multi-modal-llm">4. Advanced topics, multi-modal LLM</a></li>
  <li><a href="#efficient-inference-algorithms-for-llms" id="toc-efficient-inference-algorithms-for-llms" class="nav-link" data-scroll-target="#efficient-inference-algorithms-for-llms">5. Efficient inference algorithms for LLMs</a>
  <ul class="collapse">
  <li><a href="#quantization-smoothquant-awq-tinychat" id="toc-quantization-smoothquant-awq-tinychat" class="nav-link" data-scroll-target="#quantization-smoothquant-awq-tinychat">5.1. Quantization: SmoothQuant, AWQ, TinyChat</a></li>
  <li><a href="#pruningsparsity-spatten-h2o-moe" id="toc-pruningsparsity-spatten-h2o-moe" class="nav-link" data-scroll-target="#pruningsparsity-spatten-h2o-moe">5.2. Pruning/sparsity: SpAtten, H2O, MoE</a></li>
  </ul></li>
  <li><a href="#efficient-inference-systems-for-llms" id="toc-efficient-inference-systems-for-llms" class="nav-link" data-scroll-target="#efficient-inference-systems-for-llms">6. Efficient inference systems for LLMs</a>
  <ul class="collapse">
  <li><a href="#vllmpaged-attention" id="toc-vllmpaged-attention" class="nav-link" data-scroll-target="#vllmpaged-attention">6.1. vLLM(Paged Attention)</a></li>
  <li><a href="#streamingllm" id="toc-streamingllm" class="nav-link" data-scroll-target="#streamingllm">6.2. StreamingLLM</a></li>
  <li><a href="#flashattention" id="toc-flashattention" class="nav-link" data-scroll-target="#flashattention">6.3. FlashAttention</a></li>
  <li><a href="#speculative-decoding" id="toc-speculative-decoding" class="nav-link" data-scroll-target="#speculative-decoding">6.4. Speculative decoding</a></li>
  </ul></li>
  <li><a href="#efficient-fine-tuning-for-llms" id="toc-efficient-fine-tuning-for-llms" class="nav-link" data-scroll-target="#efficient-fine-tuning-for-llms">7. Efficient fine-tuning for LLMs</a>
  <ul class="collapse">
  <li><a href="#loraqlora" id="toc-loraqlora" class="nav-link" data-scroll-target="#loraqlora">7.1. LoRA/QLoRA</a></li>
  <li><a href="#adapter" id="toc-adapter" class="nav-link" data-scroll-target="#adapter">7.2. Adapter</a></li>
  <li><a href="#prompt-tuning" id="toc-prompt-tuning" class="nav-link" data-scroll-target="#prompt-tuning">7.3. Prompt Tuning</a></li>
  </ul></li>
  </ul>
</nav>
    </div>
<!-- main -->
<main class="content" id="quarto-document-content">

<header id="title-block-header" class="quarto-title-block default">
<div class="quarto-title">
<h1 class="title">üßë‚Äçüè´ Lecture 12-13</h1>
  <div class="quarto-categories">
    <div class="quarto-category">lecture</div>
    <div class="quarto-category">Transformer</div>
    <div class="quarto-category">LLM</div>
  </div>
  </div>

<div>
  <div class="description">
    Transformer and LLM
  </div>
</div>


<div class="quarto-title-meta">

    <div>
    <div class="quarto-title-meta-heading">Author</div>
    <div class="quarto-title-meta-contents">
             <p>Gijeong Seong </p>
          </div>
  </div>
    
    <div>
    <div class="quarto-title-meta-heading">Published</div>
    <div class="quarto-title-meta-contents">
      <p class="date">April 19, 2024</p>
    </div>
  </div>
  
    
  </div>
  


</header>


<style>
p {
  line-height: 2; /* Adjust the line-height property as needed */
  text-align: justify;
  text-justify: inter-word; /* Adjust the justification mode as needed */
}

.image-container {
  text-align: center; /* Center-align the contents horizontally */
}

.centered-text {
  text-align: center; /* Center-align the text within the container */
}
</style>
<p>Ïù¥Î≤à Í∏ÄÏóêÏÑúÎäî ÌòÑÎåÄ NLPÏùò ÌïµÏã¨ ÎèÑÍµ¨Ïù∏ transformerÏôÄ LLM, Í∑∏Î¶¨Í≥† Ïù¥Îì§ÏùÑ ÏµúÏ†ÅÌôîÌïòÎäî Ïó¨Îü¨ Î∞©Î≤ïÏóê ÎåÄÌï¥ÏÑú ÏÜåÍ∞úÌïúÎã§. transformerÎ•º ÏµúÏ†ÅÌôî ÌïòÍ∏∞ ÏúÑÌïú Î∞©Î≤ïÏúºÎ°ú positional embeddingÏùò Î≥ÄÌôîÎÇò llama2Ïóê ÏÇ¨Ïö©Îêú grouped-query-attentionÎì±ÏùÑ Îã§Î£¨Îã§. Îçî ÎÇòÏïÑÍ∞Ä, LLMsÏùò Ìö®Ïú®Ï†ÅÏù∏ Ï∂îÎ°†(inference)ÏôÄ Ï°∞Ï†ï(fine-tuning) ÏïåÍ≥†Î¶¨Ï¶òÍ≥º ÏãúÏä§ÌÖúÏóê ÎåÄÌï¥ÏÑúÎèÑ Îã§Î£¨Îã§. inferenceÏóê ÎåÄÌï¥ÏÑúÎäî vLLM, StreamingLLM, fine-tuningÏùÄ LoRA, QLoRA, AdapterÍ∏∞Î≤ï Îì±ÏùÑ ÏÜåÍ∞úÌïúÎã§.</p>
<section id="transformer-basics" class="level1">
<h1>1. Transformer basics</h1>
<p>Í∞ïÏùòÏóêÏÑúÎäî transformer Íµ¨Ï°∞Ïùò tokenizer, encoding, attentionÎì±Ïóê ÎåÄÌï¥ Í∞ÑÎûµÌûà ÏÜåÍ∞úÌïòÍ≥† ÏûàÏßÄÎßå Ïù¥ Í∏ÄÏùÄ transformer/LLMÏùò ÏµúÏ†ÅÌôîÏóê ÎåÄÌï¥ Îã§Î£®Í≥† ÏûàÍ≥†, transformer ÏûêÏ≤¥Ïùò Íµ¨Ï°∞Î•º Îã§Î£®Î©¥ Í∏ÄÏù¥ ÎÑàÎ¨¥ Í∏∏Ïñ¥Ïßà Í≤É Í∞ôÏïÑÏÑú ÏûêÏÑ∏Ìïú Íµ¨Ï°∞Îäî ÏïÑÎûòÏùò Îëê ÎßÅÌÅ¨Î•º Ï∞∏Í≥† Î∂ÄÌÉÅÎìúÎ¶ΩÎãàÎã§</p>
<p>https://wikidocs.net/31379</p>
<p>https://blogs.nvidia.co.kr/blog/what-is-a-transformer-model/</p>
</section>
<section id="transformer-design-variants" class="level1">
<h1>2. Transformer design variants</h1>
<p>Ïù¥Î≤à Ïû•ÏóêÏÑúÎäî ÏõêÎ≥∏ transformer(attention is all you need) Ïù¥ÌõÑÏóê transformerÎ•º Î∞úÏ†ÑÏãúÌÇ® Ïó¨Îü¨ Í∏∞Î≤ïÏóê ÎåÄÌï¥ Îã§Î£¨Îã§.</p>
<section id="absolute-positional-encoding---relative-positional-encoding" class="level2">
<h2 class="anchored" data-anchor-id="absolute-positional-encoding---relative-positional-encoding">Absolute positional encoding -&gt; Relative positional encoding</h2>
<p>ÏõêÎ≥∏ transformer Î™®Îç∏ÏóêÏÑúÎäî positional embeddingÏúºÎ°ú sinusoid embeddingÏùÑ ÏÇ¨Ïö©ÌïúÎã§. Ïù¥Îäî positionÎßàÎã§ ÎèÖÎ¶ΩÏ†ÅÏù¥Î©¥ÏÑúÎèÑ Ïó∞ÏÜçÏ†ÅÏù∏ embedding vectorÎ•º ÎßåÎì§Ïñ¥ÎÇ∏Îã§. <img src="../../images/lec12/Pasted image 20240502231706.png" class="img-fluid"></p>
<p>Í∑∏Îü¨ÎÇò Ïù¥Îü∞ indexÏóê ÏùòÏ°¥Ï†ÅÏù∏ absolute positional encoding Î∞©ÏãùÏóêÎäî, trainingÏ§ëÏóê Î≥¥ÏßÄ Î™ªÌïú Í∏∏Ïù¥Î•º ÎåÄÏùëÌïòÍ∏∞ Ïñ¥Î†µÎã§Îäî Î¨∏Ï†úÏ†êÏù¥ ÏûàÎã§. ÏòàÎ•º Îì§Ïñ¥, 250 tokenÍπåÏßÄÎßå ÌïôÏäµÌñàÎäîÎç∞ 251 tokenÏùò Îç∞Ïù¥ÌÑ∞Í∞Ä Îì§Ïñ¥Ïò§Îäî ÏÉÅÌô© ÎßêÏù¥Îã§.</p>
<p>Relative positional encodingÏùÑ ÏÇ¨Ïö©ÌïòÎ©¥ train short, test longÏùÑ Îã¨ÏÑ±Ìï† Ïàò ÏûàÎã§. Ïù¥Ïô∏ÏóêÎèÑ absolute positional encodingÏùÄ ÏúÑÏπò Ï†ïÎ≥¥Î•º input embeddingÏóê ÎçîÌï¥ Q/K/V Ï†ÑÏ≤¥Ïóê ÏòÅÌñ•ÏùÑ ÎØ∏ÏπòÏßÄÎßå, relative positional encodingÏùÄ Q,KÏóê biasÎ•º ÎçîÌïòÎäî Î∞©ÏãùÏúºÎ°ú attention scoreÏóê ÏòÅÌñ•ÏùÑ Ï§ÄÎã§(VÏóêÎäî ÏòÅÌñ•ÏùÑ ÎØ∏ÏπòÏßÄ ÏïäÎäîÎã§)</p>
<section id="attention-with-linear-biases-alibi" class="level3">
<h3 class="anchored" data-anchor-id="attention-with-linear-biases-alibi">Attention with Linear Biases (ALiBi)</h3>
<p>Í∞ÄÏû• Í∞ÑÎã®Ìïú Î∞©Î≤ïÏúºÎ°úÎäî ALiBiÍ∞Ä ÏûàÎã§. Ïù¥ Î∞©Î≤ïÏùÄ Îã®ÏàúÌûà attention matrixÏóê queryÏôÄ keyÏùò Í±∞Î¶¨Ïóê ÎåÄÌïú offsetÏùÑ ÎçîÌï¥Ï§ÄÎã§. <img src="../../images/lec12/Pasted image 20240502232301.png" class="img-fluid"></p>
</section>
<section id="rotary-positional-embedding-rope" class="level3">
<h3 class="anchored" data-anchor-id="rotary-positional-embedding-rope">Rotary Positional Embedding (RoPE)</h3>
<p>Îã§Î•∏ Î∞©Î≤ïÏúºÎ°úÎäî llama2ÏóêÎèÑ ÏÇ¨Ïö©Îê† Ï†ïÎèÑÎ°ú ÎÑêÎ¶¨ ÏÇ¨Ïö©ÎêòÍ≥† ÏûàÎäî RoPEÏù¥Îã§. RoPEÏùò ÏïÑÏù¥ÎîîÏñ¥Îäî ÏûÖÎ†• Îç∞Ïù¥ÌÑ∞Ïùò ÏúÑÏπò Ï†ïÎ≥¥Î•º ÌöåÏ†Ñ(rotation)ÏùÑ ÌÜµÌï¥ Ïù∏ÏΩîÎî©ÌïòÎäî Í≤ÉÏù¥Îã§. dÏ∞®ÏõêÏùò word embeddingÏúºÎ°ú d/2Í∞úÏùò ÏßùÏùÑ ÎßåÎì§Í≥†, Í∞ÅÍ∞ÅÏùò pairÎ•º 2d Ï¢åÌëúÎ°ú Í∞ÄÏ†ïÌïòÍ≥†, positionÏóê Îî∞Îùº ÌöåÏ†ÑÏãúÌÇ§Îäî Í≤ÉÏù¥Îã§. <img src="../../images/lec12/Pasted image 20240502233828.png" class="img-fluid"> ÏúÑ Í∑∏Î¶ºÏóêÏÑú x1,x2Ïóê m * thetaÎ•º Í≥±Ìï¥ÏÑú ÌöåÏ†ÑÏãúÌÇ§Í≥† ÏûàÎã§. <img src="../../images/lec12/Pasted image 20240502233920.png" class="img-fluid"> RoPEÎ•º ÏàòÏãùÏúºÎ°ú ÎÇòÌÉÄÎÇ¥Î©¥ ÏúÑÏôÄ Í∞ôÎã§</p>
<p>LLMÏùÄ Î≥¥ÌÜµ ÌïôÏäµÌï† Îïå context Í∏∏Ïù¥Ïóê Ï†úÌïúÏù¥ ÏûàÎã§. ÏòàÎ•ºÎì§Ïñ¥ llamaÎäî 2k, llama2Îäî 4kÎ°ú Ï†úÌïúÎêú Îç∞Ïù¥ÌÑ∞Î°ú ÌïôÏäµÌïúÎã§. Í∑∏Îü¨ÎÇò RoPE Î∞©Ïãù ÎçïÏóê Îçî ÌÅ∞ context Í∏∏Ïù¥ÎèÑ Îã§Î£∞ Ïàò ÏûàÎã§. <img src="../../images/lec12/Pasted image 20240502234135.png" class="img-fluid"> Îçî ÏûëÏùÄ thetaÎ•º ÏÇ¨Ïö©ÌïòÎ©¥, Îçî Ï¥òÏ¥òÌïòÍ≤å interpolate ÌïòÎ©¥ÏÑú contextÍ∏∏Ïù¥Î•º ÎäòÎ¶¥ Ïàò ÏûàÎã§. ÏúÑ Í∑∏Î¶ºÏóêÏÑú Îã®ÏàúÌûà 4096 context Í∏∏Ïù¥Îäî unseenÏù¥Îùº Ïã§Ìå®ÌïòÏßÄÎßå, thetaÎ•º Ï†àÎ∞òÏúºÎ°ú Ï§ÑÏù∏ ÏïÑÎûò Í∑∏ÎûòÌîÑÏóêÏÑúÎäî ÏõêÎûò context Í∏∏Ïù¥ ÏïàÏóê Îì§Ïñ¥Ïò§Í∏∞ ÎïåÎ¨∏Ïóê ÏÑ±Í≥µÌïòÎäî Î™®ÏäµÏùÑ Î≥º Ïàò ÏûàÎã§.</p>
</section>
</section>
<section id="kv-cache-optimizationsmulti-head-attention-mha---multi-query-attention-mqa---grouped-query-attentiongqa" class="level2">
<h2 class="anchored" data-anchor-id="kv-cache-optimizationsmulti-head-attention-mha---multi-query-attention-mqa---grouped-query-attentiongqa">KV cache optimizations(Multi-Head Attention (MHA) -&gt; Multi-Query Attention (MQA) -&gt; Grouped-Query Attention(GQA)</h2>
<p>KV cacheÎûÄ attention Îß§Ïª§ÎãàÏ¶òÏùò Key, ValueÎ•º Ï†ÄÏû•Ìï¥ÎÜìÎäî Í≤ÉÏùÑ ÎßêÌïúÎã§. transfomerÎ•º decode(gpt-style. decoder Î™®Îç∏ ÏÇ¨Ïö©Ìï¥ ÏÉùÏÑ±ÌïòÎäî Í≤ÉÏùÑ ÏùòÎØ∏Ìï®)Ìï† ÎïåÎäî ÏßÄÍ∏à ÏãúÏ†ê ÌÜ†ÌÅ∞Ïùò attentionÏùÑ Í≥ÑÏÇ∞ÌïòÍ∏∞ ÏúÑÌï¥ Ïù¥Ï†Ñ ÌÜ†ÌÅ∞Îì§Ïùò Key, ValueÍ∞íÏùÑ Î™®Îëê Ï†ÄÏû•ÌïòÍ≥† ÏûàÏñ¥Ïïº ÌïúÎã§. <img src="../../images/lec12/Pasted image 20240502234520.png" class="img-fluid"> ÏúÑ Í∑∏Î¶ºÏóêÏÑú ‚Äútrainium‚Äù ÌÜ†ÌÅ∞ÏùÑ ÏÉùÏÑ±ÌïòÍ∏∞ ÏúÑÌï¥ÏÑ† Ïù¥Ï†Ñ ‚ÄúI‚Äù, ‚Äúlove‚ÄùÏùò K,VÍ∞Ä ÌïÑÏöîÌïòÎã§(QueryÎäî ÌïÑÏöîÌïòÏßÄ ÏïäÏùå) Îã®ÏàúÌûà ÏÉÅÏÉÅÌï¥Î¥êÎèÑ, KV cacheÎ•º Ï†ÄÏû•ÌïòÍ∏∞ ÏúÑÌï¥ ÏÇ¨Ïö©ÎêòÎäî Î©îÎ™®Î¶¨Í∞Ä ÎÑàÎ¨¥ ÎßéÏù¥ ÌïÑÏöîÌïòÎã§. llama2-7b Î™®Îç∏ÏóêÏÑú KV cache ÌÅ¨Í∏∞Îäî batch_size * 32(layers) * 128(n_emd) * N(length) * 2(K,VÎãàÍπå 2Í∞ú) * 2byte(fp16) = 512KB * BS * N ÎßåÌÅº ÌïÑÏöîÌïòÎã§. llama2-70B Î™®Îç∏ÏùÑ Ïù¥Îü∞ ÏãùÏúºÎ°ú Í≥ÑÏÇ∞Ìï¥Î≥¥Î©¥, batch size 16Ïùº Îïå 4096Î≤àÏß∏ tokenÏùÑ Ï≤òÎ¶¨Ìï† Îïå KV cacheÏùò Ïö©ÎüâÏùÄ 160GBÏóê Îã¨ÌïúÎã§. <img src="../../images/lec12/Pasted image 20240502235030.png" class="img-fluid"> Îî∞ÎùºÏÑú KV cacheÏùò ÏÇ¨Ïù¥Ï¶àÎ•º Ï§ÑÏùº ÌïÑÏöîÍ∞Ä ÏûàÍ≥†, Í∑∏ Î∞©Î≤ïÏù¥ multi-query-attention(MQA), grouped-query-attention(GQA)Ïù¥Îã§. Ïù¥Ï§ë GQAÎäî llama2ÏóêÎèÑ Ï†ÅÏö©Îê† Ï†ïÎèÑÎ°ú ÎßéÏù¥ ÏÇ¨Ïö©ÎêòÎäî Î∞©ÏãùÏù¥Îã§. Í∞ÅÍ∞ÅÏùò Î∞©ÏãùÏùÑ ÏÇ¥Ìé¥Î≥¥Î©¥</p>
<p><strong>MQA</strong> : Î™®Îì† valueÏôÄ keyÎ•º ÌïòÎÇòÎ°ú ÌèâÍ∑†ÎÇ∏Îã§</p>
<p><strong>GQA</strong> : Î™®Îì† valueÏôÄ keyÎ•º GÍ∞úÎ°ú ÌèâÍ∑†ÎÇ∏Îã§(Î≥¥ÌÜµ GÎäî N/8)</p>
<p><img src="../../images/lec12/Pasted image 20240503000314.png" class="img-fluid"> ÏúÑ Í∑∏Î¶ºÏ≤òÎüº MQA, GQAÎ•º ÏÇ¨Ïö©ÌïòÎ©¥ KV cache ÌÅ¨Í∏∞Î•º ÎßéÏù¥ Ï§ÑÏùº Ïàò ÏûàÎã§.</p>
</section>
<section id="ffn-glu" class="level2">
<h2 class="anchored" data-anchor-id="ffn-glu">FFN-&gt;GLU</h2>
<p>inverted bottleneck, reluÎ•º ÏÇ¨Ïö©ÌïòÎçò Í∏∞Ï°¥ FFN Í≥ÑÏ∏µÏùÑ, GLU(Gated Linear Unit)Í≥º swish ÌôúÏÑ±Ìôî Ìï®ÏàòÎ•º ÏÇ¨Ïö©ÌïòÎ©¥ ÏÑ±Îä•Ïù¥ Îçî ÎÇòÏïÑÏßÑÎã§Í≥† ÌïúÎã§. <img src="../../images/lec12/Pasted image 20240503000645.png" class="img-fluid"> <img src="../../images/lec12/Pasted image 20240503000654.png" class="img-fluid"> Ïù¥Îïå ÏÑ±Îä•ÏùÄ PPL(perplexity)Î°ú Ï∏°Ï†ïÌïúÎã§.</p>
</section>
</section>
<section id="large-language-modelsllms" class="level1">
<h1>3. Large language models(LLMs)</h1>
<p>LLMÏù¥ Ïñ¥Îñ§ ÏùºÏùÑ Ìï† Ïàò ÏûàÎäîÏßÄ, Ïñ¥Îñ§ Ï¢ÖÎ•òÍ∞Ä ÏûàÎäîÏßÄÏóê ÎåÄÌï¥ÏÑúÎäî Ïù¥ÎØ∏ ÎßéÏù¥ Í∏ÄÍ≥º Ï†ïÎ≥¥Í∞Ä ÏûàÍ∏∞ ÎïåÎ¨∏Ïóê, 3Ïû•ÏóêÏÑúÎäî LLMÏùò Ïó¨Îü¨Í∞ÄÏßÄ ÌäπÏßïÏóê ÎåÄÌï¥ Í∞ÑÎã®Ìûà ÏÑ§Î™ÖÌïúÎã§. <img src="../../images/lec12/Pasted image 20240503000909.png" class="img-fluid"> LLMÏùò Ïã†Í∏∞Ìïú ÌäπÏßïÏ§ë ÌïòÎÇòÎäî, model sizeÍ∞Ä Ïª§ÏßÄÎã§ Î≥¥Î©¥ Ïñ¥ÎäêÏÉà ÌäπÏ†ï taskÏóê ÎåÄÌïú Îä•Î†•Ïù¥ ÏÉùÍ∏¥Îã§Îäî Í≤ÉÏù¥Îã§. ÏûÖÎ†• Îß•ÎùΩÏóê ÎßûÎäî Ïà´Ïûê Ïó∞ÏÇ∞ÏùÑ ÌïúÎã§Í±∞ÎÇò, ÏÑûÏù∏ ÏïåÌååÎ≤≥ Ï≤†ÏûêÎ•º Ï∞æÏïÑÎÇº ÏàòÎèÑ ÏûàÎã§. <img src="../../images/lec12/Pasted image 20240503001044.png" class="img-fluid"></p>
<p>ÎòêÌïú Ïù¥Ï†Ñ NLP ÏãúÎåÄÏóêÏÑúÎäî downstream taskÎ•º ÌïòÍ∏∞ ÏúÑÌï¥ÏÑú fine tuningÏùÑ Ìï¥Ïïº ÌñàÏßÄÎßå, LLMÏùÄ ÌååÏù∏ÌäúÎãù ÏóÜÏù¥ Zero-shotÏù¥ÎÇò Few-shot Î∞©ÏãùÏúºÎ°ú downstream taskÎ•º Ìï¥Í≤∞ÌïúÎã§. <img src="../../images/lec12/Pasted image 20240503001146.png" class="img-fluid"></p>
<p>Ï£ºÎ™©Î∞õÎäî LLM Î™®Îç∏Í≥º Í∞ÅÍ∞ÅÏùò ÌäπÏßïÏùÑ Í∞ÑÎûµÌûà Ï†ïÎ¶¨Ìï¥Î≥¥Î©¥, llamaÎäî SwiGLUÎ•º Ï†ÅÏö©ÌñàÍ≥†, llama2ÏóêÏÑúÎäî training tokensÏùÑ ÌÅ¨Í≤å ÎäòÎ¶∞ Ï†êÏù¥ falconÏùÄ 180BÎùºÎäî Í±∞ÎåÄÌïú model sizeÍ∞Ä mistralÏùÄ sliding window attentionÏù¥ÎùºÎäî attention Í∏∞Î≤ïÏù¥ ÎèÖÌäπÌïú Ï†êÏù¥Îã§.</p>
<p><strong>ÏπúÏπ†Îùº Î≤ïÏπô(The Chinchilla Law)</strong></p>
<p>ÏπúÏπ†Îùº Î≤ïÏπôÏùÄ model sizeÎøêÎßå ÏïÑÎãàÎùº, training dataÏùò ÌÅ¨Í∏∞ÎèÑ ÎäòÎ†§Ïïº ÏµúÏ†ÅÏùò computation-accuracy trade-off ÌïòÎäî ÏßÄÏ†êÏùÑ Ï∞æÏùÑ Ïàò ÏûàÎã§Îäî Í≤ÉÏù¥Îã§. (Î¨¥Ï°∞Í±¥ data ÌÅ¨Í∏∞Î•º ÎäòÎ¶¨ÎäîÍ≤å Ï¢ãÎã§ÎäîÍ≤å ÏïÑÎãàÎùº Îç∞Ïù¥ÌÑ∞ ÏñëÏóê Îî∞Î•∏ ÏµúÏ†Å model sizeÍ∞Ä ÏûàÎã§Îäî ÎúªÏù¥Îã§) llama-2Í∞Ä ÎπÑÍµêÏ†Å Ï†ÅÏùÄ ÌååÎùºÎØ∏ÌÑ∞ÏôÄ ÎßéÏùÄ train tokenÏúºÎ°ú Ï¢ãÏùÄ ÏÑ±Îä•ÏùÑ Î≥¥Ïó¨Ï§ÄÎã§. <img src="../../images/lec12/Pasted image 20240503001845.png" class="img-fluid"></p>
</section>
<section id="advanced-topics-multi-modal-llm" class="level1">
<h1>4. Advanced topics, multi-modal LLM</h1>
<p>Ïù¥ÌõÑ Í∞ïÏùòÏóêÏÑú ÌïúÎ≤à Îçî ÏûêÏÑ∏Ìûà Îã§Î£®Í∏∞ ÎïåÎ¨∏Ïóê Ïù¥Î≤à Í∞ïÏùò Ï†ïÎ¶¨ Í∏ÄÏóêÏÑúÎäî ÏÉùÎûµÌï©ÎãàÎã§</p>
</section>
<section id="efficient-inference-algorithms-for-llms" class="level1">
<h1>5. Efficient inference algorithms for LLMs</h1>
<p>ÏïûÏÑ† Í∞ïÏùòÎì§ÏóêÏÑú inferenceÎ•º Ìö®Ïú®Ï†ÅÏúºÎ°ú ÌïòÍ∏∞ ÏúÑÌïú Î∞©Î≤ïÏúºÎ°ú quantizationÍ≥º pruningÏùò Î∞©Î≤ïÏù¥ ÏûàÏóàÍ≥†, Ïù¥ Î∞©Î≤ïÎì§ÏùÑ LLMÏóêÎèÑ Ï†ÅÏö©Ìï¥ Î≥º Ïàò ÏûàÏùÑ Í≤ÉÏûÖÎãàÎã§.</p>
<section id="quantization-smoothquant-awq-tinychat" class="level2">
<h2 class="anchored" data-anchor-id="quantization-smoothquant-awq-tinychat">5.1. Quantization: SmoothQuant, AWQ, TinyChat</h2>
<p><img src="../../images/lec12/Pasted image 20240504222004.png" class="img-fluid"> ÌïòÏßÄÎßå, Îã®ÏàúÌûà W8A8Í≥º Í∞ôÏù¥ quantizeÌïòÎäî Í≤ÉÏùÄ, ÍµâÏû•Ìûà ÌÅ∞ ÏÑ±Îä• Ï†ÄÌïòÎ•º Î≥¥Ïó¨Ï§ÄÎã§ Ïù¥Ïú†Îäî, LLMÏóêÏÑúÎäî activationÏùò outlierÍ∞Ä Ï§ëÏöîÌïú Ïó≠Ìï†ÏùÑ ÌïòÍ∏∞ ÎïåÎ¨∏Ïù¥Îã§ <img src="../../images/lec12/Pasted image 20240504222349.png" class="img-fluid"> ÏúÑ Í∑∏Î¶ºÏùò Ïò§Î•∏Ï™ΩÏ≤òÎüº activationÏóêÎäî ÍµâÏû•Ìûà ÌÅ∞ outlierÍ∞Ä Ï°¥Ïû¨ÌïòÍ≥† weightÎäî ÎπÑÍµêÏ†Å Ìé∏Ï∞®Í∞Ä ÏóÜÎã§. Îî∞ÎùºÏÑú activationÏùÑ 10ÏúºÎ°ú ÎÇòÎàÑÍ≥†, weightÏóê 10ÏùÑ Í≥±ÌïòÎ©¥ ÏàòÏãùÏùò Í∞íÏùÄ Î≥ÄÌïòÏßÄ ÏïäÏßÄÎßå, activationÏùÑ Ï¢Ä Îçî Ìé∏ÌïòÍ≤å quantizeÌï† Ïàò ÏûàÍ≤å ÎêúÎã§(Ïò§Î•∏Ï™Ω Í∑∏Î¶º). Ïù¥Îü∞ Î∞©ÏãùÏùÑ activationÏùÑ Ï¢ÄÎçî ÌèâÌèâÌïòÍ≤å ÎßåÎì†Îã§Í≥† Ìï¥ÏÑú <strong>smoothQuant</strong>ÎùºÍ≥† ÌïúÎã§. <img src="../../images/lec12/Pasted image 20240504222805.png" class="img-fluid"> smoothQuantÎ∞©ÏãùÏùÄ llama Î™®Îç∏ÏóêÏÑúÎèÑ Îß§Ïö∞ Ïûò ÎèôÏûëÌïúÎã§.</p>
<p><img src="../../images/lec12/Pasted image 20240504222848.png" class="img-fluid"> ÏúÑ Í∑∏Î¶ºÏùò xÏ∂ïÏù∏ compute intensityÎäî FLOPs / MemoryBandwithÎ•º ÎÇòÌÉÄÎÇ∏Îã§. Ï¶â, Îç∞Ïù¥ÌÑ∞ ÌïòÎÇòÎãπ Ïó∞ÏÇ∞ÏùÑ ÏñºÎßàÎÇò Ìö®Ïú®Ï†ÅÏúºÎ°ú Ìï† Ïàò ÏûàÎäêÎÉêÏóê ÎåÄÌïú ÏßÄÌëúÏù¥Îã§. ÏúÑ Í∑∏Î¶ºÏóêÏÑú batch sizeÍ∞Ä 1Ïùº Îïå ÎÇÆÏùÄ TFLOPSÎ•º Î≥¥Ïù¥Îäî Ïù¥Ïú†Îäî Î©îÎ™®Î¶¨ ÎïåÎ¨∏Ïù¥Îã§. LLMÏóêÏÑú Îß§ ÌÜ†ÌÅ∞ÏùÑ ÏÉùÏÑ±ÌïòÍ∏∞ ÏúÑÌï¥ÏÑúÎäî ÌÅ∞ Î©îÎ™®Î¶¨ fetchÍ∞Ä ÌïÑÏöîÌïòÎã§(parameter fetch.). activationÍ≥º weight Ï§ëÏóêÏÑúÎäî weightÍ∞Ä Ìõ®Ïî¨ Îçî ÌÅ¨ÎØÄÎ°ú, weightÎ•º Ï§ÑÏù¥ÎäîÎç∞ Îçî ÏßëÏ§ëÌï¥Ïïº ÌïúÎã§.</p>
<p>ÏúÑÏóêÏÑú ÏÇ¥Ìé¥Î≥∏ W8A8 Î∞©ÏãùÏùò quantizationÏùÄ batch serving(ÌïúÎ≤àÏóê Ïó¨Îü¨ batchÎ•º Ï≤òÎ¶¨ÌïòÎäî Ïùº)ÏóêÏÑúÎäî Ïûò ÎèôÏûëÌïúÎã§. ÌïòÎÇòÎßå Ï≤òÎ¶¨ÌïòÎäî ÏûëÏóÖÏùÄ(single-batch) memory-bounded(Î©îÎ™®Î¶¨Í∞Ä Î∂ÄÏ°±ÌïòÎ©¥ bottleneckÏù¥ ÎêúÎã§)Ïù¥Îã§. <img src="../../images/lec12/Pasted image 20240504225344.png" class="img-fluid"> ÎãπÏó∞Ìûà weightÎ•º Î∞îÎ°ú quantize ÌïòÎ©¥ ÏúÑ Í∑∏Î¶ºÏ≤òÎüº ÏÑ±Îä• Ï†ÄÌïòÍ∞Ä Î∞úÏÉùÌïúÎã§ Ïò§Î•∏Ï™Ω Í∑∏Î¶ºÏùÑ ÏÇ¥Ìé¥Î≥¥Î©¥, RTNÎ∞©ÏãùÏùÑ Îã®ÏàúÌûà Ï†ÅÏö©Ìïú Í≤ΩÏö∞ PerplexityÍ∞Ä ÎßéÏù¥ ÏÉÅÏäπÌïú Í≤ÉÏùÑ Î≥º Ïàò ÏûàÎã§. <img src="../../images/lec12/Pasted image 20240504225506.png" class="img-fluid"> Îçî ÎÇòÏùÄ Î∞©Î≤ïÏùÄ, Ï§ëÏöîÌïú(salient) weightÎì§Îßå quantize ÌïòÏßÄ ÏïäÍ≥† ÎëêÎäî Í≤ÉÏù∏Îç∞, salient ÌïòÎã§Í≥† ÌåêÎã®ÌïòÎäî Í∏∞Ï§ÄÏùÑ ‚Äúactivation‚ÄùÍ∞íÏùÑ Í∏∞Î∞òÏúºÎ°ú ÌåêÎã®Ìï† Îïå(magnitude-base, Îã®ÏàúÌûà Ï†àÎåìÍ∞íÏù¥ ÌÅ¨Î©¥ Ï§ëÏöîÌïòÎã§Í≥† ÌåêÎã®) Ï¢ãÏùÄ ÏÑ±Îä•ÏùÑ Î≥¥Ïù∏Îã§. Ïù¥Îü∞ Î∞©ÏãùÏùÑ <strong>AWQ(Activation-aware Weight Quantization)</strong> ÎùºÍ≥† ÌïúÎã§.</p>
<p>SmoothQuantÏôÄ AWQÎäî Ïò§ÎäòÎÇ† ÎÑêÎ¶¨ ÏÇ¨Ïö©ÎêòÎäî Î∞©ÏãùÏù¥Îã§.</p>
</section>
<section id="pruningsparsity-spatten-h2o-moe" class="level2">
<h2 class="anchored" data-anchor-id="pruningsparsity-spatten-h2o-moe">5.2. Pruning/sparsity: SpAtten, H2O, MoE</h2>
<p>quantizationÏùÑ ÌñàÏúºÎ©¥, pruningÎèÑ Ìï¥ Î¥êÏïº ÌïúÎã§. <img src="../../images/lec12/Pasted image 20240504225913.png" class="img-fluid"> <strong>Wanda</strong>Îäî AWQÏ≤òÎüº WeightÏôÄ ActivationÏùÑ Í≥†Î†§Ìï¥ÏÑú pruning ÌïòÎäî Î∞©ÏãùÏù¥Îã§ <img src="../../images/lec12/Pasted image 20240504225956.png" class="img-fluid"> <img src="../../images/lec12/Pasted image 20240504230514.png" class="img-fluid"> SpAttenÏùÄ Ï§ëÏöîÌïòÏßÄ ÏïäÏùÄ ÌÜ†ÌÅ∞ ÏûêÏ≤¥Î•º ÏÇ≠Ï†úÌïòÎäî Î∞©ÏãùÏù¥Îã§. Ïò§Î•∏Ï™Ω attention Îßµ Í∏∞Î∞òÏúºÎ°ú, Í∞ÄÏû• ÎÇÆÏùÄ attention Ìï©Í≥ÑÎ•º Í∞ÄÏßÑ ÌÜ†ÌÅ∞ÏùÑ ÏÇ≠Ï†úÌïúÎã§. <img src="../../images/lec12/Pasted image 20240504230558.png" class="img-fluid"> <strong>H2O</strong>Îäî Heavy Hitter Token(H2)Î•º Ï§ëÏã¨ÏúºÎ°ú ÎÇ®Í∏∞Í≥†, ÎÇòÎ®∏ÏßÄÎ•º pruningÌïòÎäî Î∞©ÏãùÏù¥Îã§. Ïó¨Í∏∞ÏÑú ÎßêÌïòÎäî Heavy HitterÎûÄ attention Í∏∞Î∞òÏúºÎ°ú ÏÑ†Ï†ïÌïúÎã§. Ïù¥Ìï¥ÌïòÍ∏∞Î°úÎäî, SpAttenÏùò Î∞©ÏãùÍ≥º ÎπÑÏä∑ÌïòÎã§Í≥† ÎäêÍºàÎã§. <img src="../../images/lec12/Pasted image 20240504231032.png" class="img-fluid"> <strong>DejaVu</strong>Îäî ÏûÖÎ†•Ïóê ÏòÅÌñ•ÏùÑ Î∞õÏßÄ ÏïäÎäî attention headÎì§Ïù¥ Ï°¥Ïû¨ÌïòÍ≥†, Ïù¥Í≤ÉÏùÑ contextual sparsityÎùºÍ≥† Î∂ÄÎ•¥Î©∞, Ïù¥ Ìå®ÌÑ¥ÏùÑ MLPÎ•º ÌÜµÌï¥ ÏòàÏÉÅÌï† Ïàò ÏûàÎã§Îäî Í∞ÄÏÑ§ÏùÑ ÏÑ∏Ïõ†Îã§. Ïù¥Îü∞ contextual sparsityÎ•º Ï†úÍ±∞ÌïòÎäî Î∞©ÏãùÏùÑ DejaVuÎäî ÏÇ¨Ïö©ÌñàÎã§. <img src="../../images/lec12/Pasted image 20240504233615.png" class="img-fluid"> <strong>MoE(Mixture of Experts)</strong> Îäî FFNÏùÑ NÍ∞úÎ°ú ÎÇòÎàÑÍ≥†, ExpertÎ•º ÏÇ¨Ïö©Ìï¥ Í∑∏Ï§ëÏóê ÌïòÎÇòÎ•º Í≥†Î•¥Îäî Í∞úÎÖêÏùÑ ÎèÑÏûÖÌïúÎã§. Í∑∏Î¶º Ï§ëÍ∞ÑÏóê ÏûàÎäî RouterÎ°úÎ∂ÄÌÑ∞ ÌôïÎ•†Ï†ÅÏúºÎ°ú Ïñ¥Îñ§ FFNÏùÑ ÏÇ¨Ïö©Ìï†ÏßÄ MoEÎ∞©ÏãùÏùÄ GPT-4ÏóêÏÑú ÏÇ¨Ïö©ÌïòÍ≥† ÏûàÎã§Í≥† ÏïåÎ†§Ï†∏ ÏûàÎã§.</p>
</section>
</section>
<section id="efficient-inference-systems-for-llms" class="level1">
<h1>6. Efficient inference systems for LLMs</h1>
<p>Ïù¥ Ïû•ÏóêÏÑúÎäî systemÏ†Å Í¥ÄÏ†êÏóêÏÑú Îçî Ìö®Ïú®Ï†ÅÏúºÎ°ú LLMÏùÑ inferenceÌïòÎäî Î≤ïÏóê Îã§Î£¨Îã§.</p>
<section id="vllmpaged-attention" class="level2">
<h2 class="anchored" data-anchor-id="vllmpaged-attention">6.1. vLLM(Paged Attention)</h2>
<p>Îã§ÏàòÏùò ÏÇ¨Ïö©ÏûêÍ∞Ä LLMÏùÑ ÏÇ¨Ïö©ÌïòÎäî ÌôòÍ≤ΩÏóêÏÑ† Î¨¥ÏóáÏù¥ Î¨∏Ï†úÍ∞Ä Îê†Íπå? <img src="../../images/lec12/Pasted image 20240504235158.png" class="img-fluid"> ÏúÑ Í∑∏Î¶ºÏ≤òÎüº Ïö∞Î¶¨Îäî LLMÏùò Ï∂úÎ†•Ïù¥ ÏñºÎßàÎÇò Í∏∏Ïñ¥Ïßà ÏßÄ Î™®Î•¥Í∏∞ ÎñÑÎ¨∏Ïóê, ÏñºÎßàÎÇò Î©îÎ™®Î¶¨Î•º Ìï†ÎãπÌï¥Ïïº Ìï† ÏßÄ Ïïå Ïàò ÏóÜÎã§. Îî∞ÎùºÏÑú &lt;resv&gt; Ï≤òÎüº ÎÇ¥Î∂Ä Îã®Ìé∏Ìôî, ÌòπÏùÄ Îã§Î•∏ ÏöîÏ≤≠Í∞ÑÏùò Í∞ÑÍ≤©ÏúºÎ°ú Ïù∏Ìï¥ Ïô∏Î∂Ä Îã®Ìé∏ÌôîÍ∞Ä Î∞úÏÉùÌïòÍ≤å ÎêúÎã§. ÎßàÏπò Ïã§Ï†ú Ïö¥ÏòÅÏ≤¥Ï†úÏùò Î©îÎ™®Î¶¨Í∞ôÎã§. Í∑∏Î†áÎã§Î©¥, Ïû¨Î∞åÍ≤åÎèÑ Ïö¥ÏòÅÏ≤¥Ï†úÏóêÏÑú ÏÇ¨Ïö©ÌñàÎçò Î∞©Î≤ïÏúºÎ°ú Ïù¥Î•º Ìï¥Í≤∞Ìï† Ïàò ÏûàÍ≥†, Í∑∏ Î∞©Î≤ïÏù¥ Î∞îÎ°ú PageÎ•º ÏÇ¨Ïö©ÌïòÎäî Î∞©ÏãùÏù¥Îã§.</p>
<p><img src="../../images/lec12/Pasted image 20240504235451.png" class="img-fluid"> OSÏóêÏÑú Îã§Î•∏ ÌîÑÎ°úÏÑ∏Ïä§Í∞Ñ Î©îÎ™®Î¶¨Î•º ÏÇ¨Ïö©Ìï† Îïå pageÎã®ÏúÑÎ°ú ÏÇ¨Ïö©ÌñàÎìØÏù¥, LLMÏóêÏÑúÎèÑ Îã§Î•∏ ÏöîÏ≤≠Îì§ Í∞ÑÏóê KV cacheÎ•º page Îã®ÏúÑÎ°ú ÏÇ¨Ïö©ÌïòÎ©¥ ÎêúÎã§. <img src="../../images/lec12/Pasted image 20240504235600.png" class="img-fluid"> ÏúÑÏ≤òÎüº Îã§Î•∏ ÏöîÏ≤≠ÏùÑ pageÎã®ÏúÑÎ°ú Î∞õÏùÑ Ïàò ÏûàÎã§.</p>
<p><img src="../../images/lec12/Pasted image 20240504235658.png" class="img-fluid"> Îçî ÎÜÄÎùºÏö¥ Ï†êÏùÄ, ÌïòÎÇòÏùò KV CacheÎ•º Í≥µÏú†Ìï† Ïàò ÏûàÎã§Îäî Ï†êÏù¥Îã§. Ïïû Î¨∏Ïû•ÏùÑ Í≥µÏú†ÌïòÍ±∞ÎÇò, ÏïÑÎãàÎ©¥ PromptÍ∞ôÏù¥ ÎßéÏù¥ ÏÇ¨Ïö©ÎêòÎäî Î¨∏Ïû•Ïùò KV cacheÎ•º Í≥µÏú†Ìï¥ Ìö®Ïú®Ï†ÅÏúºÎ°ú ÎåÄÎüâÏùò inferenceÍ∞Ä Í∞ÄÎä•ÌïòÎã§.</p>
<p>Ïù¥Îü∞ Î∞©ÏãùÏùÑ <strong>Paged Attention</strong> Ïù¥ÎùºÍ≥† ÌïòÍ≥†, Ïù¥ Î∞©Î≤ïÏùÑ ÏÇ¨Ïö©Ìïú Í≤ÉÏù¥ <strong>vLLM</strong>Ïù¥ÎùºÎäî Î∞©Î≤ïÎ°†Ïù¥Îã§.</p>
</section>
<section id="streamingllm" class="level2">
<h2 class="anchored" data-anchor-id="streamingllm">6.2. StreamingLLM</h2>
<p>LLM Î∞∞Ìè¨Ïãú Îòê Îã§Î•∏ Î¨∏Ï†úÎäî Í∏∏Ïù¥ Î¨∏Ï†úÏù¥Îã§. <img src="../../images/lec12/Pasted image 20240505000351.png" class="img-fluid"> ÏóÑÏ≤≠ÎÇòÍ≤å Í∏¥ Î¨∏Ïû•Ïù¥ÎÇò, ÌòπÏùÄ Ï±óÎ¥áÏóêÏÑú ÏóÑÏ≤≠ ÏòàÏ†ÑÏóê Ïù¥ÏïºÍ∏∞ÌñàÎçò ÎÇ¥Ïö©ÍπåÏßÄ Í∏∞ÏñµÌïòÎ†§Î©¥ Î©îÎ™®Î¶¨Í∞Ä Îß§Ïö∞ ÎßéÏù¥ ÌïÑÏöîÌïòÎã§. Îã®ÏàúÌûà transformer Î∞©ÏãùÏùÑ ÏÇ¨Ïö©ÌïòÎ©¥(ÎÖ∏ÎûÄÏÉâ Í∑∏ÎûòÌîÑ) Î©îÎ™®Î¶¨Îäî ÏÑ†ÌòïÏ†ÅÏúºÎ°ú Ï¶ùÍ∞ÄÌïòÍ≥†, perplexityÎäî ÏûÖÎ†• Í∏∏Ïù¥ 4K Ïù¥ÌõÑÎ°ú Ìè≠Î∞úÏ†ÅÏúºÎ°ú Ï¶ùÍ∞ÄÌïúÎã§(trainingÏóêÏÑú Î≥¥ÏßÄ Î™ªÌïú Í∏∏Ïù¥Ïù¥Í∏∞ ÎïåÎ¨∏Ïóê) windowed attention(ÏùºÏ†ï contextÎßå Í∏∞Ïñµ, ÎÖπÏÉâ)ÏùÄ Î©îÎ™®Î¶¨ ÏÇ¨Ïö©ÎüâÏùÄ ÏùºÏ†ïÌïòÏßÄÎßå ÏûÖÎ†• Í∏∏Ïù¥Í∞Ä windowÍ∏∏Ïù¥Î•º Î≤óÏñ¥ÎÇòÎäî ÏàúÍ∞Ñ(Í∑∏Î¶ºÏóêÏÑúÎäî 1KÏ†ïÎèÑ) perplexityÍ∞Ä Í∏âÏ¶ùÌïúÎã§(Ï≤´ Î™á ÌÜ†ÌÅ∞Ïù¥ Îß§Ïö∞ Ï§ëÏöîÌïòÍ∏∞ ÎïåÎ¨∏Ïóê) <img src="../../images/lec12/Pasted image 20240505000849.png" class="img-fluid"> aÍ∞Ä ÏúÑÏóêÏÑú ÎßêÌïú Îã®Ïàú transformerÎ∞©Ïãù, bÍ∞Ä windowed attentionÏù¥Îã§. cÎäî sliding windowÎ∞©ÏãùÏù∏Îç∞, Ïù¥Ï†Ñ ÌÜ†ÌÅ∞ÏùÑ Î©îÎ™®Î¶¨Ïóê ÎëêÎäîÍ≤å ÏïÑÎãàÎùº Îã§Ïãú Í≥ÑÏÇ∞ÌïòÎäî Î∞©Î≤ïÏù¥Îã§. Ïù¥ Î∞©Î≤ïÏùÄ perplexityÎäî Í¥úÏ∞ÆÏßÄÎßå, Ïó∞ÏÇ∞ÌïòÎäîÎç∞ ÎÑàÎ¨¥ ÎßéÏùÄ ÏãúÍ∞ÑÏù¥ Îì†Îã§.</p>
<p><img src="../../images/lec12/Pasted image 20240505001038.png" class="img-fluid"> Ïù¥Îü∞ Î¨∏Ï†úÎ•º Ìï¥Í≤∞ÌïòÍ∏∞ ÏúÑÌï¥ Ï∞æÏïÑÎÇ¥Í∏∞ ÏúÑÌïú ÏïÑÏù¥ÎîîÏñ¥Î•º <strong>Attention Sink</strong>ÏóêÏÑú Ï∞æÏïòÎã§. ÏúÑ Í∑∏Î¶ºÏóêÏÑú Î≥¥Î©¥ Ï≤´Î≤àÏß∏ ÌÜ†ÌÅ∞Ïùò attention scoreÍ∞Ä Îß§Ïö∞ ÎÜíÏùÄ Í≤ÉÏùÑ Ïïå Ïàò ÏûàÎã§. Í∑∏Îü∞Îç∞, Í∑∏Îì§Ïù¥ Î¨∏Îß•Ï†ÅÏúºÎ°ú(semantically)Ï§ëÏöîÌïòÏßÄ ÏïäÏùÄ Í≤ΩÏö∞ÏóêÎèÑ Í∑∏Î†áÎã§. Ïù¥Îü∞ ÌòÑÏÉÅÏùÑ Attention SinkÎùºÍ≥† ÌïòÎäîÎç∞, Ïôú ÏùºÏñ¥ÎÇòÎäî ÌòÑÏÉÅÏùºÍπå? <img src="../../images/lec12/Pasted image 20240505001201.png" class="img-fluid"> attentionÏùÑ Íµ¨Ìï† Îïå softmaxÎ•º ÏÇ¨Ïö©ÌïòÍ≤å ÎêòÎäîÎç∞, decodingÏùÑ ÌïòÎ©¥ÏÑú Ï≤´Î≤àÏß∏ ÌÜ†ÌÅ∞ÏùÄ Î™®Îì† ÌÜ†ÌÅ∞ÏùÑ decode Ìï† Îïå Îì±Ïû•ÌïòÍ≤å ÎêòÎØÄÎ°ú, ÎãπÏó∞Ìûà Ïñ¥ÎäêÏ†ïÎèÑÏùò Í∞íÏùÑ Í≥ÑÏÜç ÎçîÌï¥Í∞ÄÏÑú ÏÉùÍ∏∞Îäî ÌòÑÏÉÅÏù¥ÎùºÎäî Í≤ÉÏù¥Îã§. <img src="../../images/lec12/Pasted image 20240505001543.png" class="img-fluid"> Í∑∏ÎûòÏÑú Ïù¥Îü∞ attention sinkÍ∞Ä ÏùºÏñ¥ÎÇòÎäî Ï≤´ ÌÜ†ÌÅ∞ÏùÄ Î¨¥Ï°∞Í±¥ ÎÇ®Í≤®ÎëêÍ≥†, windowed attentionÏùÑ ÏÇ¨Ïö©ÌïòÎ©¥ Îçî Í¥úÏ∞ÆÏùÄ Í≤∞Í≥ºÎ•º ÏñªÏùÑ Ïàò ÏûàÎã§Îäî Í≤ÉÏù¥Îã§. Ïù¥Îü∞ ÌòÑÏÉÅÏóê ÎåÄÌïú ÎÖºÎ¶¨Ï†ÅÏù∏ ÏÑ§Î™ÖÏùÄ Ï∞æÏßÄ Î™ªÌñàÏßÄÎßå, ÏïÑÎßàÎèÑ Ï≤´ ÌÜ†ÌÅ∞Ïù¥ Î¨∏Îß•Ï†ÅÏúºÎ°ú Ï§ëÏöîÌïòÏßÄ ÏïäÎçîÎùºÎèÑ ‚Äúsink(ÏåìÏïÑÎëêÎäî)‚ÄùÏùò Ïó≠Ìï†ÏùÑ ÌïòÎäî Í≤ÉÏù¥ÎùºÍ≥† ÏÉùÍ∞ÅÎêúÎã§. <img src="../../images/lec12/Pasted image 20240505002934.png" class="img-fluid"> ablation studyÏóêÏÑúÎäî ÌïòÎÇòÏùò ÌÜ†ÌÅ∞Ïù¥ ÏïÑÎãàÎùº, 4Í∞úÏùò tokenÏùÑ sinkÎ°ú ÌïòÎäîÍ≤å ÌèâÍ∑†Ï†ÅÏúºÎ°ú Ï¢ãÎã§Îäî Í≤∞Í≥ºÍ∞Ä ÏûàÎã§</p>
</section>
<section id="flashattention" class="level2">
<h2 class="anchored" data-anchor-id="flashattention">6.3. FlashAttention</h2>
<p><img src="../../images/lec12/Pasted image 20240505003016.png" class="img-fluid"> FlashAttentionÏùÄ Ï¢Ä Îçî ÌïòÎìúÏõ®Ïñ¥Ï†ÅÏù∏ Ï†ëÍ∑ºÏù¥Îã§. HBM(High Bandwith Memory)Ïóê Ï†ëÍ∑ºÌïòÎäî ÌöüÏàòÎ•º Ï§ÑÏù¥Îäî ÏïÑÏù¥ÎîîÏñ¥Ïù¥Îã§. ÌñâÎ†¨ Ïó∞ÏÇ∞ÏùÑ Ìï† Îïå Ï†ÑÏ≤¥ Î©îÎ™®Î¶¨Î•º Î∂àÎü¨Ïò§Îäî Í≤ÉÏù¥ ÏïÑÎãàÎùº ÌïòÎÇòÏî© Î∂àÎü¨ÏôÄÏÑú(Copy Block to SRAMÎ∂ÄÎ∂Ñ) GPUÏùò SRAM ÎÇ¥ÏóêÏÑú Ïó∞ÏÇ∞ÏùÑ ÏµúÎåÄÌïú ÎßàÏπòÍ≤†Îã§Îäî ÏïÑÏù¥ÎîîÏñ¥Ïù¥Îã§. ÏïûÏóêÏÑú Îã§Î§òÎçò MQA, GQAÎì±ÏùÑ Ï†ÅÏö©Ìïú FlashAttention-2ÎùºÎäî ÎÖºÎ¨∏ÎèÑ ÏûàÎã§.</p>
</section>
<section id="speculative-decoding" class="level2">
<h2 class="anchored" data-anchor-id="speculative-decoding">6.4. Speculative decoding</h2>
<p>LLMÏùò decodingÏùÄ Îß§Ïö∞ memory-boundedÌïòÎã§. ÌïòÎÇòÌïòÎÇòÏùò ÌÜ†ÌÅ∞ÏùÑ ÏÉùÏÑ±Ìï† ÎïåÎßàÎã§ Îß§Ïö∞ ÎßéÏùÄ Î©îÎ™®Î¶¨ Ïó∞ÏÇ∞Ïù¥ ÌïÑÏöîÌïòÎã§. Speculative DecodingÏùÄ Ïù¥Îü∞ Î¨∏Ï†úÎ•º Ìï¥Í≤∞ÌïòÍ∏∞ ÏúÑÌï¥ ÏûëÏùÄ Î™®Îç∏Î°ú KÍ∞úÏùò ÌÜ†ÌÅ∞ÏùÑ ÏÉùÏÑ±Ìïú Îí§ ÌÅ∞ Î™®Îç∏Î°ú Ïù¥ ÌÜ†ÌÅ∞Ïù¥ Ï¢ãÏùÄÏßÄ ÏïÑÎãåÏßÄ ÌåêÎã®ÌïòÍ≥† ÎåÄÏïàÏùÑ ÏÉùÏÑ±ÌïúÎã§(ÌÅ∞ Î™®Îç∏ÏóêÏÑúÎäî batch sizeÍ∞Ä 1Ïùº ÎïåÎÇò KÏùºÎïåÎÇò ÎπÑÏä∑ÌïòÎØÄÎ°ú) <img src="../../images/lec12/Pasted image 20240505005144.png" class="img-fluid"> KÍ∞úÏùò tokenÏùÑ ÏÉùÏÑ±Ìï† Îïå, ÌÅ∞ Î™®Îç∏ÏùÑ KÎ≤à Ìò∏Ï∂úÌïòÎäî Í≤ÉÏù¥ ÏïÑÎãàÎùº ÏûëÏùÄ Î™®Îç∏ÏùÑ KÎ≤à, ÌÅ∞ Î™®Îç∏ÏùÄ 1Î≤àÎßå Ìò∏Ï∂úÌïòÎ©¥ ÎêòÎØÄÎ°ú decoding ÏãúÍ∞ÑÏùÑ Ï†àÏïΩÌï† Ïàò ÏûàÎã§(ÎåÄÎûµ 2~3Î∞∞)</p>
</section>
</section>
<section id="efficient-fine-tuning-for-llms" class="level1">
<h1>7. Efficient fine-tuning for LLMs</h1>
<p>Ïù¥Î≤à Ïû•ÏóêÏÑúÎäî LLM fine tuningÏùÑ Ìö®Ïú®Ï†ÅÏúºÎ°ú ÌïòÎäî Î≤ïÏùÑ ÏïåÏïÑÎ≥∏Îã§.</p>
<section id="loraqlora" class="level2">
<h2 class="anchored" data-anchor-id="loraqlora">7.1. LoRA/QLoRA</h2>
<p><img src="../../images/lec12/Pasted image 20240505010052.png" class="img-fluid"> LoRAÎäî Î™®Îç∏ Ï†ÑÏ≤¥Î•º updateÌïòÎäîÍ≤å ÏïÑÎãàÎùº, ÏûëÏùÄ bypass branchÏùò weightÎßå updateÌïòÎäî Î∞©Î≤ïÏù¥Îã§. LLMÏùÑ pretrainÌïú Í∞ÄÏ§ëÏπò WÍ∞Ä ÏûàÍ≥†, full-fine tuningÌñàÏùÑ Îïå Îã¨ÎùºÏßÄÎäî Í∞ÄÏ§ëÏπòÎ•º delta WÎùºÍ≥† ÌïòÏûê. Í∑∏Îü¨Î©¥ Í∑∏ delta WÎ•º low-rank ÌñâÎ†¨Ïù∏ AÏôÄ BÏùò Í≥±(ÏúÑ Í∑∏Î¶ºÏùò Ï£ºÌô©ÏÉâ ÌñâÎ†¨)ÏúºÎ°ú ÎÇòÌÉÄÎÇ¥ÏûêÎäî ÏïÑÏù¥ÎîîÏñ¥Ïù¥Îã§.</p>
<p><img src="../../images/lec12/Pasted image 20240505010316.png" class="img-fluid"> QLoRAÎäî Í∞ÑÎã®Ìûà ÎßêÌïòÎ©¥ LoRAÏóê quantizationÏùÑ ÎçîÌïú Í≤ÉÏù¥Îã§. NF4(NormalFloat4)ÎùºÎäî normal distributionÏóê ÏµúÏ†ÅÌôîÎêú Îç∞Ïù¥ÌÑ∞ ÌÉÄÏûÖ, Double Quantization, paged optimizerÎì±Ïùò Í∏∞Î≤ïÏùÑ ÏÇ¨Ïö©ÌïúÎã§.</p>
</section>
<section id="adapter" class="level2">
<h2 class="anchored" data-anchor-id="adapter">7.2. Adapter</h2>
<p>AdapterÎäî transformer Î∏îÎ°ùÏóê learnableÌïú ÏûëÏùÄ Î∏îÎ°ùÏùÑ ÌïòÎÇò ÎÅºÏõå ÎÑ£Îäî Í≤ÉÏù¥Îã§. <img src="../../images/lec12/Pasted image 20240505011359.png" class="img-fluid"> ÏúÑ Í∑∏Î¶ºÏóêÏÑú Ïñ¥ÎåëÌÑ∞Îäî Ïò§Î•∏Ï™Ω ÎÖ∏ÎûÄÏÉâ Íµ¨Ï°∞Î•º ÎÑ£ÏùÄ Í≤ÉÏù¥Îã§. ÌïòÏßÄÎßå ÏÉàÎ°úÏö¥ layerÍ∞Ä Ï∂îÍ∞ÄÎêòÎäî Í≤ÉÏù¥Îùº, inferenceÏãú ÏãúÍ∞ÑÏù¥ Ï°∞Í∏à Îçî ÎäòÏñ¥ÎÇ† Ïàò ÏûàÎã§Îäî Î¨∏Ï†úÏ†êÏù¥ ÏûàÎã§.</p>
</section>
<section id="prompt-tuning" class="level2">
<h2 class="anchored" data-anchor-id="prompt-tuning">7.3. Prompt Tuning</h2>
<p><img src="../../images/lec12/Pasted image 20240505012942.png" class="img-fluid"> ÏúÑÏùò Î∞©Î≤ïÎì§Í≥ºÎäî Îã§Î•¥Í≤å, tuningÏóÜÏù¥ promptÎßå ÏûÖÎ†•Ìï¥ÏÑú ÌäπÏ†ïÌïú taskÏóê ÎåÄÌïú ÏÑ±Îä•ÏùÑ ÎÜíÏù¥Îäî Î∞©Î≤ïÏù¥Îã§. ÏòàÎ•º Îì§Ïñ¥, ‚ÄúÎí§Ïóê Î¨∏Ïû•ÏùÑ ÏöîÏïΩÌï¥Ï§ò :‚Äù ÎùºÎäî Î¨∏Ïû•ÏùÑ ÏûÖÎ†•Ïóê Ï∂îÍ∞ÄÌïòÎ©¥ ÏöîÏïΩ taskÏóê ÎåÄÌïú ÏÑ±Îä•Ïù¥ Ïò¨ÎùºÍ∞ÑÎã§. Ïù¥Î•º ÌôúÏöîÌïòÎ©¥, ÌïòÎÇòÏùò Î™®Îç∏Î°ú Ïó¨Îü¨Í∞ÄÏßÄ taskÏóê ÎåÄÏùëÌï† Ïàò ÏûàÍ≤å ÎêòÎ©∞, Î™®Îç∏Ïù¥ Ïª§ÏßàÏàòÎ°ù Ìï¥Îãπ taskÏóê ÎåÄÌï¥ÏÑúÎßå fine-tuningÌïú Î™®Îç∏Ïù¥Îûë ÎπÑÏä∑Ìïú ÏÑ±Îä•ÏùÑ ÎÇ¥Í≤å ÎêúÎã§.</p>


</section>
</section>

</main> <!-- /main -->
<script id="quarto-html-after-body" type="application/javascript">
window.document.addEventListener("DOMContentLoaded", function (event) {
  const toggleBodyColorMode = (bsSheetEl) => {
    const mode = bsSheetEl.getAttribute("data-mode");
    const bodyEl = window.document.querySelector("body");
    if (mode === "dark") {
      bodyEl.classList.add("quarto-dark");
      bodyEl.classList.remove("quarto-light");
    } else {
      bodyEl.classList.add("quarto-light");
      bodyEl.classList.remove("quarto-dark");
    }
  }
  const toggleBodyColorPrimary = () => {
    const bsSheetEl = window.document.querySelector("link#quarto-bootstrap");
    if (bsSheetEl) {
      toggleBodyColorMode(bsSheetEl);
    }
  }
  toggleBodyColorPrimary();  
  const icon = "Óßã";
  const anchorJS = new window.AnchorJS();
  anchorJS.options = {
    placement: 'right',
    icon: icon
  };
  anchorJS.add('.anchored');
  const isCodeAnnotation = (el) => {
    for (const clz of el.classList) {
      if (clz.startsWith('code-annotation-')) {                     
        return true;
      }
    }
    return false;
  }
  const clipboard = new window.ClipboardJS('.code-copy-button', {
    text: function(trigger) {
      const codeEl = trigger.previousElementSibling.cloneNode(true);
      for (const childEl of codeEl.children) {
        if (isCodeAnnotation(childEl)) {
          childEl.remove();
        }
      }
      return codeEl.innerText;
    }
  });
  clipboard.on('success', function(e) {
    // button target
    const button = e.trigger;
    // don't keep focus
    button.blur();
    // flash "checked"
    button.classList.add('code-copy-button-checked');
    var currentTitle = button.getAttribute("title");
    button.setAttribute("title", "Copied!");
    let tooltip;
    if (window.bootstrap) {
      button.setAttribute("data-bs-toggle", "tooltip");
      button.setAttribute("data-bs-placement", "left");
      button.setAttribute("data-bs-title", "Copied!");
      tooltip = new bootstrap.Tooltip(button, 
        { trigger: "manual", 
          customClass: "code-copy-button-tooltip",
          offset: [0, -8]});
      tooltip.show();    
    }
    setTimeout(function() {
      if (tooltip) {
        tooltip.hide();
        button.removeAttribute("data-bs-title");
        button.removeAttribute("data-bs-toggle");
        button.removeAttribute("data-bs-placement");
      }
      button.setAttribute("title", currentTitle);
      button.classList.remove('code-copy-button-checked');
    }, 1000);
    // clear code selection
    e.clearSelection();
  });
    var localhostRegex = new RegExp(/^(?:http|https):\/\/localhost\:?[0-9]*\//);
    var mailtoRegex = new RegExp(/^mailto:/);
      var filterRegex = new RegExp('/' + window.location.host + '/');
    var isInternal = (href) => {
        return filterRegex.test(href) || localhostRegex.test(href) || mailtoRegex.test(href);
    }
    // Inspect non-navigation links and adorn them if external
 	var links = window.document.querySelectorAll('a[href]:not(.nav-link):not(.navbar-brand):not(.toc-action):not(.sidebar-link):not(.sidebar-item-toggle):not(.pagination-link):not(.no-external):not([aria-hidden]):not(.dropdown-item):not(.quarto-navigation-tool)');
    for (var i=0; i<links.length; i++) {
      const link = links[i];
      if (!isInternal(link.href)) {
        // undo the damage that might have been done by quarto-nav.js in the case of
        // links that we want to consider external
        if (link.dataset.originalHref !== undefined) {
          link.href = link.dataset.originalHref;
        }
      }
    }
  function tippyHover(el, contentFn, onTriggerFn, onUntriggerFn) {
    const config = {
      allowHTML: true,
      maxWidth: 500,
      delay: 100,
      arrow: false,
      appendTo: function(el) {
          return el.parentElement;
      },
      interactive: true,
      interactiveBorder: 10,
      theme: 'quarto',
      placement: 'bottom-start',
    };
    if (contentFn) {
      config.content = contentFn;
    }
    if (onTriggerFn) {
      config.onTrigger = onTriggerFn;
    }
    if (onUntriggerFn) {
      config.onUntrigger = onUntriggerFn;
    }
    window.tippy(el, config); 
  }
  const noterefs = window.document.querySelectorAll('a[role="doc-noteref"]');
  for (var i=0; i<noterefs.length; i++) {
    const ref = noterefs[i];
    tippyHover(ref, function() {
      // use id or data attribute instead here
      let href = ref.getAttribute('data-footnote-href') || ref.getAttribute('href');
      try { href = new URL(href).hash; } catch {}
      const id = href.replace(/^#\/?/, "");
      const note = window.document.getElementById(id);
      if (note) {
        return note.innerHTML;
      } else {
        return "";
      }
    });
  }
  const xrefs = window.document.querySelectorAll('a.quarto-xref');
  const processXRef = (id, note) => {
    // Strip column container classes
    const stripColumnClz = (el) => {
      el.classList.remove("page-full", "page-columns");
      if (el.children) {
        for (const child of el.children) {
          stripColumnClz(child);
        }
      }
    }
    stripColumnClz(note)
    if (id === null || id.startsWith('sec-')) {
      // Special case sections, only their first couple elements
      const container = document.createElement("div");
      if (note.children && note.children.length > 2) {
        container.appendChild(note.children[0].cloneNode(true));
        for (let i = 1; i < note.children.length; i++) {
          const child = note.children[i];
          if (child.tagName === "P" && child.innerText === "") {
            continue;
          } else {
            container.appendChild(child.cloneNode(true));
            break;
          }
        }
        if (window.Quarto?.typesetMath) {
          window.Quarto.typesetMath(container);
        }
        return container.innerHTML
      } else {
        if (window.Quarto?.typesetMath) {
          window.Quarto.typesetMath(note);
        }
        return note.innerHTML;
      }
    } else {
      // Remove any anchor links if they are present
      const anchorLink = note.querySelector('a.anchorjs-link');
      if (anchorLink) {
        anchorLink.remove();
      }
      if (window.Quarto?.typesetMath) {
        window.Quarto.typesetMath(note);
      }
      // TODO in 1.5, we should make sure this works without a callout special case
      if (note.classList.contains("callout")) {
        return note.outerHTML;
      } else {
        return note.innerHTML;
      }
    }
  }
  for (var i=0; i<xrefs.length; i++) {
    const xref = xrefs[i];
    tippyHover(xref, undefined, function(instance) {
      instance.disable();
      let url = xref.getAttribute('href');
      let hash = undefined; 
      if (url.startsWith('#')) {
        hash = url;
      } else {
        try { hash = new URL(url).hash; } catch {}
      }
      if (hash) {
        const id = hash.replace(/^#\/?/, "");
        const note = window.document.getElementById(id);
        if (note !== null) {
          try {
            const html = processXRef(id, note.cloneNode(true));
            instance.setContent(html);
          } finally {
            instance.enable();
            instance.show();
          }
        } else {
          // See if we can fetch this
          fetch(url.split('#')[0])
          .then(res => res.text())
          .then(html => {
            const parser = new DOMParser();
            const htmlDoc = parser.parseFromString(html, "text/html");
            const note = htmlDoc.getElementById(id);
            if (note !== null) {
              const html = processXRef(id, note);
              instance.setContent(html);
            } 
          }).finally(() => {
            instance.enable();
            instance.show();
          });
        }
      } else {
        // See if we can fetch a full url (with no hash to target)
        // This is a special case and we should probably do some content thinning / targeting
        fetch(url)
        .then(res => res.text())
        .then(html => {
          const parser = new DOMParser();
          const htmlDoc = parser.parseFromString(html, "text/html");
          const note = htmlDoc.querySelector('main.content');
          if (note !== null) {
            // This should only happen for chapter cross references
            // (since there is no id in the URL)
            // remove the first header
            if (note.children.length > 0 && note.children[0].tagName === "HEADER") {
              note.children[0].remove();
            }
            const html = processXRef(null, note);
            instance.setContent(html);
          } 
        }).finally(() => {
          instance.enable();
          instance.show();
        });
      }
    }, function(instance) {
    });
  }
      let selectedAnnoteEl;
      const selectorForAnnotation = ( cell, annotation) => {
        let cellAttr = 'data-code-cell="' + cell + '"';
        let lineAttr = 'data-code-annotation="' +  annotation + '"';
        const selector = 'span[' + cellAttr + '][' + lineAttr + ']';
        return selector;
      }
      const selectCodeLines = (annoteEl) => {
        const doc = window.document;
        const targetCell = annoteEl.getAttribute("data-target-cell");
        const targetAnnotation = annoteEl.getAttribute("data-target-annotation");
        const annoteSpan = window.document.querySelector(selectorForAnnotation(targetCell, targetAnnotation));
        const lines = annoteSpan.getAttribute("data-code-lines").split(",");
        const lineIds = lines.map((line) => {
          return targetCell + "-" + line;
        })
        let top = null;
        let height = null;
        let parent = null;
        if (lineIds.length > 0) {
            //compute the position of the single el (top and bottom and make a div)
            const el = window.document.getElementById(lineIds[0]);
            top = el.offsetTop;
            height = el.offsetHeight;
            parent = el.parentElement.parentElement;
          if (lineIds.length > 1) {
            const lastEl = window.document.getElementById(lineIds[lineIds.length - 1]);
            const bottom = lastEl.offsetTop + lastEl.offsetHeight;
            height = bottom - top;
          }
          if (top !== null && height !== null && parent !== null) {
            // cook up a div (if necessary) and position it 
            let div = window.document.getElementById("code-annotation-line-highlight");
            if (div === null) {
              div = window.document.createElement("div");
              div.setAttribute("id", "code-annotation-line-highlight");
              div.style.position = 'absolute';
              parent.appendChild(div);
            }
            div.style.top = top - 2 + "px";
            div.style.height = height + 4 + "px";
            div.style.left = 0;
            let gutterDiv = window.document.getElementById("code-annotation-line-highlight-gutter");
            if (gutterDiv === null) {
              gutterDiv = window.document.createElement("div");
              gutterDiv.setAttribute("id", "code-annotation-line-highlight-gutter");
              gutterDiv.style.position = 'absolute';
              const codeCell = window.document.getElementById(targetCell);
              const gutter = codeCell.querySelector('.code-annotation-gutter');
              gutter.appendChild(gutterDiv);
            }
            gutterDiv.style.top = top - 2 + "px";
            gutterDiv.style.height = height + 4 + "px";
          }
          selectedAnnoteEl = annoteEl;
        }
      };
      const unselectCodeLines = () => {
        const elementsIds = ["code-annotation-line-highlight", "code-annotation-line-highlight-gutter"];
        elementsIds.forEach((elId) => {
          const div = window.document.getElementById(elId);
          if (div) {
            div.remove();
          }
        });
        selectedAnnoteEl = undefined;
      };
        // Handle positioning of the toggle
    window.addEventListener(
      "resize",
      throttle(() => {
        elRect = undefined;
        if (selectedAnnoteEl) {
          selectCodeLines(selectedAnnoteEl);
        }
      }, 10)
    );
    function throttle(fn, ms) {
    let throttle = false;
    let timer;
      return (...args) => {
        if(!throttle) { // first call gets through
            fn.apply(this, args);
            throttle = true;
        } else { // all the others get throttled
            if(timer) clearTimeout(timer); // cancel #2
            timer = setTimeout(() => {
              fn.apply(this, args);
              timer = throttle = false;
            }, ms);
        }
      };
    }
      // Attach click handler to the DT
      const annoteDls = window.document.querySelectorAll('dt[data-target-cell]');
      for (const annoteDlNode of annoteDls) {
        annoteDlNode.addEventListener('click', (event) => {
          const clickedEl = event.target;
          if (clickedEl !== selectedAnnoteEl) {
            unselectCodeLines();
            const activeEl = window.document.querySelector('dt[data-target-cell].code-annotation-active');
            if (activeEl) {
              activeEl.classList.remove('code-annotation-active');
            }
            selectCodeLines(clickedEl);
            clickedEl.classList.add('code-annotation-active');
          } else {
            // Unselect the line
            unselectCodeLines();
            clickedEl.classList.remove('code-annotation-active');
          }
        });
      }
  const findCites = (el) => {
    const parentEl = el.parentElement;
    if (parentEl) {
      const cites = parentEl.dataset.cites;
      if (cites) {
        return {
          el,
          cites: cites.split(' ')
        };
      } else {
        return findCites(el.parentElement)
      }
    } else {
      return undefined;
    }
  };
  var bibliorefs = window.document.querySelectorAll('a[role="doc-biblioref"]');
  for (var i=0; i<bibliorefs.length; i++) {
    const ref = bibliorefs[i];
    const citeInfo = findCites(ref);
    if (citeInfo) {
      tippyHover(citeInfo.el, function() {
        var popup = window.document.createElement('div');
        citeInfo.cites.forEach(function(cite) {
          var citeDiv = window.document.createElement('div');
          citeDiv.classList.add('hanging-indent');
          citeDiv.classList.add('csl-entry');
          var biblioDiv = window.document.getElementById('ref-' + cite);
          if (biblioDiv) {
            citeDiv.innerHTML = biblioDiv.innerHTML;
          }
          popup.appendChild(citeDiv);
        });
        return popup.innerHTML;
      });
    }
  }
});
</script>
<script src="https://giscus.app/client.js" data-repo="TINYML-KOR/blog" data-repo-id="R_kgDOLC9iGA" data-category="General" data-category-id="DIC_kwDOLC9iGM4Cc7eP" data-mapping="title" data-reactions-enabled="1" data-emit-metadata="0" data-input-position="top" data-theme="light" data-lang="en" crossorigin="anonymous" async="">
</script>
<input type="hidden" id="giscus-base-theme" value="light">
<input type="hidden" id="giscus-alt-theme" value="dark">
</div> <!-- /content -->




</body></html>