<!DOCTYPE html>
<html xmlns="http://www.w3.org/1999/xhtml" lang="en" xml:lang="en"><head>

<meta charset="utf-8">
<meta name="generator" content="quarto-1.4.549">

<meta name="viewport" content="width=device-width, initial-scale=1.0, user-scalable=yes">

<meta name="author" content="Jung Yeon Lee">
<meta name="dcterms.date" content="2024-03-06">
<meta name="description" content="K-means &amp; Linear Quantization">

<title>TinyML KOR - ğŸ‘©â€ğŸ’» Lab 2</title>
<style>
code{white-space: pre-wrap;}
span.smallcaps{font-variant: small-caps;}
div.columns{display: flex; gap: min(4vw, 1.5em);}
div.column{flex: auto; overflow-x: auto;}
div.hanging-indent{margin-left: 1.5em; text-indent: -1.5em;}
ul.task-list{list-style: none;}
ul.task-list li input[type="checkbox"] {
  width: 0.8em;
  margin: 0 0.8em 0.2em -1em; /* quarto-specific, see https://github.com/quarto-dev/quarto-cli/issues/4556 */ 
  vertical-align: middle;
}
/* CSS for syntax highlighting */
pre > code.sourceCode { white-space: pre; position: relative; }
pre > code.sourceCode > span { line-height: 1.25; }
pre > code.sourceCode > span:empty { height: 1.2em; }
.sourceCode { overflow: visible; }
code.sourceCode > span { color: inherit; text-decoration: inherit; }
div.sourceCode { margin: 1em 0; }
pre.sourceCode { margin: 0; }
@media screen {
div.sourceCode { overflow: auto; }
}
@media print {
pre > code.sourceCode { white-space: pre-wrap; }
pre > code.sourceCode > span { text-indent: -5em; padding-left: 5em; }
}
pre.numberSource code
  { counter-reset: source-line 0; }
pre.numberSource code > span
  { position: relative; left: -4em; counter-increment: source-line; }
pre.numberSource code > span > a:first-child::before
  { content: counter(source-line);
    position: relative; left: -1em; text-align: right; vertical-align: baseline;
    border: none; display: inline-block;
    -webkit-touch-callout: none; -webkit-user-select: none;
    -khtml-user-select: none; -moz-user-select: none;
    -ms-user-select: none; user-select: none;
    padding: 0 4px; width: 4em;
  }
pre.numberSource { margin-left: 3em;  padding-left: 4px; }
div.sourceCode
  {   }
@media screen {
pre > code.sourceCode > span > a:first-child::before { text-decoration: underline; }
}
</style>


<script src="https://cdnjs.cloudflare.com/ajax/libs/jquery/3.5.1/jquery.min.js" integrity="sha512-bLT0Qm9VnAYZDflyKcBaQ2gg0hSYNQrJ8RilYldYQ1FxQYoCLtUjuuRuZo+fjqhx/qtq/1itJ0C2ejDxltZVFg==" crossorigin="anonymous"></script><script src="../../site_libs/quarto-nav/quarto-nav.js"></script>
<script src="../../site_libs/quarto-nav/headroom.min.js"></script>
<script src="../../site_libs/clipboard/clipboard.min.js"></script>
<script src="../../site_libs/quarto-search/autocomplete.umd.js"></script>
<script src="../../site_libs/quarto-search/fuse.min.js"></script>
<script src="../../site_libs/quarto-search/quarto-search.js"></script>
<meta name="quarto:offset" content="../../">
<script src="../../site_libs/quarto-html/quarto.js"></script>
<script src="../../site_libs/quarto-html/popper.min.js"></script>
<script src="../../site_libs/quarto-html/tippy.umd.min.js"></script>
<script src="../../site_libs/quarto-html/anchor.min.js"></script>
<link href="../../site_libs/quarto-html/tippy.css" rel="stylesheet">
<link href="../../site_libs/quarto-html/quarto-syntax-highlighting.css" rel="stylesheet" id="quarto-text-highlighting-styles">
<script src="../../site_libs/bootstrap/bootstrap.min.js"></script>
<link href="../../site_libs/bootstrap/bootstrap-icons.css" rel="stylesheet">
<link href="../../site_libs/bootstrap/bootstrap.min.css" rel="stylesheet" id="quarto-bootstrap" data-mode="light">
<script id="quarto-search-options" type="application/json">{
  "location": "navbar",
  "copy-button": false,
  "collapse-after": 3,
  "panel-placement": "end",
  "type": "overlay",
  "limit": 50,
  "keyboard-shortcut": [
    "f",
    "/",
    "s"
  ],
  "show-item-context": false,
  "language": {
    "search-no-results-text": "No results",
    "search-matching-documents-text": "matching documents",
    "search-copy-link-title": "Copy link to search",
    "search-hide-matches-text": "Hide additional matches",
    "search-more-match-text": "more match in this document",
    "search-more-matches-text": "more matches in this document",
    "search-clear-button-title": "Clear",
    "search-text-placeholder": "",
    "search-detached-cancel-button-title": "Cancel",
    "search-submit-button-title": "Submit",
    "search-label": "Search"
  }
}</script>
<script src="https://cdnjs.cloudflare.com/ajax/libs/require.js/2.3.6/require.min.js" integrity="sha512-c3Nl8+7g4LMSTdrm621y7kf9v3SDPnhxLNhcjFJbKECVnmZHTdo+IRO05sNLTH/D3vA6u1X32ehoLC7WFVdheg==" crossorigin="anonymous"></script>

<script type="application/javascript">define('jquery', [],function() {return window.jQuery;})</script>
<script src="https://unpkg.com/@jupyter-widgets/html-manager@*/dist/embed-amd.js" crossorigin="anonymous"></script>

  <script src="https://polyfill.io/v3/polyfill.min.js?features=es6"></script>
  <script src="https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-chtml-full.js" type="text/javascript"></script>

<script type="text/javascript">
const typesetMath = (el) => {
  if (window.MathJax) {
    // MathJax Typeset
    window.MathJax.typeset([el]);
  } else if (window.katex) {
    // KaTeX Render
    var mathElements = el.getElementsByClassName("math");
    var macros = [];
    for (var i = 0; i < mathElements.length; i++) {
      var texText = mathElements[i].firstChild;
      if (mathElements[i].tagName == "SPAN") {
        window.katex.render(texText.data, mathElements[i], {
          displayMode: mathElements[i].classList.contains('display'),
          throwOnError: false,
          macros: macros,
          fleqn: false
        });
      }
    }
  }
}
window.Quarto = {
  typesetMath
};
</script>

<link rel="stylesheet" href="../../styles.css">
</head>

<body class="nav-fixed">

<div id="quarto-search-results"></div>
  <header id="quarto-header" class="headroom fixed-top">
    <nav class="navbar navbar-expand-lg " data-bs-theme="dark">
      <div class="navbar-container container-fluid">
      <div class="navbar-brand-container mx-auto">
    <a class="navbar-brand" href="../../index.html">
    <span class="navbar-title">TinyML KOR</span>
    </a>
  </div>
            <div id="quarto-search" class="" title="Search"></div>
          <button class="navbar-toggler" type="button" data-bs-toggle="collapse" data-bs-target="#navbarCollapse" aria-controls="navbarCollapse" aria-expanded="false" aria-label="Toggle navigation" onclick="if (window.quartoToggleHeadroom) { window.quartoToggleHeadroom(); }">
  <span class="navbar-toggler-icon"></span>
</button>
          <div class="collapse navbar-collapse" id="navbarCollapse">
            <ul class="navbar-nav navbar-nav-scroll ms-auto">
  <li class="nav-item">
    <a class="nav-link" href="../../about.html"> 
<span class="menu-text">TinyML Study Group</span></a>
  </li>  
  <li class="nav-item compact">
    <a class="nav-link" href="https://github.com/"> <i class="bi bi-github" role="img">
</i> 
<span class="menu-text"></span></a>
  </li>  
</ul>
          </div> <!-- /navcollapse -->
          <div class="quarto-navbar-tools">
</div>
      </div> <!-- /container-fluid -->
    </nav>
</header>
<!-- content -->
<div id="quarto-content" class="quarto-container page-columns page-rows-contents page-layout-article page-navbar">
<!-- sidebar -->
<!-- margin-sidebar -->
    <div id="quarto-margin-sidebar" class="sidebar margin-sidebar">
        <nav id="TOC" role="doc-toc" class="toc-active">
    <h2 id="toc-title">On this page</h2>
   
  <ul>
  <li><a href="#lab-2-quantization" id="toc-lab-2-quantization" class="nav-link active" data-scroll-target="#lab-2-quantization"><strong>Lab 2: Quantization</strong></a>
  <ul class="collapse">
  <li><a href="#goals" id="toc-goals" class="nav-link" data-scroll-target="#goals">Goals</a></li>
  <li><a href="#contents" id="toc-contents" class="nav-link" data-scroll-target="#contents">Contents</a></li>
  </ul></li>
  <li><a href="#ë¨¼ì €-fp32-modelì˜-ì •í™•ë„ì™€-ëª¨ë¸-í¬ê¸°ë¥¼-í‰ê°€í•´ë´…ì‹œë‹¤" id="toc-ë¨¼ì €-fp32-modelì˜-ì •í™•ë„ì™€-ëª¨ë¸-í¬ê¸°ë¥¼-í‰ê°€í•´ë´…ì‹œë‹¤" class="nav-link" data-scroll-target="#ë¨¼ì €-fp32-modelì˜-ì •í™•ë„ì™€-ëª¨ë¸-í¬ê¸°ë¥¼-í‰ê°€í•´ë´…ì‹œë‹¤">ë¨¼ì € FP32 Modelì˜ ì •í™•ë„ì™€ ëª¨ë¸ í¬ê¸°ë¥¼ í‰ê°€í•´ë´…ì‹œë‹¤</a></li>
  <li><a href="#k-means-quantization" id="toc-k-means-quantization" class="nav-link" data-scroll-target="#k-means-quantization">K-Means Quantization</a>
  <ul class="collapse">
  <li><a href="#question-1-10-pts" id="toc-question-1-10-pts" class="nav-link" data-scroll-target="#question-1-10-pts">Question 1 (10 pts)</a></li>
  <li><a href="#question-2-10-pts" id="toc-question-2-10-pts" class="nav-link" data-scroll-target="#question-2-10-pts">Question 2 (10 pts)</a>
  <ul class="collapse">
  <li><a href="#question-2.1-5-pts" id="toc-question-2.1-5-pts" class="nav-link" data-scroll-target="#question-2.1-5-pts">Question 2.1 (5 pts)</a></li>
  <li><a href="#question-2.2-5-pts" id="toc-question-2.2-5-pts" class="nav-link" data-scroll-target="#question-2.2-5-pts">Question 2.2 (5 pts)</a></li>
  </ul></li>
  <li><a href="#k-means-quantization-on-whole-model" id="toc-k-means-quantization-on-whole-model" class="nav-link" data-scroll-target="#k-means-quantization-on-whole-model">K-Means Quantization on Whole Model</a></li>
  <li><a href="#trained-k-means-quantization" id="toc-trained-k-means-quantization" class="nav-link" data-scroll-target="#trained-k-means-quantization">Trained K-Means Quantization</a>
  <ul class="collapse">
  <li><a href="#question-3-10-pts" id="toc-question-3-10-pts" class="nav-link" data-scroll-target="#question-3-10-pts">Question 3 (10 pts)</a></li>
  </ul></li>
  </ul></li>
  <li><a href="#linear-quantization" id="toc-linear-quantization" class="nav-link" data-scroll-target="#linear-quantization">Linear Quantization</a>
  <ul class="collapse">
  <li><a href="#n-bit-integer" id="toc-n-bit-integer" class="nav-link" data-scroll-target="#n-bit-integer"><em>n</em>-bit Integer</a></li>
  <li><a href="#question-4-15-pts" id="toc-question-4-15-pts" class="nav-link" data-scroll-target="#question-4-15-pts"><strong>Question 4</strong> (15 pts)</a></li>
  <li><a href="#question-5-10-pts" id="toc-question-5-10-pts" class="nav-link" data-scroll-target="#question-5-10-pts">Question 5 (10 pts)</a>
  <ul class="collapse">
  <li><a href="#scale" id="toc-scale" class="nav-link" data-scroll-target="#scale">Scale</a></li>
  <li><a href="#zero-point" id="toc-zero-point" class="nav-link" data-scroll-target="#zero-point">zero point</a></li>
  <li><a href="#question-5.3-8-pts" id="toc-question-5.3-8-pts" class="nav-link" data-scroll-target="#question-5.3-8-pts">Question 5.3 (8 pts)</a></li>
  </ul></li>
  <li><a href="#special-case-linear-quantization-on-weight-tensor" id="toc-special-case-linear-quantization-on-weight-tensor" class="nav-link" data-scroll-target="#special-case-linear-quantization-on-weight-tensor">Special case: linear quantization on weight tensor</a>
  <ul class="collapse">
  <li><a href="#per-channel-linear-quantization" id="toc-per-channel-linear-quantization" class="nav-link" data-scroll-target="#per-channel-linear-quantization">Per-channel Linear Quantization</a></li>
  <li><a href="#a-quick-peek-at-linear-quantization-on-weights" id="toc-a-quick-peek-at-linear-quantization-on-weights" class="nav-link" data-scroll-target="#a-quick-peek-at-linear-quantization-on-weights">A Quick Peek at Linear Quantization on Weights</a></li>
  </ul></li>
  <li><a href="#quantized-inference" id="toc-quantized-inference" class="nav-link" data-scroll-target="#quantized-inference">Quantized Inference</a>
  <ul class="collapse">
  <li><a href="#question-6-5-pts" id="toc-question-6-5-pts" class="nav-link" data-scroll-target="#question-6-5-pts">Question 6 (5 pts)</a></li>
  <li><a href="#quantized-fully-connected-layer" id="toc-quantized-fully-connected-layer" class="nav-link" data-scroll-target="#quantized-fully-connected-layer">Quantized Fully-Connected Layer</a></li>
  <li><a href="#quantized-convolution" id="toc-quantized-convolution" class="nav-link" data-scroll-target="#quantized-convolution">Quantized Convolution</a></li>
  </ul></li>
  <li><a href="#question-9-10-pts" id="toc-question-9-10-pts" class="nav-link" data-scroll-target="#question-9-10-pts">Question 9 (10 pts)</a>
  <ul class="collapse">
  <li><a href="#question-9.1-5-pts" id="toc-question-9.1-5-pts" class="nav-link" data-scroll-target="#question-9.1-5-pts">Question 9.1 (5 pts)</a></li>
  </ul></li>
  <li><a href="#question-9.2-bonus-question-5-pts" id="toc-question-9.2-bonus-question-5-pts" class="nav-link" data-scroll-target="#question-9.2-bonus-question-5-pts">Question 9.2 (Bonus Question; 5 pts)</a></li>
  </ul></li>
  <li><a href="#question-10-5-pts" id="toc-question-10-5-pts" class="nav-link" data-scroll-target="#question-10-5-pts">Question 10 (5 pts)</a>
  <ul class="collapse">
  <li><a href="#ì •í™•ë„" id="toc-ì •í™•ë„" class="nav-link" data-scroll-target="#ì •í™•ë„">ì •í™•ë„:</a></li>
  <li><a href="#ì§€ì—°-ì‹œê°„" id="toc-ì§€ì—°-ì‹œê°„" class="nav-link" data-scroll-target="#ì§€ì—°-ì‹œê°„">ì§€ì—° ì‹œê°„:</a></li>
  <li><a href="#í•˜ë“œì›¨ì–´-ì§€ì›" id="toc-í•˜ë“œì›¨ì–´-ì§€ì›" class="nav-link" data-scroll-target="#í•˜ë“œì›¨ì–´-ì§€ì›">í•˜ë“œì›¨ì–´ ì§€ì›:</a></li>
  <li><a href="#ìš”ì•½" id="toc-ìš”ì•½" class="nav-link" data-scroll-target="#ìš”ì•½">ìš”ì•½:</a></li>
  </ul></li>
  </ul>
</nav>
    </div>
<!-- main -->
<main class="content" id="quarto-document-content">

<header id="title-block-header" class="quarto-title-block default">
<div class="quarto-title">
<h1 class="title">ğŸ‘©â€ğŸ’» Lab 2</h1>
  <div class="quarto-categories">
    <div class="quarto-category">lab</div>
    <div class="quarto-category">quantization</div>
    <div class="quarto-category">linear</div>
    <div class="quarto-category">kmeans</div>
  </div>
  </div>

<div>
  <div class="description">
    K-means &amp; Linear Quantization
  </div>
</div>


<div class="quarto-title-meta">

    <div>
    <div class="quarto-title-meta-heading">Author</div>
    <div class="quarto-title-meta-contents">
             <p>Jung Yeon Lee </p>
          </div>
  </div>
    
    <div>
    <div class="quarto-title-meta-heading">Published</div>
    <div class="quarto-title-meta-contents">
      <p class="date">March 6, 2024</p>
    </div>
  </div>
  
    
  </div>
  


</header>


<p>Lecture 5ì™€ 6ì„ í†µí•´ ë°°ìš´ <a href="https://tinyml-kor.github.io/blog/posts/lecs/lec05.html">Quantization ë‚´ìš©</a> ì¤‘ì— K-means Quantizationê³¼ Linear Quantizationì— ëŒ€í•´ ì‹¤ìŠµí•˜ë©° ë°°ì›Œë³´ëŠ” Lab2ì— ëŒ€í•œ í’€ì´ì™€ ì„¤ëª…ì— ëŒ€í•œ í¬ìŠ¤íŒ…ì´ë‹¤. ê¸°ì¡´ì˜ ì‹¤ìŠµ ë…¸íŠ¸ëŠ” <a href="https://hanlab.mit.edu/courses/2023-fall-65940">Original ê°•ì˜</a>ì˜ <a href="https://drive.google.com/file/d/124toPMHDd3z6LiXOhOgHPy6Wvb0Xzw3E/view?usp=sharing">ë§í¬</a>ë¥¼, í•œêµ­ì–´ ë²ˆì—­ê³¼ Solutionì€ <a href="https://github.com/TINYML-KOR/assignment/blob/main/Lab2_cureiuxjy.ipynb">ì´ ë§í¬</a>ë¥¼ ì°¸ê³ í•˜ë©´ ë©ë‹ˆë‹¤. ì•„ë˜ Colaboratory ë²„íŠ¼ì„ ëˆ„ë¥´ë©´ ì‹¤ìŠµë…¸íŠ¸ë¥¼ ë°”ë¡œ ì‹¤í–‰ì‹œí‚¤ëŠ” Colab Notebookì„ ì‹¤í–‰ì‹œí‚¬ ìˆ˜ ìˆìŠµë‹ˆë‹¤.</p>
<p><a href="https://colab.research.google.com/github/TINYML-KOR/assignment/blob/main/Lab2_cureiuxjy.ipynb"><img src="http://img.shields.io/badge/Colaboratory-black?style=for-the-badge&amp;logo=google-colab.png" class="img-fluid"></a></p>
<section id="lab-2-quantization" class="level1">
<h1><strong>Lab 2: Quantization</strong></h1>
<section id="goals" class="level2">
<h2 class="anchored" data-anchor-id="goals">Goals</h2>
<p>ì´ë²ˆ ì‹¤ìŠµì—ì„œëŠ” ëª¨ë¸ í¬ê¸°ì™€ ì§€ì—° ì‹œê°„ì„ ì¤„ì´ê¸° ìœ„í•´ í´ë˜ì‹í•œ <strong>neural network model</strong>ì„ <strong>quantizing</strong>í•˜ëŠ” ì—°ìŠµì„ í•  ê²ƒì…ë‹ˆë‹¤. ì´ ì‹¤ìŠµì˜ ëª©í‘œëŠ” ë‹¤ìŒê³¼ ê°™ìŠµë‹ˆë‹¤:</p>
<ul>
<li><strong>Quantization</strong>ì˜ ê¸°ë³¸ ê°œë…ì„ ì´í•´í•©ë‹ˆë‹¤.</li>
<li><strong>k-means quantization</strong>ì„ êµ¬í˜„í•˜ê³  ì ìš©í•©ë‹ˆë‹¤.</li>
<li>k-means <strong>quantization</strong>ì— ëŒ€í•´ <strong>quantization-aware training</strong>ì„ êµ¬í˜„í•˜ê³  ì ìš©í•©ë‹ˆë‹¤.</li>
<li><strong>linear quantization</strong>ì„ êµ¬í˜„í•˜ê³  ì ìš©í•©ë‹ˆë‹¤.</li>
<li>linear <strong>quantization</strong>ì— ëŒ€í•´ <strong>integer-only inference</strong>ë¥¼ êµ¬í˜„í•˜ê³  ì ìš©í•©ë‹ˆë‹¤.</li>
<li><strong>Quantization</strong>ì—ì„œì˜ ì„±ëŠ¥ ê°œì„ (ì˜ˆ: ì†ë„ í–¥ìƒ)ì— ëŒ€í•œ ê¸°ë³¸ì ì¸ ì´í•´ë¥¼ ì–»ìŠµë‹ˆë‹¤.</li>
<li>ì´ëŸ¬í•œ <strong>quantization</strong> ì ‘ê·¼ ë°©ì‹ ì‚¬ì´ì˜ ì°¨ì´ì ê³¼ íŠ¸ë ˆì´ë“œì˜¤í”„ë¥¼ ì´í•´í•©ë‹ˆë‹¤.</li>
</ul>
</section>
<section id="contents" class="level2">
<h2 class="anchored" data-anchor-id="contents">Contents</h2>
<p>ì£¼ìš” ì„¹ì…˜ì€ <strong><em>K-Means Quantization</em></strong> ê³¼ <strong><em>Linear Quantization</em></strong> 2ê°€ì§€ë¡œ êµ¬ì„±ë˜ì–´ ìˆìŠµë‹ˆë‹¤.</p>
<p>ì´ë²ˆ ì‹¤ìŠµ ë…¸íŠ¸ì—ì„œ ì´ <strong><em>10</em></strong>ê°œì˜ ì§ˆë¬¸ì„ í†µí•´ í•™ìŠµí•˜ê²Œ ë©ë‹ˆë‹¤.:</p>
<ul>
<li><em>K-Means Quantization</em>ì— ëŒ€í•´ì„œëŠ” <strong><em>3</em></strong>ê°œì˜ ì§ˆë¬¸ì´ ìˆìŠµë‹ˆë‹¤ (Question 1-3).</li>
<li><em>Linear Quantization</em>ì— ëŒ€í•´ì„œëŠ” <strong><em>6</em></strong>ê°œì˜ ì§ˆë¬¸ì´ ìˆìŠµë‹ˆë‹¤ (Question 4-9).</li>
<li>Question 10ì€ k-means quantizationê³¼ linear quantizationì„ ë¹„êµí•©ë‹ˆë‹¤.</li>
</ul>
<blockquote class="blockquote">
<p>ì‹¤ìŠµë…¸íŠ¸ì— ëŒ€í•œ ì„¤ì • ë¶€ë¶„(Setup)ì€ Colaboratory Noteë¥¼ ì—´ë©´ í™•ì¸í•˜ì‹¤ ìˆ˜ ìˆìŠµë‹ˆë‹¤. í¬ìŠ¤íŒ…ì—ì„œëŠ” ë³´ë‹¤ ì‹¤ìŠµë‚´ìš©ì— ì§‘ì¤‘í•  ìˆ˜ ìˆë„ë¡ ìƒëµë˜ì–´ ìˆìŠµë‹ˆë‹¤.</p>
</blockquote>
</section>
</section>
<section id="ë¨¼ì €-fp32-modelì˜-ì •í™•ë„ì™€-ëª¨ë¸-í¬ê¸°ë¥¼-í‰ê°€í•´ë´…ì‹œë‹¤" class="level1">
<h1>ë¨¼ì € FP32 Modelì˜ ì •í™•ë„ì™€ ëª¨ë¸ í¬ê¸°ë¥¼ í‰ê°€í•´ë´…ì‹œë‹¤</h1>
<div id="cell-8" class="cell" data-outputid="9ad11587-1f2d-4d98-d3cc-dac6eb5240f1" data-execution_count="15">
<div class="sourceCode cell-code" id="cb1"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb1-1"><a href="#cb1-1" aria-hidden="true" tabindex="-1"></a>fp32_model_accuracy <span class="op">=</span> evaluate(model, dataloader[<span class="st">'test'</span>])</span>
<span id="cb1-2"><a href="#cb1-2" aria-hidden="true" tabindex="-1"></a>fp32_model_size <span class="op">=</span> get_model_size(model)</span>
<span id="cb1-3"><a href="#cb1-3" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>(<span class="ss">f"fp32 model has accuracy=</span><span class="sc">{</span>fp32_model_accuracy<span class="sc">:.2f}</span><span class="ss">%"</span>)</span>
<span id="cb1-4"><a href="#cb1-4" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>(<span class="ss">f"fp32 model has size=</span><span class="sc">{</span>fp32_model_size<span class="op">/</span>MiB<span class="sc">:.2f}</span><span class="ss"> MiB"</span>)</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output cell-output-display">
<script type="application/vnd.jupyter.widget-view+json">
{"model_id":"a1dea41e4428454ba0d567a9b1d96dd9","version_major":2,"version_minor":0,"quarto_mimetype":"application/vnd.jupyter.widget-view+json"}
</script>
</div>
<div class="cell-output cell-output-stdout">
<pre><code>fp32 model has accuracy=92.95%
fp32 model has size=35.20 MiB</code></pre>
</div>
</div>
</section>
<section id="k-means-quantization" class="level1">
<h1>K-Means Quantization</h1>
<p><strong>Network quantization</strong>ì€ deep networkë¥¼ í‘œí˜„í•˜ëŠ” ë° í•„ìš”í•œ ê°€ì¤‘ì¹˜ ë‹¹ ë¹„íŠ¸(bits per weight) ìˆ˜ë¥¼ ì¤„ì—¬ ë„¤íŠ¸ì›Œí¬ë¥¼ ì••ì¶•í•©ë‹ˆë‹¤. <strong>quantized network</strong>ëŠ” í•˜ë“œì›¨ì–´ ì§€ì›ì´ ìˆì„ ê²½ìš° ë” ë¹ ë¥¸ ì¶”ë¡  ì†ë„ë¥¼ ê°€ì§ˆ ìˆ˜ ìˆìŠµë‹ˆë‹¤.</p>
<p>ì´ ì„¹ì…˜ì—ì„œëŠ” <a href="https://arxiv.org/pdf/1510.00149.pdf">Deep Compression: Compressing Deep Neural Networks With Pruning, Trained Quantization And Huffman Coding</a>ì—ì„œì²˜ëŸ¼ ì‹ ê²½ë§ì— ëŒ€í•œ <strong>K-means quantization</strong>ì„ íƒêµ¬í•  ê²ƒì…ë‹ˆë‹¤.</p>
<div class="quarto-figure quarto-figure-center">
<figure class="figure">
<p><img src="../../images/lab02/kmeans.png" class="img-fluid figure-img"></p>
<figcaption>kmeans.png</figcaption>
</figure>
</div>
<blockquote class="blockquote">
<p><code>quantized_weight = codebook.centroids[codebook.labels].view_as(weight)</code></p>
</blockquote>
<p><span class="math inline">\(n\)</span>-bit k-means <strong>quantization</strong>ì€ ì‹œëƒ…ìŠ¤ë¥¼ <span class="math inline">\(2^n\)</span> ê°œì˜ í´ëŸ¬ìŠ¤í„°ë¡œ ë‚˜ëˆ„ê³ , ë™ì¼í•œ í´ëŸ¬ìŠ¤í„° ë‚´ì˜ ì‹œëƒ…ìŠ¤ëŠ” ë™ì¼í•œ ê°€ì¤‘ì¹˜ ê°’ì„ ê³µìœ í•˜ê²Œ ë©ë‹ˆë‹¤.</p>
<p>ë”°ë¼ì„œ, k-means <strong>quantization</strong>ì€ ë‹¤ìŒê³¼ ê°™ì€ codebookì„ ìƒì„±í•©ë‹ˆë‹¤: * <code>centroids</code>: <span class="math inline">\(2^n\)</span> fp32 í´ëŸ¬ìŠ¤í„° ì¤‘ì‹¬. * <code>labels</code>: ì›ë˜ fp32 ê°€ì¤‘ì¹˜ í…ì„œì™€ ë™ì¼í•œ #elementsë¥¼ ê°€ì§„ <span class="math inline">\(n\)</span>-bit ì •ìˆ˜ í…ì„œ. ê° ì •ìˆ˜ëŠ” í•´ë‹¹ í´ëŸ¬ìŠ¤í„°ê°€ ì–´ë””ì— ì†í•˜ëŠ”ì§€ë¥¼ ë‚˜íƒ€ëƒ…ë‹ˆë‹¤.</p>
<p>ì¶”ë¡ í•˜ëŠ” ë™ì•ˆ, codebookì„ ê¸°ë°˜ìœ¼ë¡œ í•œ fp32 í…ì„œê°€ ì¶”ë¡ ì„ ìœ„í•´ ìƒì„±ë©ë‹ˆë‹¤:</p>
<blockquote class="blockquote">
<p><code>quantized_weight = codebook.centroids[codebook.labels].view_as(weight)</code></p>
</blockquote>
<div id="cell-14" class="cell" data-execution_count="16">
<div class="sourceCode cell-code" id="cb3"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb3-1"><a href="#cb3-1" aria-hidden="true" tabindex="-1"></a><span class="im">from</span> collections <span class="im">import</span> namedtuple</span>
<span id="cb3-2"><a href="#cb3-2" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb3-3"><a href="#cb3-3" aria-hidden="true" tabindex="-1"></a>Codebook <span class="op">=</span> namedtuple(<span class="st">'Codebook'</span>, [<span class="st">'centroids'</span>, <span class="st">'labels'</span>])</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</div>
<section id="question-1-10-pts" class="level2">
<h2 class="anchored" data-anchor-id="question-1-10-pts">Question 1 (10 pts)</h2>
<p>ì•„ë˜ì˜ K-Means quantization functionì„ ì™„ì„±í•˜ì„¸ìš”.</p>
<div id="cell-16" class="cell" data-execution_count="17">
<div class="sourceCode cell-code" id="cb4"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb4-1"><a href="#cb4-1" aria-hidden="true" tabindex="-1"></a><span class="im">from</span> fast_pytorch_kmeans <span class="im">import</span> KMeans</span>
<span id="cb4-2"><a href="#cb4-2" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb4-3"><a href="#cb4-3" aria-hidden="true" tabindex="-1"></a><span class="kw">def</span> k_means_quantize(fp32_tensor: torch.Tensor, bitwidth<span class="op">=</span><span class="dv">4</span>, codebook<span class="op">=</span><span class="va">None</span>):</span>
<span id="cb4-4"><a href="#cb4-4" aria-hidden="true" tabindex="-1"></a>    <span class="co">"""</span></span>
<span id="cb4-5"><a href="#cb4-5" aria-hidden="true" tabindex="-1"></a><span class="co">    quantize tensor using k-means clustering</span></span>
<span id="cb4-6"><a href="#cb4-6" aria-hidden="true" tabindex="-1"></a><span class="co">    :param fp32_tensor:</span></span>
<span id="cb4-7"><a href="#cb4-7" aria-hidden="true" tabindex="-1"></a><span class="co">    :param bitwidth: [int] quantization bit width, default=4</span></span>
<span id="cb4-8"><a href="#cb4-8" aria-hidden="true" tabindex="-1"></a><span class="co">    :param codebook: [Codebook] (the cluster centroids, the cluster label tensor)</span></span>
<span id="cb4-9"><a href="#cb4-9" aria-hidden="true" tabindex="-1"></a><span class="co">    :return:</span></span>
<span id="cb4-10"><a href="#cb4-10" aria-hidden="true" tabindex="-1"></a><span class="co">        [Codebook = (centroids, labels)]</span></span>
<span id="cb4-11"><a href="#cb4-11" aria-hidden="true" tabindex="-1"></a><span class="co">            centroids: [torch.(cuda.)FloatTensor] the cluster centroids</span></span>
<span id="cb4-12"><a href="#cb4-12" aria-hidden="true" tabindex="-1"></a><span class="co">            labels: [torch.(cuda.)LongTensor] cluster label tensor</span></span>
<span id="cb4-13"><a href="#cb4-13" aria-hidden="true" tabindex="-1"></a><span class="co">    """</span></span>
<span id="cb4-14"><a href="#cb4-14" aria-hidden="true" tabindex="-1"></a>    <span class="cf">if</span> codebook <span class="kw">is</span> <span class="va">None</span>:</span>
<span id="cb4-15"><a href="#cb4-15" aria-hidden="true" tabindex="-1"></a>        <span class="co">############### YOUR CODE STARTS HERE ###############</span></span>
<span id="cb4-16"><a href="#cb4-16" aria-hidden="true" tabindex="-1"></a>        <span class="co"># get number of clusters based on the quantization precision</span></span>
<span id="cb4-17"><a href="#cb4-17" aria-hidden="true" tabindex="-1"></a>        n_clusters <span class="op">=</span> <span class="dv">2</span> <span class="op">**</span> bitwidth  <span class="co"># Calculate number of clusters as 2^bitwidth</span></span>
<span id="cb4-18"><a href="#cb4-18" aria-hidden="true" tabindex="-1"></a>        <span class="co">############### YOUR CODE ENDS HERE #################</span></span>
<span id="cb4-19"><a href="#cb4-19" aria-hidden="true" tabindex="-1"></a>        <span class="co"># use k-means to get the quantization centroids</span></span>
<span id="cb4-20"><a href="#cb4-20" aria-hidden="true" tabindex="-1"></a>        kmeans <span class="op">=</span> KMeans(n_clusters<span class="op">=</span>n_clusters, mode<span class="op">=</span><span class="st">'euclidean'</span>, verbose<span class="op">=</span><span class="dv">0</span>)</span>
<span id="cb4-21"><a href="#cb4-21" aria-hidden="true" tabindex="-1"></a>        labels <span class="op">=</span> kmeans.fit_predict(fp32_tensor.view(<span class="op">-</span><span class="dv">1</span>, <span class="dv">1</span>)).to(torch.<span class="bu">long</span>)</span>
<span id="cb4-22"><a href="#cb4-22" aria-hidden="true" tabindex="-1"></a>        centroids <span class="op">=</span> kmeans.centroids.to(torch.<span class="bu">float</span>).view(<span class="op">-</span><span class="dv">1</span>)</span>
<span id="cb4-23"><a href="#cb4-23" aria-hidden="true" tabindex="-1"></a>        codebook <span class="op">=</span> Codebook(centroids, labels)</span>
<span id="cb4-24"><a href="#cb4-24" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb4-25"><a href="#cb4-25" aria-hidden="true" tabindex="-1"></a>    <span class="co">############### YOUR CODE STARTS HERE ###############</span></span>
<span id="cb4-26"><a href="#cb4-26" aria-hidden="true" tabindex="-1"></a>    <span class="co"># decode the codebook into k-means quantized tensor for inference</span></span>
<span id="cb4-27"><a href="#cb4-27" aria-hidden="true" tabindex="-1"></a>    <span class="co"># hint: one line of code</span></span>
<span id="cb4-28"><a href="#cb4-28" aria-hidden="true" tabindex="-1"></a>    quantized_tensor <span class="op">=</span> codebook.centroids[codebook.labels].view_as(fp32_tensor)</span>
<span id="cb4-29"><a href="#cb4-29" aria-hidden="true" tabindex="-1"></a>    <span class="co">############### YOUR CODE ENDS HERE #################</span></span>
<span id="cb4-30"><a href="#cb4-30" aria-hidden="true" tabindex="-1"></a>    fp32_tensor.set_(quantized_tensor.view_as(fp32_tensor))</span>
<span id="cb4-31"><a href="#cb4-31" aria-hidden="true" tabindex="-1"></a>    <span class="cf">return</span> codebook</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</div>
<p>ìœ„ì—ì„œ ì‘ì„±í•œ k-means quantization functionì„ ë”ë¯¸ í…ì„œì— ì ìš©í•˜ì—¬ í™•ì¸í•´ë´…ì‹œë‹¤.</p>
<div id="cell-18" class="cell" data-outputid="3e3df421-5ba8-4599-8099-a3feb6c84930" data-execution_count="18">
<div class="sourceCode cell-code" id="cb5"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb5-1"><a href="#cb5-1" aria-hidden="true" tabindex="-1"></a>test_k_means_quantize()</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output cell-output-stdout">
<pre><code>tensor([[-0.3747,  0.0874,  0.3200, -0.4868,  0.4404],
        [-0.0402,  0.2322, -0.2024, -0.4986,  0.1814],
        [ 0.3102, -0.3942, -0.2030,  0.0883, -0.4741],
        [-0.1592, -0.0777, -0.3946, -0.2128,  0.2675],
        [ 0.0611, -0.1933, -0.4350,  0.2928, -0.1087]])
* Test k_means_quantize()
    target bitwidth: 2 bits
        num unique values before k-means quantization: 25
        num unique values after  k-means quantization: 4
* Test passed.</code></pre>
</div>
<div class="cell-output cell-output-display">
<div>
<figure class="figure">
<p><img src="lab02_files/figure-html/cell-5-output-2.png" class="img-fluid figure-img"></p>
</figure>
</div>
</div>
</div>
</section>
<section id="question-2-10-pts" class="level2">
<h2 class="anchored" data-anchor-id="question-2-10-pts">Question 2 (10 pts)</h2>
<p>ë§ˆì§€ë§‰ ì½”ë“œ ì…€ì€ 2ë¹„íŠ¸ k-means quantizationì„ ìˆ˜í–‰í•˜ê³  quantization ì „í›„ì˜ í…ì„œë¥¼ í”Œë¡¯í•©ë‹ˆë‹¤. ê° í´ëŸ¬ìŠ¤í„°ëŠ” ê³ ìœ í•œ ìƒ‰ìƒìœ¼ë¡œ ë Œë”ë§ë˜ë©°, quantized í…ì„œë“¤ì´ 4(<span class="math inline">\(2^2\)</span>)ê°€ì§€ ê³ ìœ í•œ ìƒ‰ìƒìœ¼ë¡œ í‘œì‹œë©ë‹ˆë‹¤.</p>
<p>ì´ëŸ¬í•œ í˜„ìƒì„ ê´€ì°°í•œ ê²ƒì„ ë°”íƒ•ìœ¼ë¡œ ì§ˆë¬¸ë“¤ì— ë‹µí•˜ì„¸ìš”.</p>
<section id="question-2.1-5-pts" class="level3">
<h3 class="anchored" data-anchor-id="question-2.1-5-pts">Question 2.1 (5 pts)</h3>
<p>4ë¹„íŠ¸ë¡œ k-means quantizationì´ ìˆ˜í–‰ë˜ë©´, quantized í…ì„œì—ëŠ” ëª‡ ê°œì˜ ê³ ìœ í•œ ìƒ‰ìƒì´ ë Œë”ë§ë ê¹Œìš”?</p>
<p><strong>Your Answer:</strong></p>
<p>4ë¹„íŠ¸ k-means quantizationì´ ìˆ˜í–‰ë˜ë©´, quantized í…ì„œì— <span class="math inline">\((2^4 = 16)\)</span>ê°œì˜ ê³ ìœ í•œ ìƒ‰ìƒì´ ë Œë”ë§ë©ë‹ˆë‹¤. ì´ëŠ” 4ë¹„íŠ¸ë¡œ 0000ë¶€í„° 1111ê¹Œì§€ì˜ 16ê°€ì§€ ë‹¤ë¥¸ ìƒíƒœë‚˜ ì¡°í•©ì„ ë‚˜íƒ€ë‚¼ ìˆ˜ ìˆìœ¼ë©°, ì´ëŠ” í…ì„œ ê°’ì´ ê·¸ë£¹í™”ë  ìˆ˜ ìˆëŠ” 16ê°œì˜ ê³ ìœ í•œ í´ëŸ¬ìŠ¤í„°ì— í•´ë‹¹í•©ë‹ˆë‹¤.</p>
</section>
<section id="question-2.2-5-pts" class="level3">
<h3 class="anchored" data-anchor-id="question-2.2-5-pts">Question 2.2 (5 pts)</h3>
<p><em>n</em>-ë¹„íŠ¸ k-means quantizationì´ ìˆ˜í–‰ë˜ë©´, quantized í…ì„œì— ëª‡ ê°œì˜ ê³ ìœ í•œ ìƒ‰ìƒì´ ë Œë”ë§ë ê¹Œìš”?</p>
<p><strong>Your Answer:</strong></p>
<p><em>n</em>-ë¹„íŠ¸ k-means quantizationì´ ìˆ˜í–‰ë˜ë©´, quantized í…ì„œì—ëŠ” <span class="math inline">\((2^n)\)</span>ê°œì˜ ê³ ìœ í•œ ìƒ‰ìƒì´ ë Œë”ë§ ë©ë‹ˆë‹¤. ì´ëŠ” <em>n</em>ë¹„íŠ¸ë¥¼ ì‚¬ìš©í•˜ì—¬ <span class="math inline">\((2^n)\)</span>ê°œì˜ ë‹¤ë¥¸ ìƒíƒœë‚˜ ì¡°í•©ì„ ë‚˜íƒ€ë‚¼ ìˆ˜ ìˆìœ¼ë©°, ì´ëŠ” í…ì„œ ê°’ì´ ê·¸ë£¹í™”ë  ìˆ˜ ìˆëŠ” <span class="math inline">\((2^n)\)</span>ê°œì˜ ê³ ìœ í•œ í´ëŸ¬ìŠ¤í„°ì— í•´ë‹¹í•©ë‹ˆë‹¤.</p>
</section>
</section>
<section id="k-means-quantization-on-whole-model" class="level2">
<h2 class="anchored" data-anchor-id="k-means-quantization-on-whole-model">K-Means Quantization on Whole Model</h2>
<p>lab 1ì—ì„œ í–ˆë˜ ê²ƒê³¼ ìœ ì‚¬í•˜ê²Œ, ì´ì œ ì „ì²´ ëª¨ë¸ì„ quantizingí•˜ê¸° ìœ„í•´ k-means quantization í•¨ìˆ˜ë¥¼ í´ë˜ìŠ¤ë¡œ ë˜í•‘í•©ë‹ˆë‹¤. <code>KMeansQuantizer</code> í´ë˜ìŠ¤ì—ì„œëŠ” ëª¨ë¸ ê°€ì¤‘ì¹˜ê°€ ë³€ê²½ë  ë•Œë§ˆë‹¤ codebooks(i.e., <code>centroids</code>ì™€ <code>labels</code>)ì„ ì ìš©í•˜ê±°ë‚˜ ì—…ë°ì´íŠ¸í•  ìˆ˜ ìˆë„ë¡ codebooksì˜ ë³€í™”ë¥¼ ê¸°ë¡í•´ì•¼ í•©ë‹ˆë‹¤.</p>
<div id="cell-26" class="cell" data-execution_count="19">
<div class="sourceCode cell-code" id="cb7"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb7-1"><a href="#cb7-1" aria-hidden="true" tabindex="-1"></a><span class="im">from</span> torch.nn <span class="im">import</span> parameter</span>
<span id="cb7-2"><a href="#cb7-2" aria-hidden="true" tabindex="-1"></a><span class="kw">class</span> KMeansQuantizer:</span>
<span id="cb7-3"><a href="#cb7-3" aria-hidden="true" tabindex="-1"></a>    <span class="kw">def</span> <span class="fu">__init__</span>(<span class="va">self</span>, model : nn.Module, bitwidth<span class="op">=</span><span class="dv">4</span>):</span>
<span id="cb7-4"><a href="#cb7-4" aria-hidden="true" tabindex="-1"></a>        <span class="va">self</span>.codebook <span class="op">=</span> KMeansQuantizer.quantize(model, bitwidth)</span>
<span id="cb7-5"><a href="#cb7-5" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb7-6"><a href="#cb7-6" aria-hidden="true" tabindex="-1"></a>    <span class="at">@torch.no_grad</span>()</span>
<span id="cb7-7"><a href="#cb7-7" aria-hidden="true" tabindex="-1"></a>    <span class="kw">def</span> <span class="bu">apply</span>(<span class="va">self</span>, model, update_centroids):</span>
<span id="cb7-8"><a href="#cb7-8" aria-hidden="true" tabindex="-1"></a>        <span class="cf">for</span> name, param <span class="kw">in</span> model.named_parameters():</span>
<span id="cb7-9"><a href="#cb7-9" aria-hidden="true" tabindex="-1"></a>            <span class="cf">if</span> name <span class="kw">in</span> <span class="va">self</span>.codebook:</span>
<span id="cb7-10"><a href="#cb7-10" aria-hidden="true" tabindex="-1"></a>                <span class="cf">if</span> update_centroids:</span>
<span id="cb7-11"><a href="#cb7-11" aria-hidden="true" tabindex="-1"></a>                    update_codebook(param, codebook<span class="op">=</span><span class="va">self</span>.codebook[name])</span>
<span id="cb7-12"><a href="#cb7-12" aria-hidden="true" tabindex="-1"></a>                <span class="va">self</span>.codebook[name] <span class="op">=</span> k_means_quantize(</span>
<span id="cb7-13"><a href="#cb7-13" aria-hidden="true" tabindex="-1"></a>                    param, codebook<span class="op">=</span><span class="va">self</span>.codebook[name])</span>
<span id="cb7-14"><a href="#cb7-14" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb7-15"><a href="#cb7-15" aria-hidden="true" tabindex="-1"></a>    <span class="at">@staticmethod</span></span>
<span id="cb7-16"><a href="#cb7-16" aria-hidden="true" tabindex="-1"></a>    <span class="at">@torch.no_grad</span>()</span>
<span id="cb7-17"><a href="#cb7-17" aria-hidden="true" tabindex="-1"></a>    <span class="kw">def</span> quantize(model: nn.Module, bitwidth<span class="op">=</span><span class="dv">4</span>):</span>
<span id="cb7-18"><a href="#cb7-18" aria-hidden="true" tabindex="-1"></a>        codebook <span class="op">=</span> <span class="bu">dict</span>()</span>
<span id="cb7-19"><a href="#cb7-19" aria-hidden="true" tabindex="-1"></a>        <span class="cf">if</span> <span class="bu">isinstance</span>(bitwidth, <span class="bu">dict</span>):</span>
<span id="cb7-20"><a href="#cb7-20" aria-hidden="true" tabindex="-1"></a>            <span class="cf">for</span> name, param <span class="kw">in</span> model.named_parameters():</span>
<span id="cb7-21"><a href="#cb7-21" aria-hidden="true" tabindex="-1"></a>                <span class="cf">if</span> name <span class="kw">in</span> bitwidth:</span>
<span id="cb7-22"><a href="#cb7-22" aria-hidden="true" tabindex="-1"></a>                    codebook[name] <span class="op">=</span> k_means_quantize(param, bitwidth<span class="op">=</span>bitwidth[name])</span>
<span id="cb7-23"><a href="#cb7-23" aria-hidden="true" tabindex="-1"></a>        <span class="cf">else</span>:</span>
<span id="cb7-24"><a href="#cb7-24" aria-hidden="true" tabindex="-1"></a>            <span class="cf">for</span> name, param <span class="kw">in</span> model.named_parameters():</span>
<span id="cb7-25"><a href="#cb7-25" aria-hidden="true" tabindex="-1"></a>                <span class="cf">if</span> param.dim() <span class="op">&gt;</span> <span class="dv">1</span>:</span>
<span id="cb7-26"><a href="#cb7-26" aria-hidden="true" tabindex="-1"></a>                    codebook[name] <span class="op">=</span> k_means_quantize(param, bitwidth<span class="op">=</span>bitwidth)</span>
<span id="cb7-27"><a href="#cb7-27" aria-hidden="true" tabindex="-1"></a>        <span class="cf">return</span> codebook</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</div>
<p>ì´ì œ K-Means Quantizationì„ ì‚¬ìš©í•˜ì—¬ ëª¨ë¸ì„ 8ë¹„íŠ¸, 4ë¹„íŠ¸, 2ë¹„íŠ¸ë¡œ quantizeí•´ë´…ì‹œë‹¤. <em>ëª¨ë¸ í¬ê¸°ë¥¼ ê³„ì‚°í•  ë•Œ codebooksì˜ ì €ì¥ ê³µê°„ì€ ë¬´ì‹œí•œë‹¤ëŠ” ì ì„ ìœ ì˜í•˜ì„¸ìš”.</em></p>
<div id="cell-28" class="cell" data-outputid="af180b7a-6aef-44e2-f657-53b9c6d6c384" data-execution_count="20">
<div class="sourceCode cell-code" id="cb8"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb8-1"><a href="#cb8-1" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>(<span class="st">'Note that the storage for codebooks is ignored when calculating the model size.'</span>)</span>
<span id="cb8-2"><a href="#cb8-2" aria-hidden="true" tabindex="-1"></a>quantizers <span class="op">=</span> <span class="bu">dict</span>()</span>
<span id="cb8-3"><a href="#cb8-3" aria-hidden="true" tabindex="-1"></a><span class="cf">for</span> bitwidth <span class="kw">in</span> [<span class="dv">8</span>, <span class="dv">4</span>, <span class="dv">2</span>]:</span>
<span id="cb8-4"><a href="#cb8-4" aria-hidden="true" tabindex="-1"></a>    recover_model()</span>
<span id="cb8-5"><a href="#cb8-5" aria-hidden="true" tabindex="-1"></a>    <span class="bu">print</span>(<span class="ss">f'k-means quantizing model into </span><span class="sc">{</span>bitwidth<span class="sc">}</span><span class="ss"> bits'</span>)</span>
<span id="cb8-6"><a href="#cb8-6" aria-hidden="true" tabindex="-1"></a>    quantizer <span class="op">=</span> KMeansQuantizer(model, bitwidth)</span>
<span id="cb8-7"><a href="#cb8-7" aria-hidden="true" tabindex="-1"></a>    quantized_model_size <span class="op">=</span> get_model_size(model, bitwidth)</span>
<span id="cb8-8"><a href="#cb8-8" aria-hidden="true" tabindex="-1"></a>    <span class="bu">print</span>(<span class="ss">f"    </span><span class="sc">{</span>bitwidth<span class="sc">}</span><span class="ss">-bit k-means quantized model has size=</span><span class="sc">{</span>quantized_model_size<span class="op">/</span>MiB<span class="sc">:.2f}</span><span class="ss"> MiB"</span>)</span>
<span id="cb8-9"><a href="#cb8-9" aria-hidden="true" tabindex="-1"></a>    quantized_model_accuracy <span class="op">=</span> evaluate(model, dataloader[<span class="st">'test'</span>])</span>
<span id="cb8-10"><a href="#cb8-10" aria-hidden="true" tabindex="-1"></a>    <span class="bu">print</span>(<span class="ss">f"    </span><span class="sc">{</span>bitwidth<span class="sc">}</span><span class="ss">-bit k-means quantized model has accuracy=</span><span class="sc">{</span>quantized_model_accuracy<span class="sc">:.2f}</span><span class="ss">%"</span>)</span>
<span id="cb8-11"><a href="#cb8-11" aria-hidden="true" tabindex="-1"></a>    quantizers[bitwidth] <span class="op">=</span> quantizer</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output cell-output-stdout">
<pre><code>Note that the storage for codebooks is ignored when calculating the model size.
k-means quantizing model into 8 bits
    8-bit k-means quantized model has size=8.80 MiB
    8-bit k-means quantized model has accuracy=92.76%
k-means quantizing model into 4 bits
    4-bit k-means quantized model has size=4.40 MiB
    4-bit k-means quantized model has accuracy=79.07%
k-means quantizing model into 2 bits
    2-bit k-means quantized model has size=2.20 MiB
    2-bit k-means quantized model has accuracy=10.00%</code></pre>
</div>
<div class="cell-output cell-output-display">
<script type="application/vnd.jupyter.widget-view+json">
{"model_id":"892325c089bc45b2b51574a424d5038a","version_major":2,"version_minor":0,"quarto_mimetype":"application/vnd.jupyter.widget-view+json"}
</script>
</div>
<div class="cell-output cell-output-display">
<script type="application/vnd.jupyter.widget-view+json">
{"model_id":"62c8dc04cb33411284907e34d1993bf0","version_major":2,"version_minor":0,"quarto_mimetype":"application/vnd.jupyter.widget-view+json"}
</script>
</div>
<div class="cell-output cell-output-display">
<script type="application/vnd.jupyter.widget-view+json">
{"model_id":"71d18c640eb54aad81c6b1babb91410a","version_major":2,"version_minor":0,"quarto_mimetype":"application/vnd.jupyter.widget-view+json"}
</script>
</div>
</div>
</section>
<section id="trained-k-means-quantization" class="level2">
<h2 class="anchored" data-anchor-id="trained-k-means-quantization">Trained K-Means Quantization</h2>
<p>ë§ˆì§€ë§‰ ì…€ì˜ ê²°ê³¼ì—ì„œ ë³¼ ìˆ˜ ìˆë“¯ì´, ëª¨ë¸ì„ ì ì€ ë¹„íŠ¸ë¡œ quantizeí•  ë•Œ ì •í™•ë„ê°€ í¬ê²Œ ë–¨ì–´ì§‘ë‹ˆë‹¤. ë”°ë¼ì„œ, ì •í™•ë„ë¥¼ íšŒë³µí•˜ê¸° ìœ„í•´ <strong>quantization-aware training</strong>ì„ í•´ì•¼ í•©ë‹ˆë‹¤.</p>
<p>k-means quantization-aware í›ˆë ¨ ë™ì•ˆ, centroidsë„ ì—…ë°ì´íŠ¸ë©ë‹ˆë‹¤. ì´ëŠ” <a href="https://arxiv.org/pdf/1510.00149.pdf">Deep Compression: Compressing Deep Neural Networks With Pruning, Trained Quantization And Huffman Coding</a>ì—ì„œ ì œì•ˆë˜ì—ˆìŠµë‹ˆë‹¤.</p>
<p>centroidsì— ëŒ€í•œ ê·¸ë˜ë””ì–¸íŠ¸ëŠ” ë‹¤ìŒê³¼ ê°™ì´ ê³„ì‚°ë©ë‹ˆë‹¤,</p>
<blockquote class="blockquote">
<p><span class="math inline">\(\frac{\partial \mathcal{L} }{\partial C_k} = \sum_{j} \frac{\partial \mathcal{L} }{\partial W_{j}} \frac{\partial W_{j} }{\partial C_k} = \sum_{j} \frac{\partial \mathcal{L} }{\partial W_{j}} \mathbf{1}(I_{j}=k)\)</span></p>
</blockquote>
<p>ì—¬ê¸°ì„œ <span class="math inline">\(\mathcal{L}\)</span>ì€ ì†ì‹¤, <span class="math inline">\(C_k\)</span>ëŠ” <em>k</em>-ë²ˆì§¸ centroid, <span class="math inline">\(I_{j}\)</span>ëŠ” ê°€ì¤‘ì¹˜ <span class="math inline">\(W_{j}\)</span>ì˜ ë¼ë²¨ì…ë‹ˆë‹¤.</p>
<p><span class="math inline">\(\mathbf{1}()\)</span>ì€ ì§€ì‹œ í•¨ìˆ˜ì´ë©°, <span class="math inline">\(\mathbf{1}(I_{j}=k)\)</span>ëŠ” <span class="math inline">\(1\;\mathrm{if}\;I_{j}=k\;\mathrm{else}\;0\)</span>, <em>ì¦‰</em>, <span class="math inline">\(I_{j}==k\)</span>ë¥¼ ì˜ë¯¸í•©ë‹ˆë‹¤.</p>
<p>labì—ì„œëŠ” <strong>ê°„ë‹¨íˆ</strong> ìµœì‹  ê°€ì¤‘ì¹˜ì— ë”°ë¼ centroidsë¥¼ ì§ì ‘ ì—…ë°ì´íŠ¸í•©ë‹ˆë‹¤:</p>
<blockquote class="blockquote">
<p><span class="math inline">\(C_k = \frac{\sum_{j}W_{j}\mathbf{1}(I_{j}=k)}{\sum_{j}\mathbf{1}(I_{j}=k)}\)</span></p>
</blockquote>
<section id="question-3-10-pts" class="level3">
<h3 class="anchored" data-anchor-id="question-3-10-pts">Question 3 (10 pts)</h3>
<p>ì•„ë˜ì˜ codebook update functionì„ ì™„ì„±í•˜ì„¸ìš”.</p>
<p><strong>Hint</strong>:</p>
<p>ìœ„ì˜ centroidsë¥¼ ì—…ë°ì´íŠ¸í•˜ëŠ” ë°©ì •ì‹ì€ ì‹¤ì œë¡œ ë™ì¼í•œ í´ëŸ¬ìŠ¤í„°ì— ìˆëŠ” ê°€ì¤‘ì¹˜ì˜ <code>í‰ê· (mean)</code>ì„ ì—…ë°ì´íŠ¸ëœ centroid ê°’ìœ¼ë¡œ ì‚¬ìš©í•˜ê³  ìˆìŠµë‹ˆë‹¤.</p>
<div id="cell-31" class="cell" data-execution_count="21">
<div class="sourceCode cell-code" id="cb10"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb10-1"><a href="#cb10-1" aria-hidden="true" tabindex="-1"></a><span class="kw">def</span> update_codebook(fp32_tensor: torch.Tensor, codebook: Codebook):</span>
<span id="cb10-2"><a href="#cb10-2" aria-hidden="true" tabindex="-1"></a>    <span class="co">"""</span></span>
<span id="cb10-3"><a href="#cb10-3" aria-hidden="true" tabindex="-1"></a><span class="co">    update the centroids in the codebook using updated fp32_tensor</span></span>
<span id="cb10-4"><a href="#cb10-4" aria-hidden="true" tabindex="-1"></a><span class="co">    :param fp32_tensor: [torch.(cuda.)Tensor]</span></span>
<span id="cb10-5"><a href="#cb10-5" aria-hidden="true" tabindex="-1"></a><span class="co">    :param codebook: [Codebook] (the cluster centroids, the cluster label tensor)</span></span>
<span id="cb10-6"><a href="#cb10-6" aria-hidden="true" tabindex="-1"></a><span class="co">    """</span></span>
<span id="cb10-7"><a href="#cb10-7" aria-hidden="true" tabindex="-1"></a>    n_clusters <span class="op">=</span> codebook.centroids.numel()</span>
<span id="cb10-8"><a href="#cb10-8" aria-hidden="true" tabindex="-1"></a>    fp32_tensor <span class="op">=</span> fp32_tensor.view(<span class="op">-</span><span class="dv">1</span>)</span>
<span id="cb10-9"><a href="#cb10-9" aria-hidden="true" tabindex="-1"></a>    <span class="cf">for</span> k <span class="kw">in</span> <span class="bu">range</span>(n_clusters):</span>
<span id="cb10-10"><a href="#cb10-10" aria-hidden="true" tabindex="-1"></a>    <span class="co">############### YOUR CODE STARTS HERE ###############</span></span>
<span id="cb10-11"><a href="#cb10-11" aria-hidden="true" tabindex="-1"></a>        codebook.centroids[k] <span class="op">=</span> fp32_tensor[codebook.labels <span class="op">==</span> k].mean()</span>
<span id="cb10-12"><a href="#cb10-12" aria-hidden="true" tabindex="-1"></a>    <span class="co">############### YOUR CODE ENDS HERE #################</span></span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</div>
<p>ì´ì œ ë‹¤ìŒ ì½”ë“œ ì…€ì„ ì‹¤í–‰í•˜ì—¬ k-means quantized ëª¨ë¸ì„ finetuningí•˜ì—¬ ì •í™•ë„ë¥¼ íšŒë³µí•´ë´…ì‹œë‹¤. ì •í™•ë„ í•˜ë½ì´ <em>0.5ë³´ë‹¤ ì‘ìœ¼ë©´</em> finetuningì„ ì¤‘ë‹¨í•©ë‹ˆë‹¤.</p>
<div id="cell-33" class="cell" data-outputid="fb05c2e6-8e2b-4593-c04d-cc0bca6cd9b5" data-execution_count="22">
<div class="sourceCode cell-code" id="cb11"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb11-1"><a href="#cb11-1" aria-hidden="true" tabindex="-1"></a>accuracy_drop_threshold <span class="op">=</span> <span class="fl">0.5</span></span>
<span id="cb11-2"><a href="#cb11-2" aria-hidden="true" tabindex="-1"></a>quantizers_before_finetune <span class="op">=</span> copy.deepcopy(quantizers)</span>
<span id="cb11-3"><a href="#cb11-3" aria-hidden="true" tabindex="-1"></a>quantizers_after_finetune <span class="op">=</span> quantizers</span>
<span id="cb11-4"><a href="#cb11-4" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb11-5"><a href="#cb11-5" aria-hidden="true" tabindex="-1"></a><span class="cf">for</span> bitwidth <span class="kw">in</span> [<span class="dv">8</span>, <span class="dv">4</span>, <span class="dv">2</span>]:</span>
<span id="cb11-6"><a href="#cb11-6" aria-hidden="true" tabindex="-1"></a>    recover_model()</span>
<span id="cb11-7"><a href="#cb11-7" aria-hidden="true" tabindex="-1"></a>    quantizer <span class="op">=</span> quantizers[bitwidth]</span>
<span id="cb11-8"><a href="#cb11-8" aria-hidden="true" tabindex="-1"></a>    <span class="bu">print</span>(<span class="ss">f'k-means quantizing model into </span><span class="sc">{</span>bitwidth<span class="sc">}</span><span class="ss"> bits'</span>)</span>
<span id="cb11-9"><a href="#cb11-9" aria-hidden="true" tabindex="-1"></a>    quantizer.<span class="bu">apply</span>(model, update_centroids<span class="op">=</span><span class="va">False</span>)</span>
<span id="cb11-10"><a href="#cb11-10" aria-hidden="true" tabindex="-1"></a>    quantized_model_size <span class="op">=</span> get_model_size(model, bitwidth)</span>
<span id="cb11-11"><a href="#cb11-11" aria-hidden="true" tabindex="-1"></a>    <span class="bu">print</span>(<span class="ss">f"    </span><span class="sc">{</span>bitwidth<span class="sc">}</span><span class="ss">-bit k-means quantized model has size=</span><span class="sc">{</span>quantized_model_size<span class="op">/</span>MiB<span class="sc">:.2f}</span><span class="ss"> MiB"</span>)</span>
<span id="cb11-12"><a href="#cb11-12" aria-hidden="true" tabindex="-1"></a>    quantized_model_accuracy <span class="op">=</span> evaluate(model, dataloader[<span class="st">'test'</span>])</span>
<span id="cb11-13"><a href="#cb11-13" aria-hidden="true" tabindex="-1"></a>    <span class="bu">print</span>(<span class="ss">f"    </span><span class="sc">{</span>bitwidth<span class="sc">}</span><span class="ss">-bit k-means quantized model has accuracy=</span><span class="sc">{</span>quantized_model_accuracy<span class="sc">:.2f}</span><span class="ss">% before quantization-aware training "</span>)</span>
<span id="cb11-14"><a href="#cb11-14" aria-hidden="true" tabindex="-1"></a>    accuracy_drop <span class="op">=</span> fp32_model_accuracy <span class="op">-</span> quantized_model_accuracy</span>
<span id="cb11-15"><a href="#cb11-15" aria-hidden="true" tabindex="-1"></a>    <span class="cf">if</span> accuracy_drop <span class="op">&gt;</span> accuracy_drop_threshold:</span>
<span id="cb11-16"><a href="#cb11-16" aria-hidden="true" tabindex="-1"></a>        <span class="bu">print</span>(<span class="ss">f"        Quantization-aware training due to accuracy drop=</span><span class="sc">{</span>accuracy_drop<span class="sc">:.2f}</span><span class="ss">% is larger than threshold=</span><span class="sc">{</span>accuracy_drop_threshold<span class="sc">:.2f}</span><span class="ss">%"</span>)</span>
<span id="cb11-17"><a href="#cb11-17" aria-hidden="true" tabindex="-1"></a>        num_finetune_epochs <span class="op">=</span> <span class="dv">5</span></span>
<span id="cb11-18"><a href="#cb11-18" aria-hidden="true" tabindex="-1"></a>        optimizer <span class="op">=</span> torch.optim.SGD(model.parameters(), lr<span class="op">=</span><span class="fl">0.01</span>, momentum<span class="op">=</span><span class="fl">0.9</span>)</span>
<span id="cb11-19"><a href="#cb11-19" aria-hidden="true" tabindex="-1"></a>        scheduler <span class="op">=</span> torch.optim.lr_scheduler.CosineAnnealingLR(optimizer, num_finetune_epochs)</span>
<span id="cb11-20"><a href="#cb11-20" aria-hidden="true" tabindex="-1"></a>        criterion <span class="op">=</span> nn.CrossEntropyLoss()</span>
<span id="cb11-21"><a href="#cb11-21" aria-hidden="true" tabindex="-1"></a>        best_accuracy <span class="op">=</span> <span class="dv">0</span></span>
<span id="cb11-22"><a href="#cb11-22" aria-hidden="true" tabindex="-1"></a>        epoch <span class="op">=</span> num_finetune_epochs</span>
<span id="cb11-23"><a href="#cb11-23" aria-hidden="true" tabindex="-1"></a>        <span class="cf">while</span> accuracy_drop <span class="op">&gt;</span> accuracy_drop_threshold <span class="kw">and</span> epoch <span class="op">&gt;</span> <span class="dv">0</span>:</span>
<span id="cb11-24"><a href="#cb11-24" aria-hidden="true" tabindex="-1"></a>            train(model, dataloader[<span class="st">'train'</span>], criterion, optimizer, scheduler,</span>
<span id="cb11-25"><a href="#cb11-25" aria-hidden="true" tabindex="-1"></a>                  callbacks<span class="op">=</span>[<span class="kw">lambda</span>: quantizer.<span class="bu">apply</span>(model, update_centroids<span class="op">=</span><span class="va">True</span>)])</span>
<span id="cb11-26"><a href="#cb11-26" aria-hidden="true" tabindex="-1"></a>            model_accuracy <span class="op">=</span> evaluate(model, dataloader[<span class="st">'test'</span>])</span>
<span id="cb11-27"><a href="#cb11-27" aria-hidden="true" tabindex="-1"></a>            is_best <span class="op">=</span> model_accuracy <span class="op">&gt;</span> best_accuracy</span>
<span id="cb11-28"><a href="#cb11-28" aria-hidden="true" tabindex="-1"></a>            best_accuracy <span class="op">=</span> <span class="bu">max</span>(model_accuracy, best_accuracy)</span>
<span id="cb11-29"><a href="#cb11-29" aria-hidden="true" tabindex="-1"></a>            <span class="bu">print</span>(<span class="ss">f'        Epoch </span><span class="sc">{</span>num_finetune_epochs<span class="op">-</span>epoch<span class="sc">}</span><span class="ss"> Accuracy </span><span class="sc">{</span>model_accuracy<span class="sc">:.2f}</span><span class="ss">% / Best Accuracy: </span><span class="sc">{</span>best_accuracy<span class="sc">:.2f}</span><span class="ss">%'</span>)</span>
<span id="cb11-30"><a href="#cb11-30" aria-hidden="true" tabindex="-1"></a>            accuracy_drop <span class="op">=</span> fp32_model_accuracy <span class="op">-</span> best_accuracy</span>
<span id="cb11-31"><a href="#cb11-31" aria-hidden="true" tabindex="-1"></a>            epoch <span class="op">-=</span> <span class="dv">1</span></span>
<span id="cb11-32"><a href="#cb11-32" aria-hidden="true" tabindex="-1"></a>    <span class="cf">else</span>:</span>
<span id="cb11-33"><a href="#cb11-33" aria-hidden="true" tabindex="-1"></a>        <span class="bu">print</span>(<span class="ss">f"        No need for quantization-aware training since accuracy drop=</span><span class="sc">{</span>accuracy_drop<span class="sc">:.2f}</span><span class="ss">% is smaller than threshold=</span><span class="sc">{</span>accuracy_drop_threshold<span class="sc">:.2f}</span><span class="ss">%"</span>)</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output cell-output-stdout">
<pre><code>k-means quantizing model into 8 bits
    8-bit k-means quantized model has size=8.80 MiB
    8-bit k-means quantized model has accuracy=92.76% before quantization-aware training 
        No need for quantization-aware training since accuracy drop=0.19% is smaller than threshold=0.50%
k-means quantizing model into 4 bits
    4-bit k-means quantized model has size=4.40 MiB
    4-bit k-means quantized model has accuracy=79.07% before quantization-aware training 
        Quantization-aware training due to accuracy drop=13.88% is larger than threshold=0.50%
        Epoch 0 Accuracy 92.47% / Best Accuracy: 92.47%
k-means quantizing model into 2 bits
    2-bit k-means quantized model has size=2.20 MiB
    2-bit k-means quantized model has accuracy=10.00% before quantization-aware training 
        Quantization-aware training due to accuracy drop=82.95% is larger than threshold=0.50%
        Epoch 0 Accuracy 90.21% / Best Accuracy: 90.21%
        Epoch 1 Accuracy 90.82% / Best Accuracy: 90.82%
        Epoch 2 Accuracy 91.00% / Best Accuracy: 91.00%
        Epoch 3 Accuracy 91.12% / Best Accuracy: 91.12%
        Epoch 4 Accuracy 91.17% / Best Accuracy: 91.17%</code></pre>
</div>
<div class="cell-output cell-output-display">
<script type="application/vnd.jupyter.widget-view+json">
{"model_id":"79186f919eb949668ed56c09c7559445","version_major":2,"version_minor":0,"quarto_mimetype":"application/vnd.jupyter.widget-view+json"}
</script>
</div>
<div class="cell-output cell-output-display">
<script type="application/vnd.jupyter.widget-view+json">
{"model_id":"456e13dfded7478cb6be376dc6ad3eb5","version_major":2,"version_minor":0,"quarto_mimetype":"application/vnd.jupyter.widget-view+json"}
</script>
</div>
<div class="cell-output cell-output-display">
<script type="application/vnd.jupyter.widget-view+json">
{"model_id":"878640f5a034497a8e3ed3450e2473da","version_major":2,"version_minor":0,"quarto_mimetype":"application/vnd.jupyter.widget-view+json"}
</script>
</div>
<div class="cell-output cell-output-display">
<script type="application/vnd.jupyter.widget-view+json">
{"model_id":"be10dc25ad21438184702da9527af6ee","version_major":2,"version_minor":0,"quarto_mimetype":"application/vnd.jupyter.widget-view+json"}
</script>
</div>
<div class="cell-output cell-output-display">
<script type="application/vnd.jupyter.widget-view+json">
{"model_id":"de4ff9cd02f2496a9f4fca3f69193f1b","version_major":2,"version_minor":0,"quarto_mimetype":"application/vnd.jupyter.widget-view+json"}
</script>
</div>
<div class="cell-output cell-output-display">
<script type="application/vnd.jupyter.widget-view+json">
{"model_id":"98efcfa3799e4cd78cc70516b6800796","version_major":2,"version_minor":0,"quarto_mimetype":"application/vnd.jupyter.widget-view+json"}
</script>
</div>
<div class="cell-output cell-output-display">
<script type="application/vnd.jupyter.widget-view+json">
{"model_id":"3c9a4bc7acb44af9b19970b33a8e4070","version_major":2,"version_minor":0,"quarto_mimetype":"application/vnd.jupyter.widget-view+json"}
</script>
</div>
<div class="cell-output cell-output-display">
<script type="application/vnd.jupyter.widget-view+json">
{"model_id":"5df64008cf134b2885bb161cbb1fb272","version_major":2,"version_minor":0,"quarto_mimetype":"application/vnd.jupyter.widget-view+json"}
</script>
</div>
<div class="cell-output cell-output-display">
<script type="application/vnd.jupyter.widget-view+json">
{"model_id":"f3d43ebe333144baabf03ee1c16fec99","version_major":2,"version_minor":0,"quarto_mimetype":"application/vnd.jupyter.widget-view+json"}
</script>
</div>
<div class="cell-output cell-output-display">
<script type="application/vnd.jupyter.widget-view+json">
{"model_id":"7728126bc72749c980b553c490ffa10c","version_major":2,"version_minor":0,"quarto_mimetype":"application/vnd.jupyter.widget-view+json"}
</script>
</div>
<div class="cell-output cell-output-display">
<script type="application/vnd.jupyter.widget-view+json">
{"model_id":"28f6a4eca3e0463d96fb6bb990b1a89a","version_major":2,"version_minor":0,"quarto_mimetype":"application/vnd.jupyter.widget-view+json"}
</script>
</div>
<div class="cell-output cell-output-display">
<script type="application/vnd.jupyter.widget-view+json">
{"model_id":"d7180d028e2f4b8d8f26451260631aa5","version_major":2,"version_minor":0,"quarto_mimetype":"application/vnd.jupyter.widget-view+json"}
</script>
</div>
<div class="cell-output cell-output-display">
<script type="application/vnd.jupyter.widget-view+json">
{"model_id":"58e7ef6e1c294277904156e2ba15bf7c","version_major":2,"version_minor":0,"quarto_mimetype":"application/vnd.jupyter.widget-view+json"}
</script>
</div>
<div class="cell-output cell-output-display">
<script type="application/vnd.jupyter.widget-view+json">
{"model_id":"d9776aabb37e4e19afe17d4a0fc40906","version_major":2,"version_minor":0,"quarto_mimetype":"application/vnd.jupyter.widget-view+json"}
</script>
</div>
<div class="cell-output cell-output-display">
<script type="application/vnd.jupyter.widget-view+json">
{"model_id":"f53af3dc696841dca7ef29cf7d786c8f","version_major":2,"version_minor":0,"quarto_mimetype":"application/vnd.jupyter.widget-view+json"}
</script>
</div>
</div>
</section>
</section>
</section>
<section id="linear-quantization" class="level1">
<h1>Linear Quantization</h1>
<p>ì´ ì„¹ì…˜ì—ì„œëŠ” linear quantizationì„ êµ¬í˜„í•˜ê³  ìˆ˜í–‰í•©ë‹ˆë‹¤.</p>
<p>Linear quantizationì€ range truncation ê³¼ scaling ê³¼ì •ì„ ê±°ì¹œ í›„ ë¶€ë™ ì†Œìˆ˜ì  ê°’ì„ ê°€ì¥ ê°€ê¹Œìš´ ì–‘ìí™”ëœ ì •ìˆ˜ë¡œ ì§ì ‘ ë°˜ì˜¬ë¦¼í•©ë‹ˆë‹¤.</p>
<p><a href="https://arxiv.org/pdf/1712.05877.pdf">Linear quantization</a>ì€ ë‹¤ìŒê³¼ ê°™ì´ í‘œí˜„ë  ìˆ˜ ìˆìŠµë‹ˆë‹¤.</p>
<p><span class="math inline">\(r = S(q-Z)\)</span></p>
<p>ì—¬ê¸°ì„œ <span class="math inline">\(r\)</span>ì€ ë¶€ë™ ì†Œìˆ˜ì  ì‹¤ìˆ˜, <span class="math inline">\(q\)</span>ëŠ” <em>n</em>-ë¹„íŠ¸ ì •ìˆ˜, <span class="math inline">\(Z\)</span>ëŠ” <em>n</em>-ë¹„íŠ¸ ì •ìˆ˜ì´ë©°, <span class="math inline">\(S\)</span>ëŠ” ë¶€ë™ ì†Œìˆ˜ì  ì‹¤ìˆ˜ì…ë‹ˆë‹¤.</p>
<p><span class="math inline">\(Z\)</span>ëŠ” quantization zero pointì´ê³  <span class="math inline">\(S\)</span>ëŠ” quantization scaling factorì…ë‹ˆë‹¤. ìƒìˆ˜ <span class="math inline">\(Z\)</span>ì™€ <span class="math inline">\(S\)</span> ëª¨ë‘ ì–‘ìí™” ë§¤ê°œë³€ìˆ˜(parameter)ì…ë‹ˆë‹¤.</p>
<section id="n-bit-integer" class="level2">
<h2 class="anchored" data-anchor-id="n-bit-integer"><em>n</em>-bit Integer</h2>
<p><em>n</em>-ë¹„íŠ¸ signed integerëŠ” ë³´í†µ <a href="https://en.wikipedia.org/wiki/Two%27s_complement">twoâ€™s complement</a> í‘œê¸°ë²•ìœ¼ë¡œ í‘œí˜„ë©ë‹ˆë‹¤.</p>
<p><em>n</em>-ë¹„íŠ¸ signed integerëŠ” ë²”ìœ„ <span class="math inline">\([-2^{n-1}, 2^{n-1}-1]\)</span> ë‚´ì˜ ì •ìˆ˜ë¥¼ ì¸ì½”ë”©í•  ìˆ˜ ìˆìŠµë‹ˆë‹¤. ì˜ˆë¥¼ ë“¤ì–´, 8ë¹„íŠ¸ ì •ìˆ˜ëŠ” [-128, 127] ë²”ìœ„ì— ì†í•©ë‹ˆë‹¤.</p>
<div id="cell-37" class="cell" data-execution_count="23">
<div class="sourceCode cell-code" id="cb13"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb13-1"><a href="#cb13-1" aria-hidden="true" tabindex="-1"></a><span class="kw">def</span> get_quantized_range(bitwidth):</span>
<span id="cb13-2"><a href="#cb13-2" aria-hidden="true" tabindex="-1"></a>    quantized_max <span class="op">=</span> (<span class="dv">1</span> <span class="op">&lt;&lt;</span> (bitwidth <span class="op">-</span> <span class="dv">1</span>)) <span class="op">-</span> <span class="dv">1</span></span>
<span id="cb13-3"><a href="#cb13-3" aria-hidden="true" tabindex="-1"></a>    quantized_min <span class="op">=</span> <span class="op">-</span>(<span class="dv">1</span> <span class="op">&lt;&lt;</span> (bitwidth <span class="op">-</span> <span class="dv">1</span>))</span>
<span id="cb13-4"><a href="#cb13-4" aria-hidden="true" tabindex="-1"></a>    <span class="cf">return</span> quantized_min, quantized_max</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</div>
</section>
<section id="question-4-15-pts" class="level2">
<h2 class="anchored" data-anchor-id="question-4-15-pts"><strong>Question 4</strong> (15 pts)</h2>
<p>ì•„ë˜ì˜ linear quantization functionì„ ì™„ì„±í•˜ì„¸ìš”.</p>
<p><strong>Hint</strong>:</p>
<ul>
<li><span class="math inline">\(r=S(q-Z)\)</span>ì—ì„œ, <span class="math inline">\(q = r/S + Z\)</span>ìœ¼ë¡œ ë°”ê¿”ì„œ ë³¼ ìˆ˜ ìˆìŠµë‹ˆë‹¤.</li>
<li><span class="math inline">\(r\)</span>ê³¼ <span class="math inline">\(S\)</span> ëª¨ë‘ ë¶€ë™ ì†Œìˆ˜ì  ìˆ«ì(floating number)ì´ë¯€ë¡œ, ì •ìˆ˜ <span class="math inline">\(Z\)</span>ë¥¼ ì§ì ‘ <span class="math inline">\(r/S\)</span>ì— ë”í•  ìˆ˜ ì—†ìŠµë‹ˆë‹¤. ë”°ë¼ì„œ <span class="math inline">\(q = \mathrm{int}(\mathrm{round}(r/S)) + Z\)</span>ì…ë‹ˆë‹¤.</li>
<li><a href="https://pytorch.org/docs/stable/tensors.html"><code>torch.FloatTensor</code></a>ë¥¼ <a href="https://pytorch.org/docs/stable/tensors.html"><code>torch.IntTensor</code></a>ë¡œ ë³€í™˜í•˜ê¸° ìœ„í•´ì„œ, <a href="https://pytorch.org/docs/stable/generated/torch.round.html#torch.round"><code>torch.round()</code></a>, <a href="https://pytorch.org/docs/stable/generated/torch.Tensor.round.html#torch.Tensor.round"><code>torch.Tensor.round()</code></a>, <a href="https://pytorch.org/docs/stable/generated/torch.Tensor.round_"><code>torch.Tensor.round_()</code></a>ì„ ì‚¬ìš©í•˜ì—¬ ëª¨ë“  ê°’ì„ ë¶€ë™ ì†Œìˆ˜ì  ì •ìˆ˜ë¡œ ë¨¼ì € ë³€í™˜í•©ë‹ˆë‹¤.</li>
<li>ê·¸ ë‹¤ìŒ <a href="https://pytorch.org/docs/stable/generated/torch.Tensor.to.html#torch.Tensor.to"><code>torch.Tensor.to(torch.int8)</code></a>ë¥¼ ì‚¬ìš©í•˜ì—¬ ë°ì´í„° íƒ€ì…ì„ <a href="https://pytorch.org/docs/stable/tensors.html"><code>torch.float</code></a>ì—ì„œ <a href="https://pytorch.org/docs/stable/tensors.html"><code>torch.int8</code></a>ë¡œ ë³€í™˜í•  ìˆ˜ ìˆìŠµë‹ˆë‹¤.</li>
</ul>
<div id="cell-39" class="cell" data-execution_count="24">
<div class="sourceCode cell-code" id="cb14"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb14-1"><a href="#cb14-1" aria-hidden="true" tabindex="-1"></a><span class="kw">def</span> linear_quantize(fp_tensor, bitwidth, scale, zero_point, dtype<span class="op">=</span>torch.int8) <span class="op">-&gt;</span> torch.Tensor:</span>
<span id="cb14-2"><a href="#cb14-2" aria-hidden="true" tabindex="-1"></a>    <span class="co">"""</span></span>
<span id="cb14-3"><a href="#cb14-3" aria-hidden="true" tabindex="-1"></a><span class="co">    linear quantization for single fp_tensor</span></span>
<span id="cb14-4"><a href="#cb14-4" aria-hidden="true" tabindex="-1"></a><span class="co">      from</span></span>
<span id="cb14-5"><a href="#cb14-5" aria-hidden="true" tabindex="-1"></a><span class="co">        fp_tensor = (quantized_tensor - zero_point) * scale</span></span>
<span id="cb14-6"><a href="#cb14-6" aria-hidden="true" tabindex="-1"></a><span class="co">      we have,</span></span>
<span id="cb14-7"><a href="#cb14-7" aria-hidden="true" tabindex="-1"></a><span class="co">        quantized_tensor = int(round(fp_tensor / scale)) + zero_point</span></span>
<span id="cb14-8"><a href="#cb14-8" aria-hidden="true" tabindex="-1"></a><span class="co">    :param tensor: [torch.(cuda.)FloatTensor] floating tensor to be quantized</span></span>
<span id="cb14-9"><a href="#cb14-9" aria-hidden="true" tabindex="-1"></a><span class="co">    :param bitwidth: [int] quantization bit width</span></span>
<span id="cb14-10"><a href="#cb14-10" aria-hidden="true" tabindex="-1"></a><span class="co">    :param scale: [torch.(cuda.)FloatTensor] scaling factor</span></span>
<span id="cb14-11"><a href="#cb14-11" aria-hidden="true" tabindex="-1"></a><span class="co">    :param zero_point: [torch.(cuda.)IntTensor] the desired centroid of tensor values</span></span>
<span id="cb14-12"><a href="#cb14-12" aria-hidden="true" tabindex="-1"></a><span class="co">    :return:</span></span>
<span id="cb14-13"><a href="#cb14-13" aria-hidden="true" tabindex="-1"></a><span class="co">        [torch.(cuda.)FloatTensor] quantized tensor whose values are integers</span></span>
<span id="cb14-14"><a href="#cb14-14" aria-hidden="true" tabindex="-1"></a><span class="co">    """</span></span>
<span id="cb14-15"><a href="#cb14-15" aria-hidden="true" tabindex="-1"></a>    <span class="cf">assert</span>(fp_tensor.dtype <span class="op">==</span> torch.<span class="bu">float</span>)</span>
<span id="cb14-16"><a href="#cb14-16" aria-hidden="true" tabindex="-1"></a>    <span class="cf">assert</span>(<span class="bu">isinstance</span>(scale, <span class="bu">float</span>) <span class="kw">or</span></span>
<span id="cb14-17"><a href="#cb14-17" aria-hidden="true" tabindex="-1"></a>           (scale.dtype <span class="op">==</span> torch.<span class="bu">float</span> <span class="kw">and</span> scale.dim() <span class="op">==</span> fp_tensor.dim()))</span>
<span id="cb14-18"><a href="#cb14-18" aria-hidden="true" tabindex="-1"></a>    <span class="cf">assert</span>(<span class="bu">isinstance</span>(zero_point, <span class="bu">int</span>) <span class="kw">or</span></span>
<span id="cb14-19"><a href="#cb14-19" aria-hidden="true" tabindex="-1"></a>           (zero_point.dtype <span class="op">==</span> dtype <span class="kw">and</span> zero_point.dim() <span class="op">==</span> fp_tensor.dim()))</span>
<span id="cb14-20"><a href="#cb14-20" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb14-21"><a href="#cb14-21" aria-hidden="true" tabindex="-1"></a>    <span class="co">############### YOUR CODE STARTS HERE ###############</span></span>
<span id="cb14-22"><a href="#cb14-22" aria-hidden="true" tabindex="-1"></a>    <span class="co"># Step 1: scale the fp_tensor</span></span>
<span id="cb14-23"><a href="#cb14-23" aria-hidden="true" tabindex="-1"></a>    scaled_tensor <span class="op">=</span> fp_tensor <span class="op">/</span> scale</span>
<span id="cb14-24"><a href="#cb14-24" aria-hidden="true" tabindex="-1"></a>    <span class="co"># Step 2: round the floating value to integer value</span></span>
<span id="cb14-25"><a href="#cb14-25" aria-hidden="true" tabindex="-1"></a>    rounded_tensor <span class="op">=</span> torch.<span class="bu">round</span>(scaled_tensor)</span>
<span id="cb14-26"><a href="#cb14-26" aria-hidden="true" tabindex="-1"></a>    <span class="co">############### YOUR CODE ENDS HERE #################</span></span>
<span id="cb14-27"><a href="#cb14-27" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb14-28"><a href="#cb14-28" aria-hidden="true" tabindex="-1"></a>    rounded_tensor <span class="op">=</span> rounded_tensor.to(dtype)</span>
<span id="cb14-29"><a href="#cb14-29" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb14-30"><a href="#cb14-30" aria-hidden="true" tabindex="-1"></a>    <span class="co">############### YOUR CODE STARTS HERE ###############</span></span>
<span id="cb14-31"><a href="#cb14-31" aria-hidden="true" tabindex="-1"></a>    <span class="co"># Step 3: shift the rounded_tensor to make zero_point 0</span></span>
<span id="cb14-32"><a href="#cb14-32" aria-hidden="true" tabindex="-1"></a>    shifted_tensor <span class="op">=</span> rounded_tensor <span class="op">+</span> zero_point</span>
<span id="cb14-33"><a href="#cb14-33" aria-hidden="true" tabindex="-1"></a>    <span class="co">############### YOUR CODE ENDS HERE #################</span></span>
<span id="cb14-34"><a href="#cb14-34" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb14-35"><a href="#cb14-35" aria-hidden="true" tabindex="-1"></a>    <span class="co"># Step 4: clamp the shifted_tensor to lie in bitwidth-bit range</span></span>
<span id="cb14-36"><a href="#cb14-36" aria-hidden="true" tabindex="-1"></a>    quantized_min, quantized_max <span class="op">=</span> get_quantized_range(bitwidth)</span>
<span id="cb14-37"><a href="#cb14-37" aria-hidden="true" tabindex="-1"></a>    quantized_tensor <span class="op">=</span> shifted_tensor.clamp_(quantized_min, quantized_max)</span>
<span id="cb14-38"><a href="#cb14-38" aria-hidden="true" tabindex="-1"></a>    <span class="cf">return</span> quantized_tensor</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</div>
<p>ìœ„ì—ì„œ ì‘ì„±í•œ linear quantization ê¸°ëŠ¥ì„ ë”ë¯¸ í…ì„œì— ì ìš©í•˜ì—¬ ê¸°ëŠ¥ì„ ê²€ì¦í•´ë´…ì‹œë‹¤.</p>
<div id="cell-41" class="cell" data-outputid="2fce8b6a-44f8-4ea0-847c-5dc58b41d5b8" data-execution_count="25">
<div class="sourceCode cell-code" id="cb15"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb15-1"><a href="#cb15-1" aria-hidden="true" tabindex="-1"></a>test_linear_quantize()</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output cell-output-stdout">
<pre><code>* Test linear_quantize()
    target bitwidth: 2 bits
        scale: 0.3333333333333333
        zero point: -1
* Test passed.</code></pre>
</div>
<div class="cell-output cell-output-display">
<div>
<figure class="figure">
<p><img src="lab02_files/figure-html/cell-12-output-2.png" class="img-fluid figure-img"></p>
</figure>
</div>
</div>
</div>
</section>
<section id="question-5-10-pts" class="level2">
<h2 class="anchored" data-anchor-id="question-5-10-pts">Question 5 (10 pts)</h2>
<p>ì´ì œ linear quantizationì„ ìœ„í•œ ìŠ¤ì¼€ì¼ë§ ì¸ì <span class="math inline">\(S\)</span>ì™€ ì œë¡œ í¬ì¸íŠ¸ <span class="math inline">\(Z\)</span>ë¥¼ ê²°ì •í•´ì•¼ í•©ë‹ˆë‹¤.</p>
<p><a href="https://arxiv.org/pdf/1712.05877.pdf">linear quantization</a>ì€ <span class="math inline">\(r = S(q-Z)\)</span> ë¡œ í‘œí˜„ë  ìˆ˜ ìˆë‹¤ëŠ” ê²ƒì„ ê¸°ì–µí•˜ì„¸ìš”.</p>
<section id="scale" class="level3">
<h3 class="anchored" data-anchor-id="scale">Scale</h3>
<p>Linear quantizationì€ ë¶€ë™ ì†Œìˆ˜ì  ë²”ìœ„ [<em>fp_min</em>, <em>fp_max</em>]ë¥¼ ì–‘ìí™”ëœ ë²”ìœ„ [<em>quantized_min</em>, <em>quantized_max</em>]ë¡œ íˆ¬ì˜(projection)í•©ë‹ˆë‹¤. ì¦‰,</p>
<blockquote class="blockquote">
<p><span class="math inline">\(r_{\mathrm{max}} = S(q_{\mathrm{max}}-Z)\)</span></p>
<p><span class="math inline">\(r_{\mathrm{min}} = S(q_{\mathrm{min}}-Z)\)</span></p>
</blockquote>
<p>ì´ ë‘ ë°©ì •ì‹ì„ ë¹¼ë©´, ìš°ë¦¬ëŠ” ë‹¤ìŒì„ ì–»ìŠµë‹ˆë‹¤,</p>
<section id="question-5.1-1-pts" class="level4">
<h4 class="anchored" data-anchor-id="question-5.1-1-pts">Question 5.1 (1 pts)</h4>
<p>ë‹¤ìŒ í…ìŠ¤íŠ¸ ì…€ì—ì„œ ì˜¬ë°”ë¥¸ ë‹µì„ ì„ íƒí•˜ê³  ì˜ëª»ëœ ë‹µì„ ì‚­ì œí•´ì£¼ì„¸ìš”.</p>
<blockquote class="blockquote">
<p><span class="math inline">\(S=r_{\mathrm{max}} / q_{\mathrm{max}}\)</span></p>
</blockquote>
<blockquote class="blockquote">
<p><span class="math inline">\(S=(r_{\mathrm{max}} + r_{\mathrm{min}}) / (q_{\mathrm{max}} + q_{\mathrm{min}})\)</span></p>
</blockquote>
<blockquote class="blockquote">
<p>âœ…<strong><span class="math inline">\(S=(r_{\mathrm{max}} - r_{\mathrm{min}}) / (q_{\mathrm{max}} - q_{\mathrm{min}})\)</span></strong></p>
</blockquote>
<blockquote class="blockquote">
<p><span class="math inline">\(S=r_{\mathrm{max}} / q_{\mathrm{max}} - r_{\mathrm{min}} / q_{\mathrm{min}}\)</span></p>
</blockquote>
<p><code>fp_tensor</code>ì˜ <span class="math inline">\(r_{\mathrm{min}}\)</span>ê³¼ <span class="math inline">\(r_{\mathrm{max}}\)</span>ë¥¼ ê²°ì •í•˜ëŠ” ë‹¤ì–‘í•œ ë°©ë²•ì´ ìˆìŠµë‹ˆë‹¤.</p>
<ul>
<li>ê°€ì¥ í”í•œ ë°©ë²•ì€ <code>fp_tensor</code>ì˜ ìµœì†Œê°’ê³¼ ìµœëŒ€ê°’ì„ ì‚¬ìš©í•˜ëŠ” ê²ƒì…ë‹ˆë‹¤.</li>
<li>ë˜ ë‹¤ë¥¸ ë„ë¦¬ ì‚¬ìš©ë˜ëŠ” ë°©ë²•ì€ Kullback-Leibler-J ë°œì‚°ì„ ìµœì†Œí™”í•˜ì—¬ <em>fp_max</em>ë¥¼ ê²°ì •í•˜ëŠ” ê²ƒì…ë‹ˆë‹¤.</li>
</ul>
</section>
</section>
<section id="zero-point" class="level3">
<h3 class="anchored" data-anchor-id="zero-point">zero point</h3>
<p>ìŠ¤ì¼€ì¼ë§ ì¸ì <span class="math inline">\(S\)</span>ë¥¼ ê²°ì •í•˜ë©´, <span class="math inline">\(r_{\mathrm{min}}\)</span>ê³¼ <span class="math inline">\(q_{\mathrm{min}}\)</span> ì‚¬ì´ì˜ ê´€ê³„ë¥¼ ì‚¬ìš©í•˜ì—¬ ì œë¡œ í¬ì¸íŠ¸ <span class="math inline">\(Z\)</span>ë¥¼ ê³„ì‚°í•  ìˆ˜ ìˆìŠµë‹ˆë‹¤.</p>
<section id="question-5.2-1-pts" class="level4">
<h4 class="anchored" data-anchor-id="question-5.2-1-pts">Question 5.2 (1 pts)</h4>
<p>ë‹¤ìŒ í…ìŠ¤íŠ¸ ì…€ì—ì„œ ì˜¬ë°”ë¥¸ ë‹µì„ ì„ íƒí•˜ê³  ì˜ëª»ëœ ë‹µì„ ì‚­ì œí•´ì£¼ì„¸ìš”.</p>
<blockquote class="blockquote">
<p><span class="math inline">\(Z = \mathrm{int}(\mathrm{round}(r_{\mathrm{min}} / S - q_{\mathrm{min}})\)</span></p>
</blockquote>
<blockquote class="blockquote">
<p><span class="math inline">\(Z = \mathrm{int}(\mathrm{round}(q_{\mathrm{min}} - r_{\mathrm{min}} / S))\)</span></p>
</blockquote>
<blockquote class="blockquote">
<p>âœ…<strong><span class="math inline">\(Z = q_{\mathrm{min}} - r_{\mathrm{min}} / S\)</span></strong></p>
</blockquote>
<blockquote class="blockquote">
<p><span class="math inline">\(Z = r_{\mathrm{min}} / S - q_{\mathrm{min}}\)</span></p>
</blockquote>
</section>
</section>
<section id="question-5.3-8-pts" class="level3">
<h3 class="anchored" data-anchor-id="question-5.3-8-pts">Question 5.3 (8 pts)</h3>
<p>floating point tensor <span class="math inline">\(r\)</span>ë¡œë¶€í„° scale <span class="math inline">\(S\)</span>ì™€ zero point <span class="math inline">\(Z\)</span>ë¥¼ ê³„ì‚°í•˜ëŠ” ì•„ë˜ì˜ í•¨ìˆ˜ë¥¼ ì™„ì„±í•˜ì„¸ìš”.</p>
<div id="cell-51" class="cell" data-execution_count="26">
<div class="sourceCode cell-code" id="cb17"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb17-1"><a href="#cb17-1" aria-hidden="true" tabindex="-1"></a><span class="kw">def</span> get_quantization_scale_and_zero_point(fp_tensor, bitwidth):</span>
<span id="cb17-2"><a href="#cb17-2" aria-hidden="true" tabindex="-1"></a>    <span class="co">"""</span></span>
<span id="cb17-3"><a href="#cb17-3" aria-hidden="true" tabindex="-1"></a><span class="co">    get quantization scale for single tensor</span></span>
<span id="cb17-4"><a href="#cb17-4" aria-hidden="true" tabindex="-1"></a><span class="co">    :param fp_tensor: [torch.(cuda.)Tensor] floating tensor to be quantized</span></span>
<span id="cb17-5"><a href="#cb17-5" aria-hidden="true" tabindex="-1"></a><span class="co">    :param bitwidth: [int] quantization bit width</span></span>
<span id="cb17-6"><a href="#cb17-6" aria-hidden="true" tabindex="-1"></a><span class="co">    :return:</span></span>
<span id="cb17-7"><a href="#cb17-7" aria-hidden="true" tabindex="-1"></a><span class="co">        [float] scale</span></span>
<span id="cb17-8"><a href="#cb17-8" aria-hidden="true" tabindex="-1"></a><span class="co">        [int] zero_point</span></span>
<span id="cb17-9"><a href="#cb17-9" aria-hidden="true" tabindex="-1"></a><span class="co">    """</span></span>
<span id="cb17-10"><a href="#cb17-10" aria-hidden="true" tabindex="-1"></a>    quantized_min, quantized_max <span class="op">=</span> get_quantized_range(bitwidth)</span>
<span id="cb17-11"><a href="#cb17-11" aria-hidden="true" tabindex="-1"></a>    fp_max <span class="op">=</span> fp_tensor.<span class="bu">max</span>().item()</span>
<span id="cb17-12"><a href="#cb17-12" aria-hidden="true" tabindex="-1"></a>    fp_min <span class="op">=</span> fp_tensor.<span class="bu">min</span>().item()</span>
<span id="cb17-13"><a href="#cb17-13" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb17-14"><a href="#cb17-14" aria-hidden="true" tabindex="-1"></a>    <span class="co">############### YOUR CODE STARTS HERE ###############</span></span>
<span id="cb17-15"><a href="#cb17-15" aria-hidden="true" tabindex="-1"></a>    <span class="co"># Calculate scale</span></span>
<span id="cb17-16"><a href="#cb17-16" aria-hidden="true" tabindex="-1"></a>    scale <span class="op">=</span> (fp_max <span class="op">-</span> fp_min) <span class="op">/</span> (quantized_max <span class="op">-</span> quantized_min)</span>
<span id="cb17-17"><a href="#cb17-17" aria-hidden="true" tabindex="-1"></a>    <span class="co"># Calculate zero_point</span></span>
<span id="cb17-18"><a href="#cb17-18" aria-hidden="true" tabindex="-1"></a>    zero_point <span class="op">=</span> quantized_min <span class="op">-</span> <span class="bu">round</span>(fp_min <span class="op">/</span> scale)</span>
<span id="cb17-19"><a href="#cb17-19" aria-hidden="true" tabindex="-1"></a>    <span class="co">############### YOUR CODE ENDS HERE #################</span></span>
<span id="cb17-20"><a href="#cb17-20" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb17-21"><a href="#cb17-21" aria-hidden="true" tabindex="-1"></a>    <span class="co"># clip the zero_point to fall in [quantized_min, quantized_max]</span></span>
<span id="cb17-22"><a href="#cb17-22" aria-hidden="true" tabindex="-1"></a>    <span class="cf">if</span> zero_point <span class="op">&lt;</span> quantized_min:</span>
<span id="cb17-23"><a href="#cb17-23" aria-hidden="true" tabindex="-1"></a>        zero_point <span class="op">=</span> quantized_min</span>
<span id="cb17-24"><a href="#cb17-24" aria-hidden="true" tabindex="-1"></a>    <span class="cf">elif</span> zero_point <span class="op">&gt;</span> quantized_max:</span>
<span id="cb17-25"><a href="#cb17-25" aria-hidden="true" tabindex="-1"></a>        zero_point <span class="op">=</span> quantized_max</span>
<span id="cb17-26"><a href="#cb17-26" aria-hidden="true" tabindex="-1"></a>    <span class="cf">else</span>: <span class="co"># convert from float to int using round()</span></span>
<span id="cb17-27"><a href="#cb17-27" aria-hidden="true" tabindex="-1"></a>        zero_point <span class="op">=</span> <span class="bu">round</span>(zero_point)</span>
<span id="cb17-28"><a href="#cb17-28" aria-hidden="true" tabindex="-1"></a>    <span class="cf">return</span> scale, <span class="bu">int</span>(zero_point)</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</div>
<p>ì´ì œ Question 4ì˜ <code>linear_quantize()</code>ì™€ Question 5ì˜ <code>get_quantization_scale_and_zero_point()</code>ì„ í•˜ë‚˜ì˜ í•¨ìˆ˜ë¡œ ë˜í•‘í•©ë‹ˆë‹¤.</p>
<div id="cell-53" class="cell" data-execution_count="27">
<div class="sourceCode cell-code" id="cb18"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb18-1"><a href="#cb18-1" aria-hidden="true" tabindex="-1"></a><span class="kw">def</span> linear_quantize_feature(fp_tensor, bitwidth):</span>
<span id="cb18-2"><a href="#cb18-2" aria-hidden="true" tabindex="-1"></a>    <span class="co">"""</span></span>
<span id="cb18-3"><a href="#cb18-3" aria-hidden="true" tabindex="-1"></a><span class="co">    linear quantization for feature tensor</span></span>
<span id="cb18-4"><a href="#cb18-4" aria-hidden="true" tabindex="-1"></a><span class="co">    :param fp_tensor: [torch.(cuda.)Tensor] floating feature to be quantized</span></span>
<span id="cb18-5"><a href="#cb18-5" aria-hidden="true" tabindex="-1"></a><span class="co">    :param bitwidth: [int] quantization bit width</span></span>
<span id="cb18-6"><a href="#cb18-6" aria-hidden="true" tabindex="-1"></a><span class="co">    :return:</span></span>
<span id="cb18-7"><a href="#cb18-7" aria-hidden="true" tabindex="-1"></a><span class="co">        [torch.(cuda.)Tensor] quantized tensor</span></span>
<span id="cb18-8"><a href="#cb18-8" aria-hidden="true" tabindex="-1"></a><span class="co">        [float] scale tensor</span></span>
<span id="cb18-9"><a href="#cb18-9" aria-hidden="true" tabindex="-1"></a><span class="co">        [int] zero point</span></span>
<span id="cb18-10"><a href="#cb18-10" aria-hidden="true" tabindex="-1"></a><span class="co">    """</span></span>
<span id="cb18-11"><a href="#cb18-11" aria-hidden="true" tabindex="-1"></a>    scale, zero_point <span class="op">=</span> get_quantization_scale_and_zero_point(fp_tensor, bitwidth)</span>
<span id="cb18-12"><a href="#cb18-12" aria-hidden="true" tabindex="-1"></a>    quantized_tensor <span class="op">=</span> linear_quantize(fp_tensor, bitwidth, scale, zero_point)</span>
<span id="cb18-13"><a href="#cb18-13" aria-hidden="true" tabindex="-1"></a>    <span class="cf">return</span> quantized_tensor, scale, zero_point</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</div>
</section>
</section>
<section id="special-case-linear-quantization-on-weight-tensor" class="level2">
<h2 class="anchored" data-anchor-id="special-case-linear-quantization-on-weight-tensor">Special case: linear quantization on weight tensor</h2>
<p>ë¨¼ì € ê°€ì¤‘ì¹˜ ê°’ì˜ ë¶„í¬ë¥¼ ì‚´í´ë´…ì‹œë‹¤.</p>
<div id="cell-55" class="cell" data-outputid="d7e7990e-9633-41c3-b0b3-905cc776e078" data-execution_count="28">
<div class="sourceCode cell-code" id="cb19"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb19-1"><a href="#cb19-1" aria-hidden="true" tabindex="-1"></a><span class="kw">def</span> plot_weight_distribution(model, bitwidth<span class="op">=</span><span class="dv">32</span>):</span>
<span id="cb19-2"><a href="#cb19-2" aria-hidden="true" tabindex="-1"></a>    <span class="co"># bins = (1 &lt;&lt; bitwidth) if bitwidth &lt;= 8 else 256</span></span>
<span id="cb19-3"><a href="#cb19-3" aria-hidden="true" tabindex="-1"></a>    <span class="cf">if</span> bitwidth <span class="op">&lt;=</span> <span class="dv">8</span>:</span>
<span id="cb19-4"><a href="#cb19-4" aria-hidden="true" tabindex="-1"></a>        qmin, qmax <span class="op">=</span> get_quantized_range(bitwidth)</span>
<span id="cb19-5"><a href="#cb19-5" aria-hidden="true" tabindex="-1"></a>        bins <span class="op">=</span> np.arange(qmin, qmax <span class="op">+</span> <span class="dv">2</span>)</span>
<span id="cb19-6"><a href="#cb19-6" aria-hidden="true" tabindex="-1"></a>        align <span class="op">=</span> <span class="st">'left'</span></span>
<span id="cb19-7"><a href="#cb19-7" aria-hidden="true" tabindex="-1"></a>    <span class="cf">else</span>:</span>
<span id="cb19-8"><a href="#cb19-8" aria-hidden="true" tabindex="-1"></a>        bins <span class="op">=</span> <span class="dv">256</span></span>
<span id="cb19-9"><a href="#cb19-9" aria-hidden="true" tabindex="-1"></a>        align <span class="op">=</span> <span class="st">'mid'</span></span>
<span id="cb19-10"><a href="#cb19-10" aria-hidden="true" tabindex="-1"></a>    fig, axes <span class="op">=</span> plt.subplots(<span class="dv">3</span>,<span class="dv">3</span>, figsize<span class="op">=</span>(<span class="dv">10</span>, <span class="dv">6</span>))</span>
<span id="cb19-11"><a href="#cb19-11" aria-hidden="true" tabindex="-1"></a>    axes <span class="op">=</span> axes.ravel()</span>
<span id="cb19-12"><a href="#cb19-12" aria-hidden="true" tabindex="-1"></a>    plot_index <span class="op">=</span> <span class="dv">0</span></span>
<span id="cb19-13"><a href="#cb19-13" aria-hidden="true" tabindex="-1"></a>    <span class="cf">for</span> name, param <span class="kw">in</span> model.named_parameters():</span>
<span id="cb19-14"><a href="#cb19-14" aria-hidden="true" tabindex="-1"></a>        <span class="cf">if</span> param.dim() <span class="op">&gt;</span> <span class="dv">1</span>:</span>
<span id="cb19-15"><a href="#cb19-15" aria-hidden="true" tabindex="-1"></a>            ax <span class="op">=</span> axes[plot_index]</span>
<span id="cb19-16"><a href="#cb19-16" aria-hidden="true" tabindex="-1"></a>            ax.hist(param.detach().view(<span class="op">-</span><span class="dv">1</span>).cpu(), bins<span class="op">=</span>bins, density<span class="op">=</span><span class="va">True</span>,</span>
<span id="cb19-17"><a href="#cb19-17" aria-hidden="true" tabindex="-1"></a>                    align<span class="op">=</span>align, color <span class="op">=</span> <span class="st">'blue'</span>, alpha <span class="op">=</span> <span class="fl">0.5</span>,</span>
<span id="cb19-18"><a href="#cb19-18" aria-hidden="true" tabindex="-1"></a>                    edgecolor<span class="op">=</span><span class="st">'black'</span> <span class="cf">if</span> bitwidth <span class="op">&lt;=</span> <span class="dv">4</span> <span class="cf">else</span> <span class="va">None</span>)</span>
<span id="cb19-19"><a href="#cb19-19" aria-hidden="true" tabindex="-1"></a>            <span class="cf">if</span> bitwidth <span class="op">&lt;=</span> <span class="dv">4</span>:</span>
<span id="cb19-20"><a href="#cb19-20" aria-hidden="true" tabindex="-1"></a>                quantized_min, quantized_max <span class="op">=</span> get_quantized_range(bitwidth)</span>
<span id="cb19-21"><a href="#cb19-21" aria-hidden="true" tabindex="-1"></a>                ax.set_xticks(np.arange(start<span class="op">=</span>quantized_min, stop<span class="op">=</span>quantized_max<span class="op">+</span><span class="dv">1</span>))</span>
<span id="cb19-22"><a href="#cb19-22" aria-hidden="true" tabindex="-1"></a>            ax.set_xlabel(name)</span>
<span id="cb19-23"><a href="#cb19-23" aria-hidden="true" tabindex="-1"></a>            ax.set_ylabel(<span class="st">'density'</span>)</span>
<span id="cb19-24"><a href="#cb19-24" aria-hidden="true" tabindex="-1"></a>            plot_index <span class="op">+=</span> <span class="dv">1</span></span>
<span id="cb19-25"><a href="#cb19-25" aria-hidden="true" tabindex="-1"></a>    fig.suptitle(<span class="ss">f'Histogram of Weights (bitwidth=</span><span class="sc">{</span>bitwidth<span class="sc">}</span><span class="ss"> bits)'</span>)</span>
<span id="cb19-26"><a href="#cb19-26" aria-hidden="true" tabindex="-1"></a>    fig.tight_layout()</span>
<span id="cb19-27"><a href="#cb19-27" aria-hidden="true" tabindex="-1"></a>    fig.subplots_adjust(top<span class="op">=</span><span class="fl">0.925</span>)</span>
<span id="cb19-28"><a href="#cb19-28" aria-hidden="true" tabindex="-1"></a>    plt.show()</span>
<span id="cb19-29"><a href="#cb19-29" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb19-30"><a href="#cb19-30" aria-hidden="true" tabindex="-1"></a>recover_model()</span>
<span id="cb19-31"><a href="#cb19-31" aria-hidden="true" tabindex="-1"></a>plot_weight_distribution(model)</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output cell-output-display">
<div>
<figure class="figure">
<p><img src="lab02_files/figure-html/cell-15-output-1.png" class="img-fluid figure-img"></p>
</figure>
</div>
</div>
</div>
<p>ìœ„ì˜ íˆìŠ¤í† ê·¸ë¨ì—ì„œ ë³¼ ìˆ˜ ìˆë“¯ì´, ê°€ì¤‘ì¹˜ ê°’ì˜ ë¶„í¬ëŠ” (ì´ ê²½ìš°ì—ëŠ” <code>classifier</code>ë¥¼ ì œì™¸í•˜ê³ ) ê±°ì˜ 0ì„ ì¤‘ì‹¬ìœ¼ë¡œ ëŒ€ì¹­ì ì…ë‹ˆë‹¤ . ë”°ë¼ì„œ ê°€ì¤‘ì¹˜ë¥¼ ì–‘ìí™”í•  ë•Œ ë³´í†µ ì œë¡œ í¬ì¸íŠ¸ <span class="math inline">\(Z=0\)</span>ìœ¼ë¡œ ì„¤ì •í•©ë‹ˆë‹¤.</p>
<p><span class="math inline">\(r = S(q-Z)\)</span>ì—ì„œ,</p>
<blockquote class="blockquote">
<p><span class="math inline">\(r_{\mathrm{max}} = S \cdot q_{\mathrm{max}}\)</span></p>
</blockquote>
<blockquote class="blockquote">
<p><span class="math inline">\(S = r_{\mathrm{max}} / q_{\mathrm{max}}\)</span></p>
</blockquote>
<p>ê°€ì¤‘ì¹˜ ê°’ì˜ ìµœëŒ€ ì ˆëŒ“ê°’ì„ <span class="math inline">\(r_{\mathrm{max}}\)</span>ë¡œ ì´ìš©í•©ë‹ˆë‹¤.</p>
<div id="cell-57" class="cell" data-execution_count="29">
<div class="sourceCode cell-code" id="cb20"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb20-1"><a href="#cb20-1" aria-hidden="true" tabindex="-1"></a><span class="kw">def</span> get_quantization_scale_for_weight(weight, bitwidth):</span>
<span id="cb20-2"><a href="#cb20-2" aria-hidden="true" tabindex="-1"></a>    <span class="co">"""</span></span>
<span id="cb20-3"><a href="#cb20-3" aria-hidden="true" tabindex="-1"></a><span class="co">    get quantization scale for single tensor of weight</span></span>
<span id="cb20-4"><a href="#cb20-4" aria-hidden="true" tabindex="-1"></a><span class="co">    :param weight: [torch.(cuda.)Tensor] floating weight to be quantized</span></span>
<span id="cb20-5"><a href="#cb20-5" aria-hidden="true" tabindex="-1"></a><span class="co">    :param bitwidth: [integer] quantization bit width</span></span>
<span id="cb20-6"><a href="#cb20-6" aria-hidden="true" tabindex="-1"></a><span class="co">    :return:</span></span>
<span id="cb20-7"><a href="#cb20-7" aria-hidden="true" tabindex="-1"></a><span class="co">        [floating scalar] scale</span></span>
<span id="cb20-8"><a href="#cb20-8" aria-hidden="true" tabindex="-1"></a><span class="co">    """</span></span>
<span id="cb20-9"><a href="#cb20-9" aria-hidden="true" tabindex="-1"></a>    <span class="co"># we just assume values in weight are symmetric</span></span>
<span id="cb20-10"><a href="#cb20-10" aria-hidden="true" tabindex="-1"></a>    <span class="co"># we also always make zero_point 0 for weight</span></span>
<span id="cb20-11"><a href="#cb20-11" aria-hidden="true" tabindex="-1"></a>    fp_max <span class="op">=</span> <span class="bu">max</span>(weight.<span class="bu">abs</span>().<span class="bu">max</span>().item(), <span class="fl">5e-7</span>)</span>
<span id="cb20-12"><a href="#cb20-12" aria-hidden="true" tabindex="-1"></a>    _, quantized_max <span class="op">=</span> get_quantized_range(bitwidth)</span>
<span id="cb20-13"><a href="#cb20-13" aria-hidden="true" tabindex="-1"></a>    <span class="cf">return</span> fp_max <span class="op">/</span> quantized_max</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</div>
<section id="per-channel-linear-quantization" class="level3">
<h3 class="anchored" data-anchor-id="per-channel-linear-quantization">Per-channel Linear Quantization</h3>
<p>2D convolutionì˜ ê²½ìš°, ê°€ì¤‘ì¹˜ í…ì„œëŠ” <code>(num_output_channels, num_input_channels, kernel_height, kernel_width)</code> ëª¨ì–‘ì˜ 4ì°¨ì› í…ì„œì…ë‹ˆë‹¤.</p>
<p>ë§ì€ ì‹¤í—˜ë“¤ì„ í†µí•´, <strong>ì„œë¡œ ë‹¤ë¥¸</strong> ì¶œë ¥ ì±„ë„ì— ëŒ€í•´ <strong>ì„œë¡œ ë‹¤ë¥¸</strong> ìŠ¤ì¼€ì¼ë§ ì¸ì <span class="math inline">\(S\)</span>ì™€ ì œë¡œ í¬ì¸íŠ¸ <span class="math inline">\(Z\)</span>ë¥¼ ì‚¬ìš©í•˜ëŠ” ê²ƒì´ ë” ë‚˜ì€ ì„±ëŠ¥ì„ ë°œíœ˜í•œë‹¤ëŠ” ê²ƒì„ ì•Œ ìˆ˜ ìˆì—ˆìŠµë‹ˆë‹¤. ë”°ë¼ì„œ ê° ì¶œë ¥ ì±„ë„ì˜ ì„œë¸Œí…ì„œì— ëŒ€í•œ ìŠ¤ì¼€ì¼ë§ ì¸ì <span class="math inline">\(S\)</span>ì™€ ì œë¡œ í¬ì¸íŠ¸ <span class="math inline">\(Z\)</span>ë¥¼ <strong>ë…ë¦½ì ìœ¼ë¡œ</strong> ì •í•´ì•¼ í•©ë‹ˆë‹¤.</p>
<div id="cell-59" class="cell" data-execution_count="30">
<div class="sourceCode cell-code" id="cb21"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb21-1"><a href="#cb21-1" aria-hidden="true" tabindex="-1"></a><span class="kw">def</span> linear_quantize_weight_per_channel(tensor, bitwidth):</span>
<span id="cb21-2"><a href="#cb21-2" aria-hidden="true" tabindex="-1"></a>    <span class="co">"""</span></span>
<span id="cb21-3"><a href="#cb21-3" aria-hidden="true" tabindex="-1"></a><span class="co">    linear quantization for weight tensor</span></span>
<span id="cb21-4"><a href="#cb21-4" aria-hidden="true" tabindex="-1"></a><span class="co">        using different scales and zero_points for different output channels</span></span>
<span id="cb21-5"><a href="#cb21-5" aria-hidden="true" tabindex="-1"></a><span class="co">    :param tensor: [torch.(cuda.)Tensor] floating weight to be quantized</span></span>
<span id="cb21-6"><a href="#cb21-6" aria-hidden="true" tabindex="-1"></a><span class="co">    :param bitwidth: [int] quantization bit width</span></span>
<span id="cb21-7"><a href="#cb21-7" aria-hidden="true" tabindex="-1"></a><span class="co">    :return:</span></span>
<span id="cb21-8"><a href="#cb21-8" aria-hidden="true" tabindex="-1"></a><span class="co">        [torch.(cuda.)Tensor] quantized tensor</span></span>
<span id="cb21-9"><a href="#cb21-9" aria-hidden="true" tabindex="-1"></a><span class="co">        [torch.(cuda.)Tensor] scale tensor</span></span>
<span id="cb21-10"><a href="#cb21-10" aria-hidden="true" tabindex="-1"></a><span class="co">        [int] zero point (which is always 0)</span></span>
<span id="cb21-11"><a href="#cb21-11" aria-hidden="true" tabindex="-1"></a><span class="co">    """</span></span>
<span id="cb21-12"><a href="#cb21-12" aria-hidden="true" tabindex="-1"></a>    dim_output_channels <span class="op">=</span> <span class="dv">0</span></span>
<span id="cb21-13"><a href="#cb21-13" aria-hidden="true" tabindex="-1"></a>    num_output_channels <span class="op">=</span> tensor.shape[dim_output_channels]</span>
<span id="cb21-14"><a href="#cb21-14" aria-hidden="true" tabindex="-1"></a>    scale <span class="op">=</span> torch.zeros(num_output_channels, device<span class="op">=</span>tensor.device)</span>
<span id="cb21-15"><a href="#cb21-15" aria-hidden="true" tabindex="-1"></a>    <span class="cf">for</span> oc <span class="kw">in</span> <span class="bu">range</span>(num_output_channels):</span>
<span id="cb21-16"><a href="#cb21-16" aria-hidden="true" tabindex="-1"></a>        _subtensor <span class="op">=</span> tensor.select(dim_output_channels, oc)</span>
<span id="cb21-17"><a href="#cb21-17" aria-hidden="true" tabindex="-1"></a>        _scale <span class="op">=</span> get_quantization_scale_for_weight(_subtensor, bitwidth)</span>
<span id="cb21-18"><a href="#cb21-18" aria-hidden="true" tabindex="-1"></a>        scale[oc] <span class="op">=</span> _scale</span>
<span id="cb21-19"><a href="#cb21-19" aria-hidden="true" tabindex="-1"></a>    scale_shape <span class="op">=</span> [<span class="dv">1</span>] <span class="op">*</span> tensor.dim()</span>
<span id="cb21-20"><a href="#cb21-20" aria-hidden="true" tabindex="-1"></a>    scale_shape[dim_output_channels] <span class="op">=</span> <span class="op">-</span><span class="dv">1</span></span>
<span id="cb21-21"><a href="#cb21-21" aria-hidden="true" tabindex="-1"></a>    scale <span class="op">=</span> scale.view(scale_shape)</span>
<span id="cb21-22"><a href="#cb21-22" aria-hidden="true" tabindex="-1"></a>    quantized_tensor <span class="op">=</span> linear_quantize(tensor, bitwidth, scale, zero_point<span class="op">=</span><span class="dv">0</span>)</span>
<span id="cb21-23"><a href="#cb21-23" aria-hidden="true" tabindex="-1"></a>    <span class="cf">return</span> quantized_tensor, scale, <span class="dv">0</span></span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</div>
</section>
<section id="a-quick-peek-at-linear-quantization-on-weights" class="level3">
<h3 class="anchored" data-anchor-id="a-quick-peek-at-linear-quantization-on-weights">A Quick Peek at Linear Quantization on Weights</h3>
<p>ì´ì œ ê°€ì¤‘ì¹˜ì— ëŒ€í•´ linear quantizationë¥¼ ì ìš©í•  ë•Œ ê°€ì¤‘ì¹˜ ë¶„í¬ì™€ ëª¨ë¸ í¬ê¸°ë¥¼ ì„œë¡œ ë‹¤ë¥¸ bitwidthsë¡œ ì‚´í´ë³´ê² ìŠµë‹ˆë‹¤.</p>
<div id="cell-61" class="cell" data-outputid="c2c7b5d1-c0c2-4afb-8e01-95e179c8dccc" data-execution_count="31">
<div class="sourceCode cell-code" id="cb22"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb22-1"><a href="#cb22-1" aria-hidden="true" tabindex="-1"></a><span class="at">@torch.no_grad</span>()</span>
<span id="cb22-2"><a href="#cb22-2" aria-hidden="true" tabindex="-1"></a><span class="kw">def</span> peek_linear_quantization():</span>
<span id="cb22-3"><a href="#cb22-3" aria-hidden="true" tabindex="-1"></a>    <span class="cf">for</span> bitwidth <span class="kw">in</span> [<span class="dv">4</span>, <span class="dv">2</span>]:</span>
<span id="cb22-4"><a href="#cb22-4" aria-hidden="true" tabindex="-1"></a>        <span class="cf">for</span> name, param <span class="kw">in</span> model.named_parameters():</span>
<span id="cb22-5"><a href="#cb22-5" aria-hidden="true" tabindex="-1"></a>            <span class="cf">if</span> param.dim() <span class="op">&gt;</span> <span class="dv">1</span>:</span>
<span id="cb22-6"><a href="#cb22-6" aria-hidden="true" tabindex="-1"></a>                quantized_param, scale, zero_point <span class="op">=</span> <span class="op">\</span></span>
<span id="cb22-7"><a href="#cb22-7" aria-hidden="true" tabindex="-1"></a>                    linear_quantize_weight_per_channel(param, bitwidth)</span>
<span id="cb22-8"><a href="#cb22-8" aria-hidden="true" tabindex="-1"></a>                param.copy_(quantized_param)</span>
<span id="cb22-9"><a href="#cb22-9" aria-hidden="true" tabindex="-1"></a>        plot_weight_distribution(model, bitwidth)</span>
<span id="cb22-10"><a href="#cb22-10" aria-hidden="true" tabindex="-1"></a>        recover_model()</span>
<span id="cb22-11"><a href="#cb22-11" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb22-12"><a href="#cb22-12" aria-hidden="true" tabindex="-1"></a>peek_linear_quantization()</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output cell-output-display">
<div>
<figure class="figure">
<p><img src="lab02_files/figure-html/cell-18-output-1.png" class="img-fluid figure-img"></p>
</figure>
</div>
</div>
<div class="cell-output cell-output-display">
<div>
<figure class="figure">
<p><img src="lab02_files/figure-html/cell-18-output-2.png" class="img-fluid figure-img"></p>
</figure>
</div>
</div>
</div>
</section>
</section>
<section id="quantized-inference" class="level2">
<h2 class="anchored" data-anchor-id="quantized-inference">Quantized Inference</h2>
<p>ì–‘ìí™” í›„, convolution ë° fully-connected layerì˜ ì¶”ë¡ ë„ ë³€ê²½ë©ë‹ˆë‹¤.</p>
<p><span class="math inline">\(r = S(q-Z)\)</span>ë¥¼ ìƒê¸°í•´ ë³´ë©´, ë‹¤ìŒê³¼ ê°™ìŠµë‹ˆë‹¤.</p>
<blockquote class="blockquote">
<p><span class="math inline">\(r_{\mathrm{input}} = S_{\mathrm{input}}(q_{\mathrm{input}}-Z_{\mathrm{input}})\)</span></p>
<p><span class="math inline">\(r_{\mathrm{weight}} = S_{\mathrm{weight}}(q_{\mathrm{weight}}-Z_{\mathrm{weight}})\)</span></p>
<p><span class="math inline">\(r_{\mathrm{bias}} = S_{\mathrm{bias}}(q_{\mathrm{bias}}-Z_{\mathrm{bias}})\)</span></p>
</blockquote>
<p><span class="math inline">\(Z_{\mathrm{weight}}=0\)</span>ì´ë¯€ë¡œ, <span class="math inline">\(r_{\mathrm{weight}} = S_{\mathrm{weight}}q_{\mathrm{weight}}\)</span>ì…ë‹ˆë‹¤.</p>
<p>ë¶€ë™ ì†Œìˆ˜ì  convolutionì€ ë‹¤ìŒê³¼ ê°™ì´ ì‘ì„±í•  ìˆ˜ ìˆìŠµë‹ˆë‹¤.</p>
<blockquote class="blockquote">
<p><span class="math inline">\(r_{\mathrm{output}} = \mathrm{CONV}[r_{\mathrm{input}}, r_{\mathrm{weight}}] + r_{\mathrm{bias}}\)</span> <span class="math inline">\(\;\;\;\;\;\;\;\;= \mathrm{CONV}[S_{\mathrm{input}}(q_{\mathrm{input}}-Z_{\mathrm{input}}), S_{\mathrm{weight}}q_{\mathrm{weight}}] + S_{\mathrm{bias}}(q_{\mathrm{bias}}-Z_{\mathrm{bias}})\)</span> <span class="math inline">\(\;\;\;\;\;\;\;\;= \mathrm{CONV}[q_{\mathrm{input}}-Z_{\mathrm{input}}, q_{\mathrm{weight}}]\cdot (S_{\mathrm{input}} \cdot S_{\mathrm{weight}}) + S_{\mathrm{bias}}(q_{\mathrm{bias}}-Z_{\mathrm{bias}})\)</span></p>
</blockquote>
<p>ê³„ì‚°ì„ ë” ê°„ë‹¨í•˜ê²Œ í•˜ê¸° ìœ„í•´</p>
<blockquote class="blockquote">
<p><span class="math inline">\(Z_{\mathrm{bias}} = 0\)</span></p>
<p><span class="math inline">\(S_{\mathrm{bias}} = S_{\mathrm{input}} \cdot S_{\mathrm{weight}}\)</span></p>
</blockquote>
<p>ë¡œ ì„¤ì •í•˜ì—¬,</p>
<blockquote class="blockquote">
<p><span class="math inline">\(r_{\mathrm{output}} = (\mathrm{CONV}[q_{\mathrm{input}}-Z_{\mathrm{input}}, q_{\mathrm{weight}}] + q_{\mathrm{bias}})\cdot (S_{\mathrm{input}} \cdot S_{\mathrm{weight}})\)</span> <span class="math inline">\(\;\;\;\;\;\;\;\;= (\mathrm{CONV}[q_{\mathrm{input}}, q_{\mathrm{weight}}] - \mathrm{CONV}[Z_{\mathrm{input}}, q_{\mathrm{weight}}] + q_{\mathrm{bias}})\cdot (S_{\mathrm{input}}S_{\mathrm{weight}})\)</span></p>
</blockquote>
<p>ì´ë©°,</p>
<blockquote class="blockquote">
<p><span class="math inline">\(r_{\mathrm{output}} = S_{\mathrm{output}}(q_{\mathrm{output}}-Z_{\mathrm{output}})\)</span></p>
</blockquote>
<p>ì´ë¯€ë¡œ</p>
<blockquote class="blockquote">
<p><span class="math inline">\(S_{\mathrm{output}}(q_{\mathrm{output}}-Z_{\mathrm{output}}) = (\mathrm{CONV}[q_{\mathrm{input}}, q_{\mathrm{weight}}] - \mathrm{CONV}[Z_{\mathrm{input}}, q_{\mathrm{weight}}] + q_{\mathrm{bias}})\cdot (S_{\mathrm{input}} S_{\mathrm{weight}})\)</span></p>
</blockquote>
<p>ë”°ë¼ì„œ</p>
<blockquote class="blockquote">
<p><span class="math inline">\(q_{\mathrm{output}} = (\mathrm{CONV}[q_{\mathrm{input}}, q_{\mathrm{weight}}] - \mathrm{CONV}[Z_{\mathrm{input}}, q_{\mathrm{weight}}] + q_{\mathrm{bias}})\cdot (S_{\mathrm{input}}S_{\mathrm{weight}} / S_{\mathrm{output}}) + Z_{\mathrm{output}}\)</span></p>
</blockquote>
<p><span class="math inline">\(Z_{\mathrm{input}}\)</span>, <span class="math inline">\(q_{\mathrm{weight}}\)</span>, <span class="math inline">\(q_{\mathrm{bias}}\)</span>ëŠ” ì¶”ë¡  ì „ì— ê²°ì •ë˜ë¯€ë¡œ,</p>
<blockquote class="blockquote">
<p><span class="math inline">\(Q_{\mathrm{bias}} = q_{\mathrm{bias}} - \mathrm{CONV}[Z_{\mathrm{input}}, q_{\mathrm{weight}}]\)</span></p>
</blockquote>
<p>ë¡œ ì„¤ì •í•˜ë©´,</p>
<blockquote class="blockquote">
<p><span class="math inline">\(q_{\mathrm{output}} = (\mathrm{Linear}[q_{\mathrm{input}}, q_{\mathrm{weight}}] + Q_{\mathrm{bias}})\cdot (S_{\mathrm{input}} \cdot S_{\mathrm{weight}} / S_{\mathrm{output}}) + Z_{\mathrm{output}}\)</span></p>
</blockquote>
<section id="question-6-5-pts" class="level3">
<h3 class="anchored" data-anchor-id="question-6-5-pts">Question 6 (5 pts)</h3>
<p>biasë¥¼ linear quantizingí•˜ëŠ” í•¨ìˆ˜ë¥¼ ì™„ì„±í•˜ì„¸ìš”.</p>
<p><strong>Hint</strong>:</p>
<p>ìœ„ì˜ ì¶”ë¡ ê³¼ì •ì—ì„œ ì•„ë˜ì™€ ê°™ì€ ìˆ˜ì‹ì„ ì–»ì—ˆìŠµë‹ˆë‹¤.</p>
<blockquote class="blockquote">
<p><span class="math inline">\(Z_{\mathrm{bias}} = 0\)</span></p>
<p><span class="math inline">\(S_{\mathrm{bias}} = S_{\mathrm{input}} \cdot S_{\mathrm{weight}}\)</span></p>
</blockquote>
<div id="cell-65" class="cell" data-execution_count="32">
<div class="sourceCode cell-code" id="cb23"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb23-1"><a href="#cb23-1" aria-hidden="true" tabindex="-1"></a><span class="kw">def</span> linear_quantize_bias_per_output_channel(bias, weight_scale, input_scale):</span>
<span id="cb23-2"><a href="#cb23-2" aria-hidden="true" tabindex="-1"></a>    <span class="co">"""</span></span>
<span id="cb23-3"><a href="#cb23-3" aria-hidden="true" tabindex="-1"></a><span class="co">    linear quantization for single bias tensor</span></span>
<span id="cb23-4"><a href="#cb23-4" aria-hidden="true" tabindex="-1"></a><span class="co">        quantized_bias = fp_bias / bias_scale</span></span>
<span id="cb23-5"><a href="#cb23-5" aria-hidden="true" tabindex="-1"></a><span class="co">    :param bias: [torch.FloatTensor] bias weight to be quantized</span></span>
<span id="cb23-6"><a href="#cb23-6" aria-hidden="true" tabindex="-1"></a><span class="co">    :param weight_scale: [float or torch.FloatTensor] weight scale tensor</span></span>
<span id="cb23-7"><a href="#cb23-7" aria-hidden="true" tabindex="-1"></a><span class="co">    :param input_scale: [float] input scale</span></span>
<span id="cb23-8"><a href="#cb23-8" aria-hidden="true" tabindex="-1"></a><span class="co">    :return:</span></span>
<span id="cb23-9"><a href="#cb23-9" aria-hidden="true" tabindex="-1"></a><span class="co">        [torch.IntTensor] quantized bias tensor</span></span>
<span id="cb23-10"><a href="#cb23-10" aria-hidden="true" tabindex="-1"></a><span class="co">    """</span></span>
<span id="cb23-11"><a href="#cb23-11" aria-hidden="true" tabindex="-1"></a>    <span class="cf">assert</span>(bias.dim() <span class="op">==</span> <span class="dv">1</span>)</span>
<span id="cb23-12"><a href="#cb23-12" aria-hidden="true" tabindex="-1"></a>    <span class="cf">assert</span>(bias.dtype <span class="op">==</span> torch.<span class="bu">float</span>)</span>
<span id="cb23-13"><a href="#cb23-13" aria-hidden="true" tabindex="-1"></a>    <span class="cf">assert</span>(<span class="bu">isinstance</span>(input_scale, <span class="bu">float</span>))</span>
<span id="cb23-14"><a href="#cb23-14" aria-hidden="true" tabindex="-1"></a>    <span class="cf">if</span> <span class="bu">isinstance</span>(weight_scale, torch.Tensor):</span>
<span id="cb23-15"><a href="#cb23-15" aria-hidden="true" tabindex="-1"></a>        <span class="cf">assert</span>(weight_scale.dtype <span class="op">==</span> torch.<span class="bu">float</span>)</span>
<span id="cb23-16"><a href="#cb23-16" aria-hidden="true" tabindex="-1"></a>        weight_scale <span class="op">=</span> weight_scale.view(<span class="op">-</span><span class="dv">1</span>)</span>
<span id="cb23-17"><a href="#cb23-17" aria-hidden="true" tabindex="-1"></a>        <span class="cf">assert</span>(bias.numel() <span class="op">==</span> weight_scale.numel())</span>
<span id="cb23-18"><a href="#cb23-18" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb23-19"><a href="#cb23-19" aria-hidden="true" tabindex="-1"></a>    <span class="co">############### YOUR CODE STARTS HERE ###############</span></span>
<span id="cb23-20"><a href="#cb23-20" aria-hidden="true" tabindex="-1"></a>    bias_scale <span class="op">=</span> weight_scale <span class="op">*</span> input_scale</span>
<span id="cb23-21"><a href="#cb23-21" aria-hidden="true" tabindex="-1"></a>    <span class="co">############### YOUR CODE ENDS HERE #################</span></span>
<span id="cb23-22"><a href="#cb23-22" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb23-23"><a href="#cb23-23" aria-hidden="true" tabindex="-1"></a>    quantized_bias <span class="op">=</span> linear_quantize(bias, <span class="dv">32</span>, bias_scale,</span>
<span id="cb23-24"><a href="#cb23-24" aria-hidden="true" tabindex="-1"></a>                                     zero_point<span class="op">=</span><span class="dv">0</span>, dtype<span class="op">=</span>torch.int32)</span>
<span id="cb23-25"><a href="#cb23-25" aria-hidden="true" tabindex="-1"></a>    <span class="cf">return</span> quantized_bias, bias_scale, <span class="dv">0</span></span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</div>
</section>
<section id="quantized-fully-connected-layer" class="level3">
<h3 class="anchored" data-anchor-id="quantized-fully-connected-layer">Quantized Fully-Connected Layer</h3>
<p>ì–‘ìí™”ëœ fully-connected layerì˜ ê²½ìš°, <span class="math inline">\(Q_{\mathrm{bias}}\)</span>ë¥¼ ë¨¼ì € ê³„ì‚°í•©ë‹ˆë‹¤. <span class="math inline">\(Q_{\mathrm{bias}} = q_{\mathrm{bias}} - \mathrm{Linear}[Z_{\mathrm{input}}, q_{\mathrm{weight}}]\)</span>ë¥¼ ê¸°ì–µí•˜ì„¸ìš”.</p>
<div id="cell-68" class="cell" data-execution_count="33">
<div class="sourceCode cell-code" id="cb24"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb24-1"><a href="#cb24-1" aria-hidden="true" tabindex="-1"></a><span class="kw">def</span> shift_quantized_linear_bias(quantized_bias, quantized_weight, input_zero_point):</span>
<span id="cb24-2"><a href="#cb24-2" aria-hidden="true" tabindex="-1"></a>    <span class="co">"""</span></span>
<span id="cb24-3"><a href="#cb24-3" aria-hidden="true" tabindex="-1"></a><span class="co">    shift quantized bias to incorporate input_zero_point for nn.Linear</span></span>
<span id="cb24-4"><a href="#cb24-4" aria-hidden="true" tabindex="-1"></a><span class="co">        shifted_quantized_bias = quantized_bias - Linear(input_zero_point, quantized_weight)</span></span>
<span id="cb24-5"><a href="#cb24-5" aria-hidden="true" tabindex="-1"></a><span class="co">    :param quantized_bias: [torch.IntTensor] quantized bias (torch.int32)</span></span>
<span id="cb24-6"><a href="#cb24-6" aria-hidden="true" tabindex="-1"></a><span class="co">    :param quantized_weight: [torch.CharTensor] quantized weight (torch.int8)</span></span>
<span id="cb24-7"><a href="#cb24-7" aria-hidden="true" tabindex="-1"></a><span class="co">    :param input_zero_point: [int] input zero point</span></span>
<span id="cb24-8"><a href="#cb24-8" aria-hidden="true" tabindex="-1"></a><span class="co">    :return:</span></span>
<span id="cb24-9"><a href="#cb24-9" aria-hidden="true" tabindex="-1"></a><span class="co">        [torch.IntTensor] shifted quantized bias tensor</span></span>
<span id="cb24-10"><a href="#cb24-10" aria-hidden="true" tabindex="-1"></a><span class="co">    """</span></span>
<span id="cb24-11"><a href="#cb24-11" aria-hidden="true" tabindex="-1"></a>    <span class="cf">assert</span>(quantized_bias.dtype <span class="op">==</span> torch.int32)</span>
<span id="cb24-12"><a href="#cb24-12" aria-hidden="true" tabindex="-1"></a>    <span class="cf">assert</span>(<span class="bu">isinstance</span>(input_zero_point, <span class="bu">int</span>))</span>
<span id="cb24-13"><a href="#cb24-13" aria-hidden="true" tabindex="-1"></a>    <span class="cf">return</span> quantized_bias <span class="op">-</span> quantized_weight.<span class="bu">sum</span>(<span class="dv">1</span>).to(torch.int32) <span class="op">*</span> input_zero_point</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</div>
<section id="question-7-15-pts" class="level4">
<h4 class="anchored" data-anchor-id="question-7-15-pts">Question 7 (15 pts)</h4>
<p>ì•„ë˜ì˜ ì–‘ìí™”ëœ fully-connected layer inference functionë¥¼ ì™„ì„±í•˜ì„¸ìš”.</p>
<p><strong>Hint</strong>:</p>
<blockquote class="blockquote">
<p><span class="math inline">\(q_{\mathrm{output}} = (\mathrm{Linear}[q_{\mathrm{input}}, q_{\mathrm{weight}}] + Q_{\mathrm{bias}})\cdot (S_{\mathrm{input}} S_{\mathrm{weight}} / S_{\mathrm{output}}) + Z_{\mathrm{output}}\)</span></p>
</blockquote>
<div id="cell-70" class="cell" data-execution_count="34">
<div class="sourceCode cell-code" id="cb25"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb25-1"><a href="#cb25-1" aria-hidden="true" tabindex="-1"></a><span class="kw">def</span> quantized_linear(<span class="bu">input</span>, weight, bias, feature_bitwidth, weight_bitwidth,</span>
<span id="cb25-2"><a href="#cb25-2" aria-hidden="true" tabindex="-1"></a>                     input_zero_point, output_zero_point,</span>
<span id="cb25-3"><a href="#cb25-3" aria-hidden="true" tabindex="-1"></a>                     input_scale, weight_scale, output_scale):</span>
<span id="cb25-4"><a href="#cb25-4" aria-hidden="true" tabindex="-1"></a>    <span class="co">"""</span></span>
<span id="cb25-5"><a href="#cb25-5" aria-hidden="true" tabindex="-1"></a><span class="co">    quantized fully-connected layer</span></span>
<span id="cb25-6"><a href="#cb25-6" aria-hidden="true" tabindex="-1"></a><span class="co">    :param input: [torch.CharTensor] quantized input (torch.int8)</span></span>
<span id="cb25-7"><a href="#cb25-7" aria-hidden="true" tabindex="-1"></a><span class="co">    :param weight: [torch.CharTensor] quantized weight (torch.int8)</span></span>
<span id="cb25-8"><a href="#cb25-8" aria-hidden="true" tabindex="-1"></a><span class="co">    :param bias: [torch.IntTensor] shifted quantized bias or None (torch.int32)</span></span>
<span id="cb25-9"><a href="#cb25-9" aria-hidden="true" tabindex="-1"></a><span class="co">    :param feature_bitwidth: [int] quantization bit width of input and output</span></span>
<span id="cb25-10"><a href="#cb25-10" aria-hidden="true" tabindex="-1"></a><span class="co">    :param weight_bitwidth: [int] quantization bit width of weight</span></span>
<span id="cb25-11"><a href="#cb25-11" aria-hidden="true" tabindex="-1"></a><span class="co">    :param input_zero_point: [int] input zero point</span></span>
<span id="cb25-12"><a href="#cb25-12" aria-hidden="true" tabindex="-1"></a><span class="co">    :param output_zero_point: [int] output zero point</span></span>
<span id="cb25-13"><a href="#cb25-13" aria-hidden="true" tabindex="-1"></a><span class="co">    :param input_scale: [float] input feature scale</span></span>
<span id="cb25-14"><a href="#cb25-14" aria-hidden="true" tabindex="-1"></a><span class="co">    :param weight_scale: [torch.FloatTensor] weight per-channel scale</span></span>
<span id="cb25-15"><a href="#cb25-15" aria-hidden="true" tabindex="-1"></a><span class="co">    :param output_scale: [float] output feature scale</span></span>
<span id="cb25-16"><a href="#cb25-16" aria-hidden="true" tabindex="-1"></a><span class="co">    :return:</span></span>
<span id="cb25-17"><a href="#cb25-17" aria-hidden="true" tabindex="-1"></a><span class="co">        [torch.CharIntTensor] quantized output feature (torch.int8)</span></span>
<span id="cb25-18"><a href="#cb25-18" aria-hidden="true" tabindex="-1"></a><span class="co">    """</span></span>
<span id="cb25-19"><a href="#cb25-19" aria-hidden="true" tabindex="-1"></a>    <span class="cf">assert</span>(<span class="bu">input</span>.dtype <span class="op">==</span> torch.int8)</span>
<span id="cb25-20"><a href="#cb25-20" aria-hidden="true" tabindex="-1"></a>    <span class="cf">assert</span>(weight.dtype <span class="op">==</span> <span class="bu">input</span>.dtype)</span>
<span id="cb25-21"><a href="#cb25-21" aria-hidden="true" tabindex="-1"></a>    <span class="cf">assert</span>(bias <span class="kw">is</span> <span class="va">None</span> <span class="kw">or</span> bias.dtype <span class="op">==</span> torch.int32)</span>
<span id="cb25-22"><a href="#cb25-22" aria-hidden="true" tabindex="-1"></a>    <span class="cf">assert</span>(<span class="bu">isinstance</span>(input_zero_point, <span class="bu">int</span>))</span>
<span id="cb25-23"><a href="#cb25-23" aria-hidden="true" tabindex="-1"></a>    <span class="cf">assert</span>(<span class="bu">isinstance</span>(output_zero_point, <span class="bu">int</span>))</span>
<span id="cb25-24"><a href="#cb25-24" aria-hidden="true" tabindex="-1"></a>    <span class="cf">assert</span>(<span class="bu">isinstance</span>(input_scale, <span class="bu">float</span>))</span>
<span id="cb25-25"><a href="#cb25-25" aria-hidden="true" tabindex="-1"></a>    <span class="cf">assert</span>(<span class="bu">isinstance</span>(output_scale, <span class="bu">float</span>))</span>
<span id="cb25-26"><a href="#cb25-26" aria-hidden="true" tabindex="-1"></a>    <span class="cf">assert</span>(weight_scale.dtype <span class="op">==</span> torch.<span class="bu">float</span>)</span>
<span id="cb25-27"><a href="#cb25-27" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb25-28"><a href="#cb25-28" aria-hidden="true" tabindex="-1"></a>    <span class="co"># Step 1: integer-based fully-connected (8-bit multiplication with 32-bit accumulation)</span></span>
<span id="cb25-29"><a href="#cb25-29" aria-hidden="true" tabindex="-1"></a>    <span class="cf">if</span> <span class="st">'cpu'</span> <span class="kw">in</span> <span class="bu">input</span>.device.<span class="bu">type</span>:</span>
<span id="cb25-30"><a href="#cb25-30" aria-hidden="true" tabindex="-1"></a>        <span class="co"># use 32-b MAC for simplicity</span></span>
<span id="cb25-31"><a href="#cb25-31" aria-hidden="true" tabindex="-1"></a>        output <span class="op">=</span> torch.nn.functional.linear(<span class="bu">input</span>.to(torch.int32), weight.to(torch.int32), bias)</span>
<span id="cb25-32"><a href="#cb25-32" aria-hidden="true" tabindex="-1"></a>    <span class="cf">else</span>:</span>
<span id="cb25-33"><a href="#cb25-33" aria-hidden="true" tabindex="-1"></a>        <span class="co"># current version pytorch does not yet support integer-based linear() on GPUs</span></span>
<span id="cb25-34"><a href="#cb25-34" aria-hidden="true" tabindex="-1"></a>        output <span class="op">=</span> torch.nn.functional.linear(<span class="bu">input</span>.<span class="bu">float</span>(), weight.<span class="bu">float</span>(), bias.<span class="bu">float</span>())</span>
<span id="cb25-35"><a href="#cb25-35" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb25-36"><a href="#cb25-36" aria-hidden="true" tabindex="-1"></a>    <span class="co">############### YOUR CODE STARTS HERE ###############</span></span>
<span id="cb25-37"><a href="#cb25-37" aria-hidden="true" tabindex="-1"></a>    <span class="co"># Step 2: scale the output</span></span>
<span id="cb25-38"><a href="#cb25-38" aria-hidden="true" tabindex="-1"></a>    <span class="co">#         hint: 1. scales are floating numbers, we need to convert output to float as well</span></span>
<span id="cb25-39"><a href="#cb25-39" aria-hidden="true" tabindex="-1"></a>    <span class="co">#               2. the shape of weight scale is [oc, 1, 1, 1] while the shape of output is [batch_size, oc]</span></span>
<span id="cb25-40"><a href="#cb25-40" aria-hidden="true" tabindex="-1"></a>    real_scale <span class="op">=</span> input_scale <span class="op">*</span> weight_scale.view(<span class="op">-</span><span class="dv">1</span>) <span class="op">/</span> output_scale</span>
<span id="cb25-41"><a href="#cb25-41" aria-hidden="true" tabindex="-1"></a>    output <span class="op">=</span> output.<span class="bu">float</span>() <span class="op">*</span> real_scale</span>
<span id="cb25-42"><a href="#cb25-42" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb25-43"><a href="#cb25-43" aria-hidden="true" tabindex="-1"></a>    <span class="co"># Step 3: Shift output by output_zero_point</span></span>
<span id="cb25-44"><a href="#cb25-44" aria-hidden="true" tabindex="-1"></a>    output <span class="op">+=</span> output_zero_point</span>
<span id="cb25-45"><a href="#cb25-45" aria-hidden="true" tabindex="-1"></a>    <span class="co">############### YOUR CODE STARTS HERE ###############</span></span>
<span id="cb25-46"><a href="#cb25-46" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb25-47"><a href="#cb25-47" aria-hidden="true" tabindex="-1"></a>    <span class="co"># Make sure all value lies in the bitwidth-bit range</span></span>
<span id="cb25-48"><a href="#cb25-48" aria-hidden="true" tabindex="-1"></a>    output <span class="op">=</span> output.<span class="bu">round</span>().clamp(<span class="op">*</span>get_quantized_range(feature_bitwidth)).to(torch.int8)</span>
<span id="cb25-49"><a href="#cb25-49" aria-hidden="true" tabindex="-1"></a>    <span class="cf">return</span> output</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</div>
<p>Letâ€™s verify the functionality of defined quantized fully connected layer.</p>
<div id="cell-72" class="cell" data-outputid="40b9bc67-2f7f-433e-af7a-cbba9add43e6" data-execution_count="35">
<div class="sourceCode cell-code" id="cb26"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb26-1"><a href="#cb26-1" aria-hidden="true" tabindex="-1"></a>test_quantized_fc()</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output cell-output-stdout">
<pre><code>* Test quantized_fc()
    target bitwidth: 2 bits
      batch size: 4
      input channels: 8
      output channels: 8
* Test passed.</code></pre>
</div>
<div class="cell-output cell-output-display">
<div>
<figure class="figure">
<p><img src="lab02_files/figure-html/cell-22-output-2.png" class="img-fluid figure-img"></p>
</figure>
</div>
</div>
</div>
</section>
</section>
<section id="quantized-convolution" class="level3">
<h3 class="anchored" data-anchor-id="quantized-convolution">Quantized Convolution</h3>
<p>ì–‘ìí™”ëœ ì»¨ë³¼ë£¨ì…˜ ë ˆì´ì–´ì˜ ê²½ìš°, ë¨¼ì € <span class="math inline">\(Q_{\mathrm{bias}}\)</span>ë¥¼ ê³„ì‚°í•©ë‹ˆë‹¤. <span class="math inline">\(Q_{\mathrm{bias}} = q_{\mathrm{bias}} - \mathrm{CONV}[Z_{\mathrm{input}}, q_{\mathrm{weight}}]\)</span>ë¥¼ ê¸°ì–µí•˜ì„¸ìš”.</p>
<div id="cell-75" class="cell" data-execution_count="36">
<div class="sourceCode cell-code" id="cb28"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb28-1"><a href="#cb28-1" aria-hidden="true" tabindex="-1"></a><span class="kw">def</span> shift_quantized_conv2d_bias(quantized_bias, quantized_weight, input_zero_point):</span>
<span id="cb28-2"><a href="#cb28-2" aria-hidden="true" tabindex="-1"></a>    <span class="co">"""</span></span>
<span id="cb28-3"><a href="#cb28-3" aria-hidden="true" tabindex="-1"></a><span class="co">    shift quantized bias to incorporate input_zero_point for nn.Conv2d</span></span>
<span id="cb28-4"><a href="#cb28-4" aria-hidden="true" tabindex="-1"></a><span class="co">        shifted_quantized_bias = quantized_bias - Conv(input_zero_point, quantized_weight)</span></span>
<span id="cb28-5"><a href="#cb28-5" aria-hidden="true" tabindex="-1"></a><span class="co">    :param quantized_bias: [torch.IntTensor] quantized bias (torch.int32)</span></span>
<span id="cb28-6"><a href="#cb28-6" aria-hidden="true" tabindex="-1"></a><span class="co">    :param quantized_weight: [torch.CharTensor] quantized weight (torch.int8)</span></span>
<span id="cb28-7"><a href="#cb28-7" aria-hidden="true" tabindex="-1"></a><span class="co">    :param input_zero_point: [int] input zero point</span></span>
<span id="cb28-8"><a href="#cb28-8" aria-hidden="true" tabindex="-1"></a><span class="co">    :return:</span></span>
<span id="cb28-9"><a href="#cb28-9" aria-hidden="true" tabindex="-1"></a><span class="co">        [torch.IntTensor] shifted quantized bias tensor</span></span>
<span id="cb28-10"><a href="#cb28-10" aria-hidden="true" tabindex="-1"></a><span class="co">    """</span></span>
<span id="cb28-11"><a href="#cb28-11" aria-hidden="true" tabindex="-1"></a>    <span class="cf">assert</span>(quantized_bias.dtype <span class="op">==</span> torch.int32)</span>
<span id="cb28-12"><a href="#cb28-12" aria-hidden="true" tabindex="-1"></a>    <span class="cf">assert</span>(<span class="bu">isinstance</span>(input_zero_point, <span class="bu">int</span>))</span>
<span id="cb28-13"><a href="#cb28-13" aria-hidden="true" tabindex="-1"></a>    <span class="cf">return</span> quantized_bias <span class="op">-</span> quantized_weight.<span class="bu">sum</span>((<span class="dv">1</span>,<span class="dv">2</span>,<span class="dv">3</span>)).to(torch.int32) <span class="op">*</span> input_zero_point</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</div>
<section id="question-8-15-pts" class="level4">
<h4 class="anchored" data-anchor-id="question-8-15-pts">Question 8 (15 pts)</h4>
<p>ì•„ë˜ì˜ quantized convolution functionì„ ì™„ì„±í•˜ì„¸ìš”.</p>
<p><strong>Hint</strong>: &gt; <span class="math inline">\(q_{\mathrm{output}} = (\mathrm{CONV}[q_{\mathrm{input}}, q_{\mathrm{weight}}] + Q_{\mathrm{bias}}) \cdot (S_{\mathrm{input}}S_{\mathrm{weight}} / S_{\mathrm{output}}) + Z_{\mathrm{output}}\)</span></p>
<div id="cell-77" class="cell" data-execution_count="37">
<div class="sourceCode cell-code" id="cb29"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb29-1"><a href="#cb29-1" aria-hidden="true" tabindex="-1"></a><span class="kw">def</span> quantized_conv2d(<span class="bu">input</span>, weight, bias, feature_bitwidth, weight_bitwidth,</span>
<span id="cb29-2"><a href="#cb29-2" aria-hidden="true" tabindex="-1"></a>                     input_zero_point, output_zero_point,</span>
<span id="cb29-3"><a href="#cb29-3" aria-hidden="true" tabindex="-1"></a>                     input_scale, weight_scale, output_scale,</span>
<span id="cb29-4"><a href="#cb29-4" aria-hidden="true" tabindex="-1"></a>                     stride, padding, dilation, groups):</span>
<span id="cb29-5"><a href="#cb29-5" aria-hidden="true" tabindex="-1"></a>    <span class="co">"""</span></span>
<span id="cb29-6"><a href="#cb29-6" aria-hidden="true" tabindex="-1"></a><span class="co">    quantized 2d convolution</span></span>
<span id="cb29-7"><a href="#cb29-7" aria-hidden="true" tabindex="-1"></a><span class="co">    :param input: [torch.CharTensor] quantized input (torch.int8)</span></span>
<span id="cb29-8"><a href="#cb29-8" aria-hidden="true" tabindex="-1"></a><span class="co">    :param weight: [torch.CharTensor] quantized weight (torch.int8)</span></span>
<span id="cb29-9"><a href="#cb29-9" aria-hidden="true" tabindex="-1"></a><span class="co">    :param bias: [torch.IntTensor] shifted quantized bias or None (torch.int32)</span></span>
<span id="cb29-10"><a href="#cb29-10" aria-hidden="true" tabindex="-1"></a><span class="co">    :param feature_bitwidth: [int] quantization bit width of input and output</span></span>
<span id="cb29-11"><a href="#cb29-11" aria-hidden="true" tabindex="-1"></a><span class="co">    :param weight_bitwidth: [int] quantization bit width of weight</span></span>
<span id="cb29-12"><a href="#cb29-12" aria-hidden="true" tabindex="-1"></a><span class="co">    :param input_zero_point: [int] input zero point</span></span>
<span id="cb29-13"><a href="#cb29-13" aria-hidden="true" tabindex="-1"></a><span class="co">    :param output_zero_point: [int] output zero point</span></span>
<span id="cb29-14"><a href="#cb29-14" aria-hidden="true" tabindex="-1"></a><span class="co">    :param input_scale: [float] input feature scale</span></span>
<span id="cb29-15"><a href="#cb29-15" aria-hidden="true" tabindex="-1"></a><span class="co">    :param weight_scale: [torch.FloatTensor] weight per-channel scale</span></span>
<span id="cb29-16"><a href="#cb29-16" aria-hidden="true" tabindex="-1"></a><span class="co">    :param output_scale: [float] output feature scale</span></span>
<span id="cb29-17"><a href="#cb29-17" aria-hidden="true" tabindex="-1"></a><span class="co">    :return:</span></span>
<span id="cb29-18"><a href="#cb29-18" aria-hidden="true" tabindex="-1"></a><span class="co">        [torch.(cuda.)CharTensor] quantized output feature</span></span>
<span id="cb29-19"><a href="#cb29-19" aria-hidden="true" tabindex="-1"></a><span class="co">    """</span></span>
<span id="cb29-20"><a href="#cb29-20" aria-hidden="true" tabindex="-1"></a>    <span class="cf">assert</span>(<span class="bu">len</span>(padding) <span class="op">==</span> <span class="dv">4</span>)</span>
<span id="cb29-21"><a href="#cb29-21" aria-hidden="true" tabindex="-1"></a>    <span class="cf">assert</span>(<span class="bu">input</span>.dtype <span class="op">==</span> torch.int8)</span>
<span id="cb29-22"><a href="#cb29-22" aria-hidden="true" tabindex="-1"></a>    <span class="cf">assert</span>(weight.dtype <span class="op">==</span> <span class="bu">input</span>.dtype)</span>
<span id="cb29-23"><a href="#cb29-23" aria-hidden="true" tabindex="-1"></a>    <span class="cf">assert</span>(bias <span class="kw">is</span> <span class="va">None</span> <span class="kw">or</span> bias.dtype <span class="op">==</span> torch.int32)</span>
<span id="cb29-24"><a href="#cb29-24" aria-hidden="true" tabindex="-1"></a>    <span class="cf">assert</span>(<span class="bu">isinstance</span>(input_zero_point, <span class="bu">int</span>))</span>
<span id="cb29-25"><a href="#cb29-25" aria-hidden="true" tabindex="-1"></a>    <span class="cf">assert</span>(<span class="bu">isinstance</span>(output_zero_point, <span class="bu">int</span>))</span>
<span id="cb29-26"><a href="#cb29-26" aria-hidden="true" tabindex="-1"></a>    <span class="cf">assert</span>(<span class="bu">isinstance</span>(input_scale, <span class="bu">float</span>))</span>
<span id="cb29-27"><a href="#cb29-27" aria-hidden="true" tabindex="-1"></a>    <span class="cf">assert</span>(<span class="bu">isinstance</span>(output_scale, <span class="bu">float</span>))</span>
<span id="cb29-28"><a href="#cb29-28" aria-hidden="true" tabindex="-1"></a>    <span class="cf">assert</span>(weight_scale.dtype <span class="op">==</span> torch.<span class="bu">float</span>)</span>
<span id="cb29-29"><a href="#cb29-29" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb29-30"><a href="#cb29-30" aria-hidden="true" tabindex="-1"></a>    <span class="co"># Step 1: calculate integer-based 2d convolution (8-bit multiplication with 32-bit accumulation)</span></span>
<span id="cb29-31"><a href="#cb29-31" aria-hidden="true" tabindex="-1"></a>    <span class="bu">input</span> <span class="op">=</span> torch.nn.functional.pad(<span class="bu">input</span>, padding, <span class="st">'constant'</span>, input_zero_point)</span>
<span id="cb29-32"><a href="#cb29-32" aria-hidden="true" tabindex="-1"></a>    <span class="cf">if</span> <span class="st">'cpu'</span> <span class="kw">in</span> <span class="bu">input</span>.device.<span class="bu">type</span>:</span>
<span id="cb29-33"><a href="#cb29-33" aria-hidden="true" tabindex="-1"></a>        <span class="co"># use 32-b MAC for simplicity</span></span>
<span id="cb29-34"><a href="#cb29-34" aria-hidden="true" tabindex="-1"></a>        output <span class="op">=</span> torch.nn.functional.conv2d(<span class="bu">input</span>.to(torch.int32), weight.to(torch.int32), <span class="va">None</span>, stride, <span class="dv">0</span>, dilation, groups)</span>
<span id="cb29-35"><a href="#cb29-35" aria-hidden="true" tabindex="-1"></a>    <span class="cf">else</span>:</span>
<span id="cb29-36"><a href="#cb29-36" aria-hidden="true" tabindex="-1"></a>        <span class="co"># current version pytorch does not yet support integer-based conv2d() on GPUs</span></span>
<span id="cb29-37"><a href="#cb29-37" aria-hidden="true" tabindex="-1"></a>        output <span class="op">=</span> torch.nn.functional.conv2d(<span class="bu">input</span>.<span class="bu">float</span>(), weight.<span class="bu">float</span>(), <span class="va">None</span>, stride, <span class="dv">0</span>, dilation, groups)</span>
<span id="cb29-38"><a href="#cb29-38" aria-hidden="true" tabindex="-1"></a>        output <span class="op">=</span> output.<span class="bu">round</span>().to(torch.int32)</span>
<span id="cb29-39"><a href="#cb29-39" aria-hidden="true" tabindex="-1"></a>    <span class="cf">if</span> bias <span class="kw">is</span> <span class="kw">not</span> <span class="va">None</span>:</span>
<span id="cb29-40"><a href="#cb29-40" aria-hidden="true" tabindex="-1"></a>        output <span class="op">=</span> output <span class="op">+</span> bias.view(<span class="dv">1</span>, <span class="op">-</span><span class="dv">1</span>, <span class="dv">1</span>, <span class="dv">1</span>)</span>
<span id="cb29-41"><a href="#cb29-41" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb29-42"><a href="#cb29-42" aria-hidden="true" tabindex="-1"></a>    <span class="co">############### YOUR CODE STARTS HERE ###############</span></span>
<span id="cb29-43"><a href="#cb29-43" aria-hidden="true" tabindex="-1"></a>    <span class="co"># hint: this code block should be the very similar to quantized_linear()</span></span>
<span id="cb29-44"><a href="#cb29-44" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb29-45"><a href="#cb29-45" aria-hidden="true" tabindex="-1"></a>    <span class="co"># Step 2: scale the output</span></span>
<span id="cb29-46"><a href="#cb29-46" aria-hidden="true" tabindex="-1"></a>    <span class="co">#         hint: 1. scales are floating numbers, we need to convert output to float as well</span></span>
<span id="cb29-47"><a href="#cb29-47" aria-hidden="true" tabindex="-1"></a>    <span class="co">#               2. the shape of weight scale is [oc, 1, 1, 1] while the shape of output is [batch_size, oc, height, width]</span></span>
<span id="cb29-48"><a href="#cb29-48" aria-hidden="true" tabindex="-1"></a>    real_scale <span class="op">=</span> input_scale <span class="op">*</span> weight_scale.view(<span class="op">-</span><span class="dv">1</span>) <span class="op">/</span> output_scale</span>
<span id="cb29-49"><a href="#cb29-49" aria-hidden="true" tabindex="-1"></a>    output <span class="op">=</span> output.<span class="bu">float</span>() <span class="op">*</span> real_scale.unsqueeze(<span class="dv">1</span>).unsqueeze(<span class="dv">2</span>)</span>
<span id="cb29-50"><a href="#cb29-50" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb29-51"><a href="#cb29-51" aria-hidden="true" tabindex="-1"></a>    <span class="co"># Step 3: shift output by output_zero_point</span></span>
<span id="cb29-52"><a href="#cb29-52" aria-hidden="true" tabindex="-1"></a>    <span class="co">#         hint: one line of code</span></span>
<span id="cb29-53"><a href="#cb29-53" aria-hidden="true" tabindex="-1"></a>    output <span class="op">+=</span> output_zero_point</span>
<span id="cb29-54"><a href="#cb29-54" aria-hidden="true" tabindex="-1"></a>    <span class="co">############### YOUR CODE STARTS HERE ###############</span></span>
<span id="cb29-55"><a href="#cb29-55" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb29-56"><a href="#cb29-56" aria-hidden="true" tabindex="-1"></a>    <span class="co"># Make sure all value lies in the bitwidth-bit range</span></span>
<span id="cb29-57"><a href="#cb29-57" aria-hidden="true" tabindex="-1"></a>    output <span class="op">=</span> output.<span class="bu">round</span>().clamp(<span class="op">*</span>get_quantized_range(feature_bitwidth)).to(torch.int8)</span>
<span id="cb29-58"><a href="#cb29-58" aria-hidden="true" tabindex="-1"></a>    <span class="cf">return</span> output</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</div>
</section>
</section>
</section>
<section id="question-9-10-pts" class="level2">
<h2 class="anchored" data-anchor-id="question-9-10-pts">Question 9 (10 pts)</h2>
<p>ë§ˆì§€ë§‰ìœ¼ë¡œ ëª¨ë“  ê²ƒì„ ì¢…í•©í•˜ì—¬ ëª¨ë¸ì— ëŒ€í•œ í›ˆë ¨ í›„ <code>int8</code> ì–‘ìí™”ë¥¼ ìˆ˜í–‰í•©ë‹ˆë‹¤. ëª¨ë¸ì˜ ì»¨ë³¼ë£¨ì…˜ ë ˆì´ì–´ì™€ ì„ í˜• ë ˆì´ì–´ë¥¼ í•˜ë‚˜ì”© ì–‘ìí™”ëœ ë²„ì „ìœ¼ë¡œ ë³€í™˜í•©ë‹ˆë‹¤.</p>
<ol type="1">
<li>ë¨¼ì €, BatchNorm ê³„ì¸µì„ ì´ì „ convolutional layerì— ìœµí•©í•  ê²ƒì´ë©°, ì´ëŠ” ì–‘ìí™” ì „ì— í•˜ëŠ” í‘œì¤€ ê´€í–‰ì…ë‹ˆë‹¤. BatchNormì„ ìœµí•©í•˜ë©´ ì¶”ë¡  ì¤‘ì— ì¶”ê°€ ê³±ì…ˆì´ ì¤„ì–´ë“­ë‹ˆë‹¤.</li>
</ol>
<p>ìœµí•© ëª¨ë¸ì¸ <code>model_fused</code>ê°€ ì›ë˜ ëª¨ë¸ê³¼ ë™ì¼í•œ ì •í™•ë„ë¥¼ ê°–ëŠ”ì§€ë„ ê²€ì¦í•  ì˜ˆì •ì…ë‹ˆë‹¤(BN fusionì€ ë„¤íŠ¸ì›Œí¬ ê¸°ëŠ¥ì„ ë³€ê²½í•˜ì§€ ì•ŠëŠ” ë™ë“±í•œ ë³€í™˜ì…ë‹ˆë‹¤).</p>
<div id="cell-80" class="cell" data-outputid="73c86c21-7ed2-4995-f611-5eaf5517a0ad" data-execution_count="38">
<div class="sourceCode cell-code" id="cb30"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb30-1"><a href="#cb30-1" aria-hidden="true" tabindex="-1"></a><span class="kw">def</span> fuse_conv_bn(conv, bn):</span>
<span id="cb30-2"><a href="#cb30-2" aria-hidden="true" tabindex="-1"></a>    <span class="co"># modified from https://mmcv.readthedocs.io/en/latest/_modules/mmcv/cnn/utils/fuse_conv_bn.html</span></span>
<span id="cb30-3"><a href="#cb30-3" aria-hidden="true" tabindex="-1"></a>    <span class="cf">assert</span> conv.bias <span class="kw">is</span> <span class="va">None</span></span>
<span id="cb30-4"><a href="#cb30-4" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb30-5"><a href="#cb30-5" aria-hidden="true" tabindex="-1"></a>    factor <span class="op">=</span> bn.weight.data <span class="op">/</span> torch.sqrt(bn.running_var.data <span class="op">+</span> bn.eps)</span>
<span id="cb30-6"><a href="#cb30-6" aria-hidden="true" tabindex="-1"></a>    conv.weight.data <span class="op">=</span> conv.weight.data <span class="op">*</span> factor.reshape(<span class="op">-</span><span class="dv">1</span>, <span class="dv">1</span>, <span class="dv">1</span>, <span class="dv">1</span>)</span>
<span id="cb30-7"><a href="#cb30-7" aria-hidden="true" tabindex="-1"></a>    conv.bias <span class="op">=</span> nn.Parameter(<span class="op">-</span> bn.running_mean.data <span class="op">*</span> factor <span class="op">+</span> bn.bias.data)</span>
<span id="cb30-8"><a href="#cb30-8" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb30-9"><a href="#cb30-9" aria-hidden="true" tabindex="-1"></a>    <span class="cf">return</span> conv</span>
<span id="cb30-10"><a href="#cb30-10" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb30-11"><a href="#cb30-11" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>(<span class="st">'Before conv-bn fusion: backbone length'</span>, <span class="bu">len</span>(model.backbone))</span>
<span id="cb30-12"><a href="#cb30-12" aria-hidden="true" tabindex="-1"></a><span class="co">#  fuse the batchnorm into conv layers</span></span>
<span id="cb30-13"><a href="#cb30-13" aria-hidden="true" tabindex="-1"></a>recover_model()</span>
<span id="cb30-14"><a href="#cb30-14" aria-hidden="true" tabindex="-1"></a>model_fused <span class="op">=</span> copy.deepcopy(model)</span>
<span id="cb30-15"><a href="#cb30-15" aria-hidden="true" tabindex="-1"></a>fused_backbone <span class="op">=</span> []</span>
<span id="cb30-16"><a href="#cb30-16" aria-hidden="true" tabindex="-1"></a>ptr <span class="op">=</span> <span class="dv">0</span></span>
<span id="cb30-17"><a href="#cb30-17" aria-hidden="true" tabindex="-1"></a><span class="cf">while</span> ptr <span class="op">&lt;</span> <span class="bu">len</span>(model_fused.backbone):</span>
<span id="cb30-18"><a href="#cb30-18" aria-hidden="true" tabindex="-1"></a>    <span class="cf">if</span> <span class="bu">isinstance</span>(model_fused.backbone[ptr], nn.Conv2d) <span class="kw">and</span> <span class="op">\</span></span>
<span id="cb30-19"><a href="#cb30-19" aria-hidden="true" tabindex="-1"></a>        <span class="bu">isinstance</span>(model_fused.backbone[ptr <span class="op">+</span> <span class="dv">1</span>], nn.BatchNorm2d):</span>
<span id="cb30-20"><a href="#cb30-20" aria-hidden="true" tabindex="-1"></a>        fused_backbone.append(fuse_conv_bn(</span>
<span id="cb30-21"><a href="#cb30-21" aria-hidden="true" tabindex="-1"></a>            model_fused.backbone[ptr], model_fused.backbone[ptr<span class="op">+</span> <span class="dv">1</span>]))</span>
<span id="cb30-22"><a href="#cb30-22" aria-hidden="true" tabindex="-1"></a>        ptr <span class="op">+=</span> <span class="dv">2</span></span>
<span id="cb30-23"><a href="#cb30-23" aria-hidden="true" tabindex="-1"></a>    <span class="cf">else</span>:</span>
<span id="cb30-24"><a href="#cb30-24" aria-hidden="true" tabindex="-1"></a>        fused_backbone.append(model_fused.backbone[ptr])</span>
<span id="cb30-25"><a href="#cb30-25" aria-hidden="true" tabindex="-1"></a>        ptr <span class="op">+=</span> <span class="dv">1</span></span>
<span id="cb30-26"><a href="#cb30-26" aria-hidden="true" tabindex="-1"></a>model_fused.backbone <span class="op">=</span> nn.Sequential(<span class="op">*</span>fused_backbone)</span>
<span id="cb30-27"><a href="#cb30-27" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb30-28"><a href="#cb30-28" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>(<span class="st">'After conv-bn fusion: backbone length'</span>, <span class="bu">len</span>(model_fused.backbone))</span>
<span id="cb30-29"><a href="#cb30-29" aria-hidden="true" tabindex="-1"></a><span class="co"># sanity check, no BN anymore</span></span>
<span id="cb30-30"><a href="#cb30-30" aria-hidden="true" tabindex="-1"></a><span class="cf">for</span> m <span class="kw">in</span> model_fused.modules():</span>
<span id="cb30-31"><a href="#cb30-31" aria-hidden="true" tabindex="-1"></a>    <span class="cf">assert</span> <span class="kw">not</span> <span class="bu">isinstance</span>(m, nn.BatchNorm2d)</span>
<span id="cb30-32"><a href="#cb30-32" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb30-33"><a href="#cb30-33" aria-hidden="true" tabindex="-1"></a><span class="co">#  the accuracy will remain the same after fusion</span></span>
<span id="cb30-34"><a href="#cb30-34" aria-hidden="true" tabindex="-1"></a>fused_acc <span class="op">=</span> evaluate(model_fused, dataloader[<span class="st">'test'</span>])</span>
<span id="cb30-35"><a href="#cb30-35" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>(<span class="ss">f'Accuracy of the fused model=</span><span class="sc">{</span>fused_acc<span class="sc">:.2f}</span><span class="ss">%'</span>)</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output cell-output-stdout">
<pre><code>Before conv-bn fusion: backbone length 29
After conv-bn fusion: backbone length 21
Accuracy of the fused model=92.95%</code></pre>
</div>
<div class="cell-output cell-output-display">
<script type="application/vnd.jupyter.widget-view+json">
{"model_id":"ec93185044b7487ab136f5be8fb1ea0c","version_major":2,"version_minor":0,"quarto_mimetype":"application/vnd.jupyter.widget-view+json"}
</script>
</div>
</div>
<ol start="2" type="1">
<li>ê° íŠ¹ì§• ë§µì˜ ë²”ìœ„ë¥¼ ì–»ê¸° ìœ„í•´ ì¼ë¶€ ìƒ˜í”Œ ë°ì´í„°ë¡œ ëª¨ë¸ì„ ì‹¤í–‰í•˜ì—¬ íŠ¹ì§• ë§µì˜ ë²”ìœ„ë¥¼ ì–»ê³ , í•´ë‹¹ ìŠ¤ì¼€ì¼ë§ íŒ©í„°ì™€ ì œë¡œ í¬ì¸íŠ¸ë¥¼ ê³„ì‚°í•  ìˆ˜ ìˆìŠµë‹ˆë‹¤.</li>
</ol>
<div id="cell-82" class="cell" data-execution_count="39">
<div class="sourceCode cell-code" id="cb32"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb32-1"><a href="#cb32-1" aria-hidden="true" tabindex="-1"></a><span class="co"># add hook to record the min max value of the activation</span></span>
<span id="cb32-2"><a href="#cb32-2" aria-hidden="true" tabindex="-1"></a>input_activation <span class="op">=</span> {}</span>
<span id="cb32-3"><a href="#cb32-3" aria-hidden="true" tabindex="-1"></a>output_activation <span class="op">=</span> {}</span>
<span id="cb32-4"><a href="#cb32-4" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb32-5"><a href="#cb32-5" aria-hidden="true" tabindex="-1"></a><span class="kw">def</span> add_range_recoder_hook(model):</span>
<span id="cb32-6"><a href="#cb32-6" aria-hidden="true" tabindex="-1"></a>    <span class="im">import</span> functools</span>
<span id="cb32-7"><a href="#cb32-7" aria-hidden="true" tabindex="-1"></a>    <span class="kw">def</span> _record_range(<span class="va">self</span>, x, y, module_name):</span>
<span id="cb32-8"><a href="#cb32-8" aria-hidden="true" tabindex="-1"></a>        x <span class="op">=</span> x[<span class="dv">0</span>]</span>
<span id="cb32-9"><a href="#cb32-9" aria-hidden="true" tabindex="-1"></a>        input_activation[module_name] <span class="op">=</span> x.detach()</span>
<span id="cb32-10"><a href="#cb32-10" aria-hidden="true" tabindex="-1"></a>        output_activation[module_name] <span class="op">=</span> y.detach()</span>
<span id="cb32-11"><a href="#cb32-11" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb32-12"><a href="#cb32-12" aria-hidden="true" tabindex="-1"></a>    all_hooks <span class="op">=</span> []</span>
<span id="cb32-13"><a href="#cb32-13" aria-hidden="true" tabindex="-1"></a>    <span class="cf">for</span> name, m <span class="kw">in</span> model.named_modules():</span>
<span id="cb32-14"><a href="#cb32-14" aria-hidden="true" tabindex="-1"></a>        <span class="cf">if</span> <span class="bu">isinstance</span>(m, (nn.Conv2d, nn.Linear, nn.ReLU)):</span>
<span id="cb32-15"><a href="#cb32-15" aria-hidden="true" tabindex="-1"></a>            all_hooks.append(m.register_forward_hook(</span>
<span id="cb32-16"><a href="#cb32-16" aria-hidden="true" tabindex="-1"></a>                functools.partial(_record_range, module_name<span class="op">=</span>name)))</span>
<span id="cb32-17"><a href="#cb32-17" aria-hidden="true" tabindex="-1"></a>    <span class="cf">return</span> all_hooks</span>
<span id="cb32-18"><a href="#cb32-18" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb32-19"><a href="#cb32-19" aria-hidden="true" tabindex="-1"></a>hooks <span class="op">=</span> add_range_recoder_hook(model_fused)</span>
<span id="cb32-20"><a href="#cb32-20" aria-hidden="true" tabindex="-1"></a>sample_data <span class="op">=</span> <span class="bu">iter</span>(dataloader[<span class="st">'train'</span>]).<span class="fu">__next__</span>()[<span class="dv">0</span>]</span>
<span id="cb32-21"><a href="#cb32-21" aria-hidden="true" tabindex="-1"></a>model_fused(sample_data.cuda())</span>
<span id="cb32-22"><a href="#cb32-22" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb32-23"><a href="#cb32-23" aria-hidden="true" tabindex="-1"></a><span class="co"># remove hooks</span></span>
<span id="cb32-24"><a href="#cb32-24" aria-hidden="true" tabindex="-1"></a><span class="cf">for</span> h <span class="kw">in</span> hooks:</span>
<span id="cb32-25"><a href="#cb32-25" aria-hidden="true" tabindex="-1"></a>    h.remove()</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</div>
<ol start="3" type="1">
<li>ë§ˆì§€ë§‰ìœ¼ë¡œ ëª¨ë¸ ì–‘ìí™”ë¥¼ í•´ë³´ê² ìŠµë‹ˆë‹¤. ë‹¤ìŒê³¼ ê°™ì€ ë§¤í•‘ìœ¼ë¡œ ëª¨ë¸ì„ ë³€í™˜í•©ë‹ˆë‹¤.</li>
</ol>
<div class="sourceCode" id="cb33"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb33-1"><a href="#cb33-1" aria-hidden="true" tabindex="-1"></a>nn.Conv2d: QuantizedConv2d,</span>
<span id="cb33-2"><a href="#cb33-2" aria-hidden="true" tabindex="-1"></a>nn.Linear: QuantizedLinear,</span>
<span id="cb33-3"><a href="#cb33-3" aria-hidden="true" tabindex="-1"></a><span class="co"># the following twos are just wrappers, as current</span></span>
<span id="cb33-4"><a href="#cb33-4" aria-hidden="true" tabindex="-1"></a><span class="co"># torch modules do not support int8 data format;</span></span>
<span id="cb33-5"><a href="#cb33-5" aria-hidden="true" tabindex="-1"></a><span class="co"># we will temporarily convert them to fp32 for computation</span></span>
<span id="cb33-6"><a href="#cb33-6" aria-hidden="true" tabindex="-1"></a>nn.MaxPool2d: QuantizedMaxPool2d,</span>
<span id="cb33-7"><a href="#cb33-7" aria-hidden="true" tabindex="-1"></a>nn.AvgPool2d: QuantizedAvgPool2d,</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div id="cell-84" class="cell" data-execution_count="40">
<div class="sourceCode cell-code" id="cb34"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb34-1"><a href="#cb34-1" aria-hidden="true" tabindex="-1"></a><span class="kw">class</span> QuantizedConv2d(nn.Module):</span>
<span id="cb34-2"><a href="#cb34-2" aria-hidden="true" tabindex="-1"></a>    <span class="kw">def</span> <span class="fu">__init__</span>(<span class="va">self</span>, weight, bias,</span>
<span id="cb34-3"><a href="#cb34-3" aria-hidden="true" tabindex="-1"></a>                 input_zero_point, output_zero_point,</span>
<span id="cb34-4"><a href="#cb34-4" aria-hidden="true" tabindex="-1"></a>                 input_scale, weight_scale, output_scale,</span>
<span id="cb34-5"><a href="#cb34-5" aria-hidden="true" tabindex="-1"></a>                 stride, padding, dilation, groups,</span>
<span id="cb34-6"><a href="#cb34-6" aria-hidden="true" tabindex="-1"></a>                 feature_bitwidth<span class="op">=</span><span class="dv">8</span>, weight_bitwidth<span class="op">=</span><span class="dv">8</span>):</span>
<span id="cb34-7"><a href="#cb34-7" aria-hidden="true" tabindex="-1"></a>        <span class="bu">super</span>().<span class="fu">__init__</span>()</span>
<span id="cb34-8"><a href="#cb34-8" aria-hidden="true" tabindex="-1"></a>        <span class="co"># current version Pytorch does not support IntTensor as nn.Parameter</span></span>
<span id="cb34-9"><a href="#cb34-9" aria-hidden="true" tabindex="-1"></a>        <span class="va">self</span>.register_buffer(<span class="st">'weight'</span>, weight)</span>
<span id="cb34-10"><a href="#cb34-10" aria-hidden="true" tabindex="-1"></a>        <span class="va">self</span>.register_buffer(<span class="st">'bias'</span>, bias)</span>
<span id="cb34-11"><a href="#cb34-11" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb34-12"><a href="#cb34-12" aria-hidden="true" tabindex="-1"></a>        <span class="va">self</span>.input_zero_point <span class="op">=</span> input_zero_point</span>
<span id="cb34-13"><a href="#cb34-13" aria-hidden="true" tabindex="-1"></a>        <span class="va">self</span>.output_zero_point <span class="op">=</span> output_zero_point</span>
<span id="cb34-14"><a href="#cb34-14" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb34-15"><a href="#cb34-15" aria-hidden="true" tabindex="-1"></a>        <span class="va">self</span>.input_scale <span class="op">=</span> input_scale</span>
<span id="cb34-16"><a href="#cb34-16" aria-hidden="true" tabindex="-1"></a>        <span class="va">self</span>.register_buffer(<span class="st">'weight_scale'</span>, weight_scale)</span>
<span id="cb34-17"><a href="#cb34-17" aria-hidden="true" tabindex="-1"></a>        <span class="va">self</span>.output_scale <span class="op">=</span> output_scale</span>
<span id="cb34-18"><a href="#cb34-18" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb34-19"><a href="#cb34-19" aria-hidden="true" tabindex="-1"></a>        <span class="va">self</span>.stride <span class="op">=</span> stride</span>
<span id="cb34-20"><a href="#cb34-20" aria-hidden="true" tabindex="-1"></a>        <span class="va">self</span>.padding <span class="op">=</span> (padding[<span class="dv">1</span>], padding[<span class="dv">1</span>], padding[<span class="dv">0</span>], padding[<span class="dv">0</span>])</span>
<span id="cb34-21"><a href="#cb34-21" aria-hidden="true" tabindex="-1"></a>        <span class="va">self</span>.dilation <span class="op">=</span> dilation</span>
<span id="cb34-22"><a href="#cb34-22" aria-hidden="true" tabindex="-1"></a>        <span class="va">self</span>.groups <span class="op">=</span> groups</span>
<span id="cb34-23"><a href="#cb34-23" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb34-24"><a href="#cb34-24" aria-hidden="true" tabindex="-1"></a>        <span class="va">self</span>.feature_bitwidth <span class="op">=</span> feature_bitwidth</span>
<span id="cb34-25"><a href="#cb34-25" aria-hidden="true" tabindex="-1"></a>        <span class="va">self</span>.weight_bitwidth <span class="op">=</span> weight_bitwidth</span>
<span id="cb34-26"><a href="#cb34-26" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb34-27"><a href="#cb34-27" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb34-28"><a href="#cb34-28" aria-hidden="true" tabindex="-1"></a>    <span class="kw">def</span> forward(<span class="va">self</span>, x):</span>
<span id="cb34-29"><a href="#cb34-29" aria-hidden="true" tabindex="-1"></a>        <span class="cf">return</span> quantized_conv2d(</span>
<span id="cb34-30"><a href="#cb34-30" aria-hidden="true" tabindex="-1"></a>            x, <span class="va">self</span>.weight, <span class="va">self</span>.bias,</span>
<span id="cb34-31"><a href="#cb34-31" aria-hidden="true" tabindex="-1"></a>            <span class="va">self</span>.feature_bitwidth, <span class="va">self</span>.weight_bitwidth,</span>
<span id="cb34-32"><a href="#cb34-32" aria-hidden="true" tabindex="-1"></a>            <span class="va">self</span>.input_zero_point, <span class="va">self</span>.output_zero_point,</span>
<span id="cb34-33"><a href="#cb34-33" aria-hidden="true" tabindex="-1"></a>            <span class="va">self</span>.input_scale, <span class="va">self</span>.weight_scale, <span class="va">self</span>.output_scale,</span>
<span id="cb34-34"><a href="#cb34-34" aria-hidden="true" tabindex="-1"></a>            <span class="va">self</span>.stride, <span class="va">self</span>.padding, <span class="va">self</span>.dilation, <span class="va">self</span>.groups</span>
<span id="cb34-35"><a href="#cb34-35" aria-hidden="true" tabindex="-1"></a>            )</span>
<span id="cb34-36"><a href="#cb34-36" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb34-37"><a href="#cb34-37" aria-hidden="true" tabindex="-1"></a><span class="kw">class</span> QuantizedLinear(nn.Module):</span>
<span id="cb34-38"><a href="#cb34-38" aria-hidden="true" tabindex="-1"></a>    <span class="kw">def</span> <span class="fu">__init__</span>(<span class="va">self</span>, weight, bias,</span>
<span id="cb34-39"><a href="#cb34-39" aria-hidden="true" tabindex="-1"></a>                 input_zero_point, output_zero_point,</span>
<span id="cb34-40"><a href="#cb34-40" aria-hidden="true" tabindex="-1"></a>                 input_scale, weight_scale, output_scale,</span>
<span id="cb34-41"><a href="#cb34-41" aria-hidden="true" tabindex="-1"></a>                 feature_bitwidth<span class="op">=</span><span class="dv">8</span>, weight_bitwidth<span class="op">=</span><span class="dv">8</span>):</span>
<span id="cb34-42"><a href="#cb34-42" aria-hidden="true" tabindex="-1"></a>        <span class="bu">super</span>().<span class="fu">__init__</span>()</span>
<span id="cb34-43"><a href="#cb34-43" aria-hidden="true" tabindex="-1"></a>        <span class="co"># current version Pytorch does not support IntTensor as nn.Parameter</span></span>
<span id="cb34-44"><a href="#cb34-44" aria-hidden="true" tabindex="-1"></a>        <span class="va">self</span>.register_buffer(<span class="st">'weight'</span>, weight)</span>
<span id="cb34-45"><a href="#cb34-45" aria-hidden="true" tabindex="-1"></a>        <span class="va">self</span>.register_buffer(<span class="st">'bias'</span>, bias)</span>
<span id="cb34-46"><a href="#cb34-46" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb34-47"><a href="#cb34-47" aria-hidden="true" tabindex="-1"></a>        <span class="va">self</span>.input_zero_point <span class="op">=</span> input_zero_point</span>
<span id="cb34-48"><a href="#cb34-48" aria-hidden="true" tabindex="-1"></a>        <span class="va">self</span>.output_zero_point <span class="op">=</span> output_zero_point</span>
<span id="cb34-49"><a href="#cb34-49" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb34-50"><a href="#cb34-50" aria-hidden="true" tabindex="-1"></a>        <span class="va">self</span>.input_scale <span class="op">=</span> input_scale</span>
<span id="cb34-51"><a href="#cb34-51" aria-hidden="true" tabindex="-1"></a>        <span class="va">self</span>.register_buffer(<span class="st">'weight_scale'</span>, weight_scale)</span>
<span id="cb34-52"><a href="#cb34-52" aria-hidden="true" tabindex="-1"></a>        <span class="va">self</span>.output_scale <span class="op">=</span> output_scale</span>
<span id="cb34-53"><a href="#cb34-53" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb34-54"><a href="#cb34-54" aria-hidden="true" tabindex="-1"></a>        <span class="va">self</span>.feature_bitwidth <span class="op">=</span> feature_bitwidth</span>
<span id="cb34-55"><a href="#cb34-55" aria-hidden="true" tabindex="-1"></a>        <span class="va">self</span>.weight_bitwidth <span class="op">=</span> weight_bitwidth</span>
<span id="cb34-56"><a href="#cb34-56" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb34-57"><a href="#cb34-57" aria-hidden="true" tabindex="-1"></a>    <span class="kw">def</span> forward(<span class="va">self</span>, x):</span>
<span id="cb34-58"><a href="#cb34-58" aria-hidden="true" tabindex="-1"></a>        <span class="cf">return</span> quantized_linear(</span>
<span id="cb34-59"><a href="#cb34-59" aria-hidden="true" tabindex="-1"></a>            x, <span class="va">self</span>.weight, <span class="va">self</span>.bias,</span>
<span id="cb34-60"><a href="#cb34-60" aria-hidden="true" tabindex="-1"></a>            <span class="va">self</span>.feature_bitwidth, <span class="va">self</span>.weight_bitwidth,</span>
<span id="cb34-61"><a href="#cb34-61" aria-hidden="true" tabindex="-1"></a>            <span class="va">self</span>.input_zero_point, <span class="va">self</span>.output_zero_point,</span>
<span id="cb34-62"><a href="#cb34-62" aria-hidden="true" tabindex="-1"></a>            <span class="va">self</span>.input_scale, <span class="va">self</span>.weight_scale, <span class="va">self</span>.output_scale</span>
<span id="cb34-63"><a href="#cb34-63" aria-hidden="true" tabindex="-1"></a>            )</span>
<span id="cb34-64"><a href="#cb34-64" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb34-65"><a href="#cb34-65" aria-hidden="true" tabindex="-1"></a><span class="kw">class</span> QuantizedMaxPool2d(nn.MaxPool2d):</span>
<span id="cb34-66"><a href="#cb34-66" aria-hidden="true" tabindex="-1"></a>    <span class="kw">def</span> forward(<span class="va">self</span>, x):</span>
<span id="cb34-67"><a href="#cb34-67" aria-hidden="true" tabindex="-1"></a>        <span class="co"># current version PyTorch does not support integer-based MaxPool</span></span>
<span id="cb34-68"><a href="#cb34-68" aria-hidden="true" tabindex="-1"></a>        <span class="cf">return</span> <span class="bu">super</span>().forward(x.<span class="bu">float</span>()).to(torch.int8)</span>
<span id="cb34-69"><a href="#cb34-69" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb34-70"><a href="#cb34-70" aria-hidden="true" tabindex="-1"></a><span class="kw">class</span> QuantizedAvgPool2d(nn.AvgPool2d):</span>
<span id="cb34-71"><a href="#cb34-71" aria-hidden="true" tabindex="-1"></a>    <span class="kw">def</span> forward(<span class="va">self</span>, x):</span>
<span id="cb34-72"><a href="#cb34-72" aria-hidden="true" tabindex="-1"></a>        <span class="co"># current version PyTorch does not support integer-based AvgPool</span></span>
<span id="cb34-73"><a href="#cb34-73" aria-hidden="true" tabindex="-1"></a>        <span class="cf">return</span> <span class="bu">super</span>().forward(x.<span class="bu">float</span>()).to(torch.int8)</span>
<span id="cb34-74"><a href="#cb34-74" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb34-75"><a href="#cb34-75" aria-hidden="true" tabindex="-1"></a><span class="co"># we use int8 quantization, which is quite popular</span></span>
<span id="cb34-76"><a href="#cb34-76" aria-hidden="true" tabindex="-1"></a>feature_bitwidth <span class="op">=</span> weight_bitwidth <span class="op">=</span> <span class="dv">8</span></span>
<span id="cb34-77"><a href="#cb34-77" aria-hidden="true" tabindex="-1"></a>quantized_model <span class="op">=</span> copy.deepcopy(model_fused)</span>
<span id="cb34-78"><a href="#cb34-78" aria-hidden="true" tabindex="-1"></a>quantized_backbone <span class="op">=</span> []</span>
<span id="cb34-79"><a href="#cb34-79" aria-hidden="true" tabindex="-1"></a>ptr <span class="op">=</span> <span class="dv">0</span></span>
<span id="cb34-80"><a href="#cb34-80" aria-hidden="true" tabindex="-1"></a><span class="cf">while</span> ptr <span class="op">&lt;</span> <span class="bu">len</span>(quantized_model.backbone):</span>
<span id="cb34-81"><a href="#cb34-81" aria-hidden="true" tabindex="-1"></a>    <span class="cf">if</span> <span class="bu">isinstance</span>(quantized_model.backbone[ptr], nn.Conv2d) <span class="kw">and</span> <span class="op">\</span></span>
<span id="cb34-82"><a href="#cb34-82" aria-hidden="true" tabindex="-1"></a>        <span class="bu">isinstance</span>(quantized_model.backbone[ptr <span class="op">+</span> <span class="dv">1</span>], nn.ReLU):</span>
<span id="cb34-83"><a href="#cb34-83" aria-hidden="true" tabindex="-1"></a>        conv <span class="op">=</span> quantized_model.backbone[ptr]</span>
<span id="cb34-84"><a href="#cb34-84" aria-hidden="true" tabindex="-1"></a>        conv_name <span class="op">=</span> <span class="ss">f'backbone.</span><span class="sc">{</span>ptr<span class="sc">}</span><span class="ss">'</span></span>
<span id="cb34-85"><a href="#cb34-85" aria-hidden="true" tabindex="-1"></a>        relu <span class="op">=</span> quantized_model.backbone[ptr <span class="op">+</span> <span class="dv">1</span>]</span>
<span id="cb34-86"><a href="#cb34-86" aria-hidden="true" tabindex="-1"></a>        relu_name <span class="op">=</span> <span class="ss">f'backbone.</span><span class="sc">{</span>ptr <span class="op">+</span> <span class="dv">1</span><span class="sc">}</span><span class="ss">'</span></span>
<span id="cb34-87"><a href="#cb34-87" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb34-88"><a href="#cb34-88" aria-hidden="true" tabindex="-1"></a>        input_scale, input_zero_point <span class="op">=</span> <span class="op">\</span></span>
<span id="cb34-89"><a href="#cb34-89" aria-hidden="true" tabindex="-1"></a>            get_quantization_scale_and_zero_point(</span>
<span id="cb34-90"><a href="#cb34-90" aria-hidden="true" tabindex="-1"></a>                input_activation[conv_name], feature_bitwidth)</span>
<span id="cb34-91"><a href="#cb34-91" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb34-92"><a href="#cb34-92" aria-hidden="true" tabindex="-1"></a>        output_scale, output_zero_point <span class="op">=</span> <span class="op">\</span></span>
<span id="cb34-93"><a href="#cb34-93" aria-hidden="true" tabindex="-1"></a>            get_quantization_scale_and_zero_point(</span>
<span id="cb34-94"><a href="#cb34-94" aria-hidden="true" tabindex="-1"></a>                output_activation[relu_name], feature_bitwidth)</span>
<span id="cb34-95"><a href="#cb34-95" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb34-96"><a href="#cb34-96" aria-hidden="true" tabindex="-1"></a>        quantized_weight, weight_scale, weight_zero_point <span class="op">=</span> <span class="op">\</span></span>
<span id="cb34-97"><a href="#cb34-97" aria-hidden="true" tabindex="-1"></a>            linear_quantize_weight_per_channel(conv.weight.data, weight_bitwidth)</span>
<span id="cb34-98"><a href="#cb34-98" aria-hidden="true" tabindex="-1"></a>        quantized_bias, bias_scale, bias_zero_point <span class="op">=</span> <span class="op">\</span></span>
<span id="cb34-99"><a href="#cb34-99" aria-hidden="true" tabindex="-1"></a>            linear_quantize_bias_per_output_channel(</span>
<span id="cb34-100"><a href="#cb34-100" aria-hidden="true" tabindex="-1"></a>                conv.bias.data, weight_scale, input_scale)</span>
<span id="cb34-101"><a href="#cb34-101" aria-hidden="true" tabindex="-1"></a>        shifted_quantized_bias <span class="op">=</span> <span class="op">\</span></span>
<span id="cb34-102"><a href="#cb34-102" aria-hidden="true" tabindex="-1"></a>            shift_quantized_conv2d_bias(quantized_bias, quantized_weight,</span>
<span id="cb34-103"><a href="#cb34-103" aria-hidden="true" tabindex="-1"></a>                                        input_zero_point)</span>
<span id="cb34-104"><a href="#cb34-104" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb34-105"><a href="#cb34-105" aria-hidden="true" tabindex="-1"></a>        quantized_conv <span class="op">=</span> QuantizedConv2d(</span>
<span id="cb34-106"><a href="#cb34-106" aria-hidden="true" tabindex="-1"></a>            quantized_weight, shifted_quantized_bias,</span>
<span id="cb34-107"><a href="#cb34-107" aria-hidden="true" tabindex="-1"></a>            input_zero_point, output_zero_point,</span>
<span id="cb34-108"><a href="#cb34-108" aria-hidden="true" tabindex="-1"></a>            input_scale, weight_scale, output_scale,</span>
<span id="cb34-109"><a href="#cb34-109" aria-hidden="true" tabindex="-1"></a>            conv.stride, conv.padding, conv.dilation, conv.groups,</span>
<span id="cb34-110"><a href="#cb34-110" aria-hidden="true" tabindex="-1"></a>            feature_bitwidth<span class="op">=</span>feature_bitwidth, weight_bitwidth<span class="op">=</span>weight_bitwidth</span>
<span id="cb34-111"><a href="#cb34-111" aria-hidden="true" tabindex="-1"></a>        )</span>
<span id="cb34-112"><a href="#cb34-112" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb34-113"><a href="#cb34-113" aria-hidden="true" tabindex="-1"></a>        quantized_backbone.append(quantized_conv)</span>
<span id="cb34-114"><a href="#cb34-114" aria-hidden="true" tabindex="-1"></a>        ptr <span class="op">+=</span> <span class="dv">2</span></span>
<span id="cb34-115"><a href="#cb34-115" aria-hidden="true" tabindex="-1"></a>    <span class="cf">elif</span> <span class="bu">isinstance</span>(quantized_model.backbone[ptr], nn.MaxPool2d):</span>
<span id="cb34-116"><a href="#cb34-116" aria-hidden="true" tabindex="-1"></a>        quantized_backbone.append(QuantizedMaxPool2d(</span>
<span id="cb34-117"><a href="#cb34-117" aria-hidden="true" tabindex="-1"></a>            kernel_size<span class="op">=</span>quantized_model.backbone[ptr].kernel_size,</span>
<span id="cb34-118"><a href="#cb34-118" aria-hidden="true" tabindex="-1"></a>            stride<span class="op">=</span>quantized_model.backbone[ptr].stride</span>
<span id="cb34-119"><a href="#cb34-119" aria-hidden="true" tabindex="-1"></a>            ))</span>
<span id="cb34-120"><a href="#cb34-120" aria-hidden="true" tabindex="-1"></a>        ptr <span class="op">+=</span> <span class="dv">1</span></span>
<span id="cb34-121"><a href="#cb34-121" aria-hidden="true" tabindex="-1"></a>    <span class="cf">elif</span> <span class="bu">isinstance</span>(quantized_model.backbone[ptr], nn.AvgPool2d):</span>
<span id="cb34-122"><a href="#cb34-122" aria-hidden="true" tabindex="-1"></a>        quantized_backbone.append(QuantizedAvgPool2d(</span>
<span id="cb34-123"><a href="#cb34-123" aria-hidden="true" tabindex="-1"></a>            kernel_size<span class="op">=</span>quantized_model.backbone[ptr].kernel_size,</span>
<span id="cb34-124"><a href="#cb34-124" aria-hidden="true" tabindex="-1"></a>            stride<span class="op">=</span>quantized_model.backbone[ptr].stride</span>
<span id="cb34-125"><a href="#cb34-125" aria-hidden="true" tabindex="-1"></a>            ))</span>
<span id="cb34-126"><a href="#cb34-126" aria-hidden="true" tabindex="-1"></a>        ptr <span class="op">+=</span> <span class="dv">1</span></span>
<span id="cb34-127"><a href="#cb34-127" aria-hidden="true" tabindex="-1"></a>    <span class="cf">else</span>:</span>
<span id="cb34-128"><a href="#cb34-128" aria-hidden="true" tabindex="-1"></a>        <span class="cf">raise</span> <span class="pp">NotImplementedError</span>(<span class="bu">type</span>(quantized_model.backbone[ptr]))  <span class="co"># should not happen</span></span>
<span id="cb34-129"><a href="#cb34-129" aria-hidden="true" tabindex="-1"></a>quantized_model.backbone <span class="op">=</span> nn.Sequential(<span class="op">*</span>quantized_backbone)</span>
<span id="cb34-130"><a href="#cb34-130" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb34-131"><a href="#cb34-131" aria-hidden="true" tabindex="-1"></a><span class="co"># finally, quantized the classifier</span></span>
<span id="cb34-132"><a href="#cb34-132" aria-hidden="true" tabindex="-1"></a>fc_name <span class="op">=</span> <span class="st">'classifier'</span></span>
<span id="cb34-133"><a href="#cb34-133" aria-hidden="true" tabindex="-1"></a>fc <span class="op">=</span> model.classifier</span>
<span id="cb34-134"><a href="#cb34-134" aria-hidden="true" tabindex="-1"></a>input_scale, input_zero_point <span class="op">=</span> <span class="op">\</span></span>
<span id="cb34-135"><a href="#cb34-135" aria-hidden="true" tabindex="-1"></a>    get_quantization_scale_and_zero_point(</span>
<span id="cb34-136"><a href="#cb34-136" aria-hidden="true" tabindex="-1"></a>        input_activation[fc_name], feature_bitwidth)</span>
<span id="cb34-137"><a href="#cb34-137" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb34-138"><a href="#cb34-138" aria-hidden="true" tabindex="-1"></a>output_scale, output_zero_point <span class="op">=</span> <span class="op">\</span></span>
<span id="cb34-139"><a href="#cb34-139" aria-hidden="true" tabindex="-1"></a>    get_quantization_scale_and_zero_point(</span>
<span id="cb34-140"><a href="#cb34-140" aria-hidden="true" tabindex="-1"></a>        output_activation[fc_name], feature_bitwidth)</span>
<span id="cb34-141"><a href="#cb34-141" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb34-142"><a href="#cb34-142" aria-hidden="true" tabindex="-1"></a>quantized_weight, weight_scale, weight_zero_point <span class="op">=</span> <span class="op">\</span></span>
<span id="cb34-143"><a href="#cb34-143" aria-hidden="true" tabindex="-1"></a>    linear_quantize_weight_per_channel(fc.weight.data, weight_bitwidth)</span>
<span id="cb34-144"><a href="#cb34-144" aria-hidden="true" tabindex="-1"></a>quantized_bias, bias_scale, bias_zero_point <span class="op">=</span> <span class="op">\</span></span>
<span id="cb34-145"><a href="#cb34-145" aria-hidden="true" tabindex="-1"></a>    linear_quantize_bias_per_output_channel(</span>
<span id="cb34-146"><a href="#cb34-146" aria-hidden="true" tabindex="-1"></a>        fc.bias.data, weight_scale, input_scale)</span>
<span id="cb34-147"><a href="#cb34-147" aria-hidden="true" tabindex="-1"></a>shifted_quantized_bias <span class="op">=</span> <span class="op">\</span></span>
<span id="cb34-148"><a href="#cb34-148" aria-hidden="true" tabindex="-1"></a>    shift_quantized_linear_bias(quantized_bias, quantized_weight,</span>
<span id="cb34-149"><a href="#cb34-149" aria-hidden="true" tabindex="-1"></a>                                input_zero_point)</span>
<span id="cb34-150"><a href="#cb34-150" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb34-151"><a href="#cb34-151" aria-hidden="true" tabindex="-1"></a>quantized_model.classifier <span class="op">=</span> QuantizedLinear(</span>
<span id="cb34-152"><a href="#cb34-152" aria-hidden="true" tabindex="-1"></a>    quantized_weight, shifted_quantized_bias,</span>
<span id="cb34-153"><a href="#cb34-153" aria-hidden="true" tabindex="-1"></a>    input_zero_point, output_zero_point,</span>
<span id="cb34-154"><a href="#cb34-154" aria-hidden="true" tabindex="-1"></a>    input_scale, weight_scale, output_scale,</span>
<span id="cb34-155"><a href="#cb34-155" aria-hidden="true" tabindex="-1"></a>    feature_bitwidth<span class="op">=</span>feature_bitwidth, weight_bitwidth<span class="op">=</span>weight_bitwidth</span>
<span id="cb34-156"><a href="#cb34-156" aria-hidden="true" tabindex="-1"></a>)</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</div>
<p>ì–‘ìí™” ê³¼ì •ì´ ì™„ë£Œë˜ì—ˆìŠµë‹ˆë‹¤! ëª¨ë¸ ì•„í‚¤í…ì²˜ë¥¼ ì¸ì‡„í•˜ê³  ì‹œê°í™”í•˜ë©° ì–‘ìí™”ëœ ëª¨ë¸ì˜ ì •í™•ì„±ë„ ê²€ì¦í•´ ë³´ê² ìŠµë‹ˆë‹¤.</p>
<section id="question-9.1-5-pts" class="level3">
<h3 class="anchored" data-anchor-id="question-9.1-5-pts">Question 9.1 (5 pts)</h3>
<p>ì–‘ìí™”ëœ ëª¨ë¸ì„ ì‹¤í–‰í•˜ê¸° ìœ„í•´ì„œëŠ” (0, 1) ë²”ìœ„ì˜ ì…ë ¥ ë°ì´í„°ë¥¼ (-128, 127) ë²”ìœ„ì˜ <code>int8</code> ë²”ìœ„ë¡œ ë§¤í•‘í•˜ëŠ” ì¶”ê°€ì ì¸ ì „ì²˜ë¦¬ê°€ í•„ìš”í•©ë‹ˆë‹¤. ì´ ì „ì²˜ë¦¬ë¥¼ ì§„í–‰í•˜ëŠ” ì•„ë˜ ì½”ë“œë¥¼ ì™„ì„±í•˜ì„¸ìš”.</p>
<p><strong>Hint</strong>: ì–‘ìí™”ëœ ëª¨ë¸ì€ <code>fp32</code> ëª¨ë¸ê³¼ ê±°ì˜ ë™ì¼í•œ ì •í™•ë„ë¥¼ ê°€ì§€ê³  ìˆìŠµë‹ˆë‹¤.</p>
<div id="cell-87" class="cell" data-outputid="e05f0b96-0345-4578-d26c-bfafd73b9cf4" data-execution_count="42">
<div class="sourceCode cell-code" id="cb35"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb35-1"><a href="#cb35-1" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>(quantized_model)</span>
<span id="cb35-2"><a href="#cb35-2" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb35-3"><a href="#cb35-3" aria-hidden="true" tabindex="-1"></a><span class="kw">def</span> extra_preprocess(x):</span>
<span id="cb35-4"><a href="#cb35-4" aria-hidden="true" tabindex="-1"></a>    <span class="co"># hint: you need to convert the original fp32 input of range (0, 1)</span></span>
<span id="cb35-5"><a href="#cb35-5" aria-hidden="true" tabindex="-1"></a>    <span class="co">#  into int8 format of range (-128, 127)</span></span>
<span id="cb35-6"><a href="#cb35-6" aria-hidden="true" tabindex="-1"></a>    <span class="co">############### YOUR CODE STARTS HERE ###############</span></span>
<span id="cb35-7"><a href="#cb35-7" aria-hidden="true" tabindex="-1"></a>    x_scaled <span class="op">=</span> x <span class="op">*</span> <span class="dv">255</span></span>
<span id="cb35-8"><a href="#cb35-8" aria-hidden="true" tabindex="-1"></a>    x_shifted <span class="op">=</span> x_scaled <span class="op">-</span> <span class="dv">128</span></span>
<span id="cb35-9"><a href="#cb35-9" aria-hidden="true" tabindex="-1"></a>    <span class="cf">return</span> x_shifted.clamp(<span class="op">-</span><span class="dv">128</span>, <span class="dv">127</span>).to(torch.int8)</span>
<span id="cb35-10"><a href="#cb35-10" aria-hidden="true" tabindex="-1"></a>    <span class="co">############### YOUR CODE ENDS HERE #################</span></span>
<span id="cb35-11"><a href="#cb35-11" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb35-12"><a href="#cb35-12" aria-hidden="true" tabindex="-1"></a>int8_model_accuracy <span class="op">=</span> evaluate(quantized_model, dataloader[<span class="st">'test'</span>],</span>
<span id="cb35-13"><a href="#cb35-13" aria-hidden="true" tabindex="-1"></a>                               extra_preprocess<span class="op">=</span>[extra_preprocess])</span>
<span id="cb35-14"><a href="#cb35-14" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>(<span class="ss">f"int8 model has accuracy=</span><span class="sc">{</span>int8_model_accuracy<span class="sc">:.2f}</span><span class="ss">%"</span>)</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output cell-output-stdout">
<pre><code>VGG(
  (backbone): Sequential(
    (0): QuantizedConv2d()
    (1): QuantizedConv2d()
    (2): QuantizedMaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)
    (3): QuantizedConv2d()
    (4): QuantizedConv2d()
    (5): QuantizedMaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)
    (6): QuantizedConv2d()
    (7): QuantizedConv2d()
    (8): QuantizedMaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)
    (9): QuantizedConv2d()
    (10): QuantizedConv2d()
    (11): QuantizedMaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)
    (12): QuantizedAvgPool2d(kernel_size=2, stride=2, padding=0)
  )
  (classifier): QuantizedLinear()
)
int8 model has accuracy=92.90%</code></pre>
</div>
<div class="cell-output cell-output-display">
<script type="application/vnd.jupyter.widget-view+json">
{"model_id":"9d3c65b4e17b47188e3a008174b0cd1a","version_major":2,"version_minor":0,"quarto_mimetype":"application/vnd.jupyter.widget-view+json"}
</script>
</div>
</div>
</section>
</section>
<section id="question-9.2-bonus-question-5-pts" class="level2">
<h2 class="anchored" data-anchor-id="question-9.2-bonus-question-5-pts">Question 9.2 (Bonus Question; 5 pts)</h2>
<p>linear quantized modelì— ReLU ì¸µì´ ì—†ëŠ” ì´ìœ ë¥¼ ì„¤ëª…í•˜ì„¸ìš”.</p>
<p><strong>Your Answer:</strong></p>
<p>ì„ í˜•(Linear) ì–‘ìí™” ëª¨ë¸ì—ì„œ ReLU(Rectified Linear Unit) ì¸µì´ ì—†ëŠ” ì´ìœ ëŠ” ì£¼ë¡œ ì–‘ìí™” ê³¼ì •ì—ì„œì˜ ë°ì´í„° í‘œí˜„ ë°©ì‹ê³¼ ì—°ì‚°ì˜ íš¨ìœ¨ì„±ê³¼ ê´€ë ¨ì´ ìˆìŠµë‹ˆë‹¤. ì–‘ìí™”ëŠ” ëª¨ë¸ì˜ ê°€ì¤‘ì¹˜ë‚˜ í™œì„±í™”ë¥¼ ê³ ì •ëœ ë¹„íŠ¸ ë„ˆë¹„(ì˜ˆ: 8ë¹„íŠ¸)ì˜ ì •ìˆ˜ë¡œ ì œí•œí•˜ì—¬ ì €ì¥í•˜ê³  ê³„ì‚°í•˜ëŠ” ê¸°ìˆ ì…ë‹ˆë‹¤. ì´ëŸ¬í•œ ì œí•œì€ ëª¨ë¸ì˜ í¬ê¸°ë¥¼ ì¤„ì´ê³ , ê³„ì‚° ì†ë„ë¥¼ í–¥ìƒì‹œí‚¤ë©°, ì €ì „ë ¥ ì¥ì¹˜ì—ì„œì˜ ì‹¤í–‰ì„ ìš©ì´í•˜ê²Œ í•©ë‹ˆë‹¤. ê·¸ëŸ¬ë‚˜ ì´ ê³¼ì •ì—ì„œ ë°ì´í„°ì˜ ì •ë°€ë„ê°€ ì†ì‹¤ë  ìˆ˜ ìˆìœ¼ë©°, ì´ëŠ” ëª¨ë¸ ì„±ëŠ¥ì— ì˜í–¥ì„ ë¯¸ì¹  ìˆ˜ ìˆìŠµë‹ˆë‹¤.</p>
<p>ReLU í™œì„±í™” í•¨ìˆ˜ëŠ” ì…ë ¥ì´ ì–‘ìˆ˜ì¼ ê²½ìš° ê·¸ëŒ€ë¡œ ì¶œë ¥í•˜ê³ , ìŒìˆ˜ì¼ ê²½ìš° 0ìœ¼ë¡œ ë§Œë“œëŠ” ê°„ë‹¨í•˜ê³  íš¨ìœ¨ì ì¸ ë¹„ì„ í˜• í•¨ìˆ˜ì…ë‹ˆë‹¤. ReLUëŠ” ë”¥ëŸ¬ë‹ ëª¨ë¸ì—ì„œ ë„ë¦¬ ì‚¬ìš©ë˜ë©°, íŠ¹íˆ ì€ë‹‰ì¸µì—ì„œ ë¹„ì„ í˜•ì„±ì„ ì¶”ê°€í•˜ì—¬ ëª¨ë¸ì˜ í‘œí˜„ë ¥ì„ í–¥ìƒì‹œí‚¤ëŠ” ë° ì¤‘ìš”í•œ ì—­í• ì„ í•©ë‹ˆë‹¤.</p>
<p>ì„ í˜• ì–‘ìí™” ëª¨ë¸ì—ì„œ ReLU ì¸µì´ ì—†ëŠ” ì£¼ëœ ì´ìœ ëŠ” ë‹¤ìŒê³¼ ê°™ìŠµë‹ˆë‹¤:</p>
<ol type="1">
<li><p><strong>ì–‘ìí™”ëœ ë°ì´í„°ì˜ ë²”ìœ„ ì œí•œ</strong>: ì •ìˆ˜ ì–‘ìí™” ê³¼ì •ì—ì„œëŠ” ë°ì´í„°ê°€ íŠ¹ì • ë²”ìœ„ ë‚´ì˜ ê°’ìœ¼ë¡œ ì œí•œë©ë‹ˆë‹¤. ì˜ˆë¥¼ ë“¤ì–´, 8ë¹„íŠ¸ ì–‘ìí™”ì—ì„œëŠ” ê°’ì´ -128ë¶€í„° 127ê¹Œì§€ì˜ ì •ìˆ˜ ë²”ìœ„ë¥¼ ê°€ì§‘ë‹ˆë‹¤. ì´ëŸ¬í•œ ë²”ìœ„ ë‚´ì—ì„œ ReLUë¥¼ ì ìš©í•˜ë©´ ìŒìˆ˜ ê°’ì´ ëª¨ë‘ 0ìœ¼ë¡œ ë³€í™˜ë˜ì–´, ì–‘ìˆ˜ ê°’ë§Œ ë‚¨ê²Œ ë©ë‹ˆë‹¤. ì´ ê³¼ì •ì—ì„œ ë°ì´í„°ì˜ ë²”ìœ„ê°€ ë”ìš± ì œí•œë˜ì–´, ì–‘ìí™”ëœ ëª¨ë¸ì˜ í‘œí˜„ë ¥ì´ ë”ìš± ê°ì†Œí•  ìˆ˜ ìˆìŠµë‹ˆë‹¤.</p></li>
<li><p><strong>íš¨ìœ¨ì„±</strong>: ì–‘ìí™”ëœ ëª¨ë¸ì€ ê°€ëŠ¥í•œ í•œ ê³„ì‚°ì„ ê°„ë‹¨í•˜ê²Œ ìœ ì§€í•˜ì—¬ ë¹ ë¥¸ ì¶”ë¡  ì†ë„ì™€ ë‚®ì€ ì „ë ¥ ì†Œëª¨ë¥¼ ë‹¬ì„±í•˜ë ¤ê³  í•©ë‹ˆë‹¤. ReLUì™€ ê°™ì€ ë¹„ì„ í˜• í•¨ìˆ˜ë¥¼ ì¶”ê°€í•˜ë©´, ì¶”ë¡  ê³¼ì •ì—ì„œ ì¶”ê°€ì ì¸ ê³„ì‚°ì´ í•„ìš”í•˜ê²Œ ë©ë‹ˆë‹¤. ì–´ë–¤ ê²½ìš°ì—ëŠ” ëª¨ë¸ì˜ êµ¬ì¡°ë‚˜ ëª©ì ì— ë”°ë¼ ì´ëŸ¬í•œ ì¶”ê°€ ê³„ì‚° ì—†ì´ë„ ì¶©ë¶„í•œ ì„±ëŠ¥ì„ ë‹¬ì„±í•  ìˆ˜ ìˆìœ¼ë¯€ë¡œ, ReLU ì¸µì„ ìƒëµí•  ìˆ˜ ìˆìŠµë‹ˆë‹¤.</p></li>
<li><p><strong>ëª¨ë¸ ì„¤ê³„ì™€ ëª©ì </strong>: íŠ¹ì • ì–‘ìí™” ëª¨ë¸ì—ì„œëŠ” ì„±ëŠ¥ ìœ ì§€ë¥¼ ìœ„í•´ ReLU ëŒ€ì‹  ë‹¤ë¥¸ ê¸°ë²•ì´ë‚˜ í™œì„±í™” í•¨ìˆ˜ë¥¼ ì‚¬ìš©í•  ìˆ˜ ìˆìŠµë‹ˆë‹¤. ì˜ˆë¥¼ ë“¤ì–´, ì–‘ìí™” ì „ ëª¨ë¸ì—ì„œ ReLUë¥¼ ì‚¬ìš©í•˜ëŠ” ëŒ€ì‹ , ì–‘ìí™” ê³¼ì •ì—ì„œ ìµœì í™”ëœ í™œì„±í™” í•¨ìˆ˜ë¥¼ ì„ íƒí•˜ê±°ë‚˜, ReLUì˜ íš¨ê³¼ë¥¼ ëª¨ë°©í•  ìˆ˜ ìˆëŠ” ë‹¤ë¥¸ ë°©ë²•ì„ ëª¨ìƒ‰í•  ìˆ˜ ìˆìŠµë‹ˆë‹¤.</p></li>
</ol>
<p>ê²°ë¡ ì ìœ¼ë¡œ, ì„ í˜• ì–‘ìí™” ëª¨ë¸ì—ì„œ ReLU ì¸µì˜ ë¶€ì¬ëŠ” ë°ì´í„°ì˜ ë²”ìœ„ ì œí•œ, ê³„ì‚° íš¨ìœ¨ì„±, ê·¸ë¦¬ê³  íŠ¹ì • ëª¨ë¸ ì„¤ê³„ì™€ ëª©ì ì— ê¸°ì¸í•  ìˆ˜ ìˆìŠµë‹ˆë‹¤. ëª¨ë¸ì˜ ì„¤ê³„ìëŠ” ì„±ëŠ¥, ì†ë„, í¬ê¸° ë“±ì˜ ìš”êµ¬ ì‚¬í•­ì„ ê· í˜• ìˆê²Œ ê³ ë ¤í•˜ì—¬ ìµœì ì˜ ëª¨ë¸ êµ¬ì¡°ë¥¼ ê²°ì •í•´ì•¼ í•©ë‹ˆë‹¤.</p>
</section>
</section>
<section id="question-10-5-pts" class="level1">
<h1>Question 10 (5 pts)</h1>
<p>k-means ê¸°ë°˜ ì–‘ìí™”ì™€ ì„ í˜• ì–‘ìí™”ì˜ ì¥ë‹¨ì ì„ ë¹„êµí•´ ë³´ì‹œê¸° ë°”ëë‹ˆë‹¤. ì •í™•ë„, ì§€ì—° ì‹œê°„, í•˜ë“œì›¨ì–´ ì§€ì› ë“±ì˜ ê´€ì ì—ì„œ ìƒê°í•´ë³´ì„¸ìš”.</p>
<p><strong>Your Answer:</strong></p>
<p>K-means ê¸°ë°˜ ì–‘ìí™”ì™€ ì„ í˜• ì–‘ìí™”ëŠ” ë°ì´í„°ì˜ ì •ë°€ë„ë¥¼ ì¤„ì´ê±°ë‚˜ í¬ê¸°ë¥¼ ì¶•ì†Œí•˜ëŠ” ë° ì‚¬ìš©ë˜ëŠ” ë‘ ê°€ì§€ ê¸°ìˆ ì…ë‹ˆë‹¤. ì´ë“¤ ê¸°ìˆ ì€ íŠ¹ì • ì‘ìš© í”„ë¡œê·¸ë¨ì— ë” ì í•©í•œ ë‹¤ì–‘í•œ íŠ¹ì„±ì„ ê°€ì§€ê³  ìˆìŠµë‹ˆë‹¤. ì—¬ê¸°ì—ì„œ ì •í™•ë„, ì§€ì—° ì‹œê°„ ë° í•˜ë“œì›¨ì–´ ì§€ì›ì˜ ê´€ì ì—ì„œ ë¹„êµí•´ ë³´ê² ìŠµë‹ˆë‹¤:</p>
<section id="ì •í™•ë„" class="level3">
<h3 class="anchored" data-anchor-id="ì •í™•ë„">ì •í™•ë„:</h3>
<ul>
<li><strong>K-means ê¸°ë°˜ ì–‘ìí™”</strong>: ì£¼ì–´ì§„ í´ëŸ¬ìŠ¤í„° ìˆ˜ì— ëŒ€í•´ ì–‘ìí™” ì˜¤ë¥˜ë¥¼ ìµœì†Œí™”í•˜ê¸° ë•Œë¬¸ì— ì¼ë°˜ì ìœ¼ë¡œ ì„ í˜• ì–‘ìí™”ë³´ë‹¤ ë” ë†’ì€ ì •í™•ë„ë¥¼ ì œê³µí•©ë‹ˆë‹¤. K-means ì–‘ìí™”ëŠ” ë¹„ìŠ·í•œ ê°’ì„ í•¨ê»˜ í´ëŸ¬ìŠ¤í„°ë§í•˜ì—¬ ë°ì´í„° ë¶„í¬ì— ì ì‘í•˜ë¯€ë¡œ, íŠ¹íˆ ë°ì´í„°ê°€ ê· ì¼í•˜ì§€ ì•Šì€ ë¶„í¬ë¥¼ ë”°ë¥¼ ë•Œ ë” ë§ì€ ì •ë³´ë¥¼ ë³´ì¡´í•©ë‹ˆë‹¤.</li>
<li><strong>ì„ í˜• ì–‘ìí™”</strong>: ì´ ë°©ë²•ì€ ë°ì´í„° ê°’ì˜ ì „ì²´ ë²”ìœ„ì— ê±¸ì³ ê· ì¼í•œ ìŠ¤ì¼€ì¼ì„ ì ìš©í•©ë‹ˆë‹¤. ë” ë‹¨ìˆœí•˜ì§€ë§Œ, K-meansë§Œí¼ ë°ì´í„° ë¶„í¬ë¥¼ íš¨ê³¼ì ìœ¼ë¡œ í¬ì°©í•˜ì§€ ëª»í•  ìˆ˜ ìˆìœ¼ë©°, íŠ¹íˆ ë°ì´í„°ê°€ ê· ì¼ ë¶„í¬ë¥¼ ë”°ë¥´ì§€ ì•ŠëŠ” ê²½ìš°ì—ëŠ” ì–‘ìí™” ì˜¤ë¥˜ê°€ ë” í´ ìˆ˜ ìˆìŠµë‹ˆë‹¤.</li>
</ul>
</section>
<section id="ì§€ì—°-ì‹œê°„" class="level3">
<h3 class="anchored" data-anchor-id="ì§€ì—°-ì‹œê°„">ì§€ì—° ì‹œê°„:</h3>
<ul>
<li><strong>K-means ê¸°ë°˜ ì–‘ìí™”</strong>: ìµœì ì˜ í´ëŸ¬ìŠ¤í„°ë¥¼ ì°¾ëŠ” ê³¼ì •ì€ ê³„ì‚°ì´ ë§ì´ í•„ìš”í•˜ê³  íŠ¹íˆ í° ë°ì´í„°ì…‹ì´ë‚˜ ë†’ì€ ì°¨ì›ì—ì„œ ë” ëŠë¦´ ìˆ˜ ìˆìŠµë‹ˆë‹¤. ì´ëŠ” K-means ì–‘ìí™”ê°€ ì„ í˜• ì–‘ìí™”ì— ë¹„í•´ ì–‘ìí™” ê³¼ì •ì—ì„œ ë” ë§ì€ ì§€ì—° ì‹œê°„ì„ ë„ì…í•  ìˆ˜ ìˆìŒì„ ì˜ë¯¸í•©ë‹ˆë‹¤.</li>
<li><strong>ì„ í˜• ì–‘ìí™”</strong>: ë‹¨ìˆœì„±ìœ¼ë¡œ ì¸í•´, ì„ í˜• ì–‘ìí™”ëŠ” K-means ê¸°ë°˜ ì–‘ìí™”ë³´ë‹¤ ì¼ë°˜ì ìœ¼ë¡œ ê³„ì‚° ì†ë„ê°€ ë” ë¹ ë¦…ë‹ˆë‹¤. ì´ëŠ” ë‹¨ìˆœí•œ ì‚°ìˆ  ì—°ì‚°ë§Œì„ í¬í•¨í•˜ê¸° ë•Œë¬¸ì—, ë‚®ì€ ì§€ì—° ì‹œê°„ì´ ì¤‘ìš”í•œ ì‹œë‚˜ë¦¬ì˜¤ì—ì„œ ë” ì í•©í•©ë‹ˆë‹¤.</li>
</ul>
</section>
<section id="í•˜ë“œì›¨ì–´-ì§€ì›" class="level3">
<h3 class="anchored" data-anchor-id="í•˜ë“œì›¨ì–´-ì§€ì›">í•˜ë“œì›¨ì–´ ì§€ì›:</h3>
<ul>
<li><strong>K-means ê¸°ë°˜ ì–‘ìí™”</strong>: í´ëŸ¬ìŠ¤í„°ë§ ì•Œê³ ë¦¬ì¦˜ì— ëŒ€í•œ ì „ìš© ì§€ì› ì—†ì´ K-means ê¸°ë°˜ ì–‘ìí™”ë¥¼ íš¨ìœ¨ì ìœ¼ë¡œ êµ¬í˜„í•˜ëŠ” ê²ƒì€ ì–´ë ¤ìš¸ ìˆ˜ ìˆìŠµë‹ˆë‹¤. í˜„ëŒ€ì˜ GPUì™€ ì „ë¬¸ ê°€ì†ê¸°(ì˜ˆ: TPU)ëŠ” ì´ëŸ¬í•œ ì‘ì—…ì„ ë” íš¨ìœ¨ì ìœ¼ë¡œ ìˆ˜í–‰í•  ìˆ˜ ìˆì§€ë§Œ, K-means ì•Œê³ ë¦¬ì¦˜ì˜ ë³µì¡ì„±ì€ ì—¬ì „íˆ ë‚®ì€ ì „ë ¥ ë˜ëŠ” ì„ë² ë””ë“œ ì¥ì¹˜ì—ì„œ ì‚¬ìš©ì„ ì œí•œí•  ìˆ˜ ìˆìŠµë‹ˆë‹¤.</li>
<li><strong>ì„ í˜• ì–‘ìí™”</strong>: ê·¸ ë‹¨ìˆœí•¨ìœ¼ë¡œ ì¸í•´ ì„ í˜• ì–‘ìí™”ëŠ” ì €ì „ë ¥ ë° ì„ë² ë””ë“œ ì¥ì¹˜ë¥¼ í¬í•¨í•œ ë‹¤ì–‘í•œ í•˜ë“œì›¨ì–´ì—ì„œ ë” ì‰½ê²Œ êµ¬í˜„ë  ìˆ˜ ìˆìŠµë‹ˆë‹¤. í´ëŸ¬ìŠ¤í„°ë§ì— ê´€ë ¨ëœ ë³µì¡í•œ ì—°ì‚°ì´ í•„ìš”í•˜ì§€ ì•Šê¸° ë•Œë¬¸ì—, ì œí•œëœ ê³„ì‚° ë¦¬ì†ŒìŠ¤ë¥¼ ê°€ì§„ ì¥ì¹˜ì—ì„œ êµ¬í˜„í•˜ê¸° ë” ì‰½ìŠµë‹ˆë‹¤.</li>
</ul>
</section>
<section id="ìš”ì•½" class="level3">
<h3 class="anchored" data-anchor-id="ìš”ì•½">ìš”ì•½:</h3>
<ul>
<li><strong>K-means ê¸°ë°˜ ì–‘ìí™”</strong>ëŠ” ê¸°ë³¸ ë°ì´í„° ë¶„í¬ì— ë” ì ì‘í•  ìˆ˜ ìˆì–´, ê³„ì‚° ë³µì¡ì„±ê³¼ ì§€ì—° ì‹œê°„ì´ ì¦ê°€í•˜ëŠ” ëŒ€ì‹  ë” ë†’ì€ ì •í™•ë„ë¥¼ ì œê³µí•  ìˆ˜ ìˆìŠµë‹ˆë‹¤. ì´ëŠ” ë°ì´í„°ì˜ ê³ ë„ì˜ ì •í™•ì„±ì„ ìœ ì§€í•˜ëŠ” ê²ƒì´ ì¤‘ìš”í•˜ê³  ê³„ì‚° ìì›ì´ ì£¼ìš” ì œì•½ ì¡°ê±´ì´ ì•„ë‹Œ ì‘ìš© í”„ë¡œê·¸ë¨ì— ê°€ì¥ ì í•©í•©ë‹ˆë‹¤.</li>
<li><strong>ì„ í˜• ì–‘ìí™”</strong>ëŠ” ë‹¨ìˆœì„±, ì†ë„ ë° ê´‘ë²”ìœ„í•œ í•˜ë“œì›¨ì–´ í˜¸í™˜ì„±ì˜ ê· í˜•ì„ ì œê³µí•˜ì—¬, ë³µì¡í•˜ê±°ë‚˜ ê· ì¼í•˜ì§€ ì•Šì€ ë°ì´í„° ë¶„í¬ì— ëŒ€í•´ ë™ì¼í•œ ìˆ˜ì¤€ì˜ ì •í™•ë„ë¥¼ í•­ìƒ ë‹¬ì„±í•  ìˆ˜ëŠ” ì—†ì§€ë§Œ ì‹¤ì‹œê°„ ì²˜ë¦¬ì™€ ì²˜ë¦¬ ëŠ¥ë ¥ì´ ì œí•œëœ ì¥ì¹˜ì— ì í•©í•©ë‹ˆë‹¤.</li>
</ul>
<p>ì‘ìš© í”„ë¡œê·¸ë¨ì˜ íŠ¹ì • ìš”êµ¬ ì‚¬í•­ì— ë”°ë¼ K-means ê¸°ë°˜ ì–‘ìí™”ì™€ ì„ í˜• ì–‘ìí™” ì‚¬ì´ì—ì„œ ì„ íƒí•´ì•¼ í•˜ë©°, ì •í™•ì„±, ì²˜ë¦¬ ì§€ì—° ì‹œê°„ ë° ì‚¬ìš© ê°€ëŠ¥í•œ ê³„ì‚° ë¦¬ì†ŒìŠ¤ì˜ ì¤‘ìš”ì„±ì„ ê³ ë ¤í•´ì•¼ í•©ë‹ˆë‹¤.</p>


</section>
</section>

</main> <!-- /main -->
<script type="application/vnd.jupyter.widget-state+json">
{"0066b1cbc43e497ebfe30c38a6174013":{"model_module":"@jupyter-widgets/base","model_module_version":"1.2.0","model_name":"LayoutModel","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"01996e2fccc44cae86d5e3d962031378":{"model_module":"@jupyter-widgets/base","model_module_version":"1.2.0","model_name":"LayoutModel","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":"hidden","width":null}},"0573b041272e4221ae361d139338af87":{"model_module":"@jupyter-widgets/base","model_module_version":"1.2.0","model_name":"LayoutModel","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"05b523786e0d4eb8b3a9c00286049a36":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"DescriptionStyleModel","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"07329c11a50d4cb28e2f4c5e393a2261":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"HTMLModel","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_0066b1cbc43e497ebfe30c38a6174013","placeholder":"â€‹","style":"IPY_MODEL_54cc76818b9140f6960f90dc01e7ed7b","value":"eval:â€‡â€‡95%"}},"074f239fb93d44f88f08d72b47a12352":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"FloatProgressModel","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"FloatProgressModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"ProgressView","bar_style":"","description":"","description_tooltip":null,"layout":"IPY_MODEL_8a4985c1c21b42d6aa1947004a2eadda","max":20,"min":0,"orientation":"horizontal","style":"IPY_MODEL_86942f19d33848c5ab819419706df89b","value":20}},"075ae3e1908d4e32bbaa4863f1295d2e":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"DescriptionStyleModel","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"0a306198966148acb3cc158e7575fca3":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"HTMLModel","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_69681a1b580541d292d677d84196cb88","placeholder":"â€‹","style":"IPY_MODEL_c25a71d3a0e647daa72a3c0fe844a076","value":"train:â€‡100%"}},"0caec2e73f9b4f5bbd2759a731926be5":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"FloatProgressModel","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"FloatProgressModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"ProgressView","bar_style":"","description":"","description_tooltip":null,"layout":"IPY_MODEL_e7013681d61e424a8b839bdb3f5c0a42","max":98,"min":0,"orientation":"horizontal","style":"IPY_MODEL_716b121dede84774a795cd429993ebad","value":98}},"0d1bcec1b3f440bab1c31167dca44076":{"model_module":"@jupyter-widgets/base","model_module_version":"1.2.0","model_name":"LayoutModel","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":"hidden","width":null}},"1040798057cf43579bf47d0316626dc7":{"model_module":"@jupyter-widgets/base","model_module_version":"1.2.0","model_name":"LayoutModel","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":"hidden","width":null}},"108b77d207af4e6e9894ea3f8dc03c2d":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"FloatProgressModel","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"FloatProgressModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"ProgressView","bar_style":"","description":"","description_tooltip":null,"layout":"IPY_MODEL_ed5b8f418c944d7e878fef4640d928f5","max":20,"min":0,"orientation":"horizontal","style":"IPY_MODEL_58d88a91496f4868bb8fad7104149788","value":20}},"10a360d3f5ee456ca2e6cb30bc794f8d":{"model_module":"@jupyter-widgets/base","model_module_version":"1.2.0","model_name":"LayoutModel","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"10a61df5f15343069d95be4c8178d907":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"ProgressStyleModel","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"ProgressStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","bar_color":null,"description_width":""}},"10ca2dffa2c64bf9af2c8323fdbda816":{"model_module":"@jupyter-widgets/base","model_module_version":"1.2.0","model_name":"LayoutModel","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"1216c21191e04d61ab4c280c1d85a7e2":{"model_module":"@jupyter-widgets/base","model_module_version":"1.2.0","model_name":"LayoutModel","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":"hidden","width":null}},"133a0bdc7fe54f0bab025af074c0762d":{"model_module":"@jupyter-widgets/base","model_module_version":"1.2.0","model_name":"LayoutModel","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"14087fca563947309b7bb795df81b113":{"model_module":"@jupyter-widgets/base","model_module_version":"1.2.0","model_name":"LayoutModel","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"143e4c8694ba42228a293442c3805363":{"model_module":"@jupyter-widgets/base","model_module_version":"1.2.0","model_name":"LayoutModel","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":"hidden","width":null}},"16895d47c331468c98ce87d08ff25d8d":{"model_module":"@jupyter-widgets/base","model_module_version":"1.2.0","model_name":"LayoutModel","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"1a25fcaa73ef49f2a7eea4ee22522f81":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"DescriptionStyleModel","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"1e47eb6961a449cb901e2bb5e171014f":{"model_module":"@jupyter-widgets/base","model_module_version":"1.2.0","model_name":"LayoutModel","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"1f07bc1b93f248479e20bf962eb43de3":{"model_module":"@jupyter-widgets/base","model_module_version":"1.2.0","model_name":"LayoutModel","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"1f8fed8d37d34f4bbd7e69d0429aad26":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"FloatProgressModel","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"FloatProgressModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"ProgressView","bar_style":"","description":"","description_tooltip":null,"layout":"IPY_MODEL_e1faaa73818e41ec880335ff21ffe48a","max":20,"min":0,"orientation":"horizontal","style":"IPY_MODEL_d34270ca755040dc8c0de858edf1c709","value":20}},"1fd1b640958f4d28b6e35ca59f0b4b98":{"model_module":"@jupyter-widgets/base","model_module_version":"1.2.0","model_name":"LayoutModel","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"202219841a0446869fb17b957f3c8c34":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"DescriptionStyleModel","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"222e114e6b13422a9d470997dd277180":{"model_module":"@jupyter-widgets/base","model_module_version":"1.2.0","model_name":"LayoutModel","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"262f3160a49f483bbcb6055d036ca1df":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"DescriptionStyleModel","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"2676c33bf31540f7b7d7e56e9dce39d3":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"FloatProgressModel","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"FloatProgressModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"ProgressView","bar_style":"","description":"","description_tooltip":null,"layout":"IPY_MODEL_8301ace1df784238b233b8d13cc9943c","max":20,"min":0,"orientation":"horizontal","style":"IPY_MODEL_45cfdfeee463470896a8881b7a382d26","value":20}},"28f6a4eca3e0463d96fb6bb990b1a89a":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"HBoxModel","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HBoxModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HBoxView","box_style":"","children":["IPY_MODEL_33a379c748954889a8daa451645751d7","IPY_MODEL_ed6dd35b928f45639db323f56b63770b","IPY_MODEL_a89f705f6f23450f9229b012a539fbad"],"layout":"IPY_MODEL_4e70b65677784b7b815a399e770f357d"}},"2b294dc599cd4a20bbae60f5969f1833":{"model_module":"@jupyter-widgets/base","model_module_version":"1.2.0","model_name":"LayoutModel","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":"hidden","width":null}},"2d0e7e3d23b9468b9fc99864e56bdc52":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"DescriptionStyleModel","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"2ec20906187841399ca685b782e76f9e":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"HTMLModel","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_3929da007a1a4654b653fbf92079791d","placeholder":"â€‹","style":"IPY_MODEL_73d130fdb3b64ec09a158bee056e02ab","value":"â€‡19/20â€‡[00:01&lt;00:00,â€‡â€‡9.17it/s]"}},"2fffee83aa7b4ca4b2b749c092e80c39":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"DescriptionStyleModel","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"30dca68821604625970a13fb1cea2de8":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"HTMLModel","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_685d206c2caf47d78dbbf1449e02fb97","placeholder":"â€‹","style":"IPY_MODEL_a79353f6e5394481aff47d82a2be6f67","value":"eval:â€‡â€‡95%"}},"31e6efb5a60b4ea68fa8b538a052e7f3":{"model_module":"@jupyter-widgets/base","model_module_version":"1.2.0","model_name":"LayoutModel","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":"hidden","width":null}},"331174c7d38147ba9b59bbcdb3ec0e0a":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"DescriptionStyleModel","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"334a45ef0e45497e8849d82ea797d6b6":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"DescriptionStyleModel","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"33a379c748954889a8daa451645751d7":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"HTMLModel","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_44f72409a20b48939aa76d4410fe9efc","placeholder":"â€‹","style":"IPY_MODEL_05b523786e0d4eb8b3a9c00286049a36","value":"eval:â€‡â€‡95%"}},"35a589253bc24280b4097e930f5230d2":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"HTMLModel","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_ac821de747e14709a4957ad20bc4c3f9","placeholder":"â€‹","style":"IPY_MODEL_3be35102ec324314aa7da4ddebcacad6","value":"â€‡98/98â€‡[00:39&lt;00:00,â€‡â€‡2.78it/s]"}},"35e740d946d6415da2810be0e35d7d54":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"HTMLModel","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_3db1c6742bb24479b93add492aea76d9","placeholder":"â€‹","style":"IPY_MODEL_655c50c05962468d977553e341facc94","value":"train:â€‡100%"}},"3616ffe673e7403787b615b60c72208e":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"HTMLModel","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_10a360d3f5ee456ca2e6cb30bc794f8d","placeholder":"â€‹","style":"IPY_MODEL_74da0fab56964a44bcea27c848edf1ba","value":"eval:â€‡â€‡95%"}},"3929da007a1a4654b653fbf92079791d":{"model_module":"@jupyter-widgets/base","model_module_version":"1.2.0","model_name":"LayoutModel","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"39f829c1d691446e99aee46fc9ccce3d":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"HTMLModel","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_e93256a3827945e7825756585385d89f","placeholder":"â€‹","style":"IPY_MODEL_40f5eb7f84364a6a8bad62a6b6bd9de4","value":"â€‡19/20â€‡[00:02&lt;00:00,â€‡â€‡7.75it/s]"}},"3aabc158e2734e29871c26d283b15c73":{"model_module":"@jupyter-widgets/base","model_module_version":"1.2.0","model_name":"LayoutModel","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"3aaf46d313004b08a9e9494bdfaf7445":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"DescriptionStyleModel","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"3be35102ec324314aa7da4ddebcacad6":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"DescriptionStyleModel","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"3c9a4bc7acb44af9b19970b33a8e4070":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"HBoxModel","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HBoxModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HBoxView","box_style":"","children":["IPY_MODEL_6fdf856502464877a0c9a300b291bed3","IPY_MODEL_2676c33bf31540f7b7d7e56e9dce39d3","IPY_MODEL_f692d80393db4c89ae5d67ac5df5900c"],"layout":"IPY_MODEL_2b294dc599cd4a20bbae60f5969f1833"}},"3d8db82ac03249d9923f3b8417732a56":{"model_module":"@jupyter-widgets/base","model_module_version":"1.2.0","model_name":"LayoutModel","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":"hidden","width":null}},"3db1c6742bb24479b93add492aea76d9":{"model_module":"@jupyter-widgets/base","model_module_version":"1.2.0","model_name":"LayoutModel","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"40f5eb7f84364a6a8bad62a6b6bd9de4":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"DescriptionStyleModel","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"4122b76ebff64e4ba0a1a1a85c704da7":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"HTMLModel","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_977688d36d4a428d8e79352c3244ff1d","placeholder":"â€‹","style":"IPY_MODEL_bd61cdad84e54c79801e7bb97fe738f5","value":"â€‡19/20â€‡[00:01&lt;00:00,â€‡13.57it/s]"}},"42728d0c22104b62a5a8f3aa81e33921":{"model_module":"@jupyter-widgets/base","model_module_version":"1.2.0","model_name":"LayoutModel","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"42ae8ce49c684be48464fd7bf635e605":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"FloatProgressModel","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"FloatProgressModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"ProgressView","bar_style":"","description":"","description_tooltip":null,"layout":"IPY_MODEL_cfc123155d634b3c90d8cede0d998b48","max":20,"min":0,"orientation":"horizontal","style":"IPY_MODEL_10a61df5f15343069d95be4c8178d907","value":20}},"437ef1d5b8cd4c05a722e16764708172":{"model_module":"@jupyter-widgets/base","model_module_version":"1.2.0","model_name":"LayoutModel","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"447b634bdb184e05a6c67c85b0b6b1ba":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"HTMLModel","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_de49b69f79d54c16ae551bfc4f18f2b4","placeholder":"â€‹","style":"IPY_MODEL_202219841a0446869fb17b957f3c8c34","value":"train:â€‡100%"}},"44b90616c4e744dcac005a7362e8693b":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"DescriptionStyleModel","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"44f72409a20b48939aa76d4410fe9efc":{"model_module":"@jupyter-widgets/base","model_module_version":"1.2.0","model_name":"LayoutModel","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"453b40203ecf4032914eca7e7f645ee6":{"model_module":"@jupyter-widgets/base","model_module_version":"1.2.0","model_name":"LayoutModel","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"456e13dfded7478cb6be376dc6ad3eb5":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"HBoxModel","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HBoxModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HBoxView","box_style":"","children":["IPY_MODEL_e494e52ee0e1472bb82ea8b269ad61d4","IPY_MODEL_074f239fb93d44f88f08d72b47a12352","IPY_MODEL_39f829c1d691446e99aee46fc9ccce3d"],"layout":"IPY_MODEL_1040798057cf43579bf47d0316626dc7"}},"45cfdfeee463470896a8881b7a382d26":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"ProgressStyleModel","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"ProgressStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","bar_color":null,"description_width":""}},"484bb157820341b49cbb97de37ad0950":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"ProgressStyleModel","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"ProgressStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","bar_color":null,"description_width":""}},"48c3c6e5f9254c959a1080678e5fc80e":{"model_module":"@jupyter-widgets/base","model_module_version":"1.2.0","model_name":"LayoutModel","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"4b6f8afa67764baaacb7624de9c0baf7":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"DescriptionStyleModel","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"4ce8a84726704c02a40742c95087abfd":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"DescriptionStyleModel","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"4e70b65677784b7b815a399e770f357d":{"model_module":"@jupyter-widgets/base","model_module_version":"1.2.0","model_name":"LayoutModel","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":"hidden","width":null}},"4ee4c739cb224cf989d2df4114747f44":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"DescriptionStyleModel","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"5227ce6720a1461fb2825d57d9237ae2":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"FloatProgressModel","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"FloatProgressModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"ProgressView","bar_style":"","description":"","description_tooltip":null,"layout":"IPY_MODEL_e9df8cea7a4f4a369996d59b6334e667","max":20,"min":0,"orientation":"horizontal","style":"IPY_MODEL_484bb157820341b49cbb97de37ad0950","value":20}},"54b67f4d12a34d20a928f39a47a33652":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"HTMLModel","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_1e47eb6961a449cb901e2bb5e171014f","placeholder":"â€‹","style":"IPY_MODEL_262f3160a49f483bbcb6055d036ca1df","value":"eval:â€‡â€‡95%"}},"54cc76818b9140f6960f90dc01e7ed7b":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"DescriptionStyleModel","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"56bc5d3fc44d49cbb163e960a41d2fe5":{"model_module":"@jupyter-widgets/base","model_module_version":"1.2.0","model_name":"LayoutModel","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":"hidden","width":null}},"571a720b338b47cda1fa4e00a5b45fb2":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"HTMLModel","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_a72b6bbc808f45738c52c7914e6637ff","placeholder":"â€‹","style":"IPY_MODEL_1a25fcaa73ef49f2a7eea4ee22522f81","value":"â€‡19/20â€‡[00:01&lt;00:00,â€‡13.17it/s]"}},"57e06c1985ae40538fde8372664ef6b4":{"model_module":"@jupyter-widgets/base","model_module_version":"1.2.0","model_name":"LayoutModel","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"581270004a9e41228c27b1839be960cf":{"model_module":"@jupyter-widgets/base","model_module_version":"1.2.0","model_name":"LayoutModel","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"5838247145d94952a9f40600f6b32b9a":{"model_module":"@jupyter-widgets/base","model_module_version":"1.2.0","model_name":"LayoutModel","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":"hidden","width":null}},"5871bbbaabc146c99c06769c3cbb946e":{"model_module":"@jupyter-widgets/base","model_module_version":"1.2.0","model_name":"LayoutModel","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"58d88a91496f4868bb8fad7104149788":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"ProgressStyleModel","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"ProgressStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","bar_color":null,"description_width":""}},"58e7ef6e1c294277904156e2ba15bf7c":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"HBoxModel","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HBoxModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HBoxView","box_style":"","children":["IPY_MODEL_ed684f87c09b4f14b26a6164f252faaa","IPY_MODEL_5227ce6720a1461fb2825d57d9237ae2","IPY_MODEL_d8f95dae0af74d56a7549575aef6088e"],"layout":"IPY_MODEL_5838247145d94952a9f40600f6b32b9a"}},"5c362921048d4a1e911e2d4e0cd97174":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"ProgressStyleModel","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"ProgressStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","bar_color":null,"description_width":""}},"5c78d17ca3e94c53a2ce2b6ff4b6cc21":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"DescriptionStyleModel","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"5df64008cf134b2885bb161cbb1fb272":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"HBoxModel","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HBoxModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HBoxView","box_style":"","children":["IPY_MODEL_0a306198966148acb3cc158e7575fca3","IPY_MODEL_d94a3d38ccec44f8a8cffe36d6a99609","IPY_MODEL_35a589253bc24280b4097e930f5230d2"],"layout":"IPY_MODEL_eb6312a978cd42d58cdda1222134d1a0"}},"5e2212f3c2c741e4ac314d001b3fe877":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"HTMLModel","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_133a0bdc7fe54f0bab025af074c0762d","placeholder":"â€‹","style":"IPY_MODEL_be2727fcf70c49a6a7ec0c3d18b14aea","value":"eval:â€‡100%"}},"5f371a42616e4cc5b4c5a532ec31fc21":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"HTMLModel","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_8f44d7fa177944cbb5b907b7f70268bd","placeholder":"â€‹","style":"IPY_MODEL_2fffee83aa7b4ca4b2b749c092e80c39","value":"eval:â€‡â€‡95%"}},"61372905162d4f3688e6efe7cd8ce18f":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"HTMLModel","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_64b905ee110c4f2ba4206b7d2af9b23d","placeholder":"â€‹","style":"IPY_MODEL_44b90616c4e744dcac005a7362e8693b","value":"â€‡19/20â€‡[00:01&lt;00:00,â€‡12.36it/s]"}},"61d9ea80089e4d00a491f2960a1c72ef":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"ProgressStyleModel","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"ProgressStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","bar_color":null,"description_width":""}},"62c8dc04cb33411284907e34d1993bf0":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"HBoxModel","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HBoxModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HBoxView","box_style":"","children":["IPY_MODEL_6d659797f4334f80be532aa1d6a834e4","IPY_MODEL_fe4b481f81e94f49ae5da9b8ed95a30c","IPY_MODEL_4122b76ebff64e4ba0a1a1a85c704da7"],"layout":"IPY_MODEL_ca1e7664741b41c78de9b425f2ee882f"}},"63f0c80b15664bd885bee132c3b7ab7b":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"DescriptionStyleModel","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"64690d7ede9f45ae96417ebc07d4c57f":{"model_module":"@jupyter-widgets/base","model_module_version":"1.2.0","model_name":"LayoutModel","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"64b905ee110c4f2ba4206b7d2af9b23d":{"model_module":"@jupyter-widgets/base","model_module_version":"1.2.0","model_name":"LayoutModel","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"655c50c05962468d977553e341facc94":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"DescriptionStyleModel","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"66271f3de108438d9dc349984e0f90d0":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"DescriptionStyleModel","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"675b027614364747baa15e1e3cff6860":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"DescriptionStyleModel","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"67de3be88bb8442190329250d23fab8d":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"DescriptionStyleModel","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"685d206c2caf47d78dbbf1449e02fb97":{"model_module":"@jupyter-widgets/base","model_module_version":"1.2.0","model_name":"LayoutModel","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"68b979d60b144deb9f263330a484701c":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"FloatProgressModel","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"FloatProgressModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"ProgressView","bar_style":"","description":"","description_tooltip":null,"layout":"IPY_MODEL_3aabc158e2734e29871c26d283b15c73","max":98,"min":0,"orientation":"horizontal","style":"IPY_MODEL_723c748c545940be9c790e7c3c54b65d","value":98}},"69681a1b580541d292d677d84196cb88":{"model_module":"@jupyter-widgets/base","model_module_version":"1.2.0","model_name":"LayoutModel","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"69bace186e2547a4b0537c5e0d66ff2a":{"model_module":"@jupyter-widgets/base","model_module_version":"1.2.0","model_name":"LayoutModel","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"6be429d8770a489bb15815a27abdc877":{"model_module":"@jupyter-widgets/base","model_module_version":"1.2.0","model_name":"LayoutModel","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"6d659797f4334f80be532aa1d6a834e4":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"HTMLModel","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_e00f523bf8a64c2eb4e2adad93f9cf25","placeholder":"â€‹","style":"IPY_MODEL_3aaf46d313004b08a9e9494bdfaf7445","value":"eval:â€‡â€‡95%"}},"6e0ade1309b94b7c8efa2d3bf927ff7f":{"model_module":"@jupyter-widgets/base","model_module_version":"1.2.0","model_name":"LayoutModel","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"6fdf856502464877a0c9a300b291bed3":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"HTMLModel","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_89c0d09cfe8a4211be85f504378ae7ed","placeholder":"â€‹","style":"IPY_MODEL_334a45ef0e45497e8849d82ea797d6b6","value":"eval:â€‡â€‡95%"}},"701bd10e4c6f4cd39556de4da0d49477":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"FloatProgressModel","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"FloatProgressModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"ProgressView","bar_style":"","description":"","description_tooltip":null,"layout":"IPY_MODEL_d90bae15c5a34cc9b2e5dff261095197","max":98,"min":0,"orientation":"horizontal","style":"IPY_MODEL_f1a8bd1bbb19495cae5bdc2cd78586df","value":98}},"716b121dede84774a795cd429993ebad":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"ProgressStyleModel","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"ProgressStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","bar_color":null,"description_width":""}},"71d18c640eb54aad81c6b1babb91410a":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"HBoxModel","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HBoxModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HBoxView","box_style":"","children":["IPY_MODEL_5f371a42616e4cc5b4c5a532ec31fc21","IPY_MODEL_108b77d207af4e6e9894ea3f8dc03c2d","IPY_MODEL_2ec20906187841399ca685b782e76f9e"],"layout":"IPY_MODEL_bb8fbb8b28f74bf691c2cb1aa8fb4db6"}},"723c748c545940be9c790e7c3c54b65d":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"ProgressStyleModel","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"ProgressStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","bar_color":null,"description_width":""}},"73a1a44dd9424dddb4c2015f82777a26":{"model_module":"@jupyter-widgets/base","model_module_version":"1.2.0","model_name":"LayoutModel","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"73d130fdb3b64ec09a158bee056e02ab":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"DescriptionStyleModel","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"74da0fab56964a44bcea27c848edf1ba":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"DescriptionStyleModel","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"7728126bc72749c980b553c490ffa10c":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"HBoxModel","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HBoxModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HBoxView","box_style":"","children":["IPY_MODEL_447b634bdb184e05a6c67c85b0b6b1ba","IPY_MODEL_0caec2e73f9b4f5bbd2759a731926be5","IPY_MODEL_ae93cddccf8e478084ab304124899d38"],"layout":"IPY_MODEL_3d8db82ac03249d9923f3b8417732a56"}},"79186f919eb949668ed56c09c7559445":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"HBoxModel","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HBoxModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HBoxView","box_style":"","children":["IPY_MODEL_d857d751f10144288242437956cfd905","IPY_MODEL_1f8fed8d37d34f4bbd7e69d0429aad26","IPY_MODEL_7a368f50d6aa4400b5f6d1ef4626f8af"],"layout":"IPY_MODEL_01996e2fccc44cae86d5e3d962031378"}},"7a368f50d6aa4400b5f6d1ef4626f8af":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"HTMLModel","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_94053fcfd8eb4498991f6968e87a8d8d","placeholder":"â€‹","style":"IPY_MODEL_8f253b6455ec46e69fdf50031d033b7b","value":"â€‡19/20â€‡[00:02&lt;00:00,â€‡â€‡8.48it/s]"}},"7b531bfcade3423cb64be695242120e0":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"ProgressStyleModel","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"ProgressStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","bar_color":null,"description_width":""}},"7e1feb29bb5041ca85ff5820ccb217bc":{"model_module":"@jupyter-widgets/base","model_module_version":"1.2.0","model_name":"LayoutModel","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"8007690880a641839351b601fa7c1d2f":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"ProgressStyleModel","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"ProgressStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","bar_color":null,"description_width":""}},"811ac006ec8445689a59933a9e04fc5f":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"ProgressStyleModel","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"ProgressStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","bar_color":null,"description_width":""}},"826a6f6dc3a84fef9192c9dd64165c57":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"HTMLModel","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_8fb4cf79469a464e8d20d26381ad0e0f","placeholder":"â€‹","style":"IPY_MODEL_c0c7d8ee39b34507a3eb7631f3e29a10","value":"â€‡19/20â€‡[00:01&lt;00:00,â€‡13.08it/s]"}},"8301ace1df784238b233b8d13cc9943c":{"model_module":"@jupyter-widgets/base","model_module_version":"1.2.0","model_name":"LayoutModel","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"86942f19d33848c5ab819419706df89b":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"ProgressStyleModel","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"ProgressStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","bar_color":null,"description_width":""}},"878640f5a034497a8e3ed3450e2473da":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"HBoxModel","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HBoxModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HBoxView","box_style":"","children":["IPY_MODEL_9b1f3904b0c2480bb0884f29ae3db97c","IPY_MODEL_701bd10e4c6f4cd39556de4da0d49477","IPY_MODEL_afbe3b3f917e45b19b9cedd5acd423eb"],"layout":"IPY_MODEL_8b6056723cbb4b76b2f2e52af7ad05a1"}},"892325c089bc45b2b51574a424d5038a":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"HBoxModel","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HBoxModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HBoxView","box_style":"","children":["IPY_MODEL_a48e91c1d1e24eb29e023964b192d8c4","IPY_MODEL_9921841229c94fd4b21d4163b76f8fc8","IPY_MODEL_dc2e1d1c09e844389fabfa6c5a1200f4"],"layout":"IPY_MODEL_1216c21191e04d61ab4c280c1d85a7e2"}},"89c0d09cfe8a4211be85f504378ae7ed":{"model_module":"@jupyter-widgets/base","model_module_version":"1.2.0","model_name":"LayoutModel","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"89ce719588124243a0f3c6a24ce4f727":{"model_module":"@jupyter-widgets/base","model_module_version":"1.2.0","model_name":"LayoutModel","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"8a4985c1c21b42d6aa1947004a2eadda":{"model_module":"@jupyter-widgets/base","model_module_version":"1.2.0","model_name":"LayoutModel","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"8aaf109e952c4ee484f3b0eaf3079996":{"model_module":"@jupyter-widgets/base","model_module_version":"1.2.0","model_name":"LayoutModel","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"8b41750a2cd0454c9bd9107fc15b9044":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"ProgressStyleModel","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"ProgressStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","bar_color":null,"description_width":""}},"8b6056723cbb4b76b2f2e52af7ad05a1":{"model_module":"@jupyter-widgets/base","model_module_version":"1.2.0","model_name":"LayoutModel","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":"hidden","width":null}},"8f253b6455ec46e69fdf50031d033b7b":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"DescriptionStyleModel","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"8f44d7fa177944cbb5b907b7f70268bd":{"model_module":"@jupyter-widgets/base","model_module_version":"1.2.0","model_name":"LayoutModel","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"8fb4cf79469a464e8d20d26381ad0e0f":{"model_module":"@jupyter-widgets/base","model_module_version":"1.2.0","model_name":"LayoutModel","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"909cd82f93374a2699cd1c3aa6998e20":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"HTMLModel","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_437ef1d5b8cd4c05a722e16764708172","placeholder":"â€‹","style":"IPY_MODEL_aaa431a4dd7e4e48bcf14f0c30ef31ae","value":"eval:â€‡â€‡95%"}},"90cfe1fa9f3d468a892d803902048cb0":{"model_module":"@jupyter-widgets/base","model_module_version":"1.2.0","model_name":"LayoutModel","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"94053fcfd8eb4498991f6968e87a8d8d":{"model_module":"@jupyter-widgets/base","model_module_version":"1.2.0","model_name":"LayoutModel","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"95d59e7001da4015b8150c33c57aeb9f":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"DescriptionStyleModel","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"977688d36d4a428d8e79352c3244ff1d":{"model_module":"@jupyter-widgets/base","model_module_version":"1.2.0","model_name":"LayoutModel","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"98efcfa3799e4cd78cc70516b6800796":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"HBoxModel","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HBoxModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HBoxView","box_style":"","children":["IPY_MODEL_35e740d946d6415da2810be0e35d7d54","IPY_MODEL_68b979d60b144deb9f263330a484701c","IPY_MODEL_c9106ee3c38f4be8a47a5d576797c246"],"layout":"IPY_MODEL_d887156cd55647f39ca551639aa02b75"}},"9921841229c94fd4b21d4163b76f8fc8":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"FloatProgressModel","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"FloatProgressModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"ProgressView","bar_style":"","description":"","description_tooltip":null,"layout":"IPY_MODEL_89ce719588124243a0f3c6a24ce4f727","max":20,"min":0,"orientation":"horizontal","style":"IPY_MODEL_5c362921048d4a1e911e2d4e0cd97174","value":20}},"9b1f3904b0c2480bb0884f29ae3db97c":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"HTMLModel","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_e9d31c73288649dda74886de88681942","placeholder":"â€‹","style":"IPY_MODEL_63f0c80b15664bd885bee132c3b7ab7b","value":"train:â€‡100%"}},"9d2959f8ced04deeb1916cafae2d38c6":{"model_module":"@jupyter-widgets/base","model_module_version":"1.2.0","model_name":"LayoutModel","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"9d3c65b4e17b47188e3a008174b0cd1a":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"HBoxModel","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HBoxModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HBoxView","box_style":"","children":["IPY_MODEL_5e2212f3c2c741e4ac314d001b3fe877","IPY_MODEL_42ae8ce49c684be48464fd7bf635e605","IPY_MODEL_ccd18c22b81b430999275478ea4da2c7"],"layout":"IPY_MODEL_0d1bcec1b3f440bab1c31167dca44076"}},"9e0b193fb8e84a60846447aefa2a45d6":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"FloatProgressModel","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"FloatProgressModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"ProgressView","bar_style":"","description":"","description_tooltip":null,"layout":"IPY_MODEL_f65e0a8dcda744f68f885ed2400d3609","max":20,"min":0,"orientation":"horizontal","style":"IPY_MODEL_811ac006ec8445689a59933a9e04fc5f","value":20}},"9e4e1097cd654550870968193db98f08":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"ProgressStyleModel","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"ProgressStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","bar_color":null,"description_width":""}},"a1dea41e4428454ba0d567a9b1d96dd9":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"HBoxModel","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HBoxModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HBoxView","box_style":"","children":["IPY_MODEL_f4e1df77f6db4782b1591b1fd1349053","IPY_MODEL_e103a84f084f40be94b95e20a4b4c211","IPY_MODEL_e4e2def5fb20480d8f5796e54bd7b69e"],"layout":"IPY_MODEL_56bc5d3fc44d49cbb163e960a41d2fe5"}},"a36c2b2ab8624a1798364b2b3d42f38e":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"FloatProgressModel","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"FloatProgressModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"ProgressView","bar_style":"","description":"","description_tooltip":null,"layout":"IPY_MODEL_6be429d8770a489bb15815a27abdc877","max":98,"min":0,"orientation":"horizontal","style":"IPY_MODEL_ede8ead3822c40a982cd5df7d4e3dbd4","value":98}},"a48e91c1d1e24eb29e023964b192d8c4":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"HTMLModel","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_7e1feb29bb5041ca85ff5820ccb217bc","placeholder":"â€‹","style":"IPY_MODEL_675b027614364747baa15e1e3cff6860","value":"eval:â€‡â€‡95%"}},"a5bac86c317e4e728c49580a482d3006":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"ProgressStyleModel","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"ProgressStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","bar_color":null,"description_width":""}},"a72b6bbc808f45738c52c7914e6637ff":{"model_module":"@jupyter-widgets/base","model_module_version":"1.2.0","model_name":"LayoutModel","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"a79353f6e5394481aff47d82a2be6f67":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"DescriptionStyleModel","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"a89f705f6f23450f9229b012a539fbad":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"HTMLModel","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_90cfe1fa9f3d468a892d803902048cb0","placeholder":"â€‹","style":"IPY_MODEL_bc0c2062f5314c4ba1e1da5a5922351d","value":"â€‡19/20â€‡[00:01&lt;00:00,â€‡12.84it/s]"}},"a9c0e0708eea478cbc2ca678f330d06c":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"DescriptionStyleModel","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"aa2d2e2aa47f47fb9416d28395c6485e":{"model_module":"@jupyter-widgets/base","model_module_version":"1.2.0","model_name":"LayoutModel","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"aaa431a4dd7e4e48bcf14f0c30ef31ae":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"DescriptionStyleModel","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"aac39b8f01f44f9d8f9b8eb10d6e306d":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"HTMLModel","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_be661f6f731446da950d9a40f46ce369","placeholder":"â€‹","style":"IPY_MODEL_f5bbcdaf2dc14d96abf85aea6299bef0","value":"â€‡98/98â€‡[00:39&lt;00:00,â€‡â€‡2.77it/s]"}},"ac821de747e14709a4957ad20bc4c3f9":{"model_module":"@jupyter-widgets/base","model_module_version":"1.2.0","model_name":"LayoutModel","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"ae93cddccf8e478084ab304124899d38":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"HTMLModel","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_10ca2dffa2c64bf9af2c8323fdbda816","placeholder":"â€‹","style":"IPY_MODEL_2d0e7e3d23b9468b9fc99864e56bdc52","value":"â€‡98/98â€‡[00:40&lt;00:00,â€‡â€‡2.50it/s]"}},"afbe3b3f917e45b19b9cedd5acd423eb":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"HTMLModel","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_64690d7ede9f45ae96417ebc07d4c57f","placeholder":"â€‹","style":"IPY_MODEL_4b6f8afa67764baaacb7624de9c0baf7","value":"â€‡98/98â€‡[00:41&lt;00:00,â€‡â€‡2.72it/s]"}},"b2dd690c121a4e839701b971987f7cf1":{"model_module":"@jupyter-widgets/base","model_module_version":"1.2.0","model_name":"LayoutModel","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"bb8fbb8b28f74bf691c2cb1aa8fb4db6":{"model_module":"@jupyter-widgets/base","model_module_version":"1.2.0","model_name":"LayoutModel","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":"hidden","width":null}},"bc0c2062f5314c4ba1e1da5a5922351d":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"DescriptionStyleModel","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"bd61cdad84e54c79801e7bb97fe738f5":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"DescriptionStyleModel","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"bd7c1312c5654c9a84dcde9e98d7bbc8":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"DescriptionStyleModel","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"be10dc25ad21438184702da9527af6ee":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"HBoxModel","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HBoxModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HBoxView","box_style":"","children":["IPY_MODEL_3616ffe673e7403787b615b60c72208e","IPY_MODEL_de504142c7314023926fb20cd3ffef88","IPY_MODEL_826a6f6dc3a84fef9192c9dd64165c57"],"layout":"IPY_MODEL_31e6efb5a60b4ea68fa8b538a052e7f3"}},"be2727fcf70c49a6a7ec0c3d18b14aea":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"DescriptionStyleModel","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"be661f6f731446da950d9a40f46ce369":{"model_module":"@jupyter-widgets/base","model_module_version":"1.2.0","model_name":"LayoutModel","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"c0c7d8ee39b34507a3eb7631f3e29a10":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"DescriptionStyleModel","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"c25a71d3a0e647daa72a3c0fe844a076":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"DescriptionStyleModel","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"c3b05046cd4b4313a39266c8c3ca6502":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"DescriptionStyleModel","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"c686627fb9224bb5bd33f55a9efc3b85":{"model_module":"@jupyter-widgets/base","model_module_version":"1.2.0","model_name":"LayoutModel","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"c9106ee3c38f4be8a47a5d576797c246":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"HTMLModel","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_69bace186e2547a4b0537c5e0d66ff2a","placeholder":"â€‹","style":"IPY_MODEL_4ee4c739cb224cf989d2df4114747f44","value":"â€‡98/98â€‡[00:39&lt;00:00,â€‡â€‡2.66it/s]"}},"ca1e7664741b41c78de9b425f2ee882f":{"model_module":"@jupyter-widgets/base","model_module_version":"1.2.0","model_name":"LayoutModel","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":"hidden","width":null}},"caebc935fe29412d93bb94e30f0a1c5b":{"model_module":"@jupyter-widgets/base","model_module_version":"1.2.0","model_name":"LayoutModel","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":"hidden","width":null}},"ccd18c22b81b430999275478ea4da2c7":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"HTMLModel","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_222e114e6b13422a9d470997dd277180","placeholder":"â€‹","style":"IPY_MODEL_95d59e7001da4015b8150c33c57aeb9f","value":"â€‡20/20â€‡[00:02&lt;00:00,â€‡â€‡7.98it/s]"}},"ccda030d6e8848eaa8fe747bc092cb54":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"HTMLModel","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_42728d0c22104b62a5a8f3aa81e33921","placeholder":"â€‹","style":"IPY_MODEL_f9b31fd7962e4e61810da88a5bf68f34","value":"â€‡19/20â€‡[00:01&lt;00:00,â€‡13.31it/s]"}},"cd7308d24bb44429a97883da24535954":{"model_module":"@jupyter-widgets/base","model_module_version":"1.2.0","model_name":"LayoutModel","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":"hidden","width":null}},"cddfb60ce7ae46dfad90f5062ccad0dc":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"ProgressStyleModel","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"ProgressStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","bar_color":null,"description_width":""}},"ceafa86a09f1479d94cc229dac946156":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"ProgressStyleModel","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"ProgressStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","bar_color":null,"description_width":""}},"cfc123155d634b3c90d8cede0d998b48":{"model_module":"@jupyter-widgets/base","model_module_version":"1.2.0","model_name":"LayoutModel","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"cfe4754798944f8c945048daf225f5f0":{"model_module":"@jupyter-widgets/base","model_module_version":"1.2.0","model_name":"LayoutModel","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":"hidden","width":null}},"d1740474ab484f05a6ec49767b8ff3d7":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"HTMLModel","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_d61bbe54c2a04298a40ccae3697dced8","placeholder":"â€‹","style":"IPY_MODEL_f3e2b9bf100045988fe69ece2b24db49","value":"â€‡19/20â€‡[00:01&lt;00:00,â€‡13.54it/s]"}},"d1ff4589f8a14e4f9159f1e3ee47c32c":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"FloatProgressModel","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"FloatProgressModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"ProgressView","bar_style":"","description":"","description_tooltip":null,"layout":"IPY_MODEL_c686627fb9224bb5bd33f55a9efc3b85","max":20,"min":0,"orientation":"horizontal","style":"IPY_MODEL_a5bac86c317e4e728c49580a482d3006","value":20}},"d34270ca755040dc8c0de858edf1c709":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"ProgressStyleModel","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"ProgressStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","bar_color":null,"description_width":""}},"d61bbe54c2a04298a40ccae3697dced8":{"model_module":"@jupyter-widgets/base","model_module_version":"1.2.0","model_name":"LayoutModel","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"d6bd3821273c4b9ab4bd9bd57101e0b6":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"HTMLModel","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_fb99d90a0e2340ddb6cf01b96294cc96","placeholder":"â€‹","style":"IPY_MODEL_67de3be88bb8442190329250d23fab8d","value":"â€‡98/98â€‡[00:40&lt;00:00,â€‡â€‡2.49it/s]"}},"d7180d028e2f4b8d8f26451260631aa5":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"HBoxModel","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HBoxModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HBoxView","box_style":"","children":["IPY_MODEL_deae0b8af3fb4fc7b16df59162070ba4","IPY_MODEL_a36c2b2ab8624a1798364b2b3d42f38e","IPY_MODEL_aac39b8f01f44f9d8f9b8eb10d6e306d"],"layout":"IPY_MODEL_cfe4754798944f8c945048daf225f5f0"}},"d857d751f10144288242437956cfd905":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"HTMLModel","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_453b40203ecf4032914eca7e7f645ee6","placeholder":"â€‹","style":"IPY_MODEL_66271f3de108438d9dc349984e0f90d0","value":"eval:â€‡â€‡95%"}},"d887156cd55647f39ca551639aa02b75":{"model_module":"@jupyter-widgets/base","model_module_version":"1.2.0","model_name":"LayoutModel","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":"hidden","width":null}},"d8f95dae0af74d56a7549575aef6088e":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"HTMLModel","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_8aaf109e952c4ee484f3b0eaf3079996","placeholder":"â€‹","style":"IPY_MODEL_c3b05046cd4b4313a39266c8c3ca6502","value":"â€‡19/20â€‡[00:01&lt;00:00,â€‡12.97it/s]"}},"d90bae15c5a34cc9b2e5dff261095197":{"model_module":"@jupyter-widgets/base","model_module_version":"1.2.0","model_name":"LayoutModel","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"d94a3d38ccec44f8a8cffe36d6a99609":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"FloatProgressModel","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"FloatProgressModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"ProgressView","bar_style":"","description":"","description_tooltip":null,"layout":"IPY_MODEL_48c3c6e5f9254c959a1080678e5fc80e","max":98,"min":0,"orientation":"horizontal","style":"IPY_MODEL_8b41750a2cd0454c9bd9107fc15b9044","value":98}},"d96278bdba38478083640c1f4ce6bb51":{"model_module":"@jupyter-widgets/base","model_module_version":"1.2.0","model_name":"LayoutModel","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"d9776aabb37e4e19afe17d4a0fc40906":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"HBoxModel","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HBoxModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HBoxView","box_style":"","children":["IPY_MODEL_f25facbfa3734aaa91d9369917d67da6","IPY_MODEL_dce77bcb0c6745c9a1b1cbdf7dc0a30b","IPY_MODEL_d6bd3821273c4b9ab4bd9bd57101e0b6"],"layout":"IPY_MODEL_cd7308d24bb44429a97883da24535954"}},"dc2e1d1c09e844389fabfa6c5a1200f4":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"HTMLModel","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_73a1a44dd9424dddb4c2015f82777a26","placeholder":"â€‹","style":"IPY_MODEL_bd7c1312c5654c9a84dcde9e98d7bbc8","value":"â€‡19/20â€‡[00:01&lt;00:00,â€‡13.72it/s]"}},"dce77bcb0c6745c9a1b1cbdf7dc0a30b":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"FloatProgressModel","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"FloatProgressModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"ProgressView","bar_style":"","description":"","description_tooltip":null,"layout":"IPY_MODEL_6e0ade1309b94b7c8efa2d3bf927ff7f","max":98,"min":0,"orientation":"horizontal","style":"IPY_MODEL_ceafa86a09f1479d94cc229dac946156","value":98}},"dcfc608db61641c9946e2a3b2922ca23":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"ProgressStyleModel","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"ProgressStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","bar_color":null,"description_width":""}},"de49b69f79d54c16ae551bfc4f18f2b4":{"model_module":"@jupyter-widgets/base","model_module_version":"1.2.0","model_name":"LayoutModel","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"de4ff9cd02f2496a9f4fca3f69193f1b":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"HBoxModel","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HBoxModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HBoxView","box_style":"","children":["IPY_MODEL_30dca68821604625970a13fb1cea2de8","IPY_MODEL_d1ff4589f8a14e4f9159f1e3ee47c32c","IPY_MODEL_d1740474ab484f05a6ec49767b8ff3d7"],"layout":"IPY_MODEL_caebc935fe29412d93bb94e30f0a1c5b"}},"de504142c7314023926fb20cd3ffef88":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"FloatProgressModel","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"FloatProgressModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"ProgressView","bar_style":"","description":"","description_tooltip":null,"layout":"IPY_MODEL_16895d47c331468c98ce87d08ff25d8d","max":20,"min":0,"orientation":"horizontal","style":"IPY_MODEL_cddfb60ce7ae46dfad90f5062ccad0dc","value":20}},"deae0b8af3fb4fc7b16df59162070ba4":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"HTMLModel","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_9d2959f8ced04deeb1916cafae2d38c6","placeholder":"â€‹","style":"IPY_MODEL_5c78d17ca3e94c53a2ce2b6ff4b6cc21","value":"train:â€‡100%"}},"e00f523bf8a64c2eb4e2adad93f9cf25":{"model_module":"@jupyter-widgets/base","model_module_version":"1.2.0","model_name":"LayoutModel","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"e103a84f084f40be94b95e20a4b4c211":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"FloatProgressModel","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"FloatProgressModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"ProgressView","bar_style":"","description":"","description_tooltip":null,"layout":"IPY_MODEL_57e06c1985ae40538fde8372664ef6b4","max":20,"min":0,"orientation":"horizontal","style":"IPY_MODEL_9e4e1097cd654550870968193db98f08","value":20}},"e1faaa73818e41ec880335ff21ffe48a":{"model_module":"@jupyter-widgets/base","model_module_version":"1.2.0","model_name":"LayoutModel","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"e37b0e95a8674beb89c6bd5dfa1f3381":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"FloatProgressModel","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"FloatProgressModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"ProgressView","bar_style":"","description":"","description_tooltip":null,"layout":"IPY_MODEL_1fd1b640958f4d28b6e35ca59f0b4b98","max":20,"min":0,"orientation":"horizontal","style":"IPY_MODEL_8007690880a641839351b601fa7c1d2f","value":20}},"e494e52ee0e1472bb82ea8b269ad61d4":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"HTMLModel","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_b2dd690c121a4e839701b971987f7cf1","placeholder":"â€‹","style":"IPY_MODEL_e7b4c5f2d6eb47a6b28017a2a3282f5c","value":"eval:â€‡â€‡95%"}},"e4e2def5fb20480d8f5796e54bd7b69e":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"HTMLModel","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_0573b041272e4221ae361d139338af87","placeholder":"â€‹","style":"IPY_MODEL_fc8f1e649a4444608945c9a38c35740f","value":"â€‡20/20â€‡[00:05&lt;00:00,â€‡â€‡5.83it/s]"}},"e7013681d61e424a8b839bdb3f5c0a42":{"model_module":"@jupyter-widgets/base","model_module_version":"1.2.0","model_name":"LayoutModel","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"e7b4c5f2d6eb47a6b28017a2a3282f5c":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"DescriptionStyleModel","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"e93256a3827945e7825756585385d89f":{"model_module":"@jupyter-widgets/base","model_module_version":"1.2.0","model_name":"LayoutModel","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"e9d31c73288649dda74886de88681942":{"model_module":"@jupyter-widgets/base","model_module_version":"1.2.0","model_name":"LayoutModel","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"e9df8cea7a4f4a369996d59b6334e667":{"model_module":"@jupyter-widgets/base","model_module_version":"1.2.0","model_name":"LayoutModel","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"eb6312a978cd42d58cdda1222134d1a0":{"model_module":"@jupyter-widgets/base","model_module_version":"1.2.0","model_name":"LayoutModel","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":"hidden","width":null}},"ec93185044b7487ab136f5be8fb1ea0c":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"HBoxModel","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HBoxModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HBoxView","box_style":"","children":["IPY_MODEL_07329c11a50d4cb28e2f4c5e393a2261","IPY_MODEL_9e0b193fb8e84a60846447aefa2a45d6","IPY_MODEL_ccda030d6e8848eaa8fe747bc092cb54"],"layout":"IPY_MODEL_fb544590050f4d6d92cb7d73701af387"}},"ed5b8f418c944d7e878fef4640d928f5":{"model_module":"@jupyter-widgets/base","model_module_version":"1.2.0","model_name":"LayoutModel","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"ed684f87c09b4f14b26a6164f252faaa":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"HTMLModel","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_aa2d2e2aa47f47fb9416d28395c6485e","placeholder":"â€‹","style":"IPY_MODEL_4ce8a84726704c02a40742c95087abfd","value":"eval:â€‡â€‡95%"}},"ed6dd35b928f45639db323f56b63770b":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"FloatProgressModel","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"FloatProgressModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"ProgressView","bar_style":"","description":"","description_tooltip":null,"layout":"IPY_MODEL_581270004a9e41228c27b1839be960cf","max":20,"min":0,"orientation":"horizontal","style":"IPY_MODEL_61d9ea80089e4d00a491f2960a1c72ef","value":20}},"ede8ead3822c40a982cd5df7d4e3dbd4":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"ProgressStyleModel","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"ProgressStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","bar_color":null,"description_width":""}},"f07ea8f1cbf143d7a15dd79dae5814a6":{"model_module":"@jupyter-widgets/base","model_module_version":"1.2.0","model_name":"LayoutModel","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":"hidden","width":null}},"f1a8bd1bbb19495cae5bdc2cd78586df":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"ProgressStyleModel","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"ProgressStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","bar_color":null,"description_width":""}},"f25facbfa3734aaa91d9369917d67da6":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"HTMLModel","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_d96278bdba38478083640c1f4ce6bb51","placeholder":"â€‹","style":"IPY_MODEL_331174c7d38147ba9b59bbcdb3ec0e0a","value":"train:â€‡100%"}},"f3d43ebe333144baabf03ee1c16fec99":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"HBoxModel","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HBoxModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HBoxView","box_style":"","children":["IPY_MODEL_909cd82f93374a2699cd1c3aa6998e20","IPY_MODEL_e37b0e95a8674beb89c6bd5dfa1f3381","IPY_MODEL_571a720b338b47cda1fa4e00a5b45fb2"],"layout":"IPY_MODEL_f07ea8f1cbf143d7a15dd79dae5814a6"}},"f3e2b9bf100045988fe69ece2b24db49":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"DescriptionStyleModel","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"f4e1df77f6db4782b1591b1fd1349053":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"HTMLModel","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_fdc2c77788104418b515b4f432c2a707","placeholder":"â€‹","style":"IPY_MODEL_a9c0e0708eea478cbc2ca678f330d06c","value":"eval:â€‡100%"}},"f53af3dc696841dca7ef29cf7d786c8f":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"HBoxModel","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HBoxModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HBoxView","box_style":"","children":["IPY_MODEL_54b67f4d12a34d20a928f39a47a33652","IPY_MODEL_fbe40a3b2b354599a31db6f7dfe7bf3d","IPY_MODEL_61372905162d4f3688e6efe7cd8ce18f"],"layout":"IPY_MODEL_143e4c8694ba42228a293442c3805363"}},"f5bbcdaf2dc14d96abf85aea6299bef0":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"DescriptionStyleModel","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"f65e0a8dcda744f68f885ed2400d3609":{"model_module":"@jupyter-widgets/base","model_module_version":"1.2.0","model_name":"LayoutModel","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"f692d80393db4c89ae5d67ac5df5900c":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"HTMLModel","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_1f07bc1b93f248479e20bf962eb43de3","placeholder":"â€‹","style":"IPY_MODEL_075ae3e1908d4e32bbaa4863f1295d2e","value":"â€‡19/20â€‡[00:01&lt;00:00,â€‡12.85it/s]"}},"f9b31fd7962e4e61810da88a5bf68f34":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"DescriptionStyleModel","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"fb544590050f4d6d92cb7d73701af387":{"model_module":"@jupyter-widgets/base","model_module_version":"1.2.0","model_name":"LayoutModel","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":"hidden","width":null}},"fb99d90a0e2340ddb6cf01b96294cc96":{"model_module":"@jupyter-widgets/base","model_module_version":"1.2.0","model_name":"LayoutModel","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"fbe40a3b2b354599a31db6f7dfe7bf3d":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"FloatProgressModel","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"FloatProgressModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"ProgressView","bar_style":"","description":"","description_tooltip":null,"layout":"IPY_MODEL_5871bbbaabc146c99c06769c3cbb946e","max":20,"min":0,"orientation":"horizontal","style":"IPY_MODEL_7b531bfcade3423cb64be695242120e0","value":20}},"fc8f1e649a4444608945c9a38c35740f":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"DescriptionStyleModel","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"fdc2c77788104418b515b4f432c2a707":{"model_module":"@jupyter-widgets/base","model_module_version":"1.2.0","model_name":"LayoutModel","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"fe4b481f81e94f49ae5da9b8ed95a30c":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"FloatProgressModel","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"FloatProgressModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"ProgressView","bar_style":"","description":"","description_tooltip":null,"layout":"IPY_MODEL_14087fca563947309b7bb795df81b113","max":20,"min":0,"orientation":"horizontal","style":"IPY_MODEL_dcfc608db61641c9946e2a3b2922ca23","value":20}}}
</script>
<script id="quarto-html-after-body" type="application/javascript">
window.document.addEventListener("DOMContentLoaded", function (event) {
  const toggleBodyColorMode = (bsSheetEl) => {
    const mode = bsSheetEl.getAttribute("data-mode");
    const bodyEl = window.document.querySelector("body");
    if (mode === "dark") {
      bodyEl.classList.add("quarto-dark");
      bodyEl.classList.remove("quarto-light");
    } else {
      bodyEl.classList.add("quarto-light");
      bodyEl.classList.remove("quarto-dark");
    }
  }
  const toggleBodyColorPrimary = () => {
    const bsSheetEl = window.document.querySelector("link#quarto-bootstrap");
    if (bsSheetEl) {
      toggleBodyColorMode(bsSheetEl);
    }
  }
  toggleBodyColorPrimary();  
  const icon = "î§‹";
  const anchorJS = new window.AnchorJS();
  anchorJS.options = {
    placement: 'right',
    icon: icon
  };
  anchorJS.add('.anchored');
  const isCodeAnnotation = (el) => {
    for (const clz of el.classList) {
      if (clz.startsWith('code-annotation-')) {                     
        return true;
      }
    }
    return false;
  }
  const clipboard = new window.ClipboardJS('.code-copy-button', {
    text: function(trigger) {
      const codeEl = trigger.previousElementSibling.cloneNode(true);
      for (const childEl of codeEl.children) {
        if (isCodeAnnotation(childEl)) {
          childEl.remove();
        }
      }
      return codeEl.innerText;
    }
  });
  clipboard.on('success', function(e) {
    // button target
    const button = e.trigger;
    // don't keep focus
    button.blur();
    // flash "checked"
    button.classList.add('code-copy-button-checked');
    var currentTitle = button.getAttribute("title");
    button.setAttribute("title", "Copied!");
    let tooltip;
    if (window.bootstrap) {
      button.setAttribute("data-bs-toggle", "tooltip");
      button.setAttribute("data-bs-placement", "left");
      button.setAttribute("data-bs-title", "Copied!");
      tooltip = new bootstrap.Tooltip(button, 
        { trigger: "manual", 
          customClass: "code-copy-button-tooltip",
          offset: [0, -8]});
      tooltip.show();    
    }
    setTimeout(function() {
      if (tooltip) {
        tooltip.hide();
        button.removeAttribute("data-bs-title");
        button.removeAttribute("data-bs-toggle");
        button.removeAttribute("data-bs-placement");
      }
      button.setAttribute("title", currentTitle);
      button.classList.remove('code-copy-button-checked');
    }, 1000);
    // clear code selection
    e.clearSelection();
  });
  function tippyHover(el, contentFn, onTriggerFn, onUntriggerFn) {
    const config = {
      allowHTML: true,
      maxWidth: 500,
      delay: 100,
      arrow: false,
      appendTo: function(el) {
          return el.parentElement;
      },
      interactive: true,
      interactiveBorder: 10,
      theme: 'quarto',
      placement: 'bottom-start',
    };
    if (contentFn) {
      config.content = contentFn;
    }
    if (onTriggerFn) {
      config.onTrigger = onTriggerFn;
    }
    if (onUntriggerFn) {
      config.onUntrigger = onUntriggerFn;
    }
    window.tippy(el, config); 
  }
  const noterefs = window.document.querySelectorAll('a[role="doc-noteref"]');
  for (var i=0; i<noterefs.length; i++) {
    const ref = noterefs[i];
    tippyHover(ref, function() {
      // use id or data attribute instead here
      let href = ref.getAttribute('data-footnote-href') || ref.getAttribute('href');
      try { href = new URL(href).hash; } catch {}
      const id = href.replace(/^#\/?/, "");
      const note = window.document.getElementById(id);
      return note.innerHTML;
    });
  }
  const xrefs = window.document.querySelectorAll('a.quarto-xref');
  const processXRef = (id, note) => {
    // Strip column container classes
    const stripColumnClz = (el) => {
      el.classList.remove("page-full", "page-columns");
      if (el.children) {
        for (const child of el.children) {
          stripColumnClz(child);
        }
      }
    }
    stripColumnClz(note)
    if (id === null || id.startsWith('sec-')) {
      // Special case sections, only their first couple elements
      const container = document.createElement("div");
      if (note.children && note.children.length > 2) {
        container.appendChild(note.children[0].cloneNode(true));
        for (let i = 1; i < note.children.length; i++) {
          const child = note.children[i];
          if (child.tagName === "P" && child.innerText === "") {
            continue;
          } else {
            container.appendChild(child.cloneNode(true));
            break;
          }
        }
        if (window.Quarto?.typesetMath) {
          window.Quarto.typesetMath(container);
        }
        return container.innerHTML
      } else {
        if (window.Quarto?.typesetMath) {
          window.Quarto.typesetMath(note);
        }
        return note.innerHTML;
      }
    } else {
      // Remove any anchor links if they are present
      const anchorLink = note.querySelector('a.anchorjs-link');
      if (anchorLink) {
        anchorLink.remove();
      }
      if (window.Quarto?.typesetMath) {
        window.Quarto.typesetMath(note);
      }
      // TODO in 1.5, we should make sure this works without a callout special case
      if (note.classList.contains("callout")) {
        return note.outerHTML;
      } else {
        return note.innerHTML;
      }
    }
  }
  for (var i=0; i<xrefs.length; i++) {
    const xref = xrefs[i];
    tippyHover(xref, undefined, function(instance) {
      instance.disable();
      let url = xref.getAttribute('href');
      let hash = undefined; 
      if (url.startsWith('#')) {
        hash = url;
      } else {
        try { hash = new URL(url).hash; } catch {}
      }
      if (hash) {
        const id = hash.replace(/^#\/?/, "");
        const note = window.document.getElementById(id);
        if (note !== null) {
          try {
            const html = processXRef(id, note.cloneNode(true));
            instance.setContent(html);
          } finally {
            instance.enable();
            instance.show();
          }
        } else {
          // See if we can fetch this
          fetch(url.split('#')[0])
          .then(res => res.text())
          .then(html => {
            const parser = new DOMParser();
            const htmlDoc = parser.parseFromString(html, "text/html");
            const note = htmlDoc.getElementById(id);
            if (note !== null) {
              const html = processXRef(id, note);
              instance.setContent(html);
            } 
          }).finally(() => {
            instance.enable();
            instance.show();
          });
        }
      } else {
        // See if we can fetch a full url (with no hash to target)
        // This is a special case and we should probably do some content thinning / targeting
        fetch(url)
        .then(res => res.text())
        .then(html => {
          const parser = new DOMParser();
          const htmlDoc = parser.parseFromString(html, "text/html");
          const note = htmlDoc.querySelector('main.content');
          if (note !== null) {
            // This should only happen for chapter cross references
            // (since there is no id in the URL)
            // remove the first header
            if (note.children.length > 0 && note.children[0].tagName === "HEADER") {
              note.children[0].remove();
            }
            const html = processXRef(null, note);
            instance.setContent(html);
          } 
        }).finally(() => {
          instance.enable();
          instance.show();
        });
      }
    }, function(instance) {
    });
  }
      let selectedAnnoteEl;
      const selectorForAnnotation = ( cell, annotation) => {
        let cellAttr = 'data-code-cell="' + cell + '"';
        let lineAttr = 'data-code-annotation="' +  annotation + '"';
        const selector = 'span[' + cellAttr + '][' + lineAttr + ']';
        return selector;
      }
      const selectCodeLines = (annoteEl) => {
        const doc = window.document;
        const targetCell = annoteEl.getAttribute("data-target-cell");
        const targetAnnotation = annoteEl.getAttribute("data-target-annotation");
        const annoteSpan = window.document.querySelector(selectorForAnnotation(targetCell, targetAnnotation));
        const lines = annoteSpan.getAttribute("data-code-lines").split(",");
        const lineIds = lines.map((line) => {
          return targetCell + "-" + line;
        })
        let top = null;
        let height = null;
        let parent = null;
        if (lineIds.length > 0) {
            //compute the position of the single el (top and bottom and make a div)
            const el = window.document.getElementById(lineIds[0]);
            top = el.offsetTop;
            height = el.offsetHeight;
            parent = el.parentElement.parentElement;
          if (lineIds.length > 1) {
            const lastEl = window.document.getElementById(lineIds[lineIds.length - 1]);
            const bottom = lastEl.offsetTop + lastEl.offsetHeight;
            height = bottom - top;
          }
          if (top !== null && height !== null && parent !== null) {
            // cook up a div (if necessary) and position it 
            let div = window.document.getElementById("code-annotation-line-highlight");
            if (div === null) {
              div = window.document.createElement("div");
              div.setAttribute("id", "code-annotation-line-highlight");
              div.style.position = 'absolute';
              parent.appendChild(div);
            }
            div.style.top = top - 2 + "px";
            div.style.height = height + 4 + "px";
            div.style.left = 0;
            let gutterDiv = window.document.getElementById("code-annotation-line-highlight-gutter");
            if (gutterDiv === null) {
              gutterDiv = window.document.createElement("div");
              gutterDiv.setAttribute("id", "code-annotation-line-highlight-gutter");
              gutterDiv.style.position = 'absolute';
              const codeCell = window.document.getElementById(targetCell);
              const gutter = codeCell.querySelector('.code-annotation-gutter');
              gutter.appendChild(gutterDiv);
            }
            gutterDiv.style.top = top - 2 + "px";
            gutterDiv.style.height = height + 4 + "px";
          }
          selectedAnnoteEl = annoteEl;
        }
      };
      const unselectCodeLines = () => {
        const elementsIds = ["code-annotation-line-highlight", "code-annotation-line-highlight-gutter"];
        elementsIds.forEach((elId) => {
          const div = window.document.getElementById(elId);
          if (div) {
            div.remove();
          }
        });
        selectedAnnoteEl = undefined;
      };
        // Handle positioning of the toggle
    window.addEventListener(
      "resize",
      throttle(() => {
        elRect = undefined;
        if (selectedAnnoteEl) {
          selectCodeLines(selectedAnnoteEl);
        }
      }, 10)
    );
    function throttle(fn, ms) {
    let throttle = false;
    let timer;
      return (...args) => {
        if(!throttle) { // first call gets through
            fn.apply(this, args);
            throttle = true;
        } else { // all the others get throttled
            if(timer) clearTimeout(timer); // cancel #2
            timer = setTimeout(() => {
              fn.apply(this, args);
              timer = throttle = false;
            }, ms);
        }
      };
    }
      // Attach click handler to the DT
      const annoteDls = window.document.querySelectorAll('dt[data-target-cell]');
      for (const annoteDlNode of annoteDls) {
        annoteDlNode.addEventListener('click', (event) => {
          const clickedEl = event.target;
          if (clickedEl !== selectedAnnoteEl) {
            unselectCodeLines();
            const activeEl = window.document.querySelector('dt[data-target-cell].code-annotation-active');
            if (activeEl) {
              activeEl.classList.remove('code-annotation-active');
            }
            selectCodeLines(clickedEl);
            clickedEl.classList.add('code-annotation-active');
          } else {
            // Unselect the line
            unselectCodeLines();
            clickedEl.classList.remove('code-annotation-active');
          }
        });
      }
  const findCites = (el) => {
    const parentEl = el.parentElement;
    if (parentEl) {
      const cites = parentEl.dataset.cites;
      if (cites) {
        return {
          el,
          cites: cites.split(' ')
        };
      } else {
        return findCites(el.parentElement)
      }
    } else {
      return undefined;
    }
  };
  var bibliorefs = window.document.querySelectorAll('a[role="doc-biblioref"]');
  for (var i=0; i<bibliorefs.length; i++) {
    const ref = bibliorefs[i];
    const citeInfo = findCites(ref);
    if (citeInfo) {
      tippyHover(citeInfo.el, function() {
        var popup = window.document.createElement('div');
        citeInfo.cites.forEach(function(cite) {
          var citeDiv = window.document.createElement('div');
          citeDiv.classList.add('hanging-indent');
          citeDiv.classList.add('csl-entry');
          var biblioDiv = window.document.getElementById('ref-' + cite);
          if (biblioDiv) {
            citeDiv.innerHTML = biblioDiv.innerHTML;
          }
          popup.appendChild(citeDiv);
        });
        return popup.innerHTML;
      });
    }
  }
});
</script>
<script src="https://giscus.app/client.js" data-repo="TINYML-KOR/blog" data-repo-id="R_kgDOLC9iGA" data-category="General" data-category-id="DIC_kwDOLC9iGM4Cc7eP" data-mapping="title" data-reactions-enabled="1" data-emit-metadata="0" data-input-position="top" data-theme="light" data-lang="en" crossorigin="anonymous" async="">
</script>
<input type="hidden" id="giscus-base-theme" value="light">
<input type="hidden" id="giscus-alt-theme" value="dark">
</div> <!-- /content -->




</body></html>