<!DOCTYPE html>
<html xmlns="http://www.w3.org/1999/xhtml" lang="en" xml:lang="en"><head>

<meta charset="utf-8">
<meta name="generator" content="quarto-1.4.549">

<meta name="viewport" content="width=device-width, initial-scale=1.0, user-scalable=yes">

<meta name="author" content="Jung Yeon Lee">
<meta name="dcterms.date" content="2024-03-06">
<meta name="description" content="K-means &amp; Linear Quantization">

<title>TinyML KOR - 👩‍💻 Lab 2</title>
<style>
code{white-space: pre-wrap;}
span.smallcaps{font-variant: small-caps;}
div.columns{display: flex; gap: min(4vw, 1.5em);}
div.column{flex: auto; overflow-x: auto;}
div.hanging-indent{margin-left: 1.5em; text-indent: -1.5em;}
ul.task-list{list-style: none;}
ul.task-list li input[type="checkbox"] {
  width: 0.8em;
  margin: 0 0.8em 0.2em -1em; /* quarto-specific, see https://github.com/quarto-dev/quarto-cli/issues/4556 */ 
  vertical-align: middle;
}
/* CSS for syntax highlighting */
pre > code.sourceCode { white-space: pre; position: relative; }
pre > code.sourceCode > span { line-height: 1.25; }
pre > code.sourceCode > span:empty { height: 1.2em; }
.sourceCode { overflow: visible; }
code.sourceCode > span { color: inherit; text-decoration: inherit; }
div.sourceCode { margin: 1em 0; }
pre.sourceCode { margin: 0; }
@media screen {
div.sourceCode { overflow: auto; }
}
@media print {
pre > code.sourceCode { white-space: pre-wrap; }
pre > code.sourceCode > span { text-indent: -5em; padding-left: 5em; }
}
pre.numberSource code
  { counter-reset: source-line 0; }
pre.numberSource code > span
  { position: relative; left: -4em; counter-increment: source-line; }
pre.numberSource code > span > a:first-child::before
  { content: counter(source-line);
    position: relative; left: -1em; text-align: right; vertical-align: baseline;
    border: none; display: inline-block;
    -webkit-touch-callout: none; -webkit-user-select: none;
    -khtml-user-select: none; -moz-user-select: none;
    -ms-user-select: none; user-select: none;
    padding: 0 4px; width: 4em;
  }
pre.numberSource { margin-left: 3em;  padding-left: 4px; }
div.sourceCode
  {   }
@media screen {
pre > code.sourceCode > span > a:first-child::before { text-decoration: underline; }
}
</style>


<script src="https://cdnjs.cloudflare.com/ajax/libs/jquery/3.5.1/jquery.min.js" integrity="sha512-bLT0Qm9VnAYZDflyKcBaQ2gg0hSYNQrJ8RilYldYQ1FxQYoCLtUjuuRuZo+fjqhx/qtq/1itJ0C2ejDxltZVFg==" crossorigin="anonymous"></script><script src="../../site_libs/quarto-nav/quarto-nav.js"></script>
<script src="../../site_libs/quarto-nav/headroom.min.js"></script>
<script src="../../site_libs/clipboard/clipboard.min.js"></script>
<script src="../../site_libs/quarto-search/autocomplete.umd.js"></script>
<script src="../../site_libs/quarto-search/fuse.min.js"></script>
<script src="../../site_libs/quarto-search/quarto-search.js"></script>
<meta name="quarto:offset" content="../../">
<script src="../../site_libs/quarto-html/quarto.js"></script>
<script src="../../site_libs/quarto-html/popper.min.js"></script>
<script src="../../site_libs/quarto-html/tippy.umd.min.js"></script>
<script src="../../site_libs/quarto-html/anchor.min.js"></script>
<link href="../../site_libs/quarto-html/tippy.css" rel="stylesheet">
<link href="../../site_libs/quarto-html/quarto-syntax-highlighting.css" rel="stylesheet" id="quarto-text-highlighting-styles">
<script src="../../site_libs/bootstrap/bootstrap.min.js"></script>
<link href="../../site_libs/bootstrap/bootstrap-icons.css" rel="stylesheet">
<link href="../../site_libs/bootstrap/bootstrap.min.css" rel="stylesheet" id="quarto-bootstrap" data-mode="light">
<script id="quarto-search-options" type="application/json">{
  "location": "navbar",
  "copy-button": false,
  "collapse-after": 3,
  "panel-placement": "end",
  "type": "overlay",
  "limit": 50,
  "keyboard-shortcut": [
    "f",
    "/",
    "s"
  ],
  "show-item-context": false,
  "language": {
    "search-no-results-text": "No results",
    "search-matching-documents-text": "matching documents",
    "search-copy-link-title": "Copy link to search",
    "search-hide-matches-text": "Hide additional matches",
    "search-more-match-text": "more match in this document",
    "search-more-matches-text": "more matches in this document",
    "search-clear-button-title": "Clear",
    "search-text-placeholder": "",
    "search-detached-cancel-button-title": "Cancel",
    "search-submit-button-title": "Submit",
    "search-label": "Search"
  }
}</script>
<script src="https://cdnjs.cloudflare.com/ajax/libs/require.js/2.3.6/require.min.js" integrity="sha512-c3Nl8+7g4LMSTdrm621y7kf9v3SDPnhxLNhcjFJbKECVnmZHTdo+IRO05sNLTH/D3vA6u1X32ehoLC7WFVdheg==" crossorigin="anonymous"></script>

<script type="application/javascript">define('jquery', [],function() {return window.jQuery;})</script>
<script src="https://unpkg.com/@jupyter-widgets/html-manager@*/dist/embed-amd.js" crossorigin="anonymous"></script>

  <script src="https://polyfill.io/v3/polyfill.min.js?features=es6"></script>
  <script src="https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-chtml-full.js" type="text/javascript"></script>

<script type="text/javascript">
const typesetMath = (el) => {
  if (window.MathJax) {
    // MathJax Typeset
    window.MathJax.typeset([el]);
  } else if (window.katex) {
    // KaTeX Render
    var mathElements = el.getElementsByClassName("math");
    var macros = [];
    for (var i = 0; i < mathElements.length; i++) {
      var texText = mathElements[i].firstChild;
      if (mathElements[i].tagName == "SPAN") {
        window.katex.render(texText.data, mathElements[i], {
          displayMode: mathElements[i].classList.contains('display'),
          throwOnError: false,
          macros: macros,
          fleqn: false
        });
      }
    }
  }
}
window.Quarto = {
  typesetMath
};
</script>

<link rel="stylesheet" href="../../styles.css">
</head>

<body class="nav-fixed">

<div id="quarto-search-results"></div>
  <header id="quarto-header" class="headroom fixed-top">
    <nav class="navbar navbar-expand-lg " data-bs-theme="dark">
      <div class="navbar-container container-fluid">
      <div class="navbar-brand-container mx-auto">
    <a class="navbar-brand" href="../../index.html">
    <span class="navbar-title">TinyML KOR</span>
    </a>
  </div>
            <div id="quarto-search" class="" title="Search"></div>
          <button class="navbar-toggler" type="button" data-bs-toggle="collapse" data-bs-target="#navbarCollapse" aria-controls="navbarCollapse" aria-expanded="false" aria-label="Toggle navigation" onclick="if (window.quartoToggleHeadroom) { window.quartoToggleHeadroom(); }">
  <span class="navbar-toggler-icon"></span>
</button>
          <div class="collapse navbar-collapse" id="navbarCollapse">
            <ul class="navbar-nav navbar-nav-scroll ms-auto">
  <li class="nav-item">
    <a class="nav-link" href="../../about.html"> 
<span class="menu-text">TinyML Study Group</span></a>
  </li>  
  <li class="nav-item compact">
    <a class="nav-link" href="https://github.com/"> <i class="bi bi-github" role="img">
</i> 
<span class="menu-text"></span></a>
  </li>  
</ul>
          </div> <!-- /navcollapse -->
          <div class="quarto-navbar-tools">
</div>
      </div> <!-- /container-fluid -->
    </nav>
</header>
<!-- content -->
<div id="quarto-content" class="quarto-container page-columns page-rows-contents page-layout-article page-navbar">
<!-- sidebar -->
<!-- margin-sidebar -->
    <div id="quarto-margin-sidebar" class="sidebar margin-sidebar">
        <nav id="TOC" role="doc-toc" class="toc-active">
    <h2 id="toc-title">On this page</h2>
   
  <ul>
  <li><a href="#lab-2-quantization" id="toc-lab-2-quantization" class="nav-link active" data-scroll-target="#lab-2-quantization"><strong>Lab 2: Quantization</strong></a>
  <ul class="collapse">
  <li><a href="#goals" id="toc-goals" class="nav-link" data-scroll-target="#goals">Goals</a></li>
  <li><a href="#contents" id="toc-contents" class="nav-link" data-scroll-target="#contents">Contents</a></li>
  </ul></li>
  <li><a href="#먼저-fp32-model의-정확도와-모델-크기를-평가해봅시다" id="toc-먼저-fp32-model의-정확도와-모델-크기를-평가해봅시다" class="nav-link" data-scroll-target="#먼저-fp32-model의-정확도와-모델-크기를-평가해봅시다">먼저 FP32 Model의 정확도와 모델 크기를 평가해봅시다</a></li>
  <li><a href="#k-means-quantization" id="toc-k-means-quantization" class="nav-link" data-scroll-target="#k-means-quantization">K-Means Quantization</a>
  <ul class="collapse">
  <li><a href="#question-1-10-pts" id="toc-question-1-10-pts" class="nav-link" data-scroll-target="#question-1-10-pts">Question 1 (10 pts)</a></li>
  <li><a href="#question-2-10-pts" id="toc-question-2-10-pts" class="nav-link" data-scroll-target="#question-2-10-pts">Question 2 (10 pts)</a>
  <ul class="collapse">
  <li><a href="#question-2.1-5-pts" id="toc-question-2.1-5-pts" class="nav-link" data-scroll-target="#question-2.1-5-pts">Question 2.1 (5 pts)</a></li>
  <li><a href="#question-2.2-5-pts" id="toc-question-2.2-5-pts" class="nav-link" data-scroll-target="#question-2.2-5-pts">Question 2.2 (5 pts)</a></li>
  </ul></li>
  <li><a href="#k-means-quantization-on-whole-model" id="toc-k-means-quantization-on-whole-model" class="nav-link" data-scroll-target="#k-means-quantization-on-whole-model">K-Means Quantization on Whole Model</a></li>
  <li><a href="#trained-k-means-quantization" id="toc-trained-k-means-quantization" class="nav-link" data-scroll-target="#trained-k-means-quantization">Trained K-Means Quantization</a>
  <ul class="collapse">
  <li><a href="#question-3-10-pts" id="toc-question-3-10-pts" class="nav-link" data-scroll-target="#question-3-10-pts">Question 3 (10 pts)</a></li>
  </ul></li>
  </ul></li>
  <li><a href="#linear-quantization" id="toc-linear-quantization" class="nav-link" data-scroll-target="#linear-quantization">Linear Quantization</a>
  <ul class="collapse">
  <li><a href="#n-bit-integer" id="toc-n-bit-integer" class="nav-link" data-scroll-target="#n-bit-integer"><em>n</em>-bit Integer</a></li>
  <li><a href="#question-4-15-pts" id="toc-question-4-15-pts" class="nav-link" data-scroll-target="#question-4-15-pts"><strong>Question 4</strong> (15 pts)</a></li>
  <li><a href="#question-5-10-pts" id="toc-question-5-10-pts" class="nav-link" data-scroll-target="#question-5-10-pts">Question 5 (10 pts)</a>
  <ul class="collapse">
  <li><a href="#scale" id="toc-scale" class="nav-link" data-scroll-target="#scale">Scale</a></li>
  <li><a href="#zero-point" id="toc-zero-point" class="nav-link" data-scroll-target="#zero-point">zero point</a></li>
  <li><a href="#question-5.3-8-pts" id="toc-question-5.3-8-pts" class="nav-link" data-scroll-target="#question-5.3-8-pts">Question 5.3 (8 pts)</a></li>
  </ul></li>
  <li><a href="#special-case-linear-quantization-on-weight-tensor" id="toc-special-case-linear-quantization-on-weight-tensor" class="nav-link" data-scroll-target="#special-case-linear-quantization-on-weight-tensor">Special case: linear quantization on weight tensor</a>
  <ul class="collapse">
  <li><a href="#per-channel-linear-quantization" id="toc-per-channel-linear-quantization" class="nav-link" data-scroll-target="#per-channel-linear-quantization">Per-channel Linear Quantization</a></li>
  <li><a href="#a-quick-peek-at-linear-quantization-on-weights" id="toc-a-quick-peek-at-linear-quantization-on-weights" class="nav-link" data-scroll-target="#a-quick-peek-at-linear-quantization-on-weights">A Quick Peek at Linear Quantization on Weights</a></li>
  </ul></li>
  <li><a href="#quantized-inference" id="toc-quantized-inference" class="nav-link" data-scroll-target="#quantized-inference">Quantized Inference</a>
  <ul class="collapse">
  <li><a href="#question-6-5-pts" id="toc-question-6-5-pts" class="nav-link" data-scroll-target="#question-6-5-pts">Question 6 (5 pts)</a></li>
  <li><a href="#quantized-fully-connected-layer" id="toc-quantized-fully-connected-layer" class="nav-link" data-scroll-target="#quantized-fully-connected-layer">Quantized Fully-Connected Layer</a></li>
  <li><a href="#quantized-convolution" id="toc-quantized-convolution" class="nav-link" data-scroll-target="#quantized-convolution">Quantized Convolution</a></li>
  </ul></li>
  <li><a href="#question-9-10-pts" id="toc-question-9-10-pts" class="nav-link" data-scroll-target="#question-9-10-pts">Question 9 (10 pts)</a>
  <ul class="collapse">
  <li><a href="#question-9.1-5-pts" id="toc-question-9.1-5-pts" class="nav-link" data-scroll-target="#question-9.1-5-pts">Question 9.1 (5 pts)</a></li>
  </ul></li>
  <li><a href="#question-9.2-bonus-question-5-pts" id="toc-question-9.2-bonus-question-5-pts" class="nav-link" data-scroll-target="#question-9.2-bonus-question-5-pts">Question 9.2 (Bonus Question; 5 pts)</a></li>
  </ul></li>
  <li><a href="#question-10-5-pts" id="toc-question-10-5-pts" class="nav-link" data-scroll-target="#question-10-5-pts">Question 10 (5 pts)</a>
  <ul class="collapse">
  <li><a href="#정확도" id="toc-정확도" class="nav-link" data-scroll-target="#정확도">정확도:</a></li>
  <li><a href="#지연-시간" id="toc-지연-시간" class="nav-link" data-scroll-target="#지연-시간">지연 시간:</a></li>
  <li><a href="#하드웨어-지원" id="toc-하드웨어-지원" class="nav-link" data-scroll-target="#하드웨어-지원">하드웨어 지원:</a></li>
  <li><a href="#요약" id="toc-요약" class="nav-link" data-scroll-target="#요약">요약:</a></li>
  </ul></li>
  </ul>
</nav>
    </div>
<!-- main -->
<main class="content" id="quarto-document-content">

<header id="title-block-header" class="quarto-title-block default">
<div class="quarto-title">
<h1 class="title">👩‍💻 Lab 2</h1>
  <div class="quarto-categories">
    <div class="quarto-category">lab</div>
    <div class="quarto-category">quantization</div>
    <div class="quarto-category">linear</div>
    <div class="quarto-category">kmeans</div>
  </div>
  </div>

<div>
  <div class="description">
    K-means &amp; Linear Quantization
  </div>
</div>


<div class="quarto-title-meta">

    <div>
    <div class="quarto-title-meta-heading">Author</div>
    <div class="quarto-title-meta-contents">
             <p>Jung Yeon Lee </p>
          </div>
  </div>
    
    <div>
    <div class="quarto-title-meta-heading">Published</div>
    <div class="quarto-title-meta-contents">
      <p class="date">March 6, 2024</p>
    </div>
  </div>
  
    
  </div>
  


</header>


<p>Lecture 5와 6을 통해 배운 <a href="https://tinyml-kor.github.io/blog/posts/lecs/lec05.html">Quantization 내용</a> 중에 K-means Quantization과 Linear Quantization에 대해 실습하며 배워보는 Lab2에 대한 풀이와 설명에 대한 포스팅이다. 기존의 실습 노트는 <a href="https://hanlab.mit.edu/courses/2023-fall-65940">Original 강의</a>의 <a href="https://drive.google.com/file/d/124toPMHDd3z6LiXOhOgHPy6Wvb0Xzw3E/view?usp=sharing">링크</a>를, 한국어 번역과 Solution은 <a href="https://github.com/TINYML-KOR/assignment/blob/main/Lab2_cureiuxjy.ipynb">이 링크</a>를 참고하면 됩니다. 아래 Colaboratory 버튼을 누르면 실습노트를 바로 실행시키는 Colab Notebook을 실행시킬 수 있습니다.</p>
<p><a href="https://colab.research.google.com/github/TINYML-KOR/assignment/blob/main/Lab2_cureiuxjy.ipynb"><img src="http://img.shields.io/badge/Colaboratory-black?style=for-the-badge&amp;logo=google-colab.png" class="img-fluid"></a></p>
<section id="lab-2-quantization" class="level1">
<h1><strong>Lab 2: Quantization</strong></h1>
<section id="goals" class="level2">
<h2 class="anchored" data-anchor-id="goals">Goals</h2>
<p>이번 실습에서는 모델 크기와 지연 시간을 줄이기 위해 클래식한 <strong>neural network model</strong>을 <strong>quantizing</strong>하는 연습을 할 것입니다. 이 실습의 목표는 다음과 같습니다:</p>
<ul>
<li><strong>Quantization</strong>의 기본 개념을 이해합니다.</li>
<li><strong>k-means quantization</strong>을 구현하고 적용합니다.</li>
<li>k-means <strong>quantization</strong>에 대해 <strong>quantization-aware training</strong>을 구현하고 적용합니다.</li>
<li><strong>linear quantization</strong>을 구현하고 적용합니다.</li>
<li>linear <strong>quantization</strong>에 대해 <strong>integer-only inference</strong>를 구현하고 적용합니다.</li>
<li><strong>Quantization</strong>에서의 성능 개선(예: 속도 향상)에 대한 기본적인 이해를 얻습니다.</li>
<li>이러한 <strong>quantization</strong> 접근 방식 사이의 차이점과 트레이드오프를 이해합니다.</li>
</ul>
</section>
<section id="contents" class="level2">
<h2 class="anchored" data-anchor-id="contents">Contents</h2>
<p>주요 섹션은 <strong><em>K-Means Quantization</em></strong> 과 <strong><em>Linear Quantization</em></strong> 2가지로 구성되어 있습니다.</p>
<p>이번 실습 노트에서 총 <strong><em>10</em></strong>개의 질문을 통해 학습하게 됩니다.:</p>
<ul>
<li><em>K-Means Quantization</em>에 대해서는 <strong><em>3</em></strong>개의 질문이 있습니다 (Question 1-3).</li>
<li><em>Linear Quantization</em>에 대해서는 <strong><em>6</em></strong>개의 질문이 있습니다 (Question 4-9).</li>
<li>Question 10은 k-means quantization과 linear quantization을 비교합니다.</li>
</ul>
<blockquote class="blockquote">
<p>실습노트에 대한 설정 부분(Setup)은 Colaboratory Note를 열면 확인하실 수 있습니다. 포스팅에서는 보다 실습내용에 집중할 수 있도록 생략되어 있습니다.</p>
</blockquote>
</section>
</section>
<section id="먼저-fp32-model의-정확도와-모델-크기를-평가해봅시다" class="level1">
<h1>먼저 FP32 Model의 정확도와 모델 크기를 평가해봅시다</h1>
<div id="cell-8" class="cell" data-outputid="9ad11587-1f2d-4d98-d3cc-dac6eb5240f1" data-execution_count="15">
<div class="sourceCode cell-code" id="cb1"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb1-1"><a href="#cb1-1" aria-hidden="true" tabindex="-1"></a>fp32_model_accuracy <span class="op">=</span> evaluate(model, dataloader[<span class="st">'test'</span>])</span>
<span id="cb1-2"><a href="#cb1-2" aria-hidden="true" tabindex="-1"></a>fp32_model_size <span class="op">=</span> get_model_size(model)</span>
<span id="cb1-3"><a href="#cb1-3" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>(<span class="ss">f"fp32 model has accuracy=</span><span class="sc">{</span>fp32_model_accuracy<span class="sc">:.2f}</span><span class="ss">%"</span>)</span>
<span id="cb1-4"><a href="#cb1-4" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>(<span class="ss">f"fp32 model has size=</span><span class="sc">{</span>fp32_model_size<span class="op">/</span>MiB<span class="sc">:.2f}</span><span class="ss"> MiB"</span>)</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output cell-output-display">
<script type="application/vnd.jupyter.widget-view+json">
{"model_id":"a1dea41e4428454ba0d567a9b1d96dd9","version_major":2,"version_minor":0,"quarto_mimetype":"application/vnd.jupyter.widget-view+json"}
</script>
</div>
<div class="cell-output cell-output-stdout">
<pre><code>fp32 model has accuracy=92.95%
fp32 model has size=35.20 MiB</code></pre>
</div>
</div>
</section>
<section id="k-means-quantization" class="level1">
<h1>K-Means Quantization</h1>
<p><strong>Network quantization</strong>은 deep network를 표현하는 데 필요한 가중치 당 비트(bits per weight) 수를 줄여 네트워크를 압축합니다. <strong>quantized network</strong>는 하드웨어 지원이 있을 경우 더 빠른 추론 속도를 가질 수 있습니다.</p>
<p>이 섹션에서는 <a href="https://arxiv.org/pdf/1510.00149.pdf">Deep Compression: Compressing Deep Neural Networks With Pruning, Trained Quantization And Huffman Coding</a>에서처럼 신경망에 대한 <strong>K-means quantization</strong>을 탐구할 것입니다.</p>
<div class="quarto-figure quarto-figure-center">
<figure class="figure">
<p><img src="../../images/lab02/kmeans.png" class="img-fluid figure-img"></p>
<figcaption>kmeans.png</figcaption>
</figure>
</div>
<blockquote class="blockquote">
<p><code>quantized_weight = codebook.centroids[codebook.labels].view_as(weight)</code></p>
</blockquote>
<p><span class="math inline">\(n\)</span>-bit k-means <strong>quantization</strong>은 시냅스를 <span class="math inline">\(2^n\)</span> 개의 클러스터로 나누고, 동일한 클러스터 내의 시냅스는 동일한 가중치 값을 공유하게 됩니다.</p>
<p>따라서, k-means <strong>quantization</strong>은 다음과 같은 codebook을 생성합니다: * <code>centroids</code>: <span class="math inline">\(2^n\)</span> fp32 클러스터 중심. * <code>labels</code>: 원래 fp32 가중치 텐서와 동일한 #elements를 가진 <span class="math inline">\(n\)</span>-bit 정수 텐서. 각 정수는 해당 클러스터가 어디에 속하는지를 나타냅니다.</p>
<p>추론하는 동안, codebook을 기반으로 한 fp32 텐서가 추론을 위해 생성됩니다:</p>
<blockquote class="blockquote">
<p><code>quantized_weight = codebook.centroids[codebook.labels].view_as(weight)</code></p>
</blockquote>
<div id="cell-14" class="cell" data-execution_count="16">
<div class="sourceCode cell-code" id="cb3"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb3-1"><a href="#cb3-1" aria-hidden="true" tabindex="-1"></a><span class="im">from</span> collections <span class="im">import</span> namedtuple</span>
<span id="cb3-2"><a href="#cb3-2" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb3-3"><a href="#cb3-3" aria-hidden="true" tabindex="-1"></a>Codebook <span class="op">=</span> namedtuple(<span class="st">'Codebook'</span>, [<span class="st">'centroids'</span>, <span class="st">'labels'</span>])</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</div>
<section id="question-1-10-pts" class="level2">
<h2 class="anchored" data-anchor-id="question-1-10-pts">Question 1 (10 pts)</h2>
<p>아래의 K-Means quantization function을 완성하세요.</p>
<div id="cell-16" class="cell" data-execution_count="17">
<div class="sourceCode cell-code" id="cb4"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb4-1"><a href="#cb4-1" aria-hidden="true" tabindex="-1"></a><span class="im">from</span> fast_pytorch_kmeans <span class="im">import</span> KMeans</span>
<span id="cb4-2"><a href="#cb4-2" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb4-3"><a href="#cb4-3" aria-hidden="true" tabindex="-1"></a><span class="kw">def</span> k_means_quantize(fp32_tensor: torch.Tensor, bitwidth<span class="op">=</span><span class="dv">4</span>, codebook<span class="op">=</span><span class="va">None</span>):</span>
<span id="cb4-4"><a href="#cb4-4" aria-hidden="true" tabindex="-1"></a>    <span class="co">"""</span></span>
<span id="cb4-5"><a href="#cb4-5" aria-hidden="true" tabindex="-1"></a><span class="co">    quantize tensor using k-means clustering</span></span>
<span id="cb4-6"><a href="#cb4-6" aria-hidden="true" tabindex="-1"></a><span class="co">    :param fp32_tensor:</span></span>
<span id="cb4-7"><a href="#cb4-7" aria-hidden="true" tabindex="-1"></a><span class="co">    :param bitwidth: [int] quantization bit width, default=4</span></span>
<span id="cb4-8"><a href="#cb4-8" aria-hidden="true" tabindex="-1"></a><span class="co">    :param codebook: [Codebook] (the cluster centroids, the cluster label tensor)</span></span>
<span id="cb4-9"><a href="#cb4-9" aria-hidden="true" tabindex="-1"></a><span class="co">    :return:</span></span>
<span id="cb4-10"><a href="#cb4-10" aria-hidden="true" tabindex="-1"></a><span class="co">        [Codebook = (centroids, labels)]</span></span>
<span id="cb4-11"><a href="#cb4-11" aria-hidden="true" tabindex="-1"></a><span class="co">            centroids: [torch.(cuda.)FloatTensor] the cluster centroids</span></span>
<span id="cb4-12"><a href="#cb4-12" aria-hidden="true" tabindex="-1"></a><span class="co">            labels: [torch.(cuda.)LongTensor] cluster label tensor</span></span>
<span id="cb4-13"><a href="#cb4-13" aria-hidden="true" tabindex="-1"></a><span class="co">    """</span></span>
<span id="cb4-14"><a href="#cb4-14" aria-hidden="true" tabindex="-1"></a>    <span class="cf">if</span> codebook <span class="kw">is</span> <span class="va">None</span>:</span>
<span id="cb4-15"><a href="#cb4-15" aria-hidden="true" tabindex="-1"></a>        <span class="co">############### YOUR CODE STARTS HERE ###############</span></span>
<span id="cb4-16"><a href="#cb4-16" aria-hidden="true" tabindex="-1"></a>        <span class="co"># get number of clusters based on the quantization precision</span></span>
<span id="cb4-17"><a href="#cb4-17" aria-hidden="true" tabindex="-1"></a>        n_clusters <span class="op">=</span> <span class="dv">2</span> <span class="op">**</span> bitwidth  <span class="co"># Calculate number of clusters as 2^bitwidth</span></span>
<span id="cb4-18"><a href="#cb4-18" aria-hidden="true" tabindex="-1"></a>        <span class="co">############### YOUR CODE ENDS HERE #################</span></span>
<span id="cb4-19"><a href="#cb4-19" aria-hidden="true" tabindex="-1"></a>        <span class="co"># use k-means to get the quantization centroids</span></span>
<span id="cb4-20"><a href="#cb4-20" aria-hidden="true" tabindex="-1"></a>        kmeans <span class="op">=</span> KMeans(n_clusters<span class="op">=</span>n_clusters, mode<span class="op">=</span><span class="st">'euclidean'</span>, verbose<span class="op">=</span><span class="dv">0</span>)</span>
<span id="cb4-21"><a href="#cb4-21" aria-hidden="true" tabindex="-1"></a>        labels <span class="op">=</span> kmeans.fit_predict(fp32_tensor.view(<span class="op">-</span><span class="dv">1</span>, <span class="dv">1</span>)).to(torch.<span class="bu">long</span>)</span>
<span id="cb4-22"><a href="#cb4-22" aria-hidden="true" tabindex="-1"></a>        centroids <span class="op">=</span> kmeans.centroids.to(torch.<span class="bu">float</span>).view(<span class="op">-</span><span class="dv">1</span>)</span>
<span id="cb4-23"><a href="#cb4-23" aria-hidden="true" tabindex="-1"></a>        codebook <span class="op">=</span> Codebook(centroids, labels)</span>
<span id="cb4-24"><a href="#cb4-24" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb4-25"><a href="#cb4-25" aria-hidden="true" tabindex="-1"></a>    <span class="co">############### YOUR CODE STARTS HERE ###############</span></span>
<span id="cb4-26"><a href="#cb4-26" aria-hidden="true" tabindex="-1"></a>    <span class="co"># decode the codebook into k-means quantized tensor for inference</span></span>
<span id="cb4-27"><a href="#cb4-27" aria-hidden="true" tabindex="-1"></a>    <span class="co"># hint: one line of code</span></span>
<span id="cb4-28"><a href="#cb4-28" aria-hidden="true" tabindex="-1"></a>    quantized_tensor <span class="op">=</span> codebook.centroids[codebook.labels].view_as(fp32_tensor)</span>
<span id="cb4-29"><a href="#cb4-29" aria-hidden="true" tabindex="-1"></a>    <span class="co">############### YOUR CODE ENDS HERE #################</span></span>
<span id="cb4-30"><a href="#cb4-30" aria-hidden="true" tabindex="-1"></a>    fp32_tensor.set_(quantized_tensor.view_as(fp32_tensor))</span>
<span id="cb4-31"><a href="#cb4-31" aria-hidden="true" tabindex="-1"></a>    <span class="cf">return</span> codebook</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</div>
<p>위에서 작성한 k-means quantization function을 더미 텐서에 적용하여 확인해봅시다.</p>
<div id="cell-18" class="cell" data-outputid="3e3df421-5ba8-4599-8099-a3feb6c84930" data-execution_count="18">
<div class="sourceCode cell-code" id="cb5"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb5-1"><a href="#cb5-1" aria-hidden="true" tabindex="-1"></a>test_k_means_quantize()</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output cell-output-stdout">
<pre><code>tensor([[-0.3747,  0.0874,  0.3200, -0.4868,  0.4404],
        [-0.0402,  0.2322, -0.2024, -0.4986,  0.1814],
        [ 0.3102, -0.3942, -0.2030,  0.0883, -0.4741],
        [-0.1592, -0.0777, -0.3946, -0.2128,  0.2675],
        [ 0.0611, -0.1933, -0.4350,  0.2928, -0.1087]])
* Test k_means_quantize()
    target bitwidth: 2 bits
        num unique values before k-means quantization: 25
        num unique values after  k-means quantization: 4
* Test passed.</code></pre>
</div>
<div class="cell-output cell-output-display">
<div>
<figure class="figure">
<p><img src="lab02_files/figure-html/cell-5-output-2.png" class="img-fluid figure-img"></p>
</figure>
</div>
</div>
</div>
</section>
<section id="question-2-10-pts" class="level2">
<h2 class="anchored" data-anchor-id="question-2-10-pts">Question 2 (10 pts)</h2>
<p>마지막 코드 셀은 2비트 k-means quantization을 수행하고 quantization 전후의 텐서를 플롯합니다. 각 클러스터는 고유한 색상으로 렌더링되며, quantized 텐서들이 4(<span class="math inline">\(2^2\)</span>)가지 고유한 색상으로 표시됩니다.</p>
<p>이러한 현상을 관찰한 것을 바탕으로 질문들에 답하세요.</p>
<section id="question-2.1-5-pts" class="level3">
<h3 class="anchored" data-anchor-id="question-2.1-5-pts">Question 2.1 (5 pts)</h3>
<p>4비트로 k-means quantization이 수행되면, quantized 텐서에는 몇 개의 고유한 색상이 렌더링될까요?</p>
<p><strong>Your Answer:</strong></p>
<p>4비트 k-means quantization이 수행되면, quantized 텐서에 <span class="math inline">\((2^4 = 16)\)</span>개의 고유한 색상이 렌더링됩니다. 이는 4비트로 0000부터 1111까지의 16가지 다른 상태나 조합을 나타낼 수 있으며, 이는 텐서 값이 그룹화될 수 있는 16개의 고유한 클러스터에 해당합니다.</p>
</section>
<section id="question-2.2-5-pts" class="level3">
<h3 class="anchored" data-anchor-id="question-2.2-5-pts">Question 2.2 (5 pts)</h3>
<p><em>n</em>-비트 k-means quantization이 수행되면, quantized 텐서에 몇 개의 고유한 색상이 렌더링될까요?</p>
<p><strong>Your Answer:</strong></p>
<p><em>n</em>-비트 k-means quantization이 수행되면, quantized 텐서에는 <span class="math inline">\((2^n)\)</span>개의 고유한 색상이 렌더링 됩니다. 이는 <em>n</em>비트를 사용하여 <span class="math inline">\((2^n)\)</span>개의 다른 상태나 조합을 나타낼 수 있으며, 이는 텐서 값이 그룹화될 수 있는 <span class="math inline">\((2^n)\)</span>개의 고유한 클러스터에 해당합니다.</p>
</section>
</section>
<section id="k-means-quantization-on-whole-model" class="level2">
<h2 class="anchored" data-anchor-id="k-means-quantization-on-whole-model">K-Means Quantization on Whole Model</h2>
<p>lab 1에서 했던 것과 유사하게, 이제 전체 모델을 quantizing하기 위해 k-means quantization 함수를 클래스로 래핑합니다. <code>KMeansQuantizer</code> 클래스에서는 모델 가중치가 변경될 때마다 codebooks(i.e., <code>centroids</code>와 <code>labels</code>)을 적용하거나 업데이트할 수 있도록 codebooks의 변화를 기록해야 합니다.</p>
<div id="cell-26" class="cell" data-execution_count="19">
<div class="sourceCode cell-code" id="cb7"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb7-1"><a href="#cb7-1" aria-hidden="true" tabindex="-1"></a><span class="im">from</span> torch.nn <span class="im">import</span> parameter</span>
<span id="cb7-2"><a href="#cb7-2" aria-hidden="true" tabindex="-1"></a><span class="kw">class</span> KMeansQuantizer:</span>
<span id="cb7-3"><a href="#cb7-3" aria-hidden="true" tabindex="-1"></a>    <span class="kw">def</span> <span class="fu">__init__</span>(<span class="va">self</span>, model : nn.Module, bitwidth<span class="op">=</span><span class="dv">4</span>):</span>
<span id="cb7-4"><a href="#cb7-4" aria-hidden="true" tabindex="-1"></a>        <span class="va">self</span>.codebook <span class="op">=</span> KMeansQuantizer.quantize(model, bitwidth)</span>
<span id="cb7-5"><a href="#cb7-5" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb7-6"><a href="#cb7-6" aria-hidden="true" tabindex="-1"></a>    <span class="at">@torch.no_grad</span>()</span>
<span id="cb7-7"><a href="#cb7-7" aria-hidden="true" tabindex="-1"></a>    <span class="kw">def</span> <span class="bu">apply</span>(<span class="va">self</span>, model, update_centroids):</span>
<span id="cb7-8"><a href="#cb7-8" aria-hidden="true" tabindex="-1"></a>        <span class="cf">for</span> name, param <span class="kw">in</span> model.named_parameters():</span>
<span id="cb7-9"><a href="#cb7-9" aria-hidden="true" tabindex="-1"></a>            <span class="cf">if</span> name <span class="kw">in</span> <span class="va">self</span>.codebook:</span>
<span id="cb7-10"><a href="#cb7-10" aria-hidden="true" tabindex="-1"></a>                <span class="cf">if</span> update_centroids:</span>
<span id="cb7-11"><a href="#cb7-11" aria-hidden="true" tabindex="-1"></a>                    update_codebook(param, codebook<span class="op">=</span><span class="va">self</span>.codebook[name])</span>
<span id="cb7-12"><a href="#cb7-12" aria-hidden="true" tabindex="-1"></a>                <span class="va">self</span>.codebook[name] <span class="op">=</span> k_means_quantize(</span>
<span id="cb7-13"><a href="#cb7-13" aria-hidden="true" tabindex="-1"></a>                    param, codebook<span class="op">=</span><span class="va">self</span>.codebook[name])</span>
<span id="cb7-14"><a href="#cb7-14" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb7-15"><a href="#cb7-15" aria-hidden="true" tabindex="-1"></a>    <span class="at">@staticmethod</span></span>
<span id="cb7-16"><a href="#cb7-16" aria-hidden="true" tabindex="-1"></a>    <span class="at">@torch.no_grad</span>()</span>
<span id="cb7-17"><a href="#cb7-17" aria-hidden="true" tabindex="-1"></a>    <span class="kw">def</span> quantize(model: nn.Module, bitwidth<span class="op">=</span><span class="dv">4</span>):</span>
<span id="cb7-18"><a href="#cb7-18" aria-hidden="true" tabindex="-1"></a>        codebook <span class="op">=</span> <span class="bu">dict</span>()</span>
<span id="cb7-19"><a href="#cb7-19" aria-hidden="true" tabindex="-1"></a>        <span class="cf">if</span> <span class="bu">isinstance</span>(bitwidth, <span class="bu">dict</span>):</span>
<span id="cb7-20"><a href="#cb7-20" aria-hidden="true" tabindex="-1"></a>            <span class="cf">for</span> name, param <span class="kw">in</span> model.named_parameters():</span>
<span id="cb7-21"><a href="#cb7-21" aria-hidden="true" tabindex="-1"></a>                <span class="cf">if</span> name <span class="kw">in</span> bitwidth:</span>
<span id="cb7-22"><a href="#cb7-22" aria-hidden="true" tabindex="-1"></a>                    codebook[name] <span class="op">=</span> k_means_quantize(param, bitwidth<span class="op">=</span>bitwidth[name])</span>
<span id="cb7-23"><a href="#cb7-23" aria-hidden="true" tabindex="-1"></a>        <span class="cf">else</span>:</span>
<span id="cb7-24"><a href="#cb7-24" aria-hidden="true" tabindex="-1"></a>            <span class="cf">for</span> name, param <span class="kw">in</span> model.named_parameters():</span>
<span id="cb7-25"><a href="#cb7-25" aria-hidden="true" tabindex="-1"></a>                <span class="cf">if</span> param.dim() <span class="op">&gt;</span> <span class="dv">1</span>:</span>
<span id="cb7-26"><a href="#cb7-26" aria-hidden="true" tabindex="-1"></a>                    codebook[name] <span class="op">=</span> k_means_quantize(param, bitwidth<span class="op">=</span>bitwidth)</span>
<span id="cb7-27"><a href="#cb7-27" aria-hidden="true" tabindex="-1"></a>        <span class="cf">return</span> codebook</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</div>
<p>이제 K-Means Quantization을 사용하여 모델을 8비트, 4비트, 2비트로 quantize해봅시다. <em>모델 크기를 계산할 때 codebooks의 저장 공간은 무시한다는 점을 유의하세요.</em></p>
<div id="cell-28" class="cell" data-outputid="af180b7a-6aef-44e2-f657-53b9c6d6c384" data-execution_count="20">
<div class="sourceCode cell-code" id="cb8"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb8-1"><a href="#cb8-1" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>(<span class="st">'Note that the storage for codebooks is ignored when calculating the model size.'</span>)</span>
<span id="cb8-2"><a href="#cb8-2" aria-hidden="true" tabindex="-1"></a>quantizers <span class="op">=</span> <span class="bu">dict</span>()</span>
<span id="cb8-3"><a href="#cb8-3" aria-hidden="true" tabindex="-1"></a><span class="cf">for</span> bitwidth <span class="kw">in</span> [<span class="dv">8</span>, <span class="dv">4</span>, <span class="dv">2</span>]:</span>
<span id="cb8-4"><a href="#cb8-4" aria-hidden="true" tabindex="-1"></a>    recover_model()</span>
<span id="cb8-5"><a href="#cb8-5" aria-hidden="true" tabindex="-1"></a>    <span class="bu">print</span>(<span class="ss">f'k-means quantizing model into </span><span class="sc">{</span>bitwidth<span class="sc">}</span><span class="ss"> bits'</span>)</span>
<span id="cb8-6"><a href="#cb8-6" aria-hidden="true" tabindex="-1"></a>    quantizer <span class="op">=</span> KMeansQuantizer(model, bitwidth)</span>
<span id="cb8-7"><a href="#cb8-7" aria-hidden="true" tabindex="-1"></a>    quantized_model_size <span class="op">=</span> get_model_size(model, bitwidth)</span>
<span id="cb8-8"><a href="#cb8-8" aria-hidden="true" tabindex="-1"></a>    <span class="bu">print</span>(<span class="ss">f"    </span><span class="sc">{</span>bitwidth<span class="sc">}</span><span class="ss">-bit k-means quantized model has size=</span><span class="sc">{</span>quantized_model_size<span class="op">/</span>MiB<span class="sc">:.2f}</span><span class="ss"> MiB"</span>)</span>
<span id="cb8-9"><a href="#cb8-9" aria-hidden="true" tabindex="-1"></a>    quantized_model_accuracy <span class="op">=</span> evaluate(model, dataloader[<span class="st">'test'</span>])</span>
<span id="cb8-10"><a href="#cb8-10" aria-hidden="true" tabindex="-1"></a>    <span class="bu">print</span>(<span class="ss">f"    </span><span class="sc">{</span>bitwidth<span class="sc">}</span><span class="ss">-bit k-means quantized model has accuracy=</span><span class="sc">{</span>quantized_model_accuracy<span class="sc">:.2f}</span><span class="ss">%"</span>)</span>
<span id="cb8-11"><a href="#cb8-11" aria-hidden="true" tabindex="-1"></a>    quantizers[bitwidth] <span class="op">=</span> quantizer</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output cell-output-stdout">
<pre><code>Note that the storage for codebooks is ignored when calculating the model size.
k-means quantizing model into 8 bits
    8-bit k-means quantized model has size=8.80 MiB
    8-bit k-means quantized model has accuracy=92.76%
k-means quantizing model into 4 bits
    4-bit k-means quantized model has size=4.40 MiB
    4-bit k-means quantized model has accuracy=79.07%
k-means quantizing model into 2 bits
    2-bit k-means quantized model has size=2.20 MiB
    2-bit k-means quantized model has accuracy=10.00%</code></pre>
</div>
<div class="cell-output cell-output-display">
<script type="application/vnd.jupyter.widget-view+json">
{"model_id":"892325c089bc45b2b51574a424d5038a","version_major":2,"version_minor":0,"quarto_mimetype":"application/vnd.jupyter.widget-view+json"}
</script>
</div>
<div class="cell-output cell-output-display">
<script type="application/vnd.jupyter.widget-view+json">
{"model_id":"62c8dc04cb33411284907e34d1993bf0","version_major":2,"version_minor":0,"quarto_mimetype":"application/vnd.jupyter.widget-view+json"}
</script>
</div>
<div class="cell-output cell-output-display">
<script type="application/vnd.jupyter.widget-view+json">
{"model_id":"71d18c640eb54aad81c6b1babb91410a","version_major":2,"version_minor":0,"quarto_mimetype":"application/vnd.jupyter.widget-view+json"}
</script>
</div>
</div>
</section>
<section id="trained-k-means-quantization" class="level2">
<h2 class="anchored" data-anchor-id="trained-k-means-quantization">Trained K-Means Quantization</h2>
<p>마지막 셀의 결과에서 볼 수 있듯이, 모델을 적은 비트로 quantize할 때 정확도가 크게 떨어집니다. 따라서, 정확도를 회복하기 위해 <strong>quantization-aware training</strong>을 해야 합니다.</p>
<p>k-means quantization-aware 훈련 동안, centroids도 업데이트됩니다. 이는 <a href="https://arxiv.org/pdf/1510.00149.pdf">Deep Compression: Compressing Deep Neural Networks With Pruning, Trained Quantization And Huffman Coding</a>에서 제안되었습니다.</p>
<p>centroids에 대한 그래디언트는 다음과 같이 계산됩니다,</p>
<blockquote class="blockquote">
<p><span class="math inline">\(\frac{\partial \mathcal{L} }{\partial C_k} = \sum_{j} \frac{\partial \mathcal{L} }{\partial W_{j}} \frac{\partial W_{j} }{\partial C_k} = \sum_{j} \frac{\partial \mathcal{L} }{\partial W_{j}} \mathbf{1}(I_{j}=k)\)</span></p>
</blockquote>
<p>여기서 <span class="math inline">\(\mathcal{L}\)</span>은 손실, <span class="math inline">\(C_k\)</span>는 <em>k</em>-번째 centroid, <span class="math inline">\(I_{j}\)</span>는 가중치 <span class="math inline">\(W_{j}\)</span>의 라벨입니다.</p>
<p><span class="math inline">\(\mathbf{1}()\)</span>은 지시 함수이며, <span class="math inline">\(\mathbf{1}(I_{j}=k)\)</span>는 <span class="math inline">\(1\;\mathrm{if}\;I_{j}=k\;\mathrm{else}\;0\)</span>, <em>즉</em>, <span class="math inline">\(I_{j}==k\)</span>를 의미합니다.</p>
<p>lab에서는 <strong>간단히</strong> 최신 가중치에 따라 centroids를 직접 업데이트합니다:</p>
<blockquote class="blockquote">
<p><span class="math inline">\(C_k = \frac{\sum_{j}W_{j}\mathbf{1}(I_{j}=k)}{\sum_{j}\mathbf{1}(I_{j}=k)}\)</span></p>
</blockquote>
<section id="question-3-10-pts" class="level3">
<h3 class="anchored" data-anchor-id="question-3-10-pts">Question 3 (10 pts)</h3>
<p>아래의 codebook update function을 완성하세요.</p>
<p><strong>Hint</strong>:</p>
<p>위의 centroids를 업데이트하는 방정식은 실제로 동일한 클러스터에 있는 가중치의 <code>평균(mean)</code>을 업데이트된 centroid 값으로 사용하고 있습니다.</p>
<div id="cell-31" class="cell" data-execution_count="21">
<div class="sourceCode cell-code" id="cb10"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb10-1"><a href="#cb10-1" aria-hidden="true" tabindex="-1"></a><span class="kw">def</span> update_codebook(fp32_tensor: torch.Tensor, codebook: Codebook):</span>
<span id="cb10-2"><a href="#cb10-2" aria-hidden="true" tabindex="-1"></a>    <span class="co">"""</span></span>
<span id="cb10-3"><a href="#cb10-3" aria-hidden="true" tabindex="-1"></a><span class="co">    update the centroids in the codebook using updated fp32_tensor</span></span>
<span id="cb10-4"><a href="#cb10-4" aria-hidden="true" tabindex="-1"></a><span class="co">    :param fp32_tensor: [torch.(cuda.)Tensor]</span></span>
<span id="cb10-5"><a href="#cb10-5" aria-hidden="true" tabindex="-1"></a><span class="co">    :param codebook: [Codebook] (the cluster centroids, the cluster label tensor)</span></span>
<span id="cb10-6"><a href="#cb10-6" aria-hidden="true" tabindex="-1"></a><span class="co">    """</span></span>
<span id="cb10-7"><a href="#cb10-7" aria-hidden="true" tabindex="-1"></a>    n_clusters <span class="op">=</span> codebook.centroids.numel()</span>
<span id="cb10-8"><a href="#cb10-8" aria-hidden="true" tabindex="-1"></a>    fp32_tensor <span class="op">=</span> fp32_tensor.view(<span class="op">-</span><span class="dv">1</span>)</span>
<span id="cb10-9"><a href="#cb10-9" aria-hidden="true" tabindex="-1"></a>    <span class="cf">for</span> k <span class="kw">in</span> <span class="bu">range</span>(n_clusters):</span>
<span id="cb10-10"><a href="#cb10-10" aria-hidden="true" tabindex="-1"></a>    <span class="co">############### YOUR CODE STARTS HERE ###############</span></span>
<span id="cb10-11"><a href="#cb10-11" aria-hidden="true" tabindex="-1"></a>        codebook.centroids[k] <span class="op">=</span> fp32_tensor[codebook.labels <span class="op">==</span> k].mean()</span>
<span id="cb10-12"><a href="#cb10-12" aria-hidden="true" tabindex="-1"></a>    <span class="co">############### YOUR CODE ENDS HERE #################</span></span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</div>
<p>이제 다음 코드 셀을 실행하여 k-means quantized 모델을 finetuning하여 정확도를 회복해봅시다. 정확도 하락이 <em>0.5보다 작으면</em> finetuning을 중단합니다.</p>
<div id="cell-33" class="cell" data-outputid="fb05c2e6-8e2b-4593-c04d-cc0bca6cd9b5" data-execution_count="22">
<div class="sourceCode cell-code" id="cb11"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb11-1"><a href="#cb11-1" aria-hidden="true" tabindex="-1"></a>accuracy_drop_threshold <span class="op">=</span> <span class="fl">0.5</span></span>
<span id="cb11-2"><a href="#cb11-2" aria-hidden="true" tabindex="-1"></a>quantizers_before_finetune <span class="op">=</span> copy.deepcopy(quantizers)</span>
<span id="cb11-3"><a href="#cb11-3" aria-hidden="true" tabindex="-1"></a>quantizers_after_finetune <span class="op">=</span> quantizers</span>
<span id="cb11-4"><a href="#cb11-4" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb11-5"><a href="#cb11-5" aria-hidden="true" tabindex="-1"></a><span class="cf">for</span> bitwidth <span class="kw">in</span> [<span class="dv">8</span>, <span class="dv">4</span>, <span class="dv">2</span>]:</span>
<span id="cb11-6"><a href="#cb11-6" aria-hidden="true" tabindex="-1"></a>    recover_model()</span>
<span id="cb11-7"><a href="#cb11-7" aria-hidden="true" tabindex="-1"></a>    quantizer <span class="op">=</span> quantizers[bitwidth]</span>
<span id="cb11-8"><a href="#cb11-8" aria-hidden="true" tabindex="-1"></a>    <span class="bu">print</span>(<span class="ss">f'k-means quantizing model into </span><span class="sc">{</span>bitwidth<span class="sc">}</span><span class="ss"> bits'</span>)</span>
<span id="cb11-9"><a href="#cb11-9" aria-hidden="true" tabindex="-1"></a>    quantizer.<span class="bu">apply</span>(model, update_centroids<span class="op">=</span><span class="va">False</span>)</span>
<span id="cb11-10"><a href="#cb11-10" aria-hidden="true" tabindex="-1"></a>    quantized_model_size <span class="op">=</span> get_model_size(model, bitwidth)</span>
<span id="cb11-11"><a href="#cb11-11" aria-hidden="true" tabindex="-1"></a>    <span class="bu">print</span>(<span class="ss">f"    </span><span class="sc">{</span>bitwidth<span class="sc">}</span><span class="ss">-bit k-means quantized model has size=</span><span class="sc">{</span>quantized_model_size<span class="op">/</span>MiB<span class="sc">:.2f}</span><span class="ss"> MiB"</span>)</span>
<span id="cb11-12"><a href="#cb11-12" aria-hidden="true" tabindex="-1"></a>    quantized_model_accuracy <span class="op">=</span> evaluate(model, dataloader[<span class="st">'test'</span>])</span>
<span id="cb11-13"><a href="#cb11-13" aria-hidden="true" tabindex="-1"></a>    <span class="bu">print</span>(<span class="ss">f"    </span><span class="sc">{</span>bitwidth<span class="sc">}</span><span class="ss">-bit k-means quantized model has accuracy=</span><span class="sc">{</span>quantized_model_accuracy<span class="sc">:.2f}</span><span class="ss">% before quantization-aware training "</span>)</span>
<span id="cb11-14"><a href="#cb11-14" aria-hidden="true" tabindex="-1"></a>    accuracy_drop <span class="op">=</span> fp32_model_accuracy <span class="op">-</span> quantized_model_accuracy</span>
<span id="cb11-15"><a href="#cb11-15" aria-hidden="true" tabindex="-1"></a>    <span class="cf">if</span> accuracy_drop <span class="op">&gt;</span> accuracy_drop_threshold:</span>
<span id="cb11-16"><a href="#cb11-16" aria-hidden="true" tabindex="-1"></a>        <span class="bu">print</span>(<span class="ss">f"        Quantization-aware training due to accuracy drop=</span><span class="sc">{</span>accuracy_drop<span class="sc">:.2f}</span><span class="ss">% is larger than threshold=</span><span class="sc">{</span>accuracy_drop_threshold<span class="sc">:.2f}</span><span class="ss">%"</span>)</span>
<span id="cb11-17"><a href="#cb11-17" aria-hidden="true" tabindex="-1"></a>        num_finetune_epochs <span class="op">=</span> <span class="dv">5</span></span>
<span id="cb11-18"><a href="#cb11-18" aria-hidden="true" tabindex="-1"></a>        optimizer <span class="op">=</span> torch.optim.SGD(model.parameters(), lr<span class="op">=</span><span class="fl">0.01</span>, momentum<span class="op">=</span><span class="fl">0.9</span>)</span>
<span id="cb11-19"><a href="#cb11-19" aria-hidden="true" tabindex="-1"></a>        scheduler <span class="op">=</span> torch.optim.lr_scheduler.CosineAnnealingLR(optimizer, num_finetune_epochs)</span>
<span id="cb11-20"><a href="#cb11-20" aria-hidden="true" tabindex="-1"></a>        criterion <span class="op">=</span> nn.CrossEntropyLoss()</span>
<span id="cb11-21"><a href="#cb11-21" aria-hidden="true" tabindex="-1"></a>        best_accuracy <span class="op">=</span> <span class="dv">0</span></span>
<span id="cb11-22"><a href="#cb11-22" aria-hidden="true" tabindex="-1"></a>        epoch <span class="op">=</span> num_finetune_epochs</span>
<span id="cb11-23"><a href="#cb11-23" aria-hidden="true" tabindex="-1"></a>        <span class="cf">while</span> accuracy_drop <span class="op">&gt;</span> accuracy_drop_threshold <span class="kw">and</span> epoch <span class="op">&gt;</span> <span class="dv">0</span>:</span>
<span id="cb11-24"><a href="#cb11-24" aria-hidden="true" tabindex="-1"></a>            train(model, dataloader[<span class="st">'train'</span>], criterion, optimizer, scheduler,</span>
<span id="cb11-25"><a href="#cb11-25" aria-hidden="true" tabindex="-1"></a>                  callbacks<span class="op">=</span>[<span class="kw">lambda</span>: quantizer.<span class="bu">apply</span>(model, update_centroids<span class="op">=</span><span class="va">True</span>)])</span>
<span id="cb11-26"><a href="#cb11-26" aria-hidden="true" tabindex="-1"></a>            model_accuracy <span class="op">=</span> evaluate(model, dataloader[<span class="st">'test'</span>])</span>
<span id="cb11-27"><a href="#cb11-27" aria-hidden="true" tabindex="-1"></a>            is_best <span class="op">=</span> model_accuracy <span class="op">&gt;</span> best_accuracy</span>
<span id="cb11-28"><a href="#cb11-28" aria-hidden="true" tabindex="-1"></a>            best_accuracy <span class="op">=</span> <span class="bu">max</span>(model_accuracy, best_accuracy)</span>
<span id="cb11-29"><a href="#cb11-29" aria-hidden="true" tabindex="-1"></a>            <span class="bu">print</span>(<span class="ss">f'        Epoch </span><span class="sc">{</span>num_finetune_epochs<span class="op">-</span>epoch<span class="sc">}</span><span class="ss"> Accuracy </span><span class="sc">{</span>model_accuracy<span class="sc">:.2f}</span><span class="ss">% / Best Accuracy: </span><span class="sc">{</span>best_accuracy<span class="sc">:.2f}</span><span class="ss">%'</span>)</span>
<span id="cb11-30"><a href="#cb11-30" aria-hidden="true" tabindex="-1"></a>            accuracy_drop <span class="op">=</span> fp32_model_accuracy <span class="op">-</span> best_accuracy</span>
<span id="cb11-31"><a href="#cb11-31" aria-hidden="true" tabindex="-1"></a>            epoch <span class="op">-=</span> <span class="dv">1</span></span>
<span id="cb11-32"><a href="#cb11-32" aria-hidden="true" tabindex="-1"></a>    <span class="cf">else</span>:</span>
<span id="cb11-33"><a href="#cb11-33" aria-hidden="true" tabindex="-1"></a>        <span class="bu">print</span>(<span class="ss">f"        No need for quantization-aware training since accuracy drop=</span><span class="sc">{</span>accuracy_drop<span class="sc">:.2f}</span><span class="ss">% is smaller than threshold=</span><span class="sc">{</span>accuracy_drop_threshold<span class="sc">:.2f}</span><span class="ss">%"</span>)</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output cell-output-stdout">
<pre><code>k-means quantizing model into 8 bits
    8-bit k-means quantized model has size=8.80 MiB
    8-bit k-means quantized model has accuracy=92.76% before quantization-aware training 
        No need for quantization-aware training since accuracy drop=0.19% is smaller than threshold=0.50%
k-means quantizing model into 4 bits
    4-bit k-means quantized model has size=4.40 MiB
    4-bit k-means quantized model has accuracy=79.07% before quantization-aware training 
        Quantization-aware training due to accuracy drop=13.88% is larger than threshold=0.50%
        Epoch 0 Accuracy 92.47% / Best Accuracy: 92.47%
k-means quantizing model into 2 bits
    2-bit k-means quantized model has size=2.20 MiB
    2-bit k-means quantized model has accuracy=10.00% before quantization-aware training 
        Quantization-aware training due to accuracy drop=82.95% is larger than threshold=0.50%
        Epoch 0 Accuracy 90.21% / Best Accuracy: 90.21%
        Epoch 1 Accuracy 90.82% / Best Accuracy: 90.82%
        Epoch 2 Accuracy 91.00% / Best Accuracy: 91.00%
        Epoch 3 Accuracy 91.12% / Best Accuracy: 91.12%
        Epoch 4 Accuracy 91.17% / Best Accuracy: 91.17%</code></pre>
</div>
<div class="cell-output cell-output-display">
<script type="application/vnd.jupyter.widget-view+json">
{"model_id":"79186f919eb949668ed56c09c7559445","version_major":2,"version_minor":0,"quarto_mimetype":"application/vnd.jupyter.widget-view+json"}
</script>
</div>
<div class="cell-output cell-output-display">
<script type="application/vnd.jupyter.widget-view+json">
{"model_id":"456e13dfded7478cb6be376dc6ad3eb5","version_major":2,"version_minor":0,"quarto_mimetype":"application/vnd.jupyter.widget-view+json"}
</script>
</div>
<div class="cell-output cell-output-display">
<script type="application/vnd.jupyter.widget-view+json">
{"model_id":"878640f5a034497a8e3ed3450e2473da","version_major":2,"version_minor":0,"quarto_mimetype":"application/vnd.jupyter.widget-view+json"}
</script>
</div>
<div class="cell-output cell-output-display">
<script type="application/vnd.jupyter.widget-view+json">
{"model_id":"be10dc25ad21438184702da9527af6ee","version_major":2,"version_minor":0,"quarto_mimetype":"application/vnd.jupyter.widget-view+json"}
</script>
</div>
<div class="cell-output cell-output-display">
<script type="application/vnd.jupyter.widget-view+json">
{"model_id":"de4ff9cd02f2496a9f4fca3f69193f1b","version_major":2,"version_minor":0,"quarto_mimetype":"application/vnd.jupyter.widget-view+json"}
</script>
</div>
<div class="cell-output cell-output-display">
<script type="application/vnd.jupyter.widget-view+json">
{"model_id":"98efcfa3799e4cd78cc70516b6800796","version_major":2,"version_minor":0,"quarto_mimetype":"application/vnd.jupyter.widget-view+json"}
</script>
</div>
<div class="cell-output cell-output-display">
<script type="application/vnd.jupyter.widget-view+json">
{"model_id":"3c9a4bc7acb44af9b19970b33a8e4070","version_major":2,"version_minor":0,"quarto_mimetype":"application/vnd.jupyter.widget-view+json"}
</script>
</div>
<div class="cell-output cell-output-display">
<script type="application/vnd.jupyter.widget-view+json">
{"model_id":"5df64008cf134b2885bb161cbb1fb272","version_major":2,"version_minor":0,"quarto_mimetype":"application/vnd.jupyter.widget-view+json"}
</script>
</div>
<div class="cell-output cell-output-display">
<script type="application/vnd.jupyter.widget-view+json">
{"model_id":"f3d43ebe333144baabf03ee1c16fec99","version_major":2,"version_minor":0,"quarto_mimetype":"application/vnd.jupyter.widget-view+json"}
</script>
</div>
<div class="cell-output cell-output-display">
<script type="application/vnd.jupyter.widget-view+json">
{"model_id":"7728126bc72749c980b553c490ffa10c","version_major":2,"version_minor":0,"quarto_mimetype":"application/vnd.jupyter.widget-view+json"}
</script>
</div>
<div class="cell-output cell-output-display">
<script type="application/vnd.jupyter.widget-view+json">
{"model_id":"28f6a4eca3e0463d96fb6bb990b1a89a","version_major":2,"version_minor":0,"quarto_mimetype":"application/vnd.jupyter.widget-view+json"}
</script>
</div>
<div class="cell-output cell-output-display">
<script type="application/vnd.jupyter.widget-view+json">
{"model_id":"d7180d028e2f4b8d8f26451260631aa5","version_major":2,"version_minor":0,"quarto_mimetype":"application/vnd.jupyter.widget-view+json"}
</script>
</div>
<div class="cell-output cell-output-display">
<script type="application/vnd.jupyter.widget-view+json">
{"model_id":"58e7ef6e1c294277904156e2ba15bf7c","version_major":2,"version_minor":0,"quarto_mimetype":"application/vnd.jupyter.widget-view+json"}
</script>
</div>
<div class="cell-output cell-output-display">
<script type="application/vnd.jupyter.widget-view+json">
{"model_id":"d9776aabb37e4e19afe17d4a0fc40906","version_major":2,"version_minor":0,"quarto_mimetype":"application/vnd.jupyter.widget-view+json"}
</script>
</div>
<div class="cell-output cell-output-display">
<script type="application/vnd.jupyter.widget-view+json">
{"model_id":"f53af3dc696841dca7ef29cf7d786c8f","version_major":2,"version_minor":0,"quarto_mimetype":"application/vnd.jupyter.widget-view+json"}
</script>
</div>
</div>
</section>
</section>
</section>
<section id="linear-quantization" class="level1">
<h1>Linear Quantization</h1>
<p>이 섹션에서는 linear quantization을 구현하고 수행합니다.</p>
<p>Linear quantization은 range truncation 과 scaling 과정을 거친 후 부동 소수점 값을 가장 가까운 양자화된 정수로 직접 반올림합니다.</p>
<p><a href="https://arxiv.org/pdf/1712.05877.pdf">Linear quantization</a>은 다음과 같이 표현될 수 있습니다.</p>
<p><span class="math inline">\(r = S(q-Z)\)</span></p>
<p>여기서 <span class="math inline">\(r\)</span>은 부동 소수점 실수, <span class="math inline">\(q\)</span>는 <em>n</em>-비트 정수, <span class="math inline">\(Z\)</span>는 <em>n</em>-비트 정수이며, <span class="math inline">\(S\)</span>는 부동 소수점 실수입니다.</p>
<p><span class="math inline">\(Z\)</span>는 quantization zero point이고 <span class="math inline">\(S\)</span>는 quantization scaling factor입니다. 상수 <span class="math inline">\(Z\)</span>와 <span class="math inline">\(S\)</span> 모두 양자화 매개변수(parameter)입니다.</p>
<section id="n-bit-integer" class="level2">
<h2 class="anchored" data-anchor-id="n-bit-integer"><em>n</em>-bit Integer</h2>
<p><em>n</em>-비트 signed integer는 보통 <a href="https://en.wikipedia.org/wiki/Two%27s_complement">two’s complement</a> 표기법으로 표현됩니다.</p>
<p><em>n</em>-비트 signed integer는 범위 <span class="math inline">\([-2^{n-1}, 2^{n-1}-1]\)</span> 내의 정수를 인코딩할 수 있습니다. 예를 들어, 8비트 정수는 [-128, 127] 범위에 속합니다.</p>
<div id="cell-37" class="cell" data-execution_count="23">
<div class="sourceCode cell-code" id="cb13"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb13-1"><a href="#cb13-1" aria-hidden="true" tabindex="-1"></a><span class="kw">def</span> get_quantized_range(bitwidth):</span>
<span id="cb13-2"><a href="#cb13-2" aria-hidden="true" tabindex="-1"></a>    quantized_max <span class="op">=</span> (<span class="dv">1</span> <span class="op">&lt;&lt;</span> (bitwidth <span class="op">-</span> <span class="dv">1</span>)) <span class="op">-</span> <span class="dv">1</span></span>
<span id="cb13-3"><a href="#cb13-3" aria-hidden="true" tabindex="-1"></a>    quantized_min <span class="op">=</span> <span class="op">-</span>(<span class="dv">1</span> <span class="op">&lt;&lt;</span> (bitwidth <span class="op">-</span> <span class="dv">1</span>))</span>
<span id="cb13-4"><a href="#cb13-4" aria-hidden="true" tabindex="-1"></a>    <span class="cf">return</span> quantized_min, quantized_max</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</div>
</section>
<section id="question-4-15-pts" class="level2">
<h2 class="anchored" data-anchor-id="question-4-15-pts"><strong>Question 4</strong> (15 pts)</h2>
<p>아래의 linear quantization function을 완성하세요.</p>
<p><strong>Hint</strong>:</p>
<ul>
<li><span class="math inline">\(r=S(q-Z)\)</span>에서, <span class="math inline">\(q = r/S + Z\)</span>으로 바꿔서 볼 수 있습니다.</li>
<li><span class="math inline">\(r\)</span>과 <span class="math inline">\(S\)</span> 모두 부동 소수점 숫자(floating number)이므로, 정수 <span class="math inline">\(Z\)</span>를 직접 <span class="math inline">\(r/S\)</span>에 더할 수 없습니다. 따라서 <span class="math inline">\(q = \mathrm{int}(\mathrm{round}(r/S)) + Z\)</span>입니다.</li>
<li><a href="https://pytorch.org/docs/stable/tensors.html"><code>torch.FloatTensor</code></a>를 <a href="https://pytorch.org/docs/stable/tensors.html"><code>torch.IntTensor</code></a>로 변환하기 위해서, <a href="https://pytorch.org/docs/stable/generated/torch.round.html#torch.round"><code>torch.round()</code></a>, <a href="https://pytorch.org/docs/stable/generated/torch.Tensor.round.html#torch.Tensor.round"><code>torch.Tensor.round()</code></a>, <a href="https://pytorch.org/docs/stable/generated/torch.Tensor.round_"><code>torch.Tensor.round_()</code></a>을 사용하여 모든 값을 부동 소수점 정수로 먼저 변환합니다.</li>
<li>그 다음 <a href="https://pytorch.org/docs/stable/generated/torch.Tensor.to.html#torch.Tensor.to"><code>torch.Tensor.to(torch.int8)</code></a>를 사용하여 데이터 타입을 <a href="https://pytorch.org/docs/stable/tensors.html"><code>torch.float</code></a>에서 <a href="https://pytorch.org/docs/stable/tensors.html"><code>torch.int8</code></a>로 변환할 수 있습니다.</li>
</ul>
<div id="cell-39" class="cell" data-execution_count="24">
<div class="sourceCode cell-code" id="cb14"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb14-1"><a href="#cb14-1" aria-hidden="true" tabindex="-1"></a><span class="kw">def</span> linear_quantize(fp_tensor, bitwidth, scale, zero_point, dtype<span class="op">=</span>torch.int8) <span class="op">-&gt;</span> torch.Tensor:</span>
<span id="cb14-2"><a href="#cb14-2" aria-hidden="true" tabindex="-1"></a>    <span class="co">"""</span></span>
<span id="cb14-3"><a href="#cb14-3" aria-hidden="true" tabindex="-1"></a><span class="co">    linear quantization for single fp_tensor</span></span>
<span id="cb14-4"><a href="#cb14-4" aria-hidden="true" tabindex="-1"></a><span class="co">      from</span></span>
<span id="cb14-5"><a href="#cb14-5" aria-hidden="true" tabindex="-1"></a><span class="co">        fp_tensor = (quantized_tensor - zero_point) * scale</span></span>
<span id="cb14-6"><a href="#cb14-6" aria-hidden="true" tabindex="-1"></a><span class="co">      we have,</span></span>
<span id="cb14-7"><a href="#cb14-7" aria-hidden="true" tabindex="-1"></a><span class="co">        quantized_tensor = int(round(fp_tensor / scale)) + zero_point</span></span>
<span id="cb14-8"><a href="#cb14-8" aria-hidden="true" tabindex="-1"></a><span class="co">    :param tensor: [torch.(cuda.)FloatTensor] floating tensor to be quantized</span></span>
<span id="cb14-9"><a href="#cb14-9" aria-hidden="true" tabindex="-1"></a><span class="co">    :param bitwidth: [int] quantization bit width</span></span>
<span id="cb14-10"><a href="#cb14-10" aria-hidden="true" tabindex="-1"></a><span class="co">    :param scale: [torch.(cuda.)FloatTensor] scaling factor</span></span>
<span id="cb14-11"><a href="#cb14-11" aria-hidden="true" tabindex="-1"></a><span class="co">    :param zero_point: [torch.(cuda.)IntTensor] the desired centroid of tensor values</span></span>
<span id="cb14-12"><a href="#cb14-12" aria-hidden="true" tabindex="-1"></a><span class="co">    :return:</span></span>
<span id="cb14-13"><a href="#cb14-13" aria-hidden="true" tabindex="-1"></a><span class="co">        [torch.(cuda.)FloatTensor] quantized tensor whose values are integers</span></span>
<span id="cb14-14"><a href="#cb14-14" aria-hidden="true" tabindex="-1"></a><span class="co">    """</span></span>
<span id="cb14-15"><a href="#cb14-15" aria-hidden="true" tabindex="-1"></a>    <span class="cf">assert</span>(fp_tensor.dtype <span class="op">==</span> torch.<span class="bu">float</span>)</span>
<span id="cb14-16"><a href="#cb14-16" aria-hidden="true" tabindex="-1"></a>    <span class="cf">assert</span>(<span class="bu">isinstance</span>(scale, <span class="bu">float</span>) <span class="kw">or</span></span>
<span id="cb14-17"><a href="#cb14-17" aria-hidden="true" tabindex="-1"></a>           (scale.dtype <span class="op">==</span> torch.<span class="bu">float</span> <span class="kw">and</span> scale.dim() <span class="op">==</span> fp_tensor.dim()))</span>
<span id="cb14-18"><a href="#cb14-18" aria-hidden="true" tabindex="-1"></a>    <span class="cf">assert</span>(<span class="bu">isinstance</span>(zero_point, <span class="bu">int</span>) <span class="kw">or</span></span>
<span id="cb14-19"><a href="#cb14-19" aria-hidden="true" tabindex="-1"></a>           (zero_point.dtype <span class="op">==</span> dtype <span class="kw">and</span> zero_point.dim() <span class="op">==</span> fp_tensor.dim()))</span>
<span id="cb14-20"><a href="#cb14-20" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb14-21"><a href="#cb14-21" aria-hidden="true" tabindex="-1"></a>    <span class="co">############### YOUR CODE STARTS HERE ###############</span></span>
<span id="cb14-22"><a href="#cb14-22" aria-hidden="true" tabindex="-1"></a>    <span class="co"># Step 1: scale the fp_tensor</span></span>
<span id="cb14-23"><a href="#cb14-23" aria-hidden="true" tabindex="-1"></a>    scaled_tensor <span class="op">=</span> fp_tensor <span class="op">/</span> scale</span>
<span id="cb14-24"><a href="#cb14-24" aria-hidden="true" tabindex="-1"></a>    <span class="co"># Step 2: round the floating value to integer value</span></span>
<span id="cb14-25"><a href="#cb14-25" aria-hidden="true" tabindex="-1"></a>    rounded_tensor <span class="op">=</span> torch.<span class="bu">round</span>(scaled_tensor)</span>
<span id="cb14-26"><a href="#cb14-26" aria-hidden="true" tabindex="-1"></a>    <span class="co">############### YOUR CODE ENDS HERE #################</span></span>
<span id="cb14-27"><a href="#cb14-27" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb14-28"><a href="#cb14-28" aria-hidden="true" tabindex="-1"></a>    rounded_tensor <span class="op">=</span> rounded_tensor.to(dtype)</span>
<span id="cb14-29"><a href="#cb14-29" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb14-30"><a href="#cb14-30" aria-hidden="true" tabindex="-1"></a>    <span class="co">############### YOUR CODE STARTS HERE ###############</span></span>
<span id="cb14-31"><a href="#cb14-31" aria-hidden="true" tabindex="-1"></a>    <span class="co"># Step 3: shift the rounded_tensor to make zero_point 0</span></span>
<span id="cb14-32"><a href="#cb14-32" aria-hidden="true" tabindex="-1"></a>    shifted_tensor <span class="op">=</span> rounded_tensor <span class="op">+</span> zero_point</span>
<span id="cb14-33"><a href="#cb14-33" aria-hidden="true" tabindex="-1"></a>    <span class="co">############### YOUR CODE ENDS HERE #################</span></span>
<span id="cb14-34"><a href="#cb14-34" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb14-35"><a href="#cb14-35" aria-hidden="true" tabindex="-1"></a>    <span class="co"># Step 4: clamp the shifted_tensor to lie in bitwidth-bit range</span></span>
<span id="cb14-36"><a href="#cb14-36" aria-hidden="true" tabindex="-1"></a>    quantized_min, quantized_max <span class="op">=</span> get_quantized_range(bitwidth)</span>
<span id="cb14-37"><a href="#cb14-37" aria-hidden="true" tabindex="-1"></a>    quantized_tensor <span class="op">=</span> shifted_tensor.clamp_(quantized_min, quantized_max)</span>
<span id="cb14-38"><a href="#cb14-38" aria-hidden="true" tabindex="-1"></a>    <span class="cf">return</span> quantized_tensor</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</div>
<p>위에서 작성한 linear quantization 기능을 더미 텐서에 적용하여 기능을 검증해봅시다.</p>
<div id="cell-41" class="cell" data-outputid="2fce8b6a-44f8-4ea0-847c-5dc58b41d5b8" data-execution_count="25">
<div class="sourceCode cell-code" id="cb15"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb15-1"><a href="#cb15-1" aria-hidden="true" tabindex="-1"></a>test_linear_quantize()</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output cell-output-stdout">
<pre><code>* Test linear_quantize()
    target bitwidth: 2 bits
        scale: 0.3333333333333333
        zero point: -1
* Test passed.</code></pre>
</div>
<div class="cell-output cell-output-display">
<div>
<figure class="figure">
<p><img src="lab02_files/figure-html/cell-12-output-2.png" class="img-fluid figure-img"></p>
</figure>
</div>
</div>
</div>
</section>
<section id="question-5-10-pts" class="level2">
<h2 class="anchored" data-anchor-id="question-5-10-pts">Question 5 (10 pts)</h2>
<p>이제 linear quantization을 위한 스케일링 인자 <span class="math inline">\(S\)</span>와 제로 포인트 <span class="math inline">\(Z\)</span>를 결정해야 합니다.</p>
<p><a href="https://arxiv.org/pdf/1712.05877.pdf">linear quantization</a>은 <span class="math inline">\(r = S(q-Z)\)</span> 로 표현될 수 있다는 것을 기억하세요.</p>
<section id="scale" class="level3">
<h3 class="anchored" data-anchor-id="scale">Scale</h3>
<p>Linear quantization은 부동 소수점 범위 [<em>fp_min</em>, <em>fp_max</em>]를 양자화된 범위 [<em>quantized_min</em>, <em>quantized_max</em>]로 투영(projection)합니다. 즉,</p>
<blockquote class="blockquote">
<p><span class="math inline">\(r_{\mathrm{max}} = S(q_{\mathrm{max}}-Z)\)</span></p>
<p><span class="math inline">\(r_{\mathrm{min}} = S(q_{\mathrm{min}}-Z)\)</span></p>
</blockquote>
<p>이 두 방정식을 빼면, 우리는 다음을 얻습니다,</p>
<section id="question-5.1-1-pts" class="level4">
<h4 class="anchored" data-anchor-id="question-5.1-1-pts">Question 5.1 (1 pts)</h4>
<p>다음 텍스트 셀에서 올바른 답을 선택하고 잘못된 답을 삭제해주세요.</p>
<blockquote class="blockquote">
<p><span class="math inline">\(S=r_{\mathrm{max}} / q_{\mathrm{max}}\)</span></p>
</blockquote>
<blockquote class="blockquote">
<p><span class="math inline">\(S=(r_{\mathrm{max}} + r_{\mathrm{min}}) / (q_{\mathrm{max}} + q_{\mathrm{min}})\)</span></p>
</blockquote>
<blockquote class="blockquote">
<p>✅<strong><span class="math inline">\(S=(r_{\mathrm{max}} - r_{\mathrm{min}}) / (q_{\mathrm{max}} - q_{\mathrm{min}})\)</span></strong></p>
</blockquote>
<blockquote class="blockquote">
<p><span class="math inline">\(S=r_{\mathrm{max}} / q_{\mathrm{max}} - r_{\mathrm{min}} / q_{\mathrm{min}}\)</span></p>
</blockquote>
<p><code>fp_tensor</code>의 <span class="math inline">\(r_{\mathrm{min}}\)</span>과 <span class="math inline">\(r_{\mathrm{max}}\)</span>를 결정하는 다양한 방법이 있습니다.</p>
<ul>
<li>가장 흔한 방법은 <code>fp_tensor</code>의 최소값과 최대값을 사용하는 것입니다.</li>
<li>또 다른 널리 사용되는 방법은 Kullback-Leibler-J 발산을 최소화하여 <em>fp_max</em>를 결정하는 것입니다.</li>
</ul>
</section>
</section>
<section id="zero-point" class="level3">
<h3 class="anchored" data-anchor-id="zero-point">zero point</h3>
<p>스케일링 인자 <span class="math inline">\(S\)</span>를 결정하면, <span class="math inline">\(r_{\mathrm{min}}\)</span>과 <span class="math inline">\(q_{\mathrm{min}}\)</span> 사이의 관계를 사용하여 제로 포인트 <span class="math inline">\(Z\)</span>를 계산할 수 있습니다.</p>
<section id="question-5.2-1-pts" class="level4">
<h4 class="anchored" data-anchor-id="question-5.2-1-pts">Question 5.2 (1 pts)</h4>
<p>다음 텍스트 셀에서 올바른 답을 선택하고 잘못된 답을 삭제해주세요.</p>
<blockquote class="blockquote">
<p><span class="math inline">\(Z = \mathrm{int}(\mathrm{round}(r_{\mathrm{min}} / S - q_{\mathrm{min}})\)</span></p>
</blockquote>
<blockquote class="blockquote">
<p><span class="math inline">\(Z = \mathrm{int}(\mathrm{round}(q_{\mathrm{min}} - r_{\mathrm{min}} / S))\)</span></p>
</blockquote>
<blockquote class="blockquote">
<p>✅<strong><span class="math inline">\(Z = q_{\mathrm{min}} - r_{\mathrm{min}} / S\)</span></strong></p>
</blockquote>
<blockquote class="blockquote">
<p><span class="math inline">\(Z = r_{\mathrm{min}} / S - q_{\mathrm{min}}\)</span></p>
</blockquote>
</section>
</section>
<section id="question-5.3-8-pts" class="level3">
<h3 class="anchored" data-anchor-id="question-5.3-8-pts">Question 5.3 (8 pts)</h3>
<p>floating point tensor <span class="math inline">\(r\)</span>로부터 scale <span class="math inline">\(S\)</span>와 zero point <span class="math inline">\(Z\)</span>를 계산하는 아래의 함수를 완성하세요.</p>
<div id="cell-51" class="cell" data-execution_count="26">
<div class="sourceCode cell-code" id="cb17"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb17-1"><a href="#cb17-1" aria-hidden="true" tabindex="-1"></a><span class="kw">def</span> get_quantization_scale_and_zero_point(fp_tensor, bitwidth):</span>
<span id="cb17-2"><a href="#cb17-2" aria-hidden="true" tabindex="-1"></a>    <span class="co">"""</span></span>
<span id="cb17-3"><a href="#cb17-3" aria-hidden="true" tabindex="-1"></a><span class="co">    get quantization scale for single tensor</span></span>
<span id="cb17-4"><a href="#cb17-4" aria-hidden="true" tabindex="-1"></a><span class="co">    :param fp_tensor: [torch.(cuda.)Tensor] floating tensor to be quantized</span></span>
<span id="cb17-5"><a href="#cb17-5" aria-hidden="true" tabindex="-1"></a><span class="co">    :param bitwidth: [int] quantization bit width</span></span>
<span id="cb17-6"><a href="#cb17-6" aria-hidden="true" tabindex="-1"></a><span class="co">    :return:</span></span>
<span id="cb17-7"><a href="#cb17-7" aria-hidden="true" tabindex="-1"></a><span class="co">        [float] scale</span></span>
<span id="cb17-8"><a href="#cb17-8" aria-hidden="true" tabindex="-1"></a><span class="co">        [int] zero_point</span></span>
<span id="cb17-9"><a href="#cb17-9" aria-hidden="true" tabindex="-1"></a><span class="co">    """</span></span>
<span id="cb17-10"><a href="#cb17-10" aria-hidden="true" tabindex="-1"></a>    quantized_min, quantized_max <span class="op">=</span> get_quantized_range(bitwidth)</span>
<span id="cb17-11"><a href="#cb17-11" aria-hidden="true" tabindex="-1"></a>    fp_max <span class="op">=</span> fp_tensor.<span class="bu">max</span>().item()</span>
<span id="cb17-12"><a href="#cb17-12" aria-hidden="true" tabindex="-1"></a>    fp_min <span class="op">=</span> fp_tensor.<span class="bu">min</span>().item()</span>
<span id="cb17-13"><a href="#cb17-13" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb17-14"><a href="#cb17-14" aria-hidden="true" tabindex="-1"></a>    <span class="co">############### YOUR CODE STARTS HERE ###############</span></span>
<span id="cb17-15"><a href="#cb17-15" aria-hidden="true" tabindex="-1"></a>    <span class="co"># Calculate scale</span></span>
<span id="cb17-16"><a href="#cb17-16" aria-hidden="true" tabindex="-1"></a>    scale <span class="op">=</span> (fp_max <span class="op">-</span> fp_min) <span class="op">/</span> (quantized_max <span class="op">-</span> quantized_min)</span>
<span id="cb17-17"><a href="#cb17-17" aria-hidden="true" tabindex="-1"></a>    <span class="co"># Calculate zero_point</span></span>
<span id="cb17-18"><a href="#cb17-18" aria-hidden="true" tabindex="-1"></a>    zero_point <span class="op">=</span> quantized_min <span class="op">-</span> <span class="bu">round</span>(fp_min <span class="op">/</span> scale)</span>
<span id="cb17-19"><a href="#cb17-19" aria-hidden="true" tabindex="-1"></a>    <span class="co">############### YOUR CODE ENDS HERE #################</span></span>
<span id="cb17-20"><a href="#cb17-20" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb17-21"><a href="#cb17-21" aria-hidden="true" tabindex="-1"></a>    <span class="co"># clip the zero_point to fall in [quantized_min, quantized_max]</span></span>
<span id="cb17-22"><a href="#cb17-22" aria-hidden="true" tabindex="-1"></a>    <span class="cf">if</span> zero_point <span class="op">&lt;</span> quantized_min:</span>
<span id="cb17-23"><a href="#cb17-23" aria-hidden="true" tabindex="-1"></a>        zero_point <span class="op">=</span> quantized_min</span>
<span id="cb17-24"><a href="#cb17-24" aria-hidden="true" tabindex="-1"></a>    <span class="cf">elif</span> zero_point <span class="op">&gt;</span> quantized_max:</span>
<span id="cb17-25"><a href="#cb17-25" aria-hidden="true" tabindex="-1"></a>        zero_point <span class="op">=</span> quantized_max</span>
<span id="cb17-26"><a href="#cb17-26" aria-hidden="true" tabindex="-1"></a>    <span class="cf">else</span>: <span class="co"># convert from float to int using round()</span></span>
<span id="cb17-27"><a href="#cb17-27" aria-hidden="true" tabindex="-1"></a>        zero_point <span class="op">=</span> <span class="bu">round</span>(zero_point)</span>
<span id="cb17-28"><a href="#cb17-28" aria-hidden="true" tabindex="-1"></a>    <span class="cf">return</span> scale, <span class="bu">int</span>(zero_point)</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</div>
<p>이제 Question 4의 <code>linear_quantize()</code>와 Question 5의 <code>get_quantization_scale_and_zero_point()</code>을 하나의 함수로 래핑합니다.</p>
<div id="cell-53" class="cell" data-execution_count="27">
<div class="sourceCode cell-code" id="cb18"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb18-1"><a href="#cb18-1" aria-hidden="true" tabindex="-1"></a><span class="kw">def</span> linear_quantize_feature(fp_tensor, bitwidth):</span>
<span id="cb18-2"><a href="#cb18-2" aria-hidden="true" tabindex="-1"></a>    <span class="co">"""</span></span>
<span id="cb18-3"><a href="#cb18-3" aria-hidden="true" tabindex="-1"></a><span class="co">    linear quantization for feature tensor</span></span>
<span id="cb18-4"><a href="#cb18-4" aria-hidden="true" tabindex="-1"></a><span class="co">    :param fp_tensor: [torch.(cuda.)Tensor] floating feature to be quantized</span></span>
<span id="cb18-5"><a href="#cb18-5" aria-hidden="true" tabindex="-1"></a><span class="co">    :param bitwidth: [int] quantization bit width</span></span>
<span id="cb18-6"><a href="#cb18-6" aria-hidden="true" tabindex="-1"></a><span class="co">    :return:</span></span>
<span id="cb18-7"><a href="#cb18-7" aria-hidden="true" tabindex="-1"></a><span class="co">        [torch.(cuda.)Tensor] quantized tensor</span></span>
<span id="cb18-8"><a href="#cb18-8" aria-hidden="true" tabindex="-1"></a><span class="co">        [float] scale tensor</span></span>
<span id="cb18-9"><a href="#cb18-9" aria-hidden="true" tabindex="-1"></a><span class="co">        [int] zero point</span></span>
<span id="cb18-10"><a href="#cb18-10" aria-hidden="true" tabindex="-1"></a><span class="co">    """</span></span>
<span id="cb18-11"><a href="#cb18-11" aria-hidden="true" tabindex="-1"></a>    scale, zero_point <span class="op">=</span> get_quantization_scale_and_zero_point(fp_tensor, bitwidth)</span>
<span id="cb18-12"><a href="#cb18-12" aria-hidden="true" tabindex="-1"></a>    quantized_tensor <span class="op">=</span> linear_quantize(fp_tensor, bitwidth, scale, zero_point)</span>
<span id="cb18-13"><a href="#cb18-13" aria-hidden="true" tabindex="-1"></a>    <span class="cf">return</span> quantized_tensor, scale, zero_point</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</div>
</section>
</section>
<section id="special-case-linear-quantization-on-weight-tensor" class="level2">
<h2 class="anchored" data-anchor-id="special-case-linear-quantization-on-weight-tensor">Special case: linear quantization on weight tensor</h2>
<p>먼저 가중치 값의 분포를 살펴봅시다.</p>
<div id="cell-55" class="cell" data-outputid="d7e7990e-9633-41c3-b0b3-905cc776e078" data-execution_count="28">
<div class="sourceCode cell-code" id="cb19"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb19-1"><a href="#cb19-1" aria-hidden="true" tabindex="-1"></a><span class="kw">def</span> plot_weight_distribution(model, bitwidth<span class="op">=</span><span class="dv">32</span>):</span>
<span id="cb19-2"><a href="#cb19-2" aria-hidden="true" tabindex="-1"></a>    <span class="co"># bins = (1 &lt;&lt; bitwidth) if bitwidth &lt;= 8 else 256</span></span>
<span id="cb19-3"><a href="#cb19-3" aria-hidden="true" tabindex="-1"></a>    <span class="cf">if</span> bitwidth <span class="op">&lt;=</span> <span class="dv">8</span>:</span>
<span id="cb19-4"><a href="#cb19-4" aria-hidden="true" tabindex="-1"></a>        qmin, qmax <span class="op">=</span> get_quantized_range(bitwidth)</span>
<span id="cb19-5"><a href="#cb19-5" aria-hidden="true" tabindex="-1"></a>        bins <span class="op">=</span> np.arange(qmin, qmax <span class="op">+</span> <span class="dv">2</span>)</span>
<span id="cb19-6"><a href="#cb19-6" aria-hidden="true" tabindex="-1"></a>        align <span class="op">=</span> <span class="st">'left'</span></span>
<span id="cb19-7"><a href="#cb19-7" aria-hidden="true" tabindex="-1"></a>    <span class="cf">else</span>:</span>
<span id="cb19-8"><a href="#cb19-8" aria-hidden="true" tabindex="-1"></a>        bins <span class="op">=</span> <span class="dv">256</span></span>
<span id="cb19-9"><a href="#cb19-9" aria-hidden="true" tabindex="-1"></a>        align <span class="op">=</span> <span class="st">'mid'</span></span>
<span id="cb19-10"><a href="#cb19-10" aria-hidden="true" tabindex="-1"></a>    fig, axes <span class="op">=</span> plt.subplots(<span class="dv">3</span>,<span class="dv">3</span>, figsize<span class="op">=</span>(<span class="dv">10</span>, <span class="dv">6</span>))</span>
<span id="cb19-11"><a href="#cb19-11" aria-hidden="true" tabindex="-1"></a>    axes <span class="op">=</span> axes.ravel()</span>
<span id="cb19-12"><a href="#cb19-12" aria-hidden="true" tabindex="-1"></a>    plot_index <span class="op">=</span> <span class="dv">0</span></span>
<span id="cb19-13"><a href="#cb19-13" aria-hidden="true" tabindex="-1"></a>    <span class="cf">for</span> name, param <span class="kw">in</span> model.named_parameters():</span>
<span id="cb19-14"><a href="#cb19-14" aria-hidden="true" tabindex="-1"></a>        <span class="cf">if</span> param.dim() <span class="op">&gt;</span> <span class="dv">1</span>:</span>
<span id="cb19-15"><a href="#cb19-15" aria-hidden="true" tabindex="-1"></a>            ax <span class="op">=</span> axes[plot_index]</span>
<span id="cb19-16"><a href="#cb19-16" aria-hidden="true" tabindex="-1"></a>            ax.hist(param.detach().view(<span class="op">-</span><span class="dv">1</span>).cpu(), bins<span class="op">=</span>bins, density<span class="op">=</span><span class="va">True</span>,</span>
<span id="cb19-17"><a href="#cb19-17" aria-hidden="true" tabindex="-1"></a>                    align<span class="op">=</span>align, color <span class="op">=</span> <span class="st">'blue'</span>, alpha <span class="op">=</span> <span class="fl">0.5</span>,</span>
<span id="cb19-18"><a href="#cb19-18" aria-hidden="true" tabindex="-1"></a>                    edgecolor<span class="op">=</span><span class="st">'black'</span> <span class="cf">if</span> bitwidth <span class="op">&lt;=</span> <span class="dv">4</span> <span class="cf">else</span> <span class="va">None</span>)</span>
<span id="cb19-19"><a href="#cb19-19" aria-hidden="true" tabindex="-1"></a>            <span class="cf">if</span> bitwidth <span class="op">&lt;=</span> <span class="dv">4</span>:</span>
<span id="cb19-20"><a href="#cb19-20" aria-hidden="true" tabindex="-1"></a>                quantized_min, quantized_max <span class="op">=</span> get_quantized_range(bitwidth)</span>
<span id="cb19-21"><a href="#cb19-21" aria-hidden="true" tabindex="-1"></a>                ax.set_xticks(np.arange(start<span class="op">=</span>quantized_min, stop<span class="op">=</span>quantized_max<span class="op">+</span><span class="dv">1</span>))</span>
<span id="cb19-22"><a href="#cb19-22" aria-hidden="true" tabindex="-1"></a>            ax.set_xlabel(name)</span>
<span id="cb19-23"><a href="#cb19-23" aria-hidden="true" tabindex="-1"></a>            ax.set_ylabel(<span class="st">'density'</span>)</span>
<span id="cb19-24"><a href="#cb19-24" aria-hidden="true" tabindex="-1"></a>            plot_index <span class="op">+=</span> <span class="dv">1</span></span>
<span id="cb19-25"><a href="#cb19-25" aria-hidden="true" tabindex="-1"></a>    fig.suptitle(<span class="ss">f'Histogram of Weights (bitwidth=</span><span class="sc">{</span>bitwidth<span class="sc">}</span><span class="ss"> bits)'</span>)</span>
<span id="cb19-26"><a href="#cb19-26" aria-hidden="true" tabindex="-1"></a>    fig.tight_layout()</span>
<span id="cb19-27"><a href="#cb19-27" aria-hidden="true" tabindex="-1"></a>    fig.subplots_adjust(top<span class="op">=</span><span class="fl">0.925</span>)</span>
<span id="cb19-28"><a href="#cb19-28" aria-hidden="true" tabindex="-1"></a>    plt.show()</span>
<span id="cb19-29"><a href="#cb19-29" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb19-30"><a href="#cb19-30" aria-hidden="true" tabindex="-1"></a>recover_model()</span>
<span id="cb19-31"><a href="#cb19-31" aria-hidden="true" tabindex="-1"></a>plot_weight_distribution(model)</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output cell-output-display">
<div>
<figure class="figure">
<p><img src="lab02_files/figure-html/cell-15-output-1.png" class="img-fluid figure-img"></p>
</figure>
</div>
</div>
</div>
<p>위의 히스토그램에서 볼 수 있듯이, 가중치 값의 분포는 (이 경우에는 <code>classifier</code>를 제외하고) 거의 0을 중심으로 대칭적입니다 . 따라서 가중치를 양자화할 때 보통 제로 포인트 <span class="math inline">\(Z=0\)</span>으로 설정합니다.</p>
<p><span class="math inline">\(r = S(q-Z)\)</span>에서,</p>
<blockquote class="blockquote">
<p><span class="math inline">\(r_{\mathrm{max}} = S \cdot q_{\mathrm{max}}\)</span></p>
</blockquote>
<blockquote class="blockquote">
<p><span class="math inline">\(S = r_{\mathrm{max}} / q_{\mathrm{max}}\)</span></p>
</blockquote>
<p>가중치 값의 최대 절댓값을 <span class="math inline">\(r_{\mathrm{max}}\)</span>로 이용합니다.</p>
<div id="cell-57" class="cell" data-execution_count="29">
<div class="sourceCode cell-code" id="cb20"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb20-1"><a href="#cb20-1" aria-hidden="true" tabindex="-1"></a><span class="kw">def</span> get_quantization_scale_for_weight(weight, bitwidth):</span>
<span id="cb20-2"><a href="#cb20-2" aria-hidden="true" tabindex="-1"></a>    <span class="co">"""</span></span>
<span id="cb20-3"><a href="#cb20-3" aria-hidden="true" tabindex="-1"></a><span class="co">    get quantization scale for single tensor of weight</span></span>
<span id="cb20-4"><a href="#cb20-4" aria-hidden="true" tabindex="-1"></a><span class="co">    :param weight: [torch.(cuda.)Tensor] floating weight to be quantized</span></span>
<span id="cb20-5"><a href="#cb20-5" aria-hidden="true" tabindex="-1"></a><span class="co">    :param bitwidth: [integer] quantization bit width</span></span>
<span id="cb20-6"><a href="#cb20-6" aria-hidden="true" tabindex="-1"></a><span class="co">    :return:</span></span>
<span id="cb20-7"><a href="#cb20-7" aria-hidden="true" tabindex="-1"></a><span class="co">        [floating scalar] scale</span></span>
<span id="cb20-8"><a href="#cb20-8" aria-hidden="true" tabindex="-1"></a><span class="co">    """</span></span>
<span id="cb20-9"><a href="#cb20-9" aria-hidden="true" tabindex="-1"></a>    <span class="co"># we just assume values in weight are symmetric</span></span>
<span id="cb20-10"><a href="#cb20-10" aria-hidden="true" tabindex="-1"></a>    <span class="co"># we also always make zero_point 0 for weight</span></span>
<span id="cb20-11"><a href="#cb20-11" aria-hidden="true" tabindex="-1"></a>    fp_max <span class="op">=</span> <span class="bu">max</span>(weight.<span class="bu">abs</span>().<span class="bu">max</span>().item(), <span class="fl">5e-7</span>)</span>
<span id="cb20-12"><a href="#cb20-12" aria-hidden="true" tabindex="-1"></a>    _, quantized_max <span class="op">=</span> get_quantized_range(bitwidth)</span>
<span id="cb20-13"><a href="#cb20-13" aria-hidden="true" tabindex="-1"></a>    <span class="cf">return</span> fp_max <span class="op">/</span> quantized_max</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</div>
<section id="per-channel-linear-quantization" class="level3">
<h3 class="anchored" data-anchor-id="per-channel-linear-quantization">Per-channel Linear Quantization</h3>
<p>2D convolution의 경우, 가중치 텐서는 <code>(num_output_channels, num_input_channels, kernel_height, kernel_width)</code> 모양의 4차원 텐서입니다.</p>
<p>많은 실험들을 통해, <strong>서로 다른</strong> 출력 채널에 대해 <strong>서로 다른</strong> 스케일링 인자 <span class="math inline">\(S\)</span>와 제로 포인트 <span class="math inline">\(Z\)</span>를 사용하는 것이 더 나은 성능을 발휘한다는 것을 알 수 있었습니다. 따라서 각 출력 채널의 서브텐서에 대한 스케일링 인자 <span class="math inline">\(S\)</span>와 제로 포인트 <span class="math inline">\(Z\)</span>를 <strong>독립적으로</strong> 정해야 합니다.</p>
<div id="cell-59" class="cell" data-execution_count="30">
<div class="sourceCode cell-code" id="cb21"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb21-1"><a href="#cb21-1" aria-hidden="true" tabindex="-1"></a><span class="kw">def</span> linear_quantize_weight_per_channel(tensor, bitwidth):</span>
<span id="cb21-2"><a href="#cb21-2" aria-hidden="true" tabindex="-1"></a>    <span class="co">"""</span></span>
<span id="cb21-3"><a href="#cb21-3" aria-hidden="true" tabindex="-1"></a><span class="co">    linear quantization for weight tensor</span></span>
<span id="cb21-4"><a href="#cb21-4" aria-hidden="true" tabindex="-1"></a><span class="co">        using different scales and zero_points for different output channels</span></span>
<span id="cb21-5"><a href="#cb21-5" aria-hidden="true" tabindex="-1"></a><span class="co">    :param tensor: [torch.(cuda.)Tensor] floating weight to be quantized</span></span>
<span id="cb21-6"><a href="#cb21-6" aria-hidden="true" tabindex="-1"></a><span class="co">    :param bitwidth: [int] quantization bit width</span></span>
<span id="cb21-7"><a href="#cb21-7" aria-hidden="true" tabindex="-1"></a><span class="co">    :return:</span></span>
<span id="cb21-8"><a href="#cb21-8" aria-hidden="true" tabindex="-1"></a><span class="co">        [torch.(cuda.)Tensor] quantized tensor</span></span>
<span id="cb21-9"><a href="#cb21-9" aria-hidden="true" tabindex="-1"></a><span class="co">        [torch.(cuda.)Tensor] scale tensor</span></span>
<span id="cb21-10"><a href="#cb21-10" aria-hidden="true" tabindex="-1"></a><span class="co">        [int] zero point (which is always 0)</span></span>
<span id="cb21-11"><a href="#cb21-11" aria-hidden="true" tabindex="-1"></a><span class="co">    """</span></span>
<span id="cb21-12"><a href="#cb21-12" aria-hidden="true" tabindex="-1"></a>    dim_output_channels <span class="op">=</span> <span class="dv">0</span></span>
<span id="cb21-13"><a href="#cb21-13" aria-hidden="true" tabindex="-1"></a>    num_output_channels <span class="op">=</span> tensor.shape[dim_output_channels]</span>
<span id="cb21-14"><a href="#cb21-14" aria-hidden="true" tabindex="-1"></a>    scale <span class="op">=</span> torch.zeros(num_output_channels, device<span class="op">=</span>tensor.device)</span>
<span id="cb21-15"><a href="#cb21-15" aria-hidden="true" tabindex="-1"></a>    <span class="cf">for</span> oc <span class="kw">in</span> <span class="bu">range</span>(num_output_channels):</span>
<span id="cb21-16"><a href="#cb21-16" aria-hidden="true" tabindex="-1"></a>        _subtensor <span class="op">=</span> tensor.select(dim_output_channels, oc)</span>
<span id="cb21-17"><a href="#cb21-17" aria-hidden="true" tabindex="-1"></a>        _scale <span class="op">=</span> get_quantization_scale_for_weight(_subtensor, bitwidth)</span>
<span id="cb21-18"><a href="#cb21-18" aria-hidden="true" tabindex="-1"></a>        scale[oc] <span class="op">=</span> _scale</span>
<span id="cb21-19"><a href="#cb21-19" aria-hidden="true" tabindex="-1"></a>    scale_shape <span class="op">=</span> [<span class="dv">1</span>] <span class="op">*</span> tensor.dim()</span>
<span id="cb21-20"><a href="#cb21-20" aria-hidden="true" tabindex="-1"></a>    scale_shape[dim_output_channels] <span class="op">=</span> <span class="op">-</span><span class="dv">1</span></span>
<span id="cb21-21"><a href="#cb21-21" aria-hidden="true" tabindex="-1"></a>    scale <span class="op">=</span> scale.view(scale_shape)</span>
<span id="cb21-22"><a href="#cb21-22" aria-hidden="true" tabindex="-1"></a>    quantized_tensor <span class="op">=</span> linear_quantize(tensor, bitwidth, scale, zero_point<span class="op">=</span><span class="dv">0</span>)</span>
<span id="cb21-23"><a href="#cb21-23" aria-hidden="true" tabindex="-1"></a>    <span class="cf">return</span> quantized_tensor, scale, <span class="dv">0</span></span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</div>
</section>
<section id="a-quick-peek-at-linear-quantization-on-weights" class="level3">
<h3 class="anchored" data-anchor-id="a-quick-peek-at-linear-quantization-on-weights">A Quick Peek at Linear Quantization on Weights</h3>
<p>이제 가중치에 대해 linear quantization를 적용할 때 가중치 분포와 모델 크기를 서로 다른 bitwidths로 살펴보겠습니다.</p>
<div id="cell-61" class="cell" data-outputid="c2c7b5d1-c0c2-4afb-8e01-95e179c8dccc" data-execution_count="31">
<div class="sourceCode cell-code" id="cb22"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb22-1"><a href="#cb22-1" aria-hidden="true" tabindex="-1"></a><span class="at">@torch.no_grad</span>()</span>
<span id="cb22-2"><a href="#cb22-2" aria-hidden="true" tabindex="-1"></a><span class="kw">def</span> peek_linear_quantization():</span>
<span id="cb22-3"><a href="#cb22-3" aria-hidden="true" tabindex="-1"></a>    <span class="cf">for</span> bitwidth <span class="kw">in</span> [<span class="dv">4</span>, <span class="dv">2</span>]:</span>
<span id="cb22-4"><a href="#cb22-4" aria-hidden="true" tabindex="-1"></a>        <span class="cf">for</span> name, param <span class="kw">in</span> model.named_parameters():</span>
<span id="cb22-5"><a href="#cb22-5" aria-hidden="true" tabindex="-1"></a>            <span class="cf">if</span> param.dim() <span class="op">&gt;</span> <span class="dv">1</span>:</span>
<span id="cb22-6"><a href="#cb22-6" aria-hidden="true" tabindex="-1"></a>                quantized_param, scale, zero_point <span class="op">=</span> <span class="op">\</span></span>
<span id="cb22-7"><a href="#cb22-7" aria-hidden="true" tabindex="-1"></a>                    linear_quantize_weight_per_channel(param, bitwidth)</span>
<span id="cb22-8"><a href="#cb22-8" aria-hidden="true" tabindex="-1"></a>                param.copy_(quantized_param)</span>
<span id="cb22-9"><a href="#cb22-9" aria-hidden="true" tabindex="-1"></a>        plot_weight_distribution(model, bitwidth)</span>
<span id="cb22-10"><a href="#cb22-10" aria-hidden="true" tabindex="-1"></a>        recover_model()</span>
<span id="cb22-11"><a href="#cb22-11" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb22-12"><a href="#cb22-12" aria-hidden="true" tabindex="-1"></a>peek_linear_quantization()</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output cell-output-display">
<div>
<figure class="figure">
<p><img src="lab02_files/figure-html/cell-18-output-1.png" class="img-fluid figure-img"></p>
</figure>
</div>
</div>
<div class="cell-output cell-output-display">
<div>
<figure class="figure">
<p><img src="lab02_files/figure-html/cell-18-output-2.png" class="img-fluid figure-img"></p>
</figure>
</div>
</div>
</div>
</section>
</section>
<section id="quantized-inference" class="level2">
<h2 class="anchored" data-anchor-id="quantized-inference">Quantized Inference</h2>
<p>양자화 후, convolution 및 fully-connected layer의 추론도 변경됩니다.</p>
<p><span class="math inline">\(r = S(q-Z)\)</span>를 상기해 보면, 다음과 같습니다.</p>
<blockquote class="blockquote">
<p><span class="math inline">\(r_{\mathrm{input}} = S_{\mathrm{input}}(q_{\mathrm{input}}-Z_{\mathrm{input}})\)</span></p>
<p><span class="math inline">\(r_{\mathrm{weight}} = S_{\mathrm{weight}}(q_{\mathrm{weight}}-Z_{\mathrm{weight}})\)</span></p>
<p><span class="math inline">\(r_{\mathrm{bias}} = S_{\mathrm{bias}}(q_{\mathrm{bias}}-Z_{\mathrm{bias}})\)</span></p>
</blockquote>
<p><span class="math inline">\(Z_{\mathrm{weight}}=0\)</span>이므로, <span class="math inline">\(r_{\mathrm{weight}} = S_{\mathrm{weight}}q_{\mathrm{weight}}\)</span>입니다.</p>
<p>부동 소수점 convolution은 다음과 같이 작성할 수 있습니다.</p>
<blockquote class="blockquote">
<p><span class="math inline">\(r_{\mathrm{output}} = \mathrm{CONV}[r_{\mathrm{input}}, r_{\mathrm{weight}}] + r_{\mathrm{bias}}\)</span> <span class="math inline">\(\;\;\;\;\;\;\;\;= \mathrm{CONV}[S_{\mathrm{input}}(q_{\mathrm{input}}-Z_{\mathrm{input}}), S_{\mathrm{weight}}q_{\mathrm{weight}}] + S_{\mathrm{bias}}(q_{\mathrm{bias}}-Z_{\mathrm{bias}})\)</span> <span class="math inline">\(\;\;\;\;\;\;\;\;= \mathrm{CONV}[q_{\mathrm{input}}-Z_{\mathrm{input}}, q_{\mathrm{weight}}]\cdot (S_{\mathrm{input}} \cdot S_{\mathrm{weight}}) + S_{\mathrm{bias}}(q_{\mathrm{bias}}-Z_{\mathrm{bias}})\)</span></p>
</blockquote>
<p>계산을 더 간단하게 하기 위해</p>
<blockquote class="blockquote">
<p><span class="math inline">\(Z_{\mathrm{bias}} = 0\)</span></p>
<p><span class="math inline">\(S_{\mathrm{bias}} = S_{\mathrm{input}} \cdot S_{\mathrm{weight}}\)</span></p>
</blockquote>
<p>로 설정하여,</p>
<blockquote class="blockquote">
<p><span class="math inline">\(r_{\mathrm{output}} = (\mathrm{CONV}[q_{\mathrm{input}}-Z_{\mathrm{input}}, q_{\mathrm{weight}}] + q_{\mathrm{bias}})\cdot (S_{\mathrm{input}} \cdot S_{\mathrm{weight}})\)</span> <span class="math inline">\(\;\;\;\;\;\;\;\;= (\mathrm{CONV}[q_{\mathrm{input}}, q_{\mathrm{weight}}] - \mathrm{CONV}[Z_{\mathrm{input}}, q_{\mathrm{weight}}] + q_{\mathrm{bias}})\cdot (S_{\mathrm{input}}S_{\mathrm{weight}})\)</span></p>
</blockquote>
<p>이며,</p>
<blockquote class="blockquote">
<p><span class="math inline">\(r_{\mathrm{output}} = S_{\mathrm{output}}(q_{\mathrm{output}}-Z_{\mathrm{output}})\)</span></p>
</blockquote>
<p>이므로</p>
<blockquote class="blockquote">
<p><span class="math inline">\(S_{\mathrm{output}}(q_{\mathrm{output}}-Z_{\mathrm{output}}) = (\mathrm{CONV}[q_{\mathrm{input}}, q_{\mathrm{weight}}] - \mathrm{CONV}[Z_{\mathrm{input}}, q_{\mathrm{weight}}] + q_{\mathrm{bias}})\cdot (S_{\mathrm{input}} S_{\mathrm{weight}})\)</span></p>
</blockquote>
<p>따라서</p>
<blockquote class="blockquote">
<p><span class="math inline">\(q_{\mathrm{output}} = (\mathrm{CONV}[q_{\mathrm{input}}, q_{\mathrm{weight}}] - \mathrm{CONV}[Z_{\mathrm{input}}, q_{\mathrm{weight}}] + q_{\mathrm{bias}})\cdot (S_{\mathrm{input}}S_{\mathrm{weight}} / S_{\mathrm{output}}) + Z_{\mathrm{output}}\)</span></p>
</blockquote>
<p><span class="math inline">\(Z_{\mathrm{input}}\)</span>, <span class="math inline">\(q_{\mathrm{weight}}\)</span>, <span class="math inline">\(q_{\mathrm{bias}}\)</span>는 추론 전에 결정되므로,</p>
<blockquote class="blockquote">
<p><span class="math inline">\(Q_{\mathrm{bias}} = q_{\mathrm{bias}} - \mathrm{CONV}[Z_{\mathrm{input}}, q_{\mathrm{weight}}]\)</span></p>
</blockquote>
<p>로 설정하면,</p>
<blockquote class="blockquote">
<p><span class="math inline">\(q_{\mathrm{output}} = (\mathrm{Linear}[q_{\mathrm{input}}, q_{\mathrm{weight}}] + Q_{\mathrm{bias}})\cdot (S_{\mathrm{input}} \cdot S_{\mathrm{weight}} / S_{\mathrm{output}}) + Z_{\mathrm{output}}\)</span></p>
</blockquote>
<section id="question-6-5-pts" class="level3">
<h3 class="anchored" data-anchor-id="question-6-5-pts">Question 6 (5 pts)</h3>
<p>bias를 linear quantizing하는 함수를 완성하세요.</p>
<p><strong>Hint</strong>:</p>
<p>위의 추론과정에서 아래와 같은 수식을 얻었습니다.</p>
<blockquote class="blockquote">
<p><span class="math inline">\(Z_{\mathrm{bias}} = 0\)</span></p>
<p><span class="math inline">\(S_{\mathrm{bias}} = S_{\mathrm{input}} \cdot S_{\mathrm{weight}}\)</span></p>
</blockquote>
<div id="cell-65" class="cell" data-execution_count="32">
<div class="sourceCode cell-code" id="cb23"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb23-1"><a href="#cb23-1" aria-hidden="true" tabindex="-1"></a><span class="kw">def</span> linear_quantize_bias_per_output_channel(bias, weight_scale, input_scale):</span>
<span id="cb23-2"><a href="#cb23-2" aria-hidden="true" tabindex="-1"></a>    <span class="co">"""</span></span>
<span id="cb23-3"><a href="#cb23-3" aria-hidden="true" tabindex="-1"></a><span class="co">    linear quantization for single bias tensor</span></span>
<span id="cb23-4"><a href="#cb23-4" aria-hidden="true" tabindex="-1"></a><span class="co">        quantized_bias = fp_bias / bias_scale</span></span>
<span id="cb23-5"><a href="#cb23-5" aria-hidden="true" tabindex="-1"></a><span class="co">    :param bias: [torch.FloatTensor] bias weight to be quantized</span></span>
<span id="cb23-6"><a href="#cb23-6" aria-hidden="true" tabindex="-1"></a><span class="co">    :param weight_scale: [float or torch.FloatTensor] weight scale tensor</span></span>
<span id="cb23-7"><a href="#cb23-7" aria-hidden="true" tabindex="-1"></a><span class="co">    :param input_scale: [float] input scale</span></span>
<span id="cb23-8"><a href="#cb23-8" aria-hidden="true" tabindex="-1"></a><span class="co">    :return:</span></span>
<span id="cb23-9"><a href="#cb23-9" aria-hidden="true" tabindex="-1"></a><span class="co">        [torch.IntTensor] quantized bias tensor</span></span>
<span id="cb23-10"><a href="#cb23-10" aria-hidden="true" tabindex="-1"></a><span class="co">    """</span></span>
<span id="cb23-11"><a href="#cb23-11" aria-hidden="true" tabindex="-1"></a>    <span class="cf">assert</span>(bias.dim() <span class="op">==</span> <span class="dv">1</span>)</span>
<span id="cb23-12"><a href="#cb23-12" aria-hidden="true" tabindex="-1"></a>    <span class="cf">assert</span>(bias.dtype <span class="op">==</span> torch.<span class="bu">float</span>)</span>
<span id="cb23-13"><a href="#cb23-13" aria-hidden="true" tabindex="-1"></a>    <span class="cf">assert</span>(<span class="bu">isinstance</span>(input_scale, <span class="bu">float</span>))</span>
<span id="cb23-14"><a href="#cb23-14" aria-hidden="true" tabindex="-1"></a>    <span class="cf">if</span> <span class="bu">isinstance</span>(weight_scale, torch.Tensor):</span>
<span id="cb23-15"><a href="#cb23-15" aria-hidden="true" tabindex="-1"></a>        <span class="cf">assert</span>(weight_scale.dtype <span class="op">==</span> torch.<span class="bu">float</span>)</span>
<span id="cb23-16"><a href="#cb23-16" aria-hidden="true" tabindex="-1"></a>        weight_scale <span class="op">=</span> weight_scale.view(<span class="op">-</span><span class="dv">1</span>)</span>
<span id="cb23-17"><a href="#cb23-17" aria-hidden="true" tabindex="-1"></a>        <span class="cf">assert</span>(bias.numel() <span class="op">==</span> weight_scale.numel())</span>
<span id="cb23-18"><a href="#cb23-18" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb23-19"><a href="#cb23-19" aria-hidden="true" tabindex="-1"></a>    <span class="co">############### YOUR CODE STARTS HERE ###############</span></span>
<span id="cb23-20"><a href="#cb23-20" aria-hidden="true" tabindex="-1"></a>    bias_scale <span class="op">=</span> weight_scale <span class="op">*</span> input_scale</span>
<span id="cb23-21"><a href="#cb23-21" aria-hidden="true" tabindex="-1"></a>    <span class="co">############### YOUR CODE ENDS HERE #################</span></span>
<span id="cb23-22"><a href="#cb23-22" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb23-23"><a href="#cb23-23" aria-hidden="true" tabindex="-1"></a>    quantized_bias <span class="op">=</span> linear_quantize(bias, <span class="dv">32</span>, bias_scale,</span>
<span id="cb23-24"><a href="#cb23-24" aria-hidden="true" tabindex="-1"></a>                                     zero_point<span class="op">=</span><span class="dv">0</span>, dtype<span class="op">=</span>torch.int32)</span>
<span id="cb23-25"><a href="#cb23-25" aria-hidden="true" tabindex="-1"></a>    <span class="cf">return</span> quantized_bias, bias_scale, <span class="dv">0</span></span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</div>
</section>
<section id="quantized-fully-connected-layer" class="level3">
<h3 class="anchored" data-anchor-id="quantized-fully-connected-layer">Quantized Fully-Connected Layer</h3>
<p>양자화된 fully-connected layer의 경우, <span class="math inline">\(Q_{\mathrm{bias}}\)</span>를 먼저 계산합니다. <span class="math inline">\(Q_{\mathrm{bias}} = q_{\mathrm{bias}} - \mathrm{Linear}[Z_{\mathrm{input}}, q_{\mathrm{weight}}]\)</span>를 기억하세요.</p>
<div id="cell-68" class="cell" data-execution_count="33">
<div class="sourceCode cell-code" id="cb24"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb24-1"><a href="#cb24-1" aria-hidden="true" tabindex="-1"></a><span class="kw">def</span> shift_quantized_linear_bias(quantized_bias, quantized_weight, input_zero_point):</span>
<span id="cb24-2"><a href="#cb24-2" aria-hidden="true" tabindex="-1"></a>    <span class="co">"""</span></span>
<span id="cb24-3"><a href="#cb24-3" aria-hidden="true" tabindex="-1"></a><span class="co">    shift quantized bias to incorporate input_zero_point for nn.Linear</span></span>
<span id="cb24-4"><a href="#cb24-4" aria-hidden="true" tabindex="-1"></a><span class="co">        shifted_quantized_bias = quantized_bias - Linear(input_zero_point, quantized_weight)</span></span>
<span id="cb24-5"><a href="#cb24-5" aria-hidden="true" tabindex="-1"></a><span class="co">    :param quantized_bias: [torch.IntTensor] quantized bias (torch.int32)</span></span>
<span id="cb24-6"><a href="#cb24-6" aria-hidden="true" tabindex="-1"></a><span class="co">    :param quantized_weight: [torch.CharTensor] quantized weight (torch.int8)</span></span>
<span id="cb24-7"><a href="#cb24-7" aria-hidden="true" tabindex="-1"></a><span class="co">    :param input_zero_point: [int] input zero point</span></span>
<span id="cb24-8"><a href="#cb24-8" aria-hidden="true" tabindex="-1"></a><span class="co">    :return:</span></span>
<span id="cb24-9"><a href="#cb24-9" aria-hidden="true" tabindex="-1"></a><span class="co">        [torch.IntTensor] shifted quantized bias tensor</span></span>
<span id="cb24-10"><a href="#cb24-10" aria-hidden="true" tabindex="-1"></a><span class="co">    """</span></span>
<span id="cb24-11"><a href="#cb24-11" aria-hidden="true" tabindex="-1"></a>    <span class="cf">assert</span>(quantized_bias.dtype <span class="op">==</span> torch.int32)</span>
<span id="cb24-12"><a href="#cb24-12" aria-hidden="true" tabindex="-1"></a>    <span class="cf">assert</span>(<span class="bu">isinstance</span>(input_zero_point, <span class="bu">int</span>))</span>
<span id="cb24-13"><a href="#cb24-13" aria-hidden="true" tabindex="-1"></a>    <span class="cf">return</span> quantized_bias <span class="op">-</span> quantized_weight.<span class="bu">sum</span>(<span class="dv">1</span>).to(torch.int32) <span class="op">*</span> input_zero_point</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</div>
<section id="question-7-15-pts" class="level4">
<h4 class="anchored" data-anchor-id="question-7-15-pts">Question 7 (15 pts)</h4>
<p>아래의 양자화된 fully-connected layer inference function를 완성하세요.</p>
<p><strong>Hint</strong>:</p>
<blockquote class="blockquote">
<p><span class="math inline">\(q_{\mathrm{output}} = (\mathrm{Linear}[q_{\mathrm{input}}, q_{\mathrm{weight}}] + Q_{\mathrm{bias}})\cdot (S_{\mathrm{input}} S_{\mathrm{weight}} / S_{\mathrm{output}}) + Z_{\mathrm{output}}\)</span></p>
</blockquote>
<div id="cell-70" class="cell" data-execution_count="34">
<div class="sourceCode cell-code" id="cb25"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb25-1"><a href="#cb25-1" aria-hidden="true" tabindex="-1"></a><span class="kw">def</span> quantized_linear(<span class="bu">input</span>, weight, bias, feature_bitwidth, weight_bitwidth,</span>
<span id="cb25-2"><a href="#cb25-2" aria-hidden="true" tabindex="-1"></a>                     input_zero_point, output_zero_point,</span>
<span id="cb25-3"><a href="#cb25-3" aria-hidden="true" tabindex="-1"></a>                     input_scale, weight_scale, output_scale):</span>
<span id="cb25-4"><a href="#cb25-4" aria-hidden="true" tabindex="-1"></a>    <span class="co">"""</span></span>
<span id="cb25-5"><a href="#cb25-5" aria-hidden="true" tabindex="-1"></a><span class="co">    quantized fully-connected layer</span></span>
<span id="cb25-6"><a href="#cb25-6" aria-hidden="true" tabindex="-1"></a><span class="co">    :param input: [torch.CharTensor] quantized input (torch.int8)</span></span>
<span id="cb25-7"><a href="#cb25-7" aria-hidden="true" tabindex="-1"></a><span class="co">    :param weight: [torch.CharTensor] quantized weight (torch.int8)</span></span>
<span id="cb25-8"><a href="#cb25-8" aria-hidden="true" tabindex="-1"></a><span class="co">    :param bias: [torch.IntTensor] shifted quantized bias or None (torch.int32)</span></span>
<span id="cb25-9"><a href="#cb25-9" aria-hidden="true" tabindex="-1"></a><span class="co">    :param feature_bitwidth: [int] quantization bit width of input and output</span></span>
<span id="cb25-10"><a href="#cb25-10" aria-hidden="true" tabindex="-1"></a><span class="co">    :param weight_bitwidth: [int] quantization bit width of weight</span></span>
<span id="cb25-11"><a href="#cb25-11" aria-hidden="true" tabindex="-1"></a><span class="co">    :param input_zero_point: [int] input zero point</span></span>
<span id="cb25-12"><a href="#cb25-12" aria-hidden="true" tabindex="-1"></a><span class="co">    :param output_zero_point: [int] output zero point</span></span>
<span id="cb25-13"><a href="#cb25-13" aria-hidden="true" tabindex="-1"></a><span class="co">    :param input_scale: [float] input feature scale</span></span>
<span id="cb25-14"><a href="#cb25-14" aria-hidden="true" tabindex="-1"></a><span class="co">    :param weight_scale: [torch.FloatTensor] weight per-channel scale</span></span>
<span id="cb25-15"><a href="#cb25-15" aria-hidden="true" tabindex="-1"></a><span class="co">    :param output_scale: [float] output feature scale</span></span>
<span id="cb25-16"><a href="#cb25-16" aria-hidden="true" tabindex="-1"></a><span class="co">    :return:</span></span>
<span id="cb25-17"><a href="#cb25-17" aria-hidden="true" tabindex="-1"></a><span class="co">        [torch.CharIntTensor] quantized output feature (torch.int8)</span></span>
<span id="cb25-18"><a href="#cb25-18" aria-hidden="true" tabindex="-1"></a><span class="co">    """</span></span>
<span id="cb25-19"><a href="#cb25-19" aria-hidden="true" tabindex="-1"></a>    <span class="cf">assert</span>(<span class="bu">input</span>.dtype <span class="op">==</span> torch.int8)</span>
<span id="cb25-20"><a href="#cb25-20" aria-hidden="true" tabindex="-1"></a>    <span class="cf">assert</span>(weight.dtype <span class="op">==</span> <span class="bu">input</span>.dtype)</span>
<span id="cb25-21"><a href="#cb25-21" aria-hidden="true" tabindex="-1"></a>    <span class="cf">assert</span>(bias <span class="kw">is</span> <span class="va">None</span> <span class="kw">or</span> bias.dtype <span class="op">==</span> torch.int32)</span>
<span id="cb25-22"><a href="#cb25-22" aria-hidden="true" tabindex="-1"></a>    <span class="cf">assert</span>(<span class="bu">isinstance</span>(input_zero_point, <span class="bu">int</span>))</span>
<span id="cb25-23"><a href="#cb25-23" aria-hidden="true" tabindex="-1"></a>    <span class="cf">assert</span>(<span class="bu">isinstance</span>(output_zero_point, <span class="bu">int</span>))</span>
<span id="cb25-24"><a href="#cb25-24" aria-hidden="true" tabindex="-1"></a>    <span class="cf">assert</span>(<span class="bu">isinstance</span>(input_scale, <span class="bu">float</span>))</span>
<span id="cb25-25"><a href="#cb25-25" aria-hidden="true" tabindex="-1"></a>    <span class="cf">assert</span>(<span class="bu">isinstance</span>(output_scale, <span class="bu">float</span>))</span>
<span id="cb25-26"><a href="#cb25-26" aria-hidden="true" tabindex="-1"></a>    <span class="cf">assert</span>(weight_scale.dtype <span class="op">==</span> torch.<span class="bu">float</span>)</span>
<span id="cb25-27"><a href="#cb25-27" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb25-28"><a href="#cb25-28" aria-hidden="true" tabindex="-1"></a>    <span class="co"># Step 1: integer-based fully-connected (8-bit multiplication with 32-bit accumulation)</span></span>
<span id="cb25-29"><a href="#cb25-29" aria-hidden="true" tabindex="-1"></a>    <span class="cf">if</span> <span class="st">'cpu'</span> <span class="kw">in</span> <span class="bu">input</span>.device.<span class="bu">type</span>:</span>
<span id="cb25-30"><a href="#cb25-30" aria-hidden="true" tabindex="-1"></a>        <span class="co"># use 32-b MAC for simplicity</span></span>
<span id="cb25-31"><a href="#cb25-31" aria-hidden="true" tabindex="-1"></a>        output <span class="op">=</span> torch.nn.functional.linear(<span class="bu">input</span>.to(torch.int32), weight.to(torch.int32), bias)</span>
<span id="cb25-32"><a href="#cb25-32" aria-hidden="true" tabindex="-1"></a>    <span class="cf">else</span>:</span>
<span id="cb25-33"><a href="#cb25-33" aria-hidden="true" tabindex="-1"></a>        <span class="co"># current version pytorch does not yet support integer-based linear() on GPUs</span></span>
<span id="cb25-34"><a href="#cb25-34" aria-hidden="true" tabindex="-1"></a>        output <span class="op">=</span> torch.nn.functional.linear(<span class="bu">input</span>.<span class="bu">float</span>(), weight.<span class="bu">float</span>(), bias.<span class="bu">float</span>())</span>
<span id="cb25-35"><a href="#cb25-35" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb25-36"><a href="#cb25-36" aria-hidden="true" tabindex="-1"></a>    <span class="co">############### YOUR CODE STARTS HERE ###############</span></span>
<span id="cb25-37"><a href="#cb25-37" aria-hidden="true" tabindex="-1"></a>    <span class="co"># Step 2: scale the output</span></span>
<span id="cb25-38"><a href="#cb25-38" aria-hidden="true" tabindex="-1"></a>    <span class="co">#         hint: 1. scales are floating numbers, we need to convert output to float as well</span></span>
<span id="cb25-39"><a href="#cb25-39" aria-hidden="true" tabindex="-1"></a>    <span class="co">#               2. the shape of weight scale is [oc, 1, 1, 1] while the shape of output is [batch_size, oc]</span></span>
<span id="cb25-40"><a href="#cb25-40" aria-hidden="true" tabindex="-1"></a>    real_scale <span class="op">=</span> input_scale <span class="op">*</span> weight_scale.view(<span class="op">-</span><span class="dv">1</span>) <span class="op">/</span> output_scale</span>
<span id="cb25-41"><a href="#cb25-41" aria-hidden="true" tabindex="-1"></a>    output <span class="op">=</span> output.<span class="bu">float</span>() <span class="op">*</span> real_scale</span>
<span id="cb25-42"><a href="#cb25-42" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb25-43"><a href="#cb25-43" aria-hidden="true" tabindex="-1"></a>    <span class="co"># Step 3: Shift output by output_zero_point</span></span>
<span id="cb25-44"><a href="#cb25-44" aria-hidden="true" tabindex="-1"></a>    output <span class="op">+=</span> output_zero_point</span>
<span id="cb25-45"><a href="#cb25-45" aria-hidden="true" tabindex="-1"></a>    <span class="co">############### YOUR CODE STARTS HERE ###############</span></span>
<span id="cb25-46"><a href="#cb25-46" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb25-47"><a href="#cb25-47" aria-hidden="true" tabindex="-1"></a>    <span class="co"># Make sure all value lies in the bitwidth-bit range</span></span>
<span id="cb25-48"><a href="#cb25-48" aria-hidden="true" tabindex="-1"></a>    output <span class="op">=</span> output.<span class="bu">round</span>().clamp(<span class="op">*</span>get_quantized_range(feature_bitwidth)).to(torch.int8)</span>
<span id="cb25-49"><a href="#cb25-49" aria-hidden="true" tabindex="-1"></a>    <span class="cf">return</span> output</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</div>
<p>Let’s verify the functionality of defined quantized fully connected layer.</p>
<div id="cell-72" class="cell" data-outputid="40b9bc67-2f7f-433e-af7a-cbba9add43e6" data-execution_count="35">
<div class="sourceCode cell-code" id="cb26"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb26-1"><a href="#cb26-1" aria-hidden="true" tabindex="-1"></a>test_quantized_fc()</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output cell-output-stdout">
<pre><code>* Test quantized_fc()
    target bitwidth: 2 bits
      batch size: 4
      input channels: 8
      output channels: 8
* Test passed.</code></pre>
</div>
<div class="cell-output cell-output-display">
<div>
<figure class="figure">
<p><img src="lab02_files/figure-html/cell-22-output-2.png" class="img-fluid figure-img"></p>
</figure>
</div>
</div>
</div>
</section>
</section>
<section id="quantized-convolution" class="level3">
<h3 class="anchored" data-anchor-id="quantized-convolution">Quantized Convolution</h3>
<p>양자화된 컨볼루션 레이어의 경우, 먼저 <span class="math inline">\(Q_{\mathrm{bias}}\)</span>를 계산합니다. <span class="math inline">\(Q_{\mathrm{bias}} = q_{\mathrm{bias}} - \mathrm{CONV}[Z_{\mathrm{input}}, q_{\mathrm{weight}}]\)</span>를 기억하세요.</p>
<div id="cell-75" class="cell" data-execution_count="36">
<div class="sourceCode cell-code" id="cb28"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb28-1"><a href="#cb28-1" aria-hidden="true" tabindex="-1"></a><span class="kw">def</span> shift_quantized_conv2d_bias(quantized_bias, quantized_weight, input_zero_point):</span>
<span id="cb28-2"><a href="#cb28-2" aria-hidden="true" tabindex="-1"></a>    <span class="co">"""</span></span>
<span id="cb28-3"><a href="#cb28-3" aria-hidden="true" tabindex="-1"></a><span class="co">    shift quantized bias to incorporate input_zero_point for nn.Conv2d</span></span>
<span id="cb28-4"><a href="#cb28-4" aria-hidden="true" tabindex="-1"></a><span class="co">        shifted_quantized_bias = quantized_bias - Conv(input_zero_point, quantized_weight)</span></span>
<span id="cb28-5"><a href="#cb28-5" aria-hidden="true" tabindex="-1"></a><span class="co">    :param quantized_bias: [torch.IntTensor] quantized bias (torch.int32)</span></span>
<span id="cb28-6"><a href="#cb28-6" aria-hidden="true" tabindex="-1"></a><span class="co">    :param quantized_weight: [torch.CharTensor] quantized weight (torch.int8)</span></span>
<span id="cb28-7"><a href="#cb28-7" aria-hidden="true" tabindex="-1"></a><span class="co">    :param input_zero_point: [int] input zero point</span></span>
<span id="cb28-8"><a href="#cb28-8" aria-hidden="true" tabindex="-1"></a><span class="co">    :return:</span></span>
<span id="cb28-9"><a href="#cb28-9" aria-hidden="true" tabindex="-1"></a><span class="co">        [torch.IntTensor] shifted quantized bias tensor</span></span>
<span id="cb28-10"><a href="#cb28-10" aria-hidden="true" tabindex="-1"></a><span class="co">    """</span></span>
<span id="cb28-11"><a href="#cb28-11" aria-hidden="true" tabindex="-1"></a>    <span class="cf">assert</span>(quantized_bias.dtype <span class="op">==</span> torch.int32)</span>
<span id="cb28-12"><a href="#cb28-12" aria-hidden="true" tabindex="-1"></a>    <span class="cf">assert</span>(<span class="bu">isinstance</span>(input_zero_point, <span class="bu">int</span>))</span>
<span id="cb28-13"><a href="#cb28-13" aria-hidden="true" tabindex="-1"></a>    <span class="cf">return</span> quantized_bias <span class="op">-</span> quantized_weight.<span class="bu">sum</span>((<span class="dv">1</span>,<span class="dv">2</span>,<span class="dv">3</span>)).to(torch.int32) <span class="op">*</span> input_zero_point</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</div>
<section id="question-8-15-pts" class="level4">
<h4 class="anchored" data-anchor-id="question-8-15-pts">Question 8 (15 pts)</h4>
<p>아래의 quantized convolution function을 완성하세요.</p>
<p><strong>Hint</strong>: &gt; <span class="math inline">\(q_{\mathrm{output}} = (\mathrm{CONV}[q_{\mathrm{input}}, q_{\mathrm{weight}}] + Q_{\mathrm{bias}}) \cdot (S_{\mathrm{input}}S_{\mathrm{weight}} / S_{\mathrm{output}}) + Z_{\mathrm{output}}\)</span></p>
<div id="cell-77" class="cell" data-execution_count="37">
<div class="sourceCode cell-code" id="cb29"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb29-1"><a href="#cb29-1" aria-hidden="true" tabindex="-1"></a><span class="kw">def</span> quantized_conv2d(<span class="bu">input</span>, weight, bias, feature_bitwidth, weight_bitwidth,</span>
<span id="cb29-2"><a href="#cb29-2" aria-hidden="true" tabindex="-1"></a>                     input_zero_point, output_zero_point,</span>
<span id="cb29-3"><a href="#cb29-3" aria-hidden="true" tabindex="-1"></a>                     input_scale, weight_scale, output_scale,</span>
<span id="cb29-4"><a href="#cb29-4" aria-hidden="true" tabindex="-1"></a>                     stride, padding, dilation, groups):</span>
<span id="cb29-5"><a href="#cb29-5" aria-hidden="true" tabindex="-1"></a>    <span class="co">"""</span></span>
<span id="cb29-6"><a href="#cb29-6" aria-hidden="true" tabindex="-1"></a><span class="co">    quantized 2d convolution</span></span>
<span id="cb29-7"><a href="#cb29-7" aria-hidden="true" tabindex="-1"></a><span class="co">    :param input: [torch.CharTensor] quantized input (torch.int8)</span></span>
<span id="cb29-8"><a href="#cb29-8" aria-hidden="true" tabindex="-1"></a><span class="co">    :param weight: [torch.CharTensor] quantized weight (torch.int8)</span></span>
<span id="cb29-9"><a href="#cb29-9" aria-hidden="true" tabindex="-1"></a><span class="co">    :param bias: [torch.IntTensor] shifted quantized bias or None (torch.int32)</span></span>
<span id="cb29-10"><a href="#cb29-10" aria-hidden="true" tabindex="-1"></a><span class="co">    :param feature_bitwidth: [int] quantization bit width of input and output</span></span>
<span id="cb29-11"><a href="#cb29-11" aria-hidden="true" tabindex="-1"></a><span class="co">    :param weight_bitwidth: [int] quantization bit width of weight</span></span>
<span id="cb29-12"><a href="#cb29-12" aria-hidden="true" tabindex="-1"></a><span class="co">    :param input_zero_point: [int] input zero point</span></span>
<span id="cb29-13"><a href="#cb29-13" aria-hidden="true" tabindex="-1"></a><span class="co">    :param output_zero_point: [int] output zero point</span></span>
<span id="cb29-14"><a href="#cb29-14" aria-hidden="true" tabindex="-1"></a><span class="co">    :param input_scale: [float] input feature scale</span></span>
<span id="cb29-15"><a href="#cb29-15" aria-hidden="true" tabindex="-1"></a><span class="co">    :param weight_scale: [torch.FloatTensor] weight per-channel scale</span></span>
<span id="cb29-16"><a href="#cb29-16" aria-hidden="true" tabindex="-1"></a><span class="co">    :param output_scale: [float] output feature scale</span></span>
<span id="cb29-17"><a href="#cb29-17" aria-hidden="true" tabindex="-1"></a><span class="co">    :return:</span></span>
<span id="cb29-18"><a href="#cb29-18" aria-hidden="true" tabindex="-1"></a><span class="co">        [torch.(cuda.)CharTensor] quantized output feature</span></span>
<span id="cb29-19"><a href="#cb29-19" aria-hidden="true" tabindex="-1"></a><span class="co">    """</span></span>
<span id="cb29-20"><a href="#cb29-20" aria-hidden="true" tabindex="-1"></a>    <span class="cf">assert</span>(<span class="bu">len</span>(padding) <span class="op">==</span> <span class="dv">4</span>)</span>
<span id="cb29-21"><a href="#cb29-21" aria-hidden="true" tabindex="-1"></a>    <span class="cf">assert</span>(<span class="bu">input</span>.dtype <span class="op">==</span> torch.int8)</span>
<span id="cb29-22"><a href="#cb29-22" aria-hidden="true" tabindex="-1"></a>    <span class="cf">assert</span>(weight.dtype <span class="op">==</span> <span class="bu">input</span>.dtype)</span>
<span id="cb29-23"><a href="#cb29-23" aria-hidden="true" tabindex="-1"></a>    <span class="cf">assert</span>(bias <span class="kw">is</span> <span class="va">None</span> <span class="kw">or</span> bias.dtype <span class="op">==</span> torch.int32)</span>
<span id="cb29-24"><a href="#cb29-24" aria-hidden="true" tabindex="-1"></a>    <span class="cf">assert</span>(<span class="bu">isinstance</span>(input_zero_point, <span class="bu">int</span>))</span>
<span id="cb29-25"><a href="#cb29-25" aria-hidden="true" tabindex="-1"></a>    <span class="cf">assert</span>(<span class="bu">isinstance</span>(output_zero_point, <span class="bu">int</span>))</span>
<span id="cb29-26"><a href="#cb29-26" aria-hidden="true" tabindex="-1"></a>    <span class="cf">assert</span>(<span class="bu">isinstance</span>(input_scale, <span class="bu">float</span>))</span>
<span id="cb29-27"><a href="#cb29-27" aria-hidden="true" tabindex="-1"></a>    <span class="cf">assert</span>(<span class="bu">isinstance</span>(output_scale, <span class="bu">float</span>))</span>
<span id="cb29-28"><a href="#cb29-28" aria-hidden="true" tabindex="-1"></a>    <span class="cf">assert</span>(weight_scale.dtype <span class="op">==</span> torch.<span class="bu">float</span>)</span>
<span id="cb29-29"><a href="#cb29-29" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb29-30"><a href="#cb29-30" aria-hidden="true" tabindex="-1"></a>    <span class="co"># Step 1: calculate integer-based 2d convolution (8-bit multiplication with 32-bit accumulation)</span></span>
<span id="cb29-31"><a href="#cb29-31" aria-hidden="true" tabindex="-1"></a>    <span class="bu">input</span> <span class="op">=</span> torch.nn.functional.pad(<span class="bu">input</span>, padding, <span class="st">'constant'</span>, input_zero_point)</span>
<span id="cb29-32"><a href="#cb29-32" aria-hidden="true" tabindex="-1"></a>    <span class="cf">if</span> <span class="st">'cpu'</span> <span class="kw">in</span> <span class="bu">input</span>.device.<span class="bu">type</span>:</span>
<span id="cb29-33"><a href="#cb29-33" aria-hidden="true" tabindex="-1"></a>        <span class="co"># use 32-b MAC for simplicity</span></span>
<span id="cb29-34"><a href="#cb29-34" aria-hidden="true" tabindex="-1"></a>        output <span class="op">=</span> torch.nn.functional.conv2d(<span class="bu">input</span>.to(torch.int32), weight.to(torch.int32), <span class="va">None</span>, stride, <span class="dv">0</span>, dilation, groups)</span>
<span id="cb29-35"><a href="#cb29-35" aria-hidden="true" tabindex="-1"></a>    <span class="cf">else</span>:</span>
<span id="cb29-36"><a href="#cb29-36" aria-hidden="true" tabindex="-1"></a>        <span class="co"># current version pytorch does not yet support integer-based conv2d() on GPUs</span></span>
<span id="cb29-37"><a href="#cb29-37" aria-hidden="true" tabindex="-1"></a>        output <span class="op">=</span> torch.nn.functional.conv2d(<span class="bu">input</span>.<span class="bu">float</span>(), weight.<span class="bu">float</span>(), <span class="va">None</span>, stride, <span class="dv">0</span>, dilation, groups)</span>
<span id="cb29-38"><a href="#cb29-38" aria-hidden="true" tabindex="-1"></a>        output <span class="op">=</span> output.<span class="bu">round</span>().to(torch.int32)</span>
<span id="cb29-39"><a href="#cb29-39" aria-hidden="true" tabindex="-1"></a>    <span class="cf">if</span> bias <span class="kw">is</span> <span class="kw">not</span> <span class="va">None</span>:</span>
<span id="cb29-40"><a href="#cb29-40" aria-hidden="true" tabindex="-1"></a>        output <span class="op">=</span> output <span class="op">+</span> bias.view(<span class="dv">1</span>, <span class="op">-</span><span class="dv">1</span>, <span class="dv">1</span>, <span class="dv">1</span>)</span>
<span id="cb29-41"><a href="#cb29-41" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb29-42"><a href="#cb29-42" aria-hidden="true" tabindex="-1"></a>    <span class="co">############### YOUR CODE STARTS HERE ###############</span></span>
<span id="cb29-43"><a href="#cb29-43" aria-hidden="true" tabindex="-1"></a>    <span class="co"># hint: this code block should be the very similar to quantized_linear()</span></span>
<span id="cb29-44"><a href="#cb29-44" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb29-45"><a href="#cb29-45" aria-hidden="true" tabindex="-1"></a>    <span class="co"># Step 2: scale the output</span></span>
<span id="cb29-46"><a href="#cb29-46" aria-hidden="true" tabindex="-1"></a>    <span class="co">#         hint: 1. scales are floating numbers, we need to convert output to float as well</span></span>
<span id="cb29-47"><a href="#cb29-47" aria-hidden="true" tabindex="-1"></a>    <span class="co">#               2. the shape of weight scale is [oc, 1, 1, 1] while the shape of output is [batch_size, oc, height, width]</span></span>
<span id="cb29-48"><a href="#cb29-48" aria-hidden="true" tabindex="-1"></a>    real_scale <span class="op">=</span> input_scale <span class="op">*</span> weight_scale.view(<span class="op">-</span><span class="dv">1</span>) <span class="op">/</span> output_scale</span>
<span id="cb29-49"><a href="#cb29-49" aria-hidden="true" tabindex="-1"></a>    output <span class="op">=</span> output.<span class="bu">float</span>() <span class="op">*</span> real_scale.unsqueeze(<span class="dv">1</span>).unsqueeze(<span class="dv">2</span>)</span>
<span id="cb29-50"><a href="#cb29-50" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb29-51"><a href="#cb29-51" aria-hidden="true" tabindex="-1"></a>    <span class="co"># Step 3: shift output by output_zero_point</span></span>
<span id="cb29-52"><a href="#cb29-52" aria-hidden="true" tabindex="-1"></a>    <span class="co">#         hint: one line of code</span></span>
<span id="cb29-53"><a href="#cb29-53" aria-hidden="true" tabindex="-1"></a>    output <span class="op">+=</span> output_zero_point</span>
<span id="cb29-54"><a href="#cb29-54" aria-hidden="true" tabindex="-1"></a>    <span class="co">############### YOUR CODE STARTS HERE ###############</span></span>
<span id="cb29-55"><a href="#cb29-55" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb29-56"><a href="#cb29-56" aria-hidden="true" tabindex="-1"></a>    <span class="co"># Make sure all value lies in the bitwidth-bit range</span></span>
<span id="cb29-57"><a href="#cb29-57" aria-hidden="true" tabindex="-1"></a>    output <span class="op">=</span> output.<span class="bu">round</span>().clamp(<span class="op">*</span>get_quantized_range(feature_bitwidth)).to(torch.int8)</span>
<span id="cb29-58"><a href="#cb29-58" aria-hidden="true" tabindex="-1"></a>    <span class="cf">return</span> output</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</div>
</section>
</section>
</section>
<section id="question-9-10-pts" class="level2">
<h2 class="anchored" data-anchor-id="question-9-10-pts">Question 9 (10 pts)</h2>
<p>마지막으로 모든 것을 종합하여 모델에 대한 훈련 후 <code>int8</code> 양자화를 수행합니다. 모델의 컨볼루션 레이어와 선형 레이어를 하나씩 양자화된 버전으로 변환합니다.</p>
<ol type="1">
<li>먼저, BatchNorm 계층을 이전 convolutional layer에 융합할 것이며, 이는 양자화 전에 하는 표준 관행입니다. BatchNorm을 융합하면 추론 중에 추가 곱셈이 줄어듭니다.</li>
</ol>
<p>융합 모델인 <code>model_fused</code>가 원래 모델과 동일한 정확도를 갖는지도 검증할 예정입니다(BN fusion은 네트워크 기능을 변경하지 않는 동등한 변환입니다).</p>
<div id="cell-80" class="cell" data-outputid="73c86c21-7ed2-4995-f611-5eaf5517a0ad" data-execution_count="38">
<div class="sourceCode cell-code" id="cb30"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb30-1"><a href="#cb30-1" aria-hidden="true" tabindex="-1"></a><span class="kw">def</span> fuse_conv_bn(conv, bn):</span>
<span id="cb30-2"><a href="#cb30-2" aria-hidden="true" tabindex="-1"></a>    <span class="co"># modified from https://mmcv.readthedocs.io/en/latest/_modules/mmcv/cnn/utils/fuse_conv_bn.html</span></span>
<span id="cb30-3"><a href="#cb30-3" aria-hidden="true" tabindex="-1"></a>    <span class="cf">assert</span> conv.bias <span class="kw">is</span> <span class="va">None</span></span>
<span id="cb30-4"><a href="#cb30-4" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb30-5"><a href="#cb30-5" aria-hidden="true" tabindex="-1"></a>    factor <span class="op">=</span> bn.weight.data <span class="op">/</span> torch.sqrt(bn.running_var.data <span class="op">+</span> bn.eps)</span>
<span id="cb30-6"><a href="#cb30-6" aria-hidden="true" tabindex="-1"></a>    conv.weight.data <span class="op">=</span> conv.weight.data <span class="op">*</span> factor.reshape(<span class="op">-</span><span class="dv">1</span>, <span class="dv">1</span>, <span class="dv">1</span>, <span class="dv">1</span>)</span>
<span id="cb30-7"><a href="#cb30-7" aria-hidden="true" tabindex="-1"></a>    conv.bias <span class="op">=</span> nn.Parameter(<span class="op">-</span> bn.running_mean.data <span class="op">*</span> factor <span class="op">+</span> bn.bias.data)</span>
<span id="cb30-8"><a href="#cb30-8" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb30-9"><a href="#cb30-9" aria-hidden="true" tabindex="-1"></a>    <span class="cf">return</span> conv</span>
<span id="cb30-10"><a href="#cb30-10" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb30-11"><a href="#cb30-11" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>(<span class="st">'Before conv-bn fusion: backbone length'</span>, <span class="bu">len</span>(model.backbone))</span>
<span id="cb30-12"><a href="#cb30-12" aria-hidden="true" tabindex="-1"></a><span class="co">#  fuse the batchnorm into conv layers</span></span>
<span id="cb30-13"><a href="#cb30-13" aria-hidden="true" tabindex="-1"></a>recover_model()</span>
<span id="cb30-14"><a href="#cb30-14" aria-hidden="true" tabindex="-1"></a>model_fused <span class="op">=</span> copy.deepcopy(model)</span>
<span id="cb30-15"><a href="#cb30-15" aria-hidden="true" tabindex="-1"></a>fused_backbone <span class="op">=</span> []</span>
<span id="cb30-16"><a href="#cb30-16" aria-hidden="true" tabindex="-1"></a>ptr <span class="op">=</span> <span class="dv">0</span></span>
<span id="cb30-17"><a href="#cb30-17" aria-hidden="true" tabindex="-1"></a><span class="cf">while</span> ptr <span class="op">&lt;</span> <span class="bu">len</span>(model_fused.backbone):</span>
<span id="cb30-18"><a href="#cb30-18" aria-hidden="true" tabindex="-1"></a>    <span class="cf">if</span> <span class="bu">isinstance</span>(model_fused.backbone[ptr], nn.Conv2d) <span class="kw">and</span> <span class="op">\</span></span>
<span id="cb30-19"><a href="#cb30-19" aria-hidden="true" tabindex="-1"></a>        <span class="bu">isinstance</span>(model_fused.backbone[ptr <span class="op">+</span> <span class="dv">1</span>], nn.BatchNorm2d):</span>
<span id="cb30-20"><a href="#cb30-20" aria-hidden="true" tabindex="-1"></a>        fused_backbone.append(fuse_conv_bn(</span>
<span id="cb30-21"><a href="#cb30-21" aria-hidden="true" tabindex="-1"></a>            model_fused.backbone[ptr], model_fused.backbone[ptr<span class="op">+</span> <span class="dv">1</span>]))</span>
<span id="cb30-22"><a href="#cb30-22" aria-hidden="true" tabindex="-1"></a>        ptr <span class="op">+=</span> <span class="dv">2</span></span>
<span id="cb30-23"><a href="#cb30-23" aria-hidden="true" tabindex="-1"></a>    <span class="cf">else</span>:</span>
<span id="cb30-24"><a href="#cb30-24" aria-hidden="true" tabindex="-1"></a>        fused_backbone.append(model_fused.backbone[ptr])</span>
<span id="cb30-25"><a href="#cb30-25" aria-hidden="true" tabindex="-1"></a>        ptr <span class="op">+=</span> <span class="dv">1</span></span>
<span id="cb30-26"><a href="#cb30-26" aria-hidden="true" tabindex="-1"></a>model_fused.backbone <span class="op">=</span> nn.Sequential(<span class="op">*</span>fused_backbone)</span>
<span id="cb30-27"><a href="#cb30-27" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb30-28"><a href="#cb30-28" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>(<span class="st">'After conv-bn fusion: backbone length'</span>, <span class="bu">len</span>(model_fused.backbone))</span>
<span id="cb30-29"><a href="#cb30-29" aria-hidden="true" tabindex="-1"></a><span class="co"># sanity check, no BN anymore</span></span>
<span id="cb30-30"><a href="#cb30-30" aria-hidden="true" tabindex="-1"></a><span class="cf">for</span> m <span class="kw">in</span> model_fused.modules():</span>
<span id="cb30-31"><a href="#cb30-31" aria-hidden="true" tabindex="-1"></a>    <span class="cf">assert</span> <span class="kw">not</span> <span class="bu">isinstance</span>(m, nn.BatchNorm2d)</span>
<span id="cb30-32"><a href="#cb30-32" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb30-33"><a href="#cb30-33" aria-hidden="true" tabindex="-1"></a><span class="co">#  the accuracy will remain the same after fusion</span></span>
<span id="cb30-34"><a href="#cb30-34" aria-hidden="true" tabindex="-1"></a>fused_acc <span class="op">=</span> evaluate(model_fused, dataloader[<span class="st">'test'</span>])</span>
<span id="cb30-35"><a href="#cb30-35" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>(<span class="ss">f'Accuracy of the fused model=</span><span class="sc">{</span>fused_acc<span class="sc">:.2f}</span><span class="ss">%'</span>)</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output cell-output-stdout">
<pre><code>Before conv-bn fusion: backbone length 29
After conv-bn fusion: backbone length 21
Accuracy of the fused model=92.95%</code></pre>
</div>
<div class="cell-output cell-output-display">
<script type="application/vnd.jupyter.widget-view+json">
{"model_id":"ec93185044b7487ab136f5be8fb1ea0c","version_major":2,"version_minor":0,"quarto_mimetype":"application/vnd.jupyter.widget-view+json"}
</script>
</div>
</div>
<ol start="2" type="1">
<li>각 특징 맵의 범위를 얻기 위해 일부 샘플 데이터로 모델을 실행하여 특징 맵의 범위를 얻고, 해당 스케일링 팩터와 제로 포인트를 계산할 수 있습니다.</li>
</ol>
<div id="cell-82" class="cell" data-execution_count="39">
<div class="sourceCode cell-code" id="cb32"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb32-1"><a href="#cb32-1" aria-hidden="true" tabindex="-1"></a><span class="co"># add hook to record the min max value of the activation</span></span>
<span id="cb32-2"><a href="#cb32-2" aria-hidden="true" tabindex="-1"></a>input_activation <span class="op">=</span> {}</span>
<span id="cb32-3"><a href="#cb32-3" aria-hidden="true" tabindex="-1"></a>output_activation <span class="op">=</span> {}</span>
<span id="cb32-4"><a href="#cb32-4" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb32-5"><a href="#cb32-5" aria-hidden="true" tabindex="-1"></a><span class="kw">def</span> add_range_recoder_hook(model):</span>
<span id="cb32-6"><a href="#cb32-6" aria-hidden="true" tabindex="-1"></a>    <span class="im">import</span> functools</span>
<span id="cb32-7"><a href="#cb32-7" aria-hidden="true" tabindex="-1"></a>    <span class="kw">def</span> _record_range(<span class="va">self</span>, x, y, module_name):</span>
<span id="cb32-8"><a href="#cb32-8" aria-hidden="true" tabindex="-1"></a>        x <span class="op">=</span> x[<span class="dv">0</span>]</span>
<span id="cb32-9"><a href="#cb32-9" aria-hidden="true" tabindex="-1"></a>        input_activation[module_name] <span class="op">=</span> x.detach()</span>
<span id="cb32-10"><a href="#cb32-10" aria-hidden="true" tabindex="-1"></a>        output_activation[module_name] <span class="op">=</span> y.detach()</span>
<span id="cb32-11"><a href="#cb32-11" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb32-12"><a href="#cb32-12" aria-hidden="true" tabindex="-1"></a>    all_hooks <span class="op">=</span> []</span>
<span id="cb32-13"><a href="#cb32-13" aria-hidden="true" tabindex="-1"></a>    <span class="cf">for</span> name, m <span class="kw">in</span> model.named_modules():</span>
<span id="cb32-14"><a href="#cb32-14" aria-hidden="true" tabindex="-1"></a>        <span class="cf">if</span> <span class="bu">isinstance</span>(m, (nn.Conv2d, nn.Linear, nn.ReLU)):</span>
<span id="cb32-15"><a href="#cb32-15" aria-hidden="true" tabindex="-1"></a>            all_hooks.append(m.register_forward_hook(</span>
<span id="cb32-16"><a href="#cb32-16" aria-hidden="true" tabindex="-1"></a>                functools.partial(_record_range, module_name<span class="op">=</span>name)))</span>
<span id="cb32-17"><a href="#cb32-17" aria-hidden="true" tabindex="-1"></a>    <span class="cf">return</span> all_hooks</span>
<span id="cb32-18"><a href="#cb32-18" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb32-19"><a href="#cb32-19" aria-hidden="true" tabindex="-1"></a>hooks <span class="op">=</span> add_range_recoder_hook(model_fused)</span>
<span id="cb32-20"><a href="#cb32-20" aria-hidden="true" tabindex="-1"></a>sample_data <span class="op">=</span> <span class="bu">iter</span>(dataloader[<span class="st">'train'</span>]).<span class="fu">__next__</span>()[<span class="dv">0</span>]</span>
<span id="cb32-21"><a href="#cb32-21" aria-hidden="true" tabindex="-1"></a>model_fused(sample_data.cuda())</span>
<span id="cb32-22"><a href="#cb32-22" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb32-23"><a href="#cb32-23" aria-hidden="true" tabindex="-1"></a><span class="co"># remove hooks</span></span>
<span id="cb32-24"><a href="#cb32-24" aria-hidden="true" tabindex="-1"></a><span class="cf">for</span> h <span class="kw">in</span> hooks:</span>
<span id="cb32-25"><a href="#cb32-25" aria-hidden="true" tabindex="-1"></a>    h.remove()</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</div>
<ol start="3" type="1">
<li>마지막으로 모델 양자화를 해보겠습니다. 다음과 같은 매핑으로 모델을 변환합니다.</li>
</ol>
<div class="sourceCode" id="cb33"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb33-1"><a href="#cb33-1" aria-hidden="true" tabindex="-1"></a>nn.Conv2d: QuantizedConv2d,</span>
<span id="cb33-2"><a href="#cb33-2" aria-hidden="true" tabindex="-1"></a>nn.Linear: QuantizedLinear,</span>
<span id="cb33-3"><a href="#cb33-3" aria-hidden="true" tabindex="-1"></a><span class="co"># the following twos are just wrappers, as current</span></span>
<span id="cb33-4"><a href="#cb33-4" aria-hidden="true" tabindex="-1"></a><span class="co"># torch modules do not support int8 data format;</span></span>
<span id="cb33-5"><a href="#cb33-5" aria-hidden="true" tabindex="-1"></a><span class="co"># we will temporarily convert them to fp32 for computation</span></span>
<span id="cb33-6"><a href="#cb33-6" aria-hidden="true" tabindex="-1"></a>nn.MaxPool2d: QuantizedMaxPool2d,</span>
<span id="cb33-7"><a href="#cb33-7" aria-hidden="true" tabindex="-1"></a>nn.AvgPool2d: QuantizedAvgPool2d,</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div id="cell-84" class="cell" data-execution_count="40">
<div class="sourceCode cell-code" id="cb34"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb34-1"><a href="#cb34-1" aria-hidden="true" tabindex="-1"></a><span class="kw">class</span> QuantizedConv2d(nn.Module):</span>
<span id="cb34-2"><a href="#cb34-2" aria-hidden="true" tabindex="-1"></a>    <span class="kw">def</span> <span class="fu">__init__</span>(<span class="va">self</span>, weight, bias,</span>
<span id="cb34-3"><a href="#cb34-3" aria-hidden="true" tabindex="-1"></a>                 input_zero_point, output_zero_point,</span>
<span id="cb34-4"><a href="#cb34-4" aria-hidden="true" tabindex="-1"></a>                 input_scale, weight_scale, output_scale,</span>
<span id="cb34-5"><a href="#cb34-5" aria-hidden="true" tabindex="-1"></a>                 stride, padding, dilation, groups,</span>
<span id="cb34-6"><a href="#cb34-6" aria-hidden="true" tabindex="-1"></a>                 feature_bitwidth<span class="op">=</span><span class="dv">8</span>, weight_bitwidth<span class="op">=</span><span class="dv">8</span>):</span>
<span id="cb34-7"><a href="#cb34-7" aria-hidden="true" tabindex="-1"></a>        <span class="bu">super</span>().<span class="fu">__init__</span>()</span>
<span id="cb34-8"><a href="#cb34-8" aria-hidden="true" tabindex="-1"></a>        <span class="co"># current version Pytorch does not support IntTensor as nn.Parameter</span></span>
<span id="cb34-9"><a href="#cb34-9" aria-hidden="true" tabindex="-1"></a>        <span class="va">self</span>.register_buffer(<span class="st">'weight'</span>, weight)</span>
<span id="cb34-10"><a href="#cb34-10" aria-hidden="true" tabindex="-1"></a>        <span class="va">self</span>.register_buffer(<span class="st">'bias'</span>, bias)</span>
<span id="cb34-11"><a href="#cb34-11" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb34-12"><a href="#cb34-12" aria-hidden="true" tabindex="-1"></a>        <span class="va">self</span>.input_zero_point <span class="op">=</span> input_zero_point</span>
<span id="cb34-13"><a href="#cb34-13" aria-hidden="true" tabindex="-1"></a>        <span class="va">self</span>.output_zero_point <span class="op">=</span> output_zero_point</span>
<span id="cb34-14"><a href="#cb34-14" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb34-15"><a href="#cb34-15" aria-hidden="true" tabindex="-1"></a>        <span class="va">self</span>.input_scale <span class="op">=</span> input_scale</span>
<span id="cb34-16"><a href="#cb34-16" aria-hidden="true" tabindex="-1"></a>        <span class="va">self</span>.register_buffer(<span class="st">'weight_scale'</span>, weight_scale)</span>
<span id="cb34-17"><a href="#cb34-17" aria-hidden="true" tabindex="-1"></a>        <span class="va">self</span>.output_scale <span class="op">=</span> output_scale</span>
<span id="cb34-18"><a href="#cb34-18" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb34-19"><a href="#cb34-19" aria-hidden="true" tabindex="-1"></a>        <span class="va">self</span>.stride <span class="op">=</span> stride</span>
<span id="cb34-20"><a href="#cb34-20" aria-hidden="true" tabindex="-1"></a>        <span class="va">self</span>.padding <span class="op">=</span> (padding[<span class="dv">1</span>], padding[<span class="dv">1</span>], padding[<span class="dv">0</span>], padding[<span class="dv">0</span>])</span>
<span id="cb34-21"><a href="#cb34-21" aria-hidden="true" tabindex="-1"></a>        <span class="va">self</span>.dilation <span class="op">=</span> dilation</span>
<span id="cb34-22"><a href="#cb34-22" aria-hidden="true" tabindex="-1"></a>        <span class="va">self</span>.groups <span class="op">=</span> groups</span>
<span id="cb34-23"><a href="#cb34-23" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb34-24"><a href="#cb34-24" aria-hidden="true" tabindex="-1"></a>        <span class="va">self</span>.feature_bitwidth <span class="op">=</span> feature_bitwidth</span>
<span id="cb34-25"><a href="#cb34-25" aria-hidden="true" tabindex="-1"></a>        <span class="va">self</span>.weight_bitwidth <span class="op">=</span> weight_bitwidth</span>
<span id="cb34-26"><a href="#cb34-26" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb34-27"><a href="#cb34-27" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb34-28"><a href="#cb34-28" aria-hidden="true" tabindex="-1"></a>    <span class="kw">def</span> forward(<span class="va">self</span>, x):</span>
<span id="cb34-29"><a href="#cb34-29" aria-hidden="true" tabindex="-1"></a>        <span class="cf">return</span> quantized_conv2d(</span>
<span id="cb34-30"><a href="#cb34-30" aria-hidden="true" tabindex="-1"></a>            x, <span class="va">self</span>.weight, <span class="va">self</span>.bias,</span>
<span id="cb34-31"><a href="#cb34-31" aria-hidden="true" tabindex="-1"></a>            <span class="va">self</span>.feature_bitwidth, <span class="va">self</span>.weight_bitwidth,</span>
<span id="cb34-32"><a href="#cb34-32" aria-hidden="true" tabindex="-1"></a>            <span class="va">self</span>.input_zero_point, <span class="va">self</span>.output_zero_point,</span>
<span id="cb34-33"><a href="#cb34-33" aria-hidden="true" tabindex="-1"></a>            <span class="va">self</span>.input_scale, <span class="va">self</span>.weight_scale, <span class="va">self</span>.output_scale,</span>
<span id="cb34-34"><a href="#cb34-34" aria-hidden="true" tabindex="-1"></a>            <span class="va">self</span>.stride, <span class="va">self</span>.padding, <span class="va">self</span>.dilation, <span class="va">self</span>.groups</span>
<span id="cb34-35"><a href="#cb34-35" aria-hidden="true" tabindex="-1"></a>            )</span>
<span id="cb34-36"><a href="#cb34-36" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb34-37"><a href="#cb34-37" aria-hidden="true" tabindex="-1"></a><span class="kw">class</span> QuantizedLinear(nn.Module):</span>
<span id="cb34-38"><a href="#cb34-38" aria-hidden="true" tabindex="-1"></a>    <span class="kw">def</span> <span class="fu">__init__</span>(<span class="va">self</span>, weight, bias,</span>
<span id="cb34-39"><a href="#cb34-39" aria-hidden="true" tabindex="-1"></a>                 input_zero_point, output_zero_point,</span>
<span id="cb34-40"><a href="#cb34-40" aria-hidden="true" tabindex="-1"></a>                 input_scale, weight_scale, output_scale,</span>
<span id="cb34-41"><a href="#cb34-41" aria-hidden="true" tabindex="-1"></a>                 feature_bitwidth<span class="op">=</span><span class="dv">8</span>, weight_bitwidth<span class="op">=</span><span class="dv">8</span>):</span>
<span id="cb34-42"><a href="#cb34-42" aria-hidden="true" tabindex="-1"></a>        <span class="bu">super</span>().<span class="fu">__init__</span>()</span>
<span id="cb34-43"><a href="#cb34-43" aria-hidden="true" tabindex="-1"></a>        <span class="co"># current version Pytorch does not support IntTensor as nn.Parameter</span></span>
<span id="cb34-44"><a href="#cb34-44" aria-hidden="true" tabindex="-1"></a>        <span class="va">self</span>.register_buffer(<span class="st">'weight'</span>, weight)</span>
<span id="cb34-45"><a href="#cb34-45" aria-hidden="true" tabindex="-1"></a>        <span class="va">self</span>.register_buffer(<span class="st">'bias'</span>, bias)</span>
<span id="cb34-46"><a href="#cb34-46" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb34-47"><a href="#cb34-47" aria-hidden="true" tabindex="-1"></a>        <span class="va">self</span>.input_zero_point <span class="op">=</span> input_zero_point</span>
<span id="cb34-48"><a href="#cb34-48" aria-hidden="true" tabindex="-1"></a>        <span class="va">self</span>.output_zero_point <span class="op">=</span> output_zero_point</span>
<span id="cb34-49"><a href="#cb34-49" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb34-50"><a href="#cb34-50" aria-hidden="true" tabindex="-1"></a>        <span class="va">self</span>.input_scale <span class="op">=</span> input_scale</span>
<span id="cb34-51"><a href="#cb34-51" aria-hidden="true" tabindex="-1"></a>        <span class="va">self</span>.register_buffer(<span class="st">'weight_scale'</span>, weight_scale)</span>
<span id="cb34-52"><a href="#cb34-52" aria-hidden="true" tabindex="-1"></a>        <span class="va">self</span>.output_scale <span class="op">=</span> output_scale</span>
<span id="cb34-53"><a href="#cb34-53" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb34-54"><a href="#cb34-54" aria-hidden="true" tabindex="-1"></a>        <span class="va">self</span>.feature_bitwidth <span class="op">=</span> feature_bitwidth</span>
<span id="cb34-55"><a href="#cb34-55" aria-hidden="true" tabindex="-1"></a>        <span class="va">self</span>.weight_bitwidth <span class="op">=</span> weight_bitwidth</span>
<span id="cb34-56"><a href="#cb34-56" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb34-57"><a href="#cb34-57" aria-hidden="true" tabindex="-1"></a>    <span class="kw">def</span> forward(<span class="va">self</span>, x):</span>
<span id="cb34-58"><a href="#cb34-58" aria-hidden="true" tabindex="-1"></a>        <span class="cf">return</span> quantized_linear(</span>
<span id="cb34-59"><a href="#cb34-59" aria-hidden="true" tabindex="-1"></a>            x, <span class="va">self</span>.weight, <span class="va">self</span>.bias,</span>
<span id="cb34-60"><a href="#cb34-60" aria-hidden="true" tabindex="-1"></a>            <span class="va">self</span>.feature_bitwidth, <span class="va">self</span>.weight_bitwidth,</span>
<span id="cb34-61"><a href="#cb34-61" aria-hidden="true" tabindex="-1"></a>            <span class="va">self</span>.input_zero_point, <span class="va">self</span>.output_zero_point,</span>
<span id="cb34-62"><a href="#cb34-62" aria-hidden="true" tabindex="-1"></a>            <span class="va">self</span>.input_scale, <span class="va">self</span>.weight_scale, <span class="va">self</span>.output_scale</span>
<span id="cb34-63"><a href="#cb34-63" aria-hidden="true" tabindex="-1"></a>            )</span>
<span id="cb34-64"><a href="#cb34-64" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb34-65"><a href="#cb34-65" aria-hidden="true" tabindex="-1"></a><span class="kw">class</span> QuantizedMaxPool2d(nn.MaxPool2d):</span>
<span id="cb34-66"><a href="#cb34-66" aria-hidden="true" tabindex="-1"></a>    <span class="kw">def</span> forward(<span class="va">self</span>, x):</span>
<span id="cb34-67"><a href="#cb34-67" aria-hidden="true" tabindex="-1"></a>        <span class="co"># current version PyTorch does not support integer-based MaxPool</span></span>
<span id="cb34-68"><a href="#cb34-68" aria-hidden="true" tabindex="-1"></a>        <span class="cf">return</span> <span class="bu">super</span>().forward(x.<span class="bu">float</span>()).to(torch.int8)</span>
<span id="cb34-69"><a href="#cb34-69" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb34-70"><a href="#cb34-70" aria-hidden="true" tabindex="-1"></a><span class="kw">class</span> QuantizedAvgPool2d(nn.AvgPool2d):</span>
<span id="cb34-71"><a href="#cb34-71" aria-hidden="true" tabindex="-1"></a>    <span class="kw">def</span> forward(<span class="va">self</span>, x):</span>
<span id="cb34-72"><a href="#cb34-72" aria-hidden="true" tabindex="-1"></a>        <span class="co"># current version PyTorch does not support integer-based AvgPool</span></span>
<span id="cb34-73"><a href="#cb34-73" aria-hidden="true" tabindex="-1"></a>        <span class="cf">return</span> <span class="bu">super</span>().forward(x.<span class="bu">float</span>()).to(torch.int8)</span>
<span id="cb34-74"><a href="#cb34-74" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb34-75"><a href="#cb34-75" aria-hidden="true" tabindex="-1"></a><span class="co"># we use int8 quantization, which is quite popular</span></span>
<span id="cb34-76"><a href="#cb34-76" aria-hidden="true" tabindex="-1"></a>feature_bitwidth <span class="op">=</span> weight_bitwidth <span class="op">=</span> <span class="dv">8</span></span>
<span id="cb34-77"><a href="#cb34-77" aria-hidden="true" tabindex="-1"></a>quantized_model <span class="op">=</span> copy.deepcopy(model_fused)</span>
<span id="cb34-78"><a href="#cb34-78" aria-hidden="true" tabindex="-1"></a>quantized_backbone <span class="op">=</span> []</span>
<span id="cb34-79"><a href="#cb34-79" aria-hidden="true" tabindex="-1"></a>ptr <span class="op">=</span> <span class="dv">0</span></span>
<span id="cb34-80"><a href="#cb34-80" aria-hidden="true" tabindex="-1"></a><span class="cf">while</span> ptr <span class="op">&lt;</span> <span class="bu">len</span>(quantized_model.backbone):</span>
<span id="cb34-81"><a href="#cb34-81" aria-hidden="true" tabindex="-1"></a>    <span class="cf">if</span> <span class="bu">isinstance</span>(quantized_model.backbone[ptr], nn.Conv2d) <span class="kw">and</span> <span class="op">\</span></span>
<span id="cb34-82"><a href="#cb34-82" aria-hidden="true" tabindex="-1"></a>        <span class="bu">isinstance</span>(quantized_model.backbone[ptr <span class="op">+</span> <span class="dv">1</span>], nn.ReLU):</span>
<span id="cb34-83"><a href="#cb34-83" aria-hidden="true" tabindex="-1"></a>        conv <span class="op">=</span> quantized_model.backbone[ptr]</span>
<span id="cb34-84"><a href="#cb34-84" aria-hidden="true" tabindex="-1"></a>        conv_name <span class="op">=</span> <span class="ss">f'backbone.</span><span class="sc">{</span>ptr<span class="sc">}</span><span class="ss">'</span></span>
<span id="cb34-85"><a href="#cb34-85" aria-hidden="true" tabindex="-1"></a>        relu <span class="op">=</span> quantized_model.backbone[ptr <span class="op">+</span> <span class="dv">1</span>]</span>
<span id="cb34-86"><a href="#cb34-86" aria-hidden="true" tabindex="-1"></a>        relu_name <span class="op">=</span> <span class="ss">f'backbone.</span><span class="sc">{</span>ptr <span class="op">+</span> <span class="dv">1</span><span class="sc">}</span><span class="ss">'</span></span>
<span id="cb34-87"><a href="#cb34-87" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb34-88"><a href="#cb34-88" aria-hidden="true" tabindex="-1"></a>        input_scale, input_zero_point <span class="op">=</span> <span class="op">\</span></span>
<span id="cb34-89"><a href="#cb34-89" aria-hidden="true" tabindex="-1"></a>            get_quantization_scale_and_zero_point(</span>
<span id="cb34-90"><a href="#cb34-90" aria-hidden="true" tabindex="-1"></a>                input_activation[conv_name], feature_bitwidth)</span>
<span id="cb34-91"><a href="#cb34-91" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb34-92"><a href="#cb34-92" aria-hidden="true" tabindex="-1"></a>        output_scale, output_zero_point <span class="op">=</span> <span class="op">\</span></span>
<span id="cb34-93"><a href="#cb34-93" aria-hidden="true" tabindex="-1"></a>            get_quantization_scale_and_zero_point(</span>
<span id="cb34-94"><a href="#cb34-94" aria-hidden="true" tabindex="-1"></a>                output_activation[relu_name], feature_bitwidth)</span>
<span id="cb34-95"><a href="#cb34-95" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb34-96"><a href="#cb34-96" aria-hidden="true" tabindex="-1"></a>        quantized_weight, weight_scale, weight_zero_point <span class="op">=</span> <span class="op">\</span></span>
<span id="cb34-97"><a href="#cb34-97" aria-hidden="true" tabindex="-1"></a>            linear_quantize_weight_per_channel(conv.weight.data, weight_bitwidth)</span>
<span id="cb34-98"><a href="#cb34-98" aria-hidden="true" tabindex="-1"></a>        quantized_bias, bias_scale, bias_zero_point <span class="op">=</span> <span class="op">\</span></span>
<span id="cb34-99"><a href="#cb34-99" aria-hidden="true" tabindex="-1"></a>            linear_quantize_bias_per_output_channel(</span>
<span id="cb34-100"><a href="#cb34-100" aria-hidden="true" tabindex="-1"></a>                conv.bias.data, weight_scale, input_scale)</span>
<span id="cb34-101"><a href="#cb34-101" aria-hidden="true" tabindex="-1"></a>        shifted_quantized_bias <span class="op">=</span> <span class="op">\</span></span>
<span id="cb34-102"><a href="#cb34-102" aria-hidden="true" tabindex="-1"></a>            shift_quantized_conv2d_bias(quantized_bias, quantized_weight,</span>
<span id="cb34-103"><a href="#cb34-103" aria-hidden="true" tabindex="-1"></a>                                        input_zero_point)</span>
<span id="cb34-104"><a href="#cb34-104" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb34-105"><a href="#cb34-105" aria-hidden="true" tabindex="-1"></a>        quantized_conv <span class="op">=</span> QuantizedConv2d(</span>
<span id="cb34-106"><a href="#cb34-106" aria-hidden="true" tabindex="-1"></a>            quantized_weight, shifted_quantized_bias,</span>
<span id="cb34-107"><a href="#cb34-107" aria-hidden="true" tabindex="-1"></a>            input_zero_point, output_zero_point,</span>
<span id="cb34-108"><a href="#cb34-108" aria-hidden="true" tabindex="-1"></a>            input_scale, weight_scale, output_scale,</span>
<span id="cb34-109"><a href="#cb34-109" aria-hidden="true" tabindex="-1"></a>            conv.stride, conv.padding, conv.dilation, conv.groups,</span>
<span id="cb34-110"><a href="#cb34-110" aria-hidden="true" tabindex="-1"></a>            feature_bitwidth<span class="op">=</span>feature_bitwidth, weight_bitwidth<span class="op">=</span>weight_bitwidth</span>
<span id="cb34-111"><a href="#cb34-111" aria-hidden="true" tabindex="-1"></a>        )</span>
<span id="cb34-112"><a href="#cb34-112" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb34-113"><a href="#cb34-113" aria-hidden="true" tabindex="-1"></a>        quantized_backbone.append(quantized_conv)</span>
<span id="cb34-114"><a href="#cb34-114" aria-hidden="true" tabindex="-1"></a>        ptr <span class="op">+=</span> <span class="dv">2</span></span>
<span id="cb34-115"><a href="#cb34-115" aria-hidden="true" tabindex="-1"></a>    <span class="cf">elif</span> <span class="bu">isinstance</span>(quantized_model.backbone[ptr], nn.MaxPool2d):</span>
<span id="cb34-116"><a href="#cb34-116" aria-hidden="true" tabindex="-1"></a>        quantized_backbone.append(QuantizedMaxPool2d(</span>
<span id="cb34-117"><a href="#cb34-117" aria-hidden="true" tabindex="-1"></a>            kernel_size<span class="op">=</span>quantized_model.backbone[ptr].kernel_size,</span>
<span id="cb34-118"><a href="#cb34-118" aria-hidden="true" tabindex="-1"></a>            stride<span class="op">=</span>quantized_model.backbone[ptr].stride</span>
<span id="cb34-119"><a href="#cb34-119" aria-hidden="true" tabindex="-1"></a>            ))</span>
<span id="cb34-120"><a href="#cb34-120" aria-hidden="true" tabindex="-1"></a>        ptr <span class="op">+=</span> <span class="dv">1</span></span>
<span id="cb34-121"><a href="#cb34-121" aria-hidden="true" tabindex="-1"></a>    <span class="cf">elif</span> <span class="bu">isinstance</span>(quantized_model.backbone[ptr], nn.AvgPool2d):</span>
<span id="cb34-122"><a href="#cb34-122" aria-hidden="true" tabindex="-1"></a>        quantized_backbone.append(QuantizedAvgPool2d(</span>
<span id="cb34-123"><a href="#cb34-123" aria-hidden="true" tabindex="-1"></a>            kernel_size<span class="op">=</span>quantized_model.backbone[ptr].kernel_size,</span>
<span id="cb34-124"><a href="#cb34-124" aria-hidden="true" tabindex="-1"></a>            stride<span class="op">=</span>quantized_model.backbone[ptr].stride</span>
<span id="cb34-125"><a href="#cb34-125" aria-hidden="true" tabindex="-1"></a>            ))</span>
<span id="cb34-126"><a href="#cb34-126" aria-hidden="true" tabindex="-1"></a>        ptr <span class="op">+=</span> <span class="dv">1</span></span>
<span id="cb34-127"><a href="#cb34-127" aria-hidden="true" tabindex="-1"></a>    <span class="cf">else</span>:</span>
<span id="cb34-128"><a href="#cb34-128" aria-hidden="true" tabindex="-1"></a>        <span class="cf">raise</span> <span class="pp">NotImplementedError</span>(<span class="bu">type</span>(quantized_model.backbone[ptr]))  <span class="co"># should not happen</span></span>
<span id="cb34-129"><a href="#cb34-129" aria-hidden="true" tabindex="-1"></a>quantized_model.backbone <span class="op">=</span> nn.Sequential(<span class="op">*</span>quantized_backbone)</span>
<span id="cb34-130"><a href="#cb34-130" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb34-131"><a href="#cb34-131" aria-hidden="true" tabindex="-1"></a><span class="co"># finally, quantized the classifier</span></span>
<span id="cb34-132"><a href="#cb34-132" aria-hidden="true" tabindex="-1"></a>fc_name <span class="op">=</span> <span class="st">'classifier'</span></span>
<span id="cb34-133"><a href="#cb34-133" aria-hidden="true" tabindex="-1"></a>fc <span class="op">=</span> model.classifier</span>
<span id="cb34-134"><a href="#cb34-134" aria-hidden="true" tabindex="-1"></a>input_scale, input_zero_point <span class="op">=</span> <span class="op">\</span></span>
<span id="cb34-135"><a href="#cb34-135" aria-hidden="true" tabindex="-1"></a>    get_quantization_scale_and_zero_point(</span>
<span id="cb34-136"><a href="#cb34-136" aria-hidden="true" tabindex="-1"></a>        input_activation[fc_name], feature_bitwidth)</span>
<span id="cb34-137"><a href="#cb34-137" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb34-138"><a href="#cb34-138" aria-hidden="true" tabindex="-1"></a>output_scale, output_zero_point <span class="op">=</span> <span class="op">\</span></span>
<span id="cb34-139"><a href="#cb34-139" aria-hidden="true" tabindex="-1"></a>    get_quantization_scale_and_zero_point(</span>
<span id="cb34-140"><a href="#cb34-140" aria-hidden="true" tabindex="-1"></a>        output_activation[fc_name], feature_bitwidth)</span>
<span id="cb34-141"><a href="#cb34-141" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb34-142"><a href="#cb34-142" aria-hidden="true" tabindex="-1"></a>quantized_weight, weight_scale, weight_zero_point <span class="op">=</span> <span class="op">\</span></span>
<span id="cb34-143"><a href="#cb34-143" aria-hidden="true" tabindex="-1"></a>    linear_quantize_weight_per_channel(fc.weight.data, weight_bitwidth)</span>
<span id="cb34-144"><a href="#cb34-144" aria-hidden="true" tabindex="-1"></a>quantized_bias, bias_scale, bias_zero_point <span class="op">=</span> <span class="op">\</span></span>
<span id="cb34-145"><a href="#cb34-145" aria-hidden="true" tabindex="-1"></a>    linear_quantize_bias_per_output_channel(</span>
<span id="cb34-146"><a href="#cb34-146" aria-hidden="true" tabindex="-1"></a>        fc.bias.data, weight_scale, input_scale)</span>
<span id="cb34-147"><a href="#cb34-147" aria-hidden="true" tabindex="-1"></a>shifted_quantized_bias <span class="op">=</span> <span class="op">\</span></span>
<span id="cb34-148"><a href="#cb34-148" aria-hidden="true" tabindex="-1"></a>    shift_quantized_linear_bias(quantized_bias, quantized_weight,</span>
<span id="cb34-149"><a href="#cb34-149" aria-hidden="true" tabindex="-1"></a>                                input_zero_point)</span>
<span id="cb34-150"><a href="#cb34-150" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb34-151"><a href="#cb34-151" aria-hidden="true" tabindex="-1"></a>quantized_model.classifier <span class="op">=</span> QuantizedLinear(</span>
<span id="cb34-152"><a href="#cb34-152" aria-hidden="true" tabindex="-1"></a>    quantized_weight, shifted_quantized_bias,</span>
<span id="cb34-153"><a href="#cb34-153" aria-hidden="true" tabindex="-1"></a>    input_zero_point, output_zero_point,</span>
<span id="cb34-154"><a href="#cb34-154" aria-hidden="true" tabindex="-1"></a>    input_scale, weight_scale, output_scale,</span>
<span id="cb34-155"><a href="#cb34-155" aria-hidden="true" tabindex="-1"></a>    feature_bitwidth<span class="op">=</span>feature_bitwidth, weight_bitwidth<span class="op">=</span>weight_bitwidth</span>
<span id="cb34-156"><a href="#cb34-156" aria-hidden="true" tabindex="-1"></a>)</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</div>
<p>양자화 과정이 완료되었습니다! 모델 아키텍처를 인쇄하고 시각화하며 양자화된 모델의 정확성도 검증해 보겠습니다.</p>
<section id="question-9.1-5-pts" class="level3">
<h3 class="anchored" data-anchor-id="question-9.1-5-pts">Question 9.1 (5 pts)</h3>
<p>양자화된 모델을 실행하기 위해서는 (0, 1) 범위의 입력 데이터를 (-128, 127) 범위의 <code>int8</code> 범위로 매핑하는 추가적인 전처리가 필요합니다. 이 전처리를 진행하는 아래 코드를 완성하세요.</p>
<p><strong>Hint</strong>: 양자화된 모델은 <code>fp32</code> 모델과 거의 동일한 정확도를 가지고 있습니다.</p>
<div id="cell-87" class="cell" data-outputid="e05f0b96-0345-4578-d26c-bfafd73b9cf4" data-execution_count="42">
<div class="sourceCode cell-code" id="cb35"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb35-1"><a href="#cb35-1" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>(quantized_model)</span>
<span id="cb35-2"><a href="#cb35-2" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb35-3"><a href="#cb35-3" aria-hidden="true" tabindex="-1"></a><span class="kw">def</span> extra_preprocess(x):</span>
<span id="cb35-4"><a href="#cb35-4" aria-hidden="true" tabindex="-1"></a>    <span class="co"># hint: you need to convert the original fp32 input of range (0, 1)</span></span>
<span id="cb35-5"><a href="#cb35-5" aria-hidden="true" tabindex="-1"></a>    <span class="co">#  into int8 format of range (-128, 127)</span></span>
<span id="cb35-6"><a href="#cb35-6" aria-hidden="true" tabindex="-1"></a>    <span class="co">############### YOUR CODE STARTS HERE ###############</span></span>
<span id="cb35-7"><a href="#cb35-7" aria-hidden="true" tabindex="-1"></a>    x_scaled <span class="op">=</span> x <span class="op">*</span> <span class="dv">255</span></span>
<span id="cb35-8"><a href="#cb35-8" aria-hidden="true" tabindex="-1"></a>    x_shifted <span class="op">=</span> x_scaled <span class="op">-</span> <span class="dv">128</span></span>
<span id="cb35-9"><a href="#cb35-9" aria-hidden="true" tabindex="-1"></a>    <span class="cf">return</span> x_shifted.clamp(<span class="op">-</span><span class="dv">128</span>, <span class="dv">127</span>).to(torch.int8)</span>
<span id="cb35-10"><a href="#cb35-10" aria-hidden="true" tabindex="-1"></a>    <span class="co">############### YOUR CODE ENDS HERE #################</span></span>
<span id="cb35-11"><a href="#cb35-11" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb35-12"><a href="#cb35-12" aria-hidden="true" tabindex="-1"></a>int8_model_accuracy <span class="op">=</span> evaluate(quantized_model, dataloader[<span class="st">'test'</span>],</span>
<span id="cb35-13"><a href="#cb35-13" aria-hidden="true" tabindex="-1"></a>                               extra_preprocess<span class="op">=</span>[extra_preprocess])</span>
<span id="cb35-14"><a href="#cb35-14" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>(<span class="ss">f"int8 model has accuracy=</span><span class="sc">{</span>int8_model_accuracy<span class="sc">:.2f}</span><span class="ss">%"</span>)</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output cell-output-stdout">
<pre><code>VGG(
  (backbone): Sequential(
    (0): QuantizedConv2d()
    (1): QuantizedConv2d()
    (2): QuantizedMaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)
    (3): QuantizedConv2d()
    (4): QuantizedConv2d()
    (5): QuantizedMaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)
    (6): QuantizedConv2d()
    (7): QuantizedConv2d()
    (8): QuantizedMaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)
    (9): QuantizedConv2d()
    (10): QuantizedConv2d()
    (11): QuantizedMaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)
    (12): QuantizedAvgPool2d(kernel_size=2, stride=2, padding=0)
  )
  (classifier): QuantizedLinear()
)
int8 model has accuracy=92.90%</code></pre>
</div>
<div class="cell-output cell-output-display">
<script type="application/vnd.jupyter.widget-view+json">
{"model_id":"9d3c65b4e17b47188e3a008174b0cd1a","version_major":2,"version_minor":0,"quarto_mimetype":"application/vnd.jupyter.widget-view+json"}
</script>
</div>
</div>
</section>
</section>
<section id="question-9.2-bonus-question-5-pts" class="level2">
<h2 class="anchored" data-anchor-id="question-9.2-bonus-question-5-pts">Question 9.2 (Bonus Question; 5 pts)</h2>
<p>linear quantized model에 ReLU 층이 없는 이유를 설명하세요.</p>
<p><strong>Your Answer:</strong></p>
<p>선형(Linear) 양자화 모델에서 ReLU(Rectified Linear Unit) 층이 없는 이유는 주로 양자화 과정에서의 데이터 표현 방식과 연산의 효율성과 관련이 있습니다. 양자화는 모델의 가중치나 활성화를 고정된 비트 너비(예: 8비트)의 정수로 제한하여 저장하고 계산하는 기술입니다. 이러한 제한은 모델의 크기를 줄이고, 계산 속도를 향상시키며, 저전력 장치에서의 실행을 용이하게 합니다. 그러나 이 과정에서 데이터의 정밀도가 손실될 수 있으며, 이는 모델 성능에 영향을 미칠 수 있습니다.</p>
<p>ReLU 활성화 함수는 입력이 양수일 경우 그대로 출력하고, 음수일 경우 0으로 만드는 간단하고 효율적인 비선형 함수입니다. ReLU는 딥러닝 모델에서 널리 사용되며, 특히 은닉층에서 비선형성을 추가하여 모델의 표현력을 향상시키는 데 중요한 역할을 합니다.</p>
<p>선형 양자화 모델에서 ReLU 층이 없는 주된 이유는 다음과 같습니다:</p>
<ol type="1">
<li><p><strong>양자화된 데이터의 범위 제한</strong>: 정수 양자화 과정에서는 데이터가 특정 범위 내의 값으로 제한됩니다. 예를 들어, 8비트 양자화에서는 값이 -128부터 127까지의 정수 범위를 가집니다. 이러한 범위 내에서 ReLU를 적용하면 음수 값이 모두 0으로 변환되어, 양수 값만 남게 됩니다. 이 과정에서 데이터의 범위가 더욱 제한되어, 양자화된 모델의 표현력이 더욱 감소할 수 있습니다.</p></li>
<li><p><strong>효율성</strong>: 양자화된 모델은 가능한 한 계산을 간단하게 유지하여 빠른 추론 속도와 낮은 전력 소모를 달성하려고 합니다. ReLU와 같은 비선형 함수를 추가하면, 추론 과정에서 추가적인 계산이 필요하게 됩니다. 어떤 경우에는 모델의 구조나 목적에 따라 이러한 추가 계산 없이도 충분한 성능을 달성할 수 있으므로, ReLU 층을 생략할 수 있습니다.</p></li>
<li><p><strong>모델 설계와 목적</strong>: 특정 양자화 모델에서는 성능 유지를 위해 ReLU 대신 다른 기법이나 활성화 함수를 사용할 수 있습니다. 예를 들어, 양자화 전 모델에서 ReLU를 사용하는 대신, 양자화 과정에서 최적화된 활성화 함수를 선택하거나, ReLU의 효과를 모방할 수 있는 다른 방법을 모색할 수 있습니다.</p></li>
</ol>
<p>결론적으로, 선형 양자화 모델에서 ReLU 층의 부재는 데이터의 범위 제한, 계산 효율성, 그리고 특정 모델 설계와 목적에 기인할 수 있습니다. 모델의 설계자는 성능, 속도, 크기 등의 요구 사항을 균형 있게 고려하여 최적의 모델 구조를 결정해야 합니다.</p>
</section>
</section>
<section id="question-10-5-pts" class="level1">
<h1>Question 10 (5 pts)</h1>
<p>k-means 기반 양자화와 선형 양자화의 장단점을 비교해 보시기 바랍니다. 정확도, 지연 시간, 하드웨어 지원 등의 관점에서 생각해보세요.</p>
<p><strong>Your Answer:</strong></p>
<p>K-means 기반 양자화와 선형 양자화는 데이터의 정밀도를 줄이거나 크기를 축소하는 데 사용되는 두 가지 기술입니다. 이들 기술은 특정 응용 프로그램에 더 적합한 다양한 특성을 가지고 있습니다. 여기에서 정확도, 지연 시간 및 하드웨어 지원의 관점에서 비교해 보겠습니다:</p>
<section id="정확도" class="level3">
<h3 class="anchored" data-anchor-id="정확도">정확도:</h3>
<ul>
<li><strong>K-means 기반 양자화</strong>: 주어진 클러스터 수에 대해 양자화 오류를 최소화하기 때문에 일반적으로 선형 양자화보다 더 높은 정확도를 제공합니다. K-means 양자화는 비슷한 값을 함께 클러스터링하여 데이터 분포에 적응하므로, 특히 데이터가 균일하지 않은 분포를 따를 때 더 많은 정보를 보존합니다.</li>
<li><strong>선형 양자화</strong>: 이 방법은 데이터 값의 전체 범위에 걸쳐 균일한 스케일을 적용합니다. 더 단순하지만, K-means만큼 데이터 분포를 효과적으로 포착하지 못할 수 있으며, 특히 데이터가 균일 분포를 따르지 않는 경우에는 양자화 오류가 더 클 수 있습니다.</li>
</ul>
</section>
<section id="지연-시간" class="level3">
<h3 class="anchored" data-anchor-id="지연-시간">지연 시간:</h3>
<ul>
<li><strong>K-means 기반 양자화</strong>: 최적의 클러스터를 찾는 과정은 계산이 많이 필요하고 특히 큰 데이터셋이나 높은 차원에서 더 느릴 수 있습니다. 이는 K-means 양자화가 선형 양자화에 비해 양자화 과정에서 더 많은 지연 시간을 도입할 수 있음을 의미합니다.</li>
<li><strong>선형 양자화</strong>: 단순성으로 인해, 선형 양자화는 K-means 기반 양자화보다 일반적으로 계산 속도가 더 빠릅니다. 이는 단순한 산술 연산만을 포함하기 때문에, 낮은 지연 시간이 중요한 시나리오에서 더 적합합니다.</li>
</ul>
</section>
<section id="하드웨어-지원" class="level3">
<h3 class="anchored" data-anchor-id="하드웨어-지원">하드웨어 지원:</h3>
<ul>
<li><strong>K-means 기반 양자화</strong>: 클러스터링 알고리즘에 대한 전용 지원 없이 K-means 기반 양자화를 효율적으로 구현하는 것은 어려울 수 있습니다. 현대의 GPU와 전문 가속기(예: TPU)는 이러한 작업을 더 효율적으로 수행할 수 있지만, K-means 알고리즘의 복잡성은 여전히 낮은 전력 또는 임베디드 장치에서 사용을 제한할 수 있습니다.</li>
<li><strong>선형 양자화</strong>: 그 단순함으로 인해 선형 양자화는 저전력 및 임베디드 장치를 포함한 다양한 하드웨어에서 더 쉽게 구현될 수 있습니다. 클러스터링에 관련된 복잡한 연산이 필요하지 않기 때문에, 제한된 계산 리소스를 가진 장치에서 구현하기 더 쉽습니다.</li>
</ul>
</section>
<section id="요약" class="level3">
<h3 class="anchored" data-anchor-id="요약">요약:</h3>
<ul>
<li><strong>K-means 기반 양자화</strong>는 기본 데이터 분포에 더 적응할 수 있어, 계산 복잡성과 지연 시간이 증가하는 대신 더 높은 정확도를 제공할 수 있습니다. 이는 데이터의 고도의 정확성을 유지하는 것이 중요하고 계산 자원이 주요 제약 조건이 아닌 응용 프로그램에 가장 적합합니다.</li>
<li><strong>선형 양자화</strong>는 단순성, 속도 및 광범위한 하드웨어 호환성의 균형을 제공하여, 복잡하거나 균일하지 않은 데이터 분포에 대해 동일한 수준의 정확도를 항상 달성할 수는 없지만 실시간 처리와 처리 능력이 제한된 장치에 적합합니다.</li>
</ul>
<p>응용 프로그램의 특정 요구 사항에 따라 K-means 기반 양자화와 선형 양자화 사이에서 선택해야 하며, 정확성, 처리 지연 시간 및 사용 가능한 계산 리소스의 중요성을 고려해야 합니다.</p>


</section>
</section>

</main> <!-- /main -->
<script type="application/vnd.jupyter.widget-state+json">
{"0066b1cbc43e497ebfe30c38a6174013":{"model_module":"@jupyter-widgets/base","model_module_version":"1.2.0","model_name":"LayoutModel","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"01996e2fccc44cae86d5e3d962031378":{"model_module":"@jupyter-widgets/base","model_module_version":"1.2.0","model_name":"LayoutModel","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":"hidden","width":null}},"0573b041272e4221ae361d139338af87":{"model_module":"@jupyter-widgets/base","model_module_version":"1.2.0","model_name":"LayoutModel","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"05b523786e0d4eb8b3a9c00286049a36":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"DescriptionStyleModel","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"07329c11a50d4cb28e2f4c5e393a2261":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"HTMLModel","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_0066b1cbc43e497ebfe30c38a6174013","placeholder":"​","style":"IPY_MODEL_54cc76818b9140f6960f90dc01e7ed7b","value":"eval:  95%"}},"074f239fb93d44f88f08d72b47a12352":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"FloatProgressModel","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"FloatProgressModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"ProgressView","bar_style":"","description":"","description_tooltip":null,"layout":"IPY_MODEL_8a4985c1c21b42d6aa1947004a2eadda","max":20,"min":0,"orientation":"horizontal","style":"IPY_MODEL_86942f19d33848c5ab819419706df89b","value":20}},"075ae3e1908d4e32bbaa4863f1295d2e":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"DescriptionStyleModel","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"0a306198966148acb3cc158e7575fca3":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"HTMLModel","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_69681a1b580541d292d677d84196cb88","placeholder":"​","style":"IPY_MODEL_c25a71d3a0e647daa72a3c0fe844a076","value":"train: 100%"}},"0caec2e73f9b4f5bbd2759a731926be5":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"FloatProgressModel","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"FloatProgressModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"ProgressView","bar_style":"","description":"","description_tooltip":null,"layout":"IPY_MODEL_e7013681d61e424a8b839bdb3f5c0a42","max":98,"min":0,"orientation":"horizontal","style":"IPY_MODEL_716b121dede84774a795cd429993ebad","value":98}},"0d1bcec1b3f440bab1c31167dca44076":{"model_module":"@jupyter-widgets/base","model_module_version":"1.2.0","model_name":"LayoutModel","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":"hidden","width":null}},"1040798057cf43579bf47d0316626dc7":{"model_module":"@jupyter-widgets/base","model_module_version":"1.2.0","model_name":"LayoutModel","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":"hidden","width":null}},"108b77d207af4e6e9894ea3f8dc03c2d":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"FloatProgressModel","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"FloatProgressModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"ProgressView","bar_style":"","description":"","description_tooltip":null,"layout":"IPY_MODEL_ed5b8f418c944d7e878fef4640d928f5","max":20,"min":0,"orientation":"horizontal","style":"IPY_MODEL_58d88a91496f4868bb8fad7104149788","value":20}},"10a360d3f5ee456ca2e6cb30bc794f8d":{"model_module":"@jupyter-widgets/base","model_module_version":"1.2.0","model_name":"LayoutModel","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"10a61df5f15343069d95be4c8178d907":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"ProgressStyleModel","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"ProgressStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","bar_color":null,"description_width":""}},"10ca2dffa2c64bf9af2c8323fdbda816":{"model_module":"@jupyter-widgets/base","model_module_version":"1.2.0","model_name":"LayoutModel","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"1216c21191e04d61ab4c280c1d85a7e2":{"model_module":"@jupyter-widgets/base","model_module_version":"1.2.0","model_name":"LayoutModel","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":"hidden","width":null}},"133a0bdc7fe54f0bab025af074c0762d":{"model_module":"@jupyter-widgets/base","model_module_version":"1.2.0","model_name":"LayoutModel","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"14087fca563947309b7bb795df81b113":{"model_module":"@jupyter-widgets/base","model_module_version":"1.2.0","model_name":"LayoutModel","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"143e4c8694ba42228a293442c3805363":{"model_module":"@jupyter-widgets/base","model_module_version":"1.2.0","model_name":"LayoutModel","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":"hidden","width":null}},"16895d47c331468c98ce87d08ff25d8d":{"model_module":"@jupyter-widgets/base","model_module_version":"1.2.0","model_name":"LayoutModel","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"1a25fcaa73ef49f2a7eea4ee22522f81":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"DescriptionStyleModel","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"1e47eb6961a449cb901e2bb5e171014f":{"model_module":"@jupyter-widgets/base","model_module_version":"1.2.0","model_name":"LayoutModel","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"1f07bc1b93f248479e20bf962eb43de3":{"model_module":"@jupyter-widgets/base","model_module_version":"1.2.0","model_name":"LayoutModel","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"1f8fed8d37d34f4bbd7e69d0429aad26":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"FloatProgressModel","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"FloatProgressModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"ProgressView","bar_style":"","description":"","description_tooltip":null,"layout":"IPY_MODEL_e1faaa73818e41ec880335ff21ffe48a","max":20,"min":0,"orientation":"horizontal","style":"IPY_MODEL_d34270ca755040dc8c0de858edf1c709","value":20}},"1fd1b640958f4d28b6e35ca59f0b4b98":{"model_module":"@jupyter-widgets/base","model_module_version":"1.2.0","model_name":"LayoutModel","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"202219841a0446869fb17b957f3c8c34":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"DescriptionStyleModel","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"222e114e6b13422a9d470997dd277180":{"model_module":"@jupyter-widgets/base","model_module_version":"1.2.0","model_name":"LayoutModel","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"262f3160a49f483bbcb6055d036ca1df":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"DescriptionStyleModel","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"2676c33bf31540f7b7d7e56e9dce39d3":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"FloatProgressModel","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"FloatProgressModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"ProgressView","bar_style":"","description":"","description_tooltip":null,"layout":"IPY_MODEL_8301ace1df784238b233b8d13cc9943c","max":20,"min":0,"orientation":"horizontal","style":"IPY_MODEL_45cfdfeee463470896a8881b7a382d26","value":20}},"28f6a4eca3e0463d96fb6bb990b1a89a":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"HBoxModel","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HBoxModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HBoxView","box_style":"","children":["IPY_MODEL_33a379c748954889a8daa451645751d7","IPY_MODEL_ed6dd35b928f45639db323f56b63770b","IPY_MODEL_a89f705f6f23450f9229b012a539fbad"],"layout":"IPY_MODEL_4e70b65677784b7b815a399e770f357d"}},"2b294dc599cd4a20bbae60f5969f1833":{"model_module":"@jupyter-widgets/base","model_module_version":"1.2.0","model_name":"LayoutModel","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":"hidden","width":null}},"2d0e7e3d23b9468b9fc99864e56bdc52":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"DescriptionStyleModel","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"2ec20906187841399ca685b782e76f9e":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"HTMLModel","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_3929da007a1a4654b653fbf92079791d","placeholder":"​","style":"IPY_MODEL_73d130fdb3b64ec09a158bee056e02ab","value":" 19/20 [00:01&lt;00:00,  9.17it/s]"}},"2fffee83aa7b4ca4b2b749c092e80c39":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"DescriptionStyleModel","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"30dca68821604625970a13fb1cea2de8":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"HTMLModel","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_685d206c2caf47d78dbbf1449e02fb97","placeholder":"​","style":"IPY_MODEL_a79353f6e5394481aff47d82a2be6f67","value":"eval:  95%"}},"31e6efb5a60b4ea68fa8b538a052e7f3":{"model_module":"@jupyter-widgets/base","model_module_version":"1.2.0","model_name":"LayoutModel","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":"hidden","width":null}},"331174c7d38147ba9b59bbcdb3ec0e0a":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"DescriptionStyleModel","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"334a45ef0e45497e8849d82ea797d6b6":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"DescriptionStyleModel","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"33a379c748954889a8daa451645751d7":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"HTMLModel","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_44f72409a20b48939aa76d4410fe9efc","placeholder":"​","style":"IPY_MODEL_05b523786e0d4eb8b3a9c00286049a36","value":"eval:  95%"}},"35a589253bc24280b4097e930f5230d2":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"HTMLModel","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_ac821de747e14709a4957ad20bc4c3f9","placeholder":"​","style":"IPY_MODEL_3be35102ec324314aa7da4ddebcacad6","value":" 98/98 [00:39&lt;00:00,  2.78it/s]"}},"35e740d946d6415da2810be0e35d7d54":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"HTMLModel","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_3db1c6742bb24479b93add492aea76d9","placeholder":"​","style":"IPY_MODEL_655c50c05962468d977553e341facc94","value":"train: 100%"}},"3616ffe673e7403787b615b60c72208e":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"HTMLModel","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_10a360d3f5ee456ca2e6cb30bc794f8d","placeholder":"​","style":"IPY_MODEL_74da0fab56964a44bcea27c848edf1ba","value":"eval:  95%"}},"3929da007a1a4654b653fbf92079791d":{"model_module":"@jupyter-widgets/base","model_module_version":"1.2.0","model_name":"LayoutModel","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"39f829c1d691446e99aee46fc9ccce3d":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"HTMLModel","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_e93256a3827945e7825756585385d89f","placeholder":"​","style":"IPY_MODEL_40f5eb7f84364a6a8bad62a6b6bd9de4","value":" 19/20 [00:02&lt;00:00,  7.75it/s]"}},"3aabc158e2734e29871c26d283b15c73":{"model_module":"@jupyter-widgets/base","model_module_version":"1.2.0","model_name":"LayoutModel","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"3aaf46d313004b08a9e9494bdfaf7445":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"DescriptionStyleModel","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"3be35102ec324314aa7da4ddebcacad6":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"DescriptionStyleModel","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"3c9a4bc7acb44af9b19970b33a8e4070":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"HBoxModel","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HBoxModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HBoxView","box_style":"","children":["IPY_MODEL_6fdf856502464877a0c9a300b291bed3","IPY_MODEL_2676c33bf31540f7b7d7e56e9dce39d3","IPY_MODEL_f692d80393db4c89ae5d67ac5df5900c"],"layout":"IPY_MODEL_2b294dc599cd4a20bbae60f5969f1833"}},"3d8db82ac03249d9923f3b8417732a56":{"model_module":"@jupyter-widgets/base","model_module_version":"1.2.0","model_name":"LayoutModel","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":"hidden","width":null}},"3db1c6742bb24479b93add492aea76d9":{"model_module":"@jupyter-widgets/base","model_module_version":"1.2.0","model_name":"LayoutModel","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"40f5eb7f84364a6a8bad62a6b6bd9de4":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"DescriptionStyleModel","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"4122b76ebff64e4ba0a1a1a85c704da7":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"HTMLModel","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_977688d36d4a428d8e79352c3244ff1d","placeholder":"​","style":"IPY_MODEL_bd61cdad84e54c79801e7bb97fe738f5","value":" 19/20 [00:01&lt;00:00, 13.57it/s]"}},"42728d0c22104b62a5a8f3aa81e33921":{"model_module":"@jupyter-widgets/base","model_module_version":"1.2.0","model_name":"LayoutModel","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"42ae8ce49c684be48464fd7bf635e605":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"FloatProgressModel","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"FloatProgressModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"ProgressView","bar_style":"","description":"","description_tooltip":null,"layout":"IPY_MODEL_cfc123155d634b3c90d8cede0d998b48","max":20,"min":0,"orientation":"horizontal","style":"IPY_MODEL_10a61df5f15343069d95be4c8178d907","value":20}},"437ef1d5b8cd4c05a722e16764708172":{"model_module":"@jupyter-widgets/base","model_module_version":"1.2.0","model_name":"LayoutModel","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"447b634bdb184e05a6c67c85b0b6b1ba":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"HTMLModel","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_de49b69f79d54c16ae551bfc4f18f2b4","placeholder":"​","style":"IPY_MODEL_202219841a0446869fb17b957f3c8c34","value":"train: 100%"}},"44b90616c4e744dcac005a7362e8693b":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"DescriptionStyleModel","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"44f72409a20b48939aa76d4410fe9efc":{"model_module":"@jupyter-widgets/base","model_module_version":"1.2.0","model_name":"LayoutModel","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"453b40203ecf4032914eca7e7f645ee6":{"model_module":"@jupyter-widgets/base","model_module_version":"1.2.0","model_name":"LayoutModel","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"456e13dfded7478cb6be376dc6ad3eb5":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"HBoxModel","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HBoxModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HBoxView","box_style":"","children":["IPY_MODEL_e494e52ee0e1472bb82ea8b269ad61d4","IPY_MODEL_074f239fb93d44f88f08d72b47a12352","IPY_MODEL_39f829c1d691446e99aee46fc9ccce3d"],"layout":"IPY_MODEL_1040798057cf43579bf47d0316626dc7"}},"45cfdfeee463470896a8881b7a382d26":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"ProgressStyleModel","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"ProgressStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","bar_color":null,"description_width":""}},"484bb157820341b49cbb97de37ad0950":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"ProgressStyleModel","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"ProgressStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","bar_color":null,"description_width":""}},"48c3c6e5f9254c959a1080678e5fc80e":{"model_module":"@jupyter-widgets/base","model_module_version":"1.2.0","model_name":"LayoutModel","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"4b6f8afa67764baaacb7624de9c0baf7":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"DescriptionStyleModel","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"4ce8a84726704c02a40742c95087abfd":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"DescriptionStyleModel","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"4e70b65677784b7b815a399e770f357d":{"model_module":"@jupyter-widgets/base","model_module_version":"1.2.0","model_name":"LayoutModel","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":"hidden","width":null}},"4ee4c739cb224cf989d2df4114747f44":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"DescriptionStyleModel","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"5227ce6720a1461fb2825d57d9237ae2":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"FloatProgressModel","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"FloatProgressModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"ProgressView","bar_style":"","description":"","description_tooltip":null,"layout":"IPY_MODEL_e9df8cea7a4f4a369996d59b6334e667","max":20,"min":0,"orientation":"horizontal","style":"IPY_MODEL_484bb157820341b49cbb97de37ad0950","value":20}},"54b67f4d12a34d20a928f39a47a33652":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"HTMLModel","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_1e47eb6961a449cb901e2bb5e171014f","placeholder":"​","style":"IPY_MODEL_262f3160a49f483bbcb6055d036ca1df","value":"eval:  95%"}},"54cc76818b9140f6960f90dc01e7ed7b":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"DescriptionStyleModel","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"56bc5d3fc44d49cbb163e960a41d2fe5":{"model_module":"@jupyter-widgets/base","model_module_version":"1.2.0","model_name":"LayoutModel","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":"hidden","width":null}},"571a720b338b47cda1fa4e00a5b45fb2":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"HTMLModel","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_a72b6bbc808f45738c52c7914e6637ff","placeholder":"​","style":"IPY_MODEL_1a25fcaa73ef49f2a7eea4ee22522f81","value":" 19/20 [00:01&lt;00:00, 13.17it/s]"}},"57e06c1985ae40538fde8372664ef6b4":{"model_module":"@jupyter-widgets/base","model_module_version":"1.2.0","model_name":"LayoutModel","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"581270004a9e41228c27b1839be960cf":{"model_module":"@jupyter-widgets/base","model_module_version":"1.2.0","model_name":"LayoutModel","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"5838247145d94952a9f40600f6b32b9a":{"model_module":"@jupyter-widgets/base","model_module_version":"1.2.0","model_name":"LayoutModel","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":"hidden","width":null}},"5871bbbaabc146c99c06769c3cbb946e":{"model_module":"@jupyter-widgets/base","model_module_version":"1.2.0","model_name":"LayoutModel","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"58d88a91496f4868bb8fad7104149788":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"ProgressStyleModel","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"ProgressStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","bar_color":null,"description_width":""}},"58e7ef6e1c294277904156e2ba15bf7c":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"HBoxModel","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HBoxModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HBoxView","box_style":"","children":["IPY_MODEL_ed684f87c09b4f14b26a6164f252faaa","IPY_MODEL_5227ce6720a1461fb2825d57d9237ae2","IPY_MODEL_d8f95dae0af74d56a7549575aef6088e"],"layout":"IPY_MODEL_5838247145d94952a9f40600f6b32b9a"}},"5c362921048d4a1e911e2d4e0cd97174":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"ProgressStyleModel","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"ProgressStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","bar_color":null,"description_width":""}},"5c78d17ca3e94c53a2ce2b6ff4b6cc21":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"DescriptionStyleModel","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"5df64008cf134b2885bb161cbb1fb272":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"HBoxModel","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HBoxModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HBoxView","box_style":"","children":["IPY_MODEL_0a306198966148acb3cc158e7575fca3","IPY_MODEL_d94a3d38ccec44f8a8cffe36d6a99609","IPY_MODEL_35a589253bc24280b4097e930f5230d2"],"layout":"IPY_MODEL_eb6312a978cd42d58cdda1222134d1a0"}},"5e2212f3c2c741e4ac314d001b3fe877":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"HTMLModel","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_133a0bdc7fe54f0bab025af074c0762d","placeholder":"​","style":"IPY_MODEL_be2727fcf70c49a6a7ec0c3d18b14aea","value":"eval: 100%"}},"5f371a42616e4cc5b4c5a532ec31fc21":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"HTMLModel","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_8f44d7fa177944cbb5b907b7f70268bd","placeholder":"​","style":"IPY_MODEL_2fffee83aa7b4ca4b2b749c092e80c39","value":"eval:  95%"}},"61372905162d4f3688e6efe7cd8ce18f":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"HTMLModel","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_64b905ee110c4f2ba4206b7d2af9b23d","placeholder":"​","style":"IPY_MODEL_44b90616c4e744dcac005a7362e8693b","value":" 19/20 [00:01&lt;00:00, 12.36it/s]"}},"61d9ea80089e4d00a491f2960a1c72ef":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"ProgressStyleModel","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"ProgressStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","bar_color":null,"description_width":""}},"62c8dc04cb33411284907e34d1993bf0":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"HBoxModel","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HBoxModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HBoxView","box_style":"","children":["IPY_MODEL_6d659797f4334f80be532aa1d6a834e4","IPY_MODEL_fe4b481f81e94f49ae5da9b8ed95a30c","IPY_MODEL_4122b76ebff64e4ba0a1a1a85c704da7"],"layout":"IPY_MODEL_ca1e7664741b41c78de9b425f2ee882f"}},"63f0c80b15664bd885bee132c3b7ab7b":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"DescriptionStyleModel","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"64690d7ede9f45ae96417ebc07d4c57f":{"model_module":"@jupyter-widgets/base","model_module_version":"1.2.0","model_name":"LayoutModel","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"64b905ee110c4f2ba4206b7d2af9b23d":{"model_module":"@jupyter-widgets/base","model_module_version":"1.2.0","model_name":"LayoutModel","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"655c50c05962468d977553e341facc94":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"DescriptionStyleModel","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"66271f3de108438d9dc349984e0f90d0":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"DescriptionStyleModel","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"675b027614364747baa15e1e3cff6860":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"DescriptionStyleModel","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"67de3be88bb8442190329250d23fab8d":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"DescriptionStyleModel","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"685d206c2caf47d78dbbf1449e02fb97":{"model_module":"@jupyter-widgets/base","model_module_version":"1.2.0","model_name":"LayoutModel","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"68b979d60b144deb9f263330a484701c":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"FloatProgressModel","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"FloatProgressModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"ProgressView","bar_style":"","description":"","description_tooltip":null,"layout":"IPY_MODEL_3aabc158e2734e29871c26d283b15c73","max":98,"min":0,"orientation":"horizontal","style":"IPY_MODEL_723c748c545940be9c790e7c3c54b65d","value":98}},"69681a1b580541d292d677d84196cb88":{"model_module":"@jupyter-widgets/base","model_module_version":"1.2.0","model_name":"LayoutModel","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"69bace186e2547a4b0537c5e0d66ff2a":{"model_module":"@jupyter-widgets/base","model_module_version":"1.2.0","model_name":"LayoutModel","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"6be429d8770a489bb15815a27abdc877":{"model_module":"@jupyter-widgets/base","model_module_version":"1.2.0","model_name":"LayoutModel","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"6d659797f4334f80be532aa1d6a834e4":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"HTMLModel","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_e00f523bf8a64c2eb4e2adad93f9cf25","placeholder":"​","style":"IPY_MODEL_3aaf46d313004b08a9e9494bdfaf7445","value":"eval:  95%"}},"6e0ade1309b94b7c8efa2d3bf927ff7f":{"model_module":"@jupyter-widgets/base","model_module_version":"1.2.0","model_name":"LayoutModel","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"6fdf856502464877a0c9a300b291bed3":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"HTMLModel","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_89c0d09cfe8a4211be85f504378ae7ed","placeholder":"​","style":"IPY_MODEL_334a45ef0e45497e8849d82ea797d6b6","value":"eval:  95%"}},"701bd10e4c6f4cd39556de4da0d49477":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"FloatProgressModel","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"FloatProgressModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"ProgressView","bar_style":"","description":"","description_tooltip":null,"layout":"IPY_MODEL_d90bae15c5a34cc9b2e5dff261095197","max":98,"min":0,"orientation":"horizontal","style":"IPY_MODEL_f1a8bd1bbb19495cae5bdc2cd78586df","value":98}},"716b121dede84774a795cd429993ebad":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"ProgressStyleModel","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"ProgressStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","bar_color":null,"description_width":""}},"71d18c640eb54aad81c6b1babb91410a":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"HBoxModel","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HBoxModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HBoxView","box_style":"","children":["IPY_MODEL_5f371a42616e4cc5b4c5a532ec31fc21","IPY_MODEL_108b77d207af4e6e9894ea3f8dc03c2d","IPY_MODEL_2ec20906187841399ca685b782e76f9e"],"layout":"IPY_MODEL_bb8fbb8b28f74bf691c2cb1aa8fb4db6"}},"723c748c545940be9c790e7c3c54b65d":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"ProgressStyleModel","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"ProgressStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","bar_color":null,"description_width":""}},"73a1a44dd9424dddb4c2015f82777a26":{"model_module":"@jupyter-widgets/base","model_module_version":"1.2.0","model_name":"LayoutModel","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"73d130fdb3b64ec09a158bee056e02ab":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"DescriptionStyleModel","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"74da0fab56964a44bcea27c848edf1ba":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"DescriptionStyleModel","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"7728126bc72749c980b553c490ffa10c":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"HBoxModel","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HBoxModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HBoxView","box_style":"","children":["IPY_MODEL_447b634bdb184e05a6c67c85b0b6b1ba","IPY_MODEL_0caec2e73f9b4f5bbd2759a731926be5","IPY_MODEL_ae93cddccf8e478084ab304124899d38"],"layout":"IPY_MODEL_3d8db82ac03249d9923f3b8417732a56"}},"79186f919eb949668ed56c09c7559445":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"HBoxModel","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HBoxModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HBoxView","box_style":"","children":["IPY_MODEL_d857d751f10144288242437956cfd905","IPY_MODEL_1f8fed8d37d34f4bbd7e69d0429aad26","IPY_MODEL_7a368f50d6aa4400b5f6d1ef4626f8af"],"layout":"IPY_MODEL_01996e2fccc44cae86d5e3d962031378"}},"7a368f50d6aa4400b5f6d1ef4626f8af":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"HTMLModel","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_94053fcfd8eb4498991f6968e87a8d8d","placeholder":"​","style":"IPY_MODEL_8f253b6455ec46e69fdf50031d033b7b","value":" 19/20 [00:02&lt;00:00,  8.48it/s]"}},"7b531bfcade3423cb64be695242120e0":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"ProgressStyleModel","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"ProgressStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","bar_color":null,"description_width":""}},"7e1feb29bb5041ca85ff5820ccb217bc":{"model_module":"@jupyter-widgets/base","model_module_version":"1.2.0","model_name":"LayoutModel","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"8007690880a641839351b601fa7c1d2f":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"ProgressStyleModel","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"ProgressStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","bar_color":null,"description_width":""}},"811ac006ec8445689a59933a9e04fc5f":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"ProgressStyleModel","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"ProgressStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","bar_color":null,"description_width":""}},"826a6f6dc3a84fef9192c9dd64165c57":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"HTMLModel","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_8fb4cf79469a464e8d20d26381ad0e0f","placeholder":"​","style":"IPY_MODEL_c0c7d8ee39b34507a3eb7631f3e29a10","value":" 19/20 [00:01&lt;00:00, 13.08it/s]"}},"8301ace1df784238b233b8d13cc9943c":{"model_module":"@jupyter-widgets/base","model_module_version":"1.2.0","model_name":"LayoutModel","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"86942f19d33848c5ab819419706df89b":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"ProgressStyleModel","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"ProgressStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","bar_color":null,"description_width":""}},"878640f5a034497a8e3ed3450e2473da":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"HBoxModel","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HBoxModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HBoxView","box_style":"","children":["IPY_MODEL_9b1f3904b0c2480bb0884f29ae3db97c","IPY_MODEL_701bd10e4c6f4cd39556de4da0d49477","IPY_MODEL_afbe3b3f917e45b19b9cedd5acd423eb"],"layout":"IPY_MODEL_8b6056723cbb4b76b2f2e52af7ad05a1"}},"892325c089bc45b2b51574a424d5038a":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"HBoxModel","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HBoxModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HBoxView","box_style":"","children":["IPY_MODEL_a48e91c1d1e24eb29e023964b192d8c4","IPY_MODEL_9921841229c94fd4b21d4163b76f8fc8","IPY_MODEL_dc2e1d1c09e844389fabfa6c5a1200f4"],"layout":"IPY_MODEL_1216c21191e04d61ab4c280c1d85a7e2"}},"89c0d09cfe8a4211be85f504378ae7ed":{"model_module":"@jupyter-widgets/base","model_module_version":"1.2.0","model_name":"LayoutModel","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"89ce719588124243a0f3c6a24ce4f727":{"model_module":"@jupyter-widgets/base","model_module_version":"1.2.0","model_name":"LayoutModel","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"8a4985c1c21b42d6aa1947004a2eadda":{"model_module":"@jupyter-widgets/base","model_module_version":"1.2.0","model_name":"LayoutModel","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"8aaf109e952c4ee484f3b0eaf3079996":{"model_module":"@jupyter-widgets/base","model_module_version":"1.2.0","model_name":"LayoutModel","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"8b41750a2cd0454c9bd9107fc15b9044":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"ProgressStyleModel","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"ProgressStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","bar_color":null,"description_width":""}},"8b6056723cbb4b76b2f2e52af7ad05a1":{"model_module":"@jupyter-widgets/base","model_module_version":"1.2.0","model_name":"LayoutModel","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":"hidden","width":null}},"8f253b6455ec46e69fdf50031d033b7b":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"DescriptionStyleModel","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"8f44d7fa177944cbb5b907b7f70268bd":{"model_module":"@jupyter-widgets/base","model_module_version":"1.2.0","model_name":"LayoutModel","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"8fb4cf79469a464e8d20d26381ad0e0f":{"model_module":"@jupyter-widgets/base","model_module_version":"1.2.0","model_name":"LayoutModel","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"909cd82f93374a2699cd1c3aa6998e20":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"HTMLModel","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_437ef1d5b8cd4c05a722e16764708172","placeholder":"​","style":"IPY_MODEL_aaa431a4dd7e4e48bcf14f0c30ef31ae","value":"eval:  95%"}},"90cfe1fa9f3d468a892d803902048cb0":{"model_module":"@jupyter-widgets/base","model_module_version":"1.2.0","model_name":"LayoutModel","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"94053fcfd8eb4498991f6968e87a8d8d":{"model_module":"@jupyter-widgets/base","model_module_version":"1.2.0","model_name":"LayoutModel","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"95d59e7001da4015b8150c33c57aeb9f":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"DescriptionStyleModel","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"977688d36d4a428d8e79352c3244ff1d":{"model_module":"@jupyter-widgets/base","model_module_version":"1.2.0","model_name":"LayoutModel","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"98efcfa3799e4cd78cc70516b6800796":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"HBoxModel","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HBoxModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HBoxView","box_style":"","children":["IPY_MODEL_35e740d946d6415da2810be0e35d7d54","IPY_MODEL_68b979d60b144deb9f263330a484701c","IPY_MODEL_c9106ee3c38f4be8a47a5d576797c246"],"layout":"IPY_MODEL_d887156cd55647f39ca551639aa02b75"}},"9921841229c94fd4b21d4163b76f8fc8":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"FloatProgressModel","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"FloatProgressModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"ProgressView","bar_style":"","description":"","description_tooltip":null,"layout":"IPY_MODEL_89ce719588124243a0f3c6a24ce4f727","max":20,"min":0,"orientation":"horizontal","style":"IPY_MODEL_5c362921048d4a1e911e2d4e0cd97174","value":20}},"9b1f3904b0c2480bb0884f29ae3db97c":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"HTMLModel","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_e9d31c73288649dda74886de88681942","placeholder":"​","style":"IPY_MODEL_63f0c80b15664bd885bee132c3b7ab7b","value":"train: 100%"}},"9d2959f8ced04deeb1916cafae2d38c6":{"model_module":"@jupyter-widgets/base","model_module_version":"1.2.0","model_name":"LayoutModel","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"9d3c65b4e17b47188e3a008174b0cd1a":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"HBoxModel","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HBoxModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HBoxView","box_style":"","children":["IPY_MODEL_5e2212f3c2c741e4ac314d001b3fe877","IPY_MODEL_42ae8ce49c684be48464fd7bf635e605","IPY_MODEL_ccd18c22b81b430999275478ea4da2c7"],"layout":"IPY_MODEL_0d1bcec1b3f440bab1c31167dca44076"}},"9e0b193fb8e84a60846447aefa2a45d6":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"FloatProgressModel","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"FloatProgressModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"ProgressView","bar_style":"","description":"","description_tooltip":null,"layout":"IPY_MODEL_f65e0a8dcda744f68f885ed2400d3609","max":20,"min":0,"orientation":"horizontal","style":"IPY_MODEL_811ac006ec8445689a59933a9e04fc5f","value":20}},"9e4e1097cd654550870968193db98f08":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"ProgressStyleModel","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"ProgressStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","bar_color":null,"description_width":""}},"a1dea41e4428454ba0d567a9b1d96dd9":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"HBoxModel","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HBoxModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HBoxView","box_style":"","children":["IPY_MODEL_f4e1df77f6db4782b1591b1fd1349053","IPY_MODEL_e103a84f084f40be94b95e20a4b4c211","IPY_MODEL_e4e2def5fb20480d8f5796e54bd7b69e"],"layout":"IPY_MODEL_56bc5d3fc44d49cbb163e960a41d2fe5"}},"a36c2b2ab8624a1798364b2b3d42f38e":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"FloatProgressModel","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"FloatProgressModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"ProgressView","bar_style":"","description":"","description_tooltip":null,"layout":"IPY_MODEL_6be429d8770a489bb15815a27abdc877","max":98,"min":0,"orientation":"horizontal","style":"IPY_MODEL_ede8ead3822c40a982cd5df7d4e3dbd4","value":98}},"a48e91c1d1e24eb29e023964b192d8c4":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"HTMLModel","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_7e1feb29bb5041ca85ff5820ccb217bc","placeholder":"​","style":"IPY_MODEL_675b027614364747baa15e1e3cff6860","value":"eval:  95%"}},"a5bac86c317e4e728c49580a482d3006":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"ProgressStyleModel","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"ProgressStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","bar_color":null,"description_width":""}},"a72b6bbc808f45738c52c7914e6637ff":{"model_module":"@jupyter-widgets/base","model_module_version":"1.2.0","model_name":"LayoutModel","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"a79353f6e5394481aff47d82a2be6f67":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"DescriptionStyleModel","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"a89f705f6f23450f9229b012a539fbad":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"HTMLModel","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_90cfe1fa9f3d468a892d803902048cb0","placeholder":"​","style":"IPY_MODEL_bc0c2062f5314c4ba1e1da5a5922351d","value":" 19/20 [00:01&lt;00:00, 12.84it/s]"}},"a9c0e0708eea478cbc2ca678f330d06c":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"DescriptionStyleModel","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"aa2d2e2aa47f47fb9416d28395c6485e":{"model_module":"@jupyter-widgets/base","model_module_version":"1.2.0","model_name":"LayoutModel","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"aaa431a4dd7e4e48bcf14f0c30ef31ae":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"DescriptionStyleModel","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"aac39b8f01f44f9d8f9b8eb10d6e306d":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"HTMLModel","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_be661f6f731446da950d9a40f46ce369","placeholder":"​","style":"IPY_MODEL_f5bbcdaf2dc14d96abf85aea6299bef0","value":" 98/98 [00:39&lt;00:00,  2.77it/s]"}},"ac821de747e14709a4957ad20bc4c3f9":{"model_module":"@jupyter-widgets/base","model_module_version":"1.2.0","model_name":"LayoutModel","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"ae93cddccf8e478084ab304124899d38":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"HTMLModel","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_10ca2dffa2c64bf9af2c8323fdbda816","placeholder":"​","style":"IPY_MODEL_2d0e7e3d23b9468b9fc99864e56bdc52","value":" 98/98 [00:40&lt;00:00,  2.50it/s]"}},"afbe3b3f917e45b19b9cedd5acd423eb":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"HTMLModel","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_64690d7ede9f45ae96417ebc07d4c57f","placeholder":"​","style":"IPY_MODEL_4b6f8afa67764baaacb7624de9c0baf7","value":" 98/98 [00:41&lt;00:00,  2.72it/s]"}},"b2dd690c121a4e839701b971987f7cf1":{"model_module":"@jupyter-widgets/base","model_module_version":"1.2.0","model_name":"LayoutModel","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"bb8fbb8b28f74bf691c2cb1aa8fb4db6":{"model_module":"@jupyter-widgets/base","model_module_version":"1.2.0","model_name":"LayoutModel","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":"hidden","width":null}},"bc0c2062f5314c4ba1e1da5a5922351d":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"DescriptionStyleModel","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"bd61cdad84e54c79801e7bb97fe738f5":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"DescriptionStyleModel","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"bd7c1312c5654c9a84dcde9e98d7bbc8":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"DescriptionStyleModel","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"be10dc25ad21438184702da9527af6ee":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"HBoxModel","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HBoxModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HBoxView","box_style":"","children":["IPY_MODEL_3616ffe673e7403787b615b60c72208e","IPY_MODEL_de504142c7314023926fb20cd3ffef88","IPY_MODEL_826a6f6dc3a84fef9192c9dd64165c57"],"layout":"IPY_MODEL_31e6efb5a60b4ea68fa8b538a052e7f3"}},"be2727fcf70c49a6a7ec0c3d18b14aea":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"DescriptionStyleModel","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"be661f6f731446da950d9a40f46ce369":{"model_module":"@jupyter-widgets/base","model_module_version":"1.2.0","model_name":"LayoutModel","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"c0c7d8ee39b34507a3eb7631f3e29a10":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"DescriptionStyleModel","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"c25a71d3a0e647daa72a3c0fe844a076":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"DescriptionStyleModel","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"c3b05046cd4b4313a39266c8c3ca6502":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"DescriptionStyleModel","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"c686627fb9224bb5bd33f55a9efc3b85":{"model_module":"@jupyter-widgets/base","model_module_version":"1.2.0","model_name":"LayoutModel","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"c9106ee3c38f4be8a47a5d576797c246":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"HTMLModel","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_69bace186e2547a4b0537c5e0d66ff2a","placeholder":"​","style":"IPY_MODEL_4ee4c739cb224cf989d2df4114747f44","value":" 98/98 [00:39&lt;00:00,  2.66it/s]"}},"ca1e7664741b41c78de9b425f2ee882f":{"model_module":"@jupyter-widgets/base","model_module_version":"1.2.0","model_name":"LayoutModel","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":"hidden","width":null}},"caebc935fe29412d93bb94e30f0a1c5b":{"model_module":"@jupyter-widgets/base","model_module_version":"1.2.0","model_name":"LayoutModel","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":"hidden","width":null}},"ccd18c22b81b430999275478ea4da2c7":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"HTMLModel","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_222e114e6b13422a9d470997dd277180","placeholder":"​","style":"IPY_MODEL_95d59e7001da4015b8150c33c57aeb9f","value":" 20/20 [00:02&lt;00:00,  7.98it/s]"}},"ccda030d6e8848eaa8fe747bc092cb54":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"HTMLModel","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_42728d0c22104b62a5a8f3aa81e33921","placeholder":"​","style":"IPY_MODEL_f9b31fd7962e4e61810da88a5bf68f34","value":" 19/20 [00:01&lt;00:00, 13.31it/s]"}},"cd7308d24bb44429a97883da24535954":{"model_module":"@jupyter-widgets/base","model_module_version":"1.2.0","model_name":"LayoutModel","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":"hidden","width":null}},"cddfb60ce7ae46dfad90f5062ccad0dc":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"ProgressStyleModel","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"ProgressStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","bar_color":null,"description_width":""}},"ceafa86a09f1479d94cc229dac946156":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"ProgressStyleModel","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"ProgressStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","bar_color":null,"description_width":""}},"cfc123155d634b3c90d8cede0d998b48":{"model_module":"@jupyter-widgets/base","model_module_version":"1.2.0","model_name":"LayoutModel","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"cfe4754798944f8c945048daf225f5f0":{"model_module":"@jupyter-widgets/base","model_module_version":"1.2.0","model_name":"LayoutModel","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":"hidden","width":null}},"d1740474ab484f05a6ec49767b8ff3d7":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"HTMLModel","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_d61bbe54c2a04298a40ccae3697dced8","placeholder":"​","style":"IPY_MODEL_f3e2b9bf100045988fe69ece2b24db49","value":" 19/20 [00:01&lt;00:00, 13.54it/s]"}},"d1ff4589f8a14e4f9159f1e3ee47c32c":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"FloatProgressModel","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"FloatProgressModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"ProgressView","bar_style":"","description":"","description_tooltip":null,"layout":"IPY_MODEL_c686627fb9224bb5bd33f55a9efc3b85","max":20,"min":0,"orientation":"horizontal","style":"IPY_MODEL_a5bac86c317e4e728c49580a482d3006","value":20}},"d34270ca755040dc8c0de858edf1c709":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"ProgressStyleModel","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"ProgressStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","bar_color":null,"description_width":""}},"d61bbe54c2a04298a40ccae3697dced8":{"model_module":"@jupyter-widgets/base","model_module_version":"1.2.0","model_name":"LayoutModel","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"d6bd3821273c4b9ab4bd9bd57101e0b6":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"HTMLModel","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_fb99d90a0e2340ddb6cf01b96294cc96","placeholder":"​","style":"IPY_MODEL_67de3be88bb8442190329250d23fab8d","value":" 98/98 [00:40&lt;00:00,  2.49it/s]"}},"d7180d028e2f4b8d8f26451260631aa5":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"HBoxModel","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HBoxModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HBoxView","box_style":"","children":["IPY_MODEL_deae0b8af3fb4fc7b16df59162070ba4","IPY_MODEL_a36c2b2ab8624a1798364b2b3d42f38e","IPY_MODEL_aac39b8f01f44f9d8f9b8eb10d6e306d"],"layout":"IPY_MODEL_cfe4754798944f8c945048daf225f5f0"}},"d857d751f10144288242437956cfd905":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"HTMLModel","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_453b40203ecf4032914eca7e7f645ee6","placeholder":"​","style":"IPY_MODEL_66271f3de108438d9dc349984e0f90d0","value":"eval:  95%"}},"d887156cd55647f39ca551639aa02b75":{"model_module":"@jupyter-widgets/base","model_module_version":"1.2.0","model_name":"LayoutModel","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":"hidden","width":null}},"d8f95dae0af74d56a7549575aef6088e":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"HTMLModel","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_8aaf109e952c4ee484f3b0eaf3079996","placeholder":"​","style":"IPY_MODEL_c3b05046cd4b4313a39266c8c3ca6502","value":" 19/20 [00:01&lt;00:00, 12.97it/s]"}},"d90bae15c5a34cc9b2e5dff261095197":{"model_module":"@jupyter-widgets/base","model_module_version":"1.2.0","model_name":"LayoutModel","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"d94a3d38ccec44f8a8cffe36d6a99609":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"FloatProgressModel","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"FloatProgressModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"ProgressView","bar_style":"","description":"","description_tooltip":null,"layout":"IPY_MODEL_48c3c6e5f9254c959a1080678e5fc80e","max":98,"min":0,"orientation":"horizontal","style":"IPY_MODEL_8b41750a2cd0454c9bd9107fc15b9044","value":98}},"d96278bdba38478083640c1f4ce6bb51":{"model_module":"@jupyter-widgets/base","model_module_version":"1.2.0","model_name":"LayoutModel","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"d9776aabb37e4e19afe17d4a0fc40906":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"HBoxModel","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HBoxModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HBoxView","box_style":"","children":["IPY_MODEL_f25facbfa3734aaa91d9369917d67da6","IPY_MODEL_dce77bcb0c6745c9a1b1cbdf7dc0a30b","IPY_MODEL_d6bd3821273c4b9ab4bd9bd57101e0b6"],"layout":"IPY_MODEL_cd7308d24bb44429a97883da24535954"}},"dc2e1d1c09e844389fabfa6c5a1200f4":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"HTMLModel","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_73a1a44dd9424dddb4c2015f82777a26","placeholder":"​","style":"IPY_MODEL_bd7c1312c5654c9a84dcde9e98d7bbc8","value":" 19/20 [00:01&lt;00:00, 13.72it/s]"}},"dce77bcb0c6745c9a1b1cbdf7dc0a30b":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"FloatProgressModel","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"FloatProgressModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"ProgressView","bar_style":"","description":"","description_tooltip":null,"layout":"IPY_MODEL_6e0ade1309b94b7c8efa2d3bf927ff7f","max":98,"min":0,"orientation":"horizontal","style":"IPY_MODEL_ceafa86a09f1479d94cc229dac946156","value":98}},"dcfc608db61641c9946e2a3b2922ca23":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"ProgressStyleModel","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"ProgressStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","bar_color":null,"description_width":""}},"de49b69f79d54c16ae551bfc4f18f2b4":{"model_module":"@jupyter-widgets/base","model_module_version":"1.2.0","model_name":"LayoutModel","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"de4ff9cd02f2496a9f4fca3f69193f1b":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"HBoxModel","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HBoxModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HBoxView","box_style":"","children":["IPY_MODEL_30dca68821604625970a13fb1cea2de8","IPY_MODEL_d1ff4589f8a14e4f9159f1e3ee47c32c","IPY_MODEL_d1740474ab484f05a6ec49767b8ff3d7"],"layout":"IPY_MODEL_caebc935fe29412d93bb94e30f0a1c5b"}},"de504142c7314023926fb20cd3ffef88":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"FloatProgressModel","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"FloatProgressModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"ProgressView","bar_style":"","description":"","description_tooltip":null,"layout":"IPY_MODEL_16895d47c331468c98ce87d08ff25d8d","max":20,"min":0,"orientation":"horizontal","style":"IPY_MODEL_cddfb60ce7ae46dfad90f5062ccad0dc","value":20}},"deae0b8af3fb4fc7b16df59162070ba4":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"HTMLModel","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_9d2959f8ced04deeb1916cafae2d38c6","placeholder":"​","style":"IPY_MODEL_5c78d17ca3e94c53a2ce2b6ff4b6cc21","value":"train: 100%"}},"e00f523bf8a64c2eb4e2adad93f9cf25":{"model_module":"@jupyter-widgets/base","model_module_version":"1.2.0","model_name":"LayoutModel","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"e103a84f084f40be94b95e20a4b4c211":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"FloatProgressModel","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"FloatProgressModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"ProgressView","bar_style":"","description":"","description_tooltip":null,"layout":"IPY_MODEL_57e06c1985ae40538fde8372664ef6b4","max":20,"min":0,"orientation":"horizontal","style":"IPY_MODEL_9e4e1097cd654550870968193db98f08","value":20}},"e1faaa73818e41ec880335ff21ffe48a":{"model_module":"@jupyter-widgets/base","model_module_version":"1.2.0","model_name":"LayoutModel","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"e37b0e95a8674beb89c6bd5dfa1f3381":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"FloatProgressModel","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"FloatProgressModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"ProgressView","bar_style":"","description":"","description_tooltip":null,"layout":"IPY_MODEL_1fd1b640958f4d28b6e35ca59f0b4b98","max":20,"min":0,"orientation":"horizontal","style":"IPY_MODEL_8007690880a641839351b601fa7c1d2f","value":20}},"e494e52ee0e1472bb82ea8b269ad61d4":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"HTMLModel","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_b2dd690c121a4e839701b971987f7cf1","placeholder":"​","style":"IPY_MODEL_e7b4c5f2d6eb47a6b28017a2a3282f5c","value":"eval:  95%"}},"e4e2def5fb20480d8f5796e54bd7b69e":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"HTMLModel","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_0573b041272e4221ae361d139338af87","placeholder":"​","style":"IPY_MODEL_fc8f1e649a4444608945c9a38c35740f","value":" 20/20 [00:05&lt;00:00,  5.83it/s]"}},"e7013681d61e424a8b839bdb3f5c0a42":{"model_module":"@jupyter-widgets/base","model_module_version":"1.2.0","model_name":"LayoutModel","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"e7b4c5f2d6eb47a6b28017a2a3282f5c":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"DescriptionStyleModel","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"e93256a3827945e7825756585385d89f":{"model_module":"@jupyter-widgets/base","model_module_version":"1.2.0","model_name":"LayoutModel","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"e9d31c73288649dda74886de88681942":{"model_module":"@jupyter-widgets/base","model_module_version":"1.2.0","model_name":"LayoutModel","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"e9df8cea7a4f4a369996d59b6334e667":{"model_module":"@jupyter-widgets/base","model_module_version":"1.2.0","model_name":"LayoutModel","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"eb6312a978cd42d58cdda1222134d1a0":{"model_module":"@jupyter-widgets/base","model_module_version":"1.2.0","model_name":"LayoutModel","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":"hidden","width":null}},"ec93185044b7487ab136f5be8fb1ea0c":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"HBoxModel","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HBoxModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HBoxView","box_style":"","children":["IPY_MODEL_07329c11a50d4cb28e2f4c5e393a2261","IPY_MODEL_9e0b193fb8e84a60846447aefa2a45d6","IPY_MODEL_ccda030d6e8848eaa8fe747bc092cb54"],"layout":"IPY_MODEL_fb544590050f4d6d92cb7d73701af387"}},"ed5b8f418c944d7e878fef4640d928f5":{"model_module":"@jupyter-widgets/base","model_module_version":"1.2.0","model_name":"LayoutModel","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"ed684f87c09b4f14b26a6164f252faaa":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"HTMLModel","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_aa2d2e2aa47f47fb9416d28395c6485e","placeholder":"​","style":"IPY_MODEL_4ce8a84726704c02a40742c95087abfd","value":"eval:  95%"}},"ed6dd35b928f45639db323f56b63770b":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"FloatProgressModel","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"FloatProgressModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"ProgressView","bar_style":"","description":"","description_tooltip":null,"layout":"IPY_MODEL_581270004a9e41228c27b1839be960cf","max":20,"min":0,"orientation":"horizontal","style":"IPY_MODEL_61d9ea80089e4d00a491f2960a1c72ef","value":20}},"ede8ead3822c40a982cd5df7d4e3dbd4":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"ProgressStyleModel","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"ProgressStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","bar_color":null,"description_width":""}},"f07ea8f1cbf143d7a15dd79dae5814a6":{"model_module":"@jupyter-widgets/base","model_module_version":"1.2.0","model_name":"LayoutModel","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":"hidden","width":null}},"f1a8bd1bbb19495cae5bdc2cd78586df":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"ProgressStyleModel","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"ProgressStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","bar_color":null,"description_width":""}},"f25facbfa3734aaa91d9369917d67da6":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"HTMLModel","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_d96278bdba38478083640c1f4ce6bb51","placeholder":"​","style":"IPY_MODEL_331174c7d38147ba9b59bbcdb3ec0e0a","value":"train: 100%"}},"f3d43ebe333144baabf03ee1c16fec99":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"HBoxModel","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HBoxModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HBoxView","box_style":"","children":["IPY_MODEL_909cd82f93374a2699cd1c3aa6998e20","IPY_MODEL_e37b0e95a8674beb89c6bd5dfa1f3381","IPY_MODEL_571a720b338b47cda1fa4e00a5b45fb2"],"layout":"IPY_MODEL_f07ea8f1cbf143d7a15dd79dae5814a6"}},"f3e2b9bf100045988fe69ece2b24db49":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"DescriptionStyleModel","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"f4e1df77f6db4782b1591b1fd1349053":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"HTMLModel","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_fdc2c77788104418b515b4f432c2a707","placeholder":"​","style":"IPY_MODEL_a9c0e0708eea478cbc2ca678f330d06c","value":"eval: 100%"}},"f53af3dc696841dca7ef29cf7d786c8f":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"HBoxModel","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HBoxModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HBoxView","box_style":"","children":["IPY_MODEL_54b67f4d12a34d20a928f39a47a33652","IPY_MODEL_fbe40a3b2b354599a31db6f7dfe7bf3d","IPY_MODEL_61372905162d4f3688e6efe7cd8ce18f"],"layout":"IPY_MODEL_143e4c8694ba42228a293442c3805363"}},"f5bbcdaf2dc14d96abf85aea6299bef0":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"DescriptionStyleModel","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"f65e0a8dcda744f68f885ed2400d3609":{"model_module":"@jupyter-widgets/base","model_module_version":"1.2.0","model_name":"LayoutModel","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"f692d80393db4c89ae5d67ac5df5900c":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"HTMLModel","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_1f07bc1b93f248479e20bf962eb43de3","placeholder":"​","style":"IPY_MODEL_075ae3e1908d4e32bbaa4863f1295d2e","value":" 19/20 [00:01&lt;00:00, 12.85it/s]"}},"f9b31fd7962e4e61810da88a5bf68f34":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"DescriptionStyleModel","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"fb544590050f4d6d92cb7d73701af387":{"model_module":"@jupyter-widgets/base","model_module_version":"1.2.0","model_name":"LayoutModel","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":"hidden","width":null}},"fb99d90a0e2340ddb6cf01b96294cc96":{"model_module":"@jupyter-widgets/base","model_module_version":"1.2.0","model_name":"LayoutModel","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"fbe40a3b2b354599a31db6f7dfe7bf3d":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"FloatProgressModel","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"FloatProgressModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"ProgressView","bar_style":"","description":"","description_tooltip":null,"layout":"IPY_MODEL_5871bbbaabc146c99c06769c3cbb946e","max":20,"min":0,"orientation":"horizontal","style":"IPY_MODEL_7b531bfcade3423cb64be695242120e0","value":20}},"fc8f1e649a4444608945c9a38c35740f":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"DescriptionStyleModel","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"fdc2c77788104418b515b4f432c2a707":{"model_module":"@jupyter-widgets/base","model_module_version":"1.2.0","model_name":"LayoutModel","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"fe4b481f81e94f49ae5da9b8ed95a30c":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"FloatProgressModel","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"FloatProgressModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"ProgressView","bar_style":"","description":"","description_tooltip":null,"layout":"IPY_MODEL_14087fca563947309b7bb795df81b113","max":20,"min":0,"orientation":"horizontal","style":"IPY_MODEL_dcfc608db61641c9946e2a3b2922ca23","value":20}}}
</script>
<script id="quarto-html-after-body" type="application/javascript">
window.document.addEventListener("DOMContentLoaded", function (event) {
  const toggleBodyColorMode = (bsSheetEl) => {
    const mode = bsSheetEl.getAttribute("data-mode");
    const bodyEl = window.document.querySelector("body");
    if (mode === "dark") {
      bodyEl.classList.add("quarto-dark");
      bodyEl.classList.remove("quarto-light");
    } else {
      bodyEl.classList.add("quarto-light");
      bodyEl.classList.remove("quarto-dark");
    }
  }
  const toggleBodyColorPrimary = () => {
    const bsSheetEl = window.document.querySelector("link#quarto-bootstrap");
    if (bsSheetEl) {
      toggleBodyColorMode(bsSheetEl);
    }
  }
  toggleBodyColorPrimary();  
  const icon = "";
  const anchorJS = new window.AnchorJS();
  anchorJS.options = {
    placement: 'right',
    icon: icon
  };
  anchorJS.add('.anchored');
  const isCodeAnnotation = (el) => {
    for (const clz of el.classList) {
      if (clz.startsWith('code-annotation-')) {                     
        return true;
      }
    }
    return false;
  }
  const clipboard = new window.ClipboardJS('.code-copy-button', {
    text: function(trigger) {
      const codeEl = trigger.previousElementSibling.cloneNode(true);
      for (const childEl of codeEl.children) {
        if (isCodeAnnotation(childEl)) {
          childEl.remove();
        }
      }
      return codeEl.innerText;
    }
  });
  clipboard.on('success', function(e) {
    // button target
    const button = e.trigger;
    // don't keep focus
    button.blur();
    // flash "checked"
    button.classList.add('code-copy-button-checked');
    var currentTitle = button.getAttribute("title");
    button.setAttribute("title", "Copied!");
    let tooltip;
    if (window.bootstrap) {
      button.setAttribute("data-bs-toggle", "tooltip");
      button.setAttribute("data-bs-placement", "left");
      button.setAttribute("data-bs-title", "Copied!");
      tooltip = new bootstrap.Tooltip(button, 
        { trigger: "manual", 
          customClass: "code-copy-button-tooltip",
          offset: [0, -8]});
      tooltip.show();    
    }
    setTimeout(function() {
      if (tooltip) {
        tooltip.hide();
        button.removeAttribute("data-bs-title");
        button.removeAttribute("data-bs-toggle");
        button.removeAttribute("data-bs-placement");
      }
      button.setAttribute("title", currentTitle);
      button.classList.remove('code-copy-button-checked');
    }, 1000);
    // clear code selection
    e.clearSelection();
  });
  function tippyHover(el, contentFn, onTriggerFn, onUntriggerFn) {
    const config = {
      allowHTML: true,
      maxWidth: 500,
      delay: 100,
      arrow: false,
      appendTo: function(el) {
          return el.parentElement;
      },
      interactive: true,
      interactiveBorder: 10,
      theme: 'quarto',
      placement: 'bottom-start',
    };
    if (contentFn) {
      config.content = contentFn;
    }
    if (onTriggerFn) {
      config.onTrigger = onTriggerFn;
    }
    if (onUntriggerFn) {
      config.onUntrigger = onUntriggerFn;
    }
    window.tippy(el, config); 
  }
  const noterefs = window.document.querySelectorAll('a[role="doc-noteref"]');
  for (var i=0; i<noterefs.length; i++) {
    const ref = noterefs[i];
    tippyHover(ref, function() {
      // use id or data attribute instead here
      let href = ref.getAttribute('data-footnote-href') || ref.getAttribute('href');
      try { href = new URL(href).hash; } catch {}
      const id = href.replace(/^#\/?/, "");
      const note = window.document.getElementById(id);
      return note.innerHTML;
    });
  }
  const xrefs = window.document.querySelectorAll('a.quarto-xref');
  const processXRef = (id, note) => {
    // Strip column container classes
    const stripColumnClz = (el) => {
      el.classList.remove("page-full", "page-columns");
      if (el.children) {
        for (const child of el.children) {
          stripColumnClz(child);
        }
      }
    }
    stripColumnClz(note)
    if (id === null || id.startsWith('sec-')) {
      // Special case sections, only their first couple elements
      const container = document.createElement("div");
      if (note.children && note.children.length > 2) {
        container.appendChild(note.children[0].cloneNode(true));
        for (let i = 1; i < note.children.length; i++) {
          const child = note.children[i];
          if (child.tagName === "P" && child.innerText === "") {
            continue;
          } else {
            container.appendChild(child.cloneNode(true));
            break;
          }
        }
        if (window.Quarto?.typesetMath) {
          window.Quarto.typesetMath(container);
        }
        return container.innerHTML
      } else {
        if (window.Quarto?.typesetMath) {
          window.Quarto.typesetMath(note);
        }
        return note.innerHTML;
      }
    } else {
      // Remove any anchor links if they are present
      const anchorLink = note.querySelector('a.anchorjs-link');
      if (anchorLink) {
        anchorLink.remove();
      }
      if (window.Quarto?.typesetMath) {
        window.Quarto.typesetMath(note);
      }
      // TODO in 1.5, we should make sure this works without a callout special case
      if (note.classList.contains("callout")) {
        return note.outerHTML;
      } else {
        return note.innerHTML;
      }
    }
  }
  for (var i=0; i<xrefs.length; i++) {
    const xref = xrefs[i];
    tippyHover(xref, undefined, function(instance) {
      instance.disable();
      let url = xref.getAttribute('href');
      let hash = undefined; 
      if (url.startsWith('#')) {
        hash = url;
      } else {
        try { hash = new URL(url).hash; } catch {}
      }
      if (hash) {
        const id = hash.replace(/^#\/?/, "");
        const note = window.document.getElementById(id);
        if (note !== null) {
          try {
            const html = processXRef(id, note.cloneNode(true));
            instance.setContent(html);
          } finally {
            instance.enable();
            instance.show();
          }
        } else {
          // See if we can fetch this
          fetch(url.split('#')[0])
          .then(res => res.text())
          .then(html => {
            const parser = new DOMParser();
            const htmlDoc = parser.parseFromString(html, "text/html");
            const note = htmlDoc.getElementById(id);
            if (note !== null) {
              const html = processXRef(id, note);
              instance.setContent(html);
            } 
          }).finally(() => {
            instance.enable();
            instance.show();
          });
        }
      } else {
        // See if we can fetch a full url (with no hash to target)
        // This is a special case and we should probably do some content thinning / targeting
        fetch(url)
        .then(res => res.text())
        .then(html => {
          const parser = new DOMParser();
          const htmlDoc = parser.parseFromString(html, "text/html");
          const note = htmlDoc.querySelector('main.content');
          if (note !== null) {
            // This should only happen for chapter cross references
            // (since there is no id in the URL)
            // remove the first header
            if (note.children.length > 0 && note.children[0].tagName === "HEADER") {
              note.children[0].remove();
            }
            const html = processXRef(null, note);
            instance.setContent(html);
          } 
        }).finally(() => {
          instance.enable();
          instance.show();
        });
      }
    }, function(instance) {
    });
  }
      let selectedAnnoteEl;
      const selectorForAnnotation = ( cell, annotation) => {
        let cellAttr = 'data-code-cell="' + cell + '"';
        let lineAttr = 'data-code-annotation="' +  annotation + '"';
        const selector = 'span[' + cellAttr + '][' + lineAttr + ']';
        return selector;
      }
      const selectCodeLines = (annoteEl) => {
        const doc = window.document;
        const targetCell = annoteEl.getAttribute("data-target-cell");
        const targetAnnotation = annoteEl.getAttribute("data-target-annotation");
        const annoteSpan = window.document.querySelector(selectorForAnnotation(targetCell, targetAnnotation));
        const lines = annoteSpan.getAttribute("data-code-lines").split(",");
        const lineIds = lines.map((line) => {
          return targetCell + "-" + line;
        })
        let top = null;
        let height = null;
        let parent = null;
        if (lineIds.length > 0) {
            //compute the position of the single el (top and bottom and make a div)
            const el = window.document.getElementById(lineIds[0]);
            top = el.offsetTop;
            height = el.offsetHeight;
            parent = el.parentElement.parentElement;
          if (lineIds.length > 1) {
            const lastEl = window.document.getElementById(lineIds[lineIds.length - 1]);
            const bottom = lastEl.offsetTop + lastEl.offsetHeight;
            height = bottom - top;
          }
          if (top !== null && height !== null && parent !== null) {
            // cook up a div (if necessary) and position it 
            let div = window.document.getElementById("code-annotation-line-highlight");
            if (div === null) {
              div = window.document.createElement("div");
              div.setAttribute("id", "code-annotation-line-highlight");
              div.style.position = 'absolute';
              parent.appendChild(div);
            }
            div.style.top = top - 2 + "px";
            div.style.height = height + 4 + "px";
            div.style.left = 0;
            let gutterDiv = window.document.getElementById("code-annotation-line-highlight-gutter");
            if (gutterDiv === null) {
              gutterDiv = window.document.createElement("div");
              gutterDiv.setAttribute("id", "code-annotation-line-highlight-gutter");
              gutterDiv.style.position = 'absolute';
              const codeCell = window.document.getElementById(targetCell);
              const gutter = codeCell.querySelector('.code-annotation-gutter');
              gutter.appendChild(gutterDiv);
            }
            gutterDiv.style.top = top - 2 + "px";
            gutterDiv.style.height = height + 4 + "px";
          }
          selectedAnnoteEl = annoteEl;
        }
      };
      const unselectCodeLines = () => {
        const elementsIds = ["code-annotation-line-highlight", "code-annotation-line-highlight-gutter"];
        elementsIds.forEach((elId) => {
          const div = window.document.getElementById(elId);
          if (div) {
            div.remove();
          }
        });
        selectedAnnoteEl = undefined;
      };
        // Handle positioning of the toggle
    window.addEventListener(
      "resize",
      throttle(() => {
        elRect = undefined;
        if (selectedAnnoteEl) {
          selectCodeLines(selectedAnnoteEl);
        }
      }, 10)
    );
    function throttle(fn, ms) {
    let throttle = false;
    let timer;
      return (...args) => {
        if(!throttle) { // first call gets through
            fn.apply(this, args);
            throttle = true;
        } else { // all the others get throttled
            if(timer) clearTimeout(timer); // cancel #2
            timer = setTimeout(() => {
              fn.apply(this, args);
              timer = throttle = false;
            }, ms);
        }
      };
    }
      // Attach click handler to the DT
      const annoteDls = window.document.querySelectorAll('dt[data-target-cell]');
      for (const annoteDlNode of annoteDls) {
        annoteDlNode.addEventListener('click', (event) => {
          const clickedEl = event.target;
          if (clickedEl !== selectedAnnoteEl) {
            unselectCodeLines();
            const activeEl = window.document.querySelector('dt[data-target-cell].code-annotation-active');
            if (activeEl) {
              activeEl.classList.remove('code-annotation-active');
            }
            selectCodeLines(clickedEl);
            clickedEl.classList.add('code-annotation-active');
          } else {
            // Unselect the line
            unselectCodeLines();
            clickedEl.classList.remove('code-annotation-active');
          }
        });
      }
  const findCites = (el) => {
    const parentEl = el.parentElement;
    if (parentEl) {
      const cites = parentEl.dataset.cites;
      if (cites) {
        return {
          el,
          cites: cites.split(' ')
        };
      } else {
        return findCites(el.parentElement)
      }
    } else {
      return undefined;
    }
  };
  var bibliorefs = window.document.querySelectorAll('a[role="doc-biblioref"]');
  for (var i=0; i<bibliorefs.length; i++) {
    const ref = bibliorefs[i];
    const citeInfo = findCites(ref);
    if (citeInfo) {
      tippyHover(citeInfo.el, function() {
        var popup = window.document.createElement('div');
        citeInfo.cites.forEach(function(cite) {
          var citeDiv = window.document.createElement('div');
          citeDiv.classList.add('hanging-indent');
          citeDiv.classList.add('csl-entry');
          var biblioDiv = window.document.getElementById('ref-' + cite);
          if (biblioDiv) {
            citeDiv.innerHTML = biblioDiv.innerHTML;
          }
          popup.appendChild(citeDiv);
        });
        return popup.innerHTML;
      });
    }
  }
});
</script>
<script src="https://giscus.app/client.js" data-repo="TINYML-KOR/blog" data-repo-id="R_kgDOLC9iGA" data-category="General" data-category-id="DIC_kwDOLC9iGM4Cc7eP" data-mapping="title" data-reactions-enabled="1" data-emit-metadata="0" data-input-position="top" data-theme="light" data-lang="en" crossorigin="anonymous" async="">
</script>
<input type="hidden" id="giscus-base-theme" value="light">
<input type="hidden" id="giscus-alt-theme" value="dark">
</div> <!-- /content -->




</body></html>