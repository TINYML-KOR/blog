[
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "Study Archive",
    "section": "",
    "text": "ìŠ¤í„°ë”” ìë£Œì™€ ê´€ë ¨í•´ì„œ ì–´ë–¤ í† ì˜ë‚˜ ì˜ê²¬ ëª¨ë‘ ê°ì‚¬í•©ë‹ˆë‹¤! Github Discussionì— ê¸€ì„ ë‚¨ê²¨ì£¼ì…”ë„ ì¢‹ê³  ê° í¬ìŠ¤íŒ… í•˜ë‹¨ì— ìˆëŠ” Giscus ëŒ“ê¸€ì°½ì— ì½”ë©˜íŠ¸ë“¤ì„ ë‚¨ê²¨ì£¼ì‹œë©´ ë©ë‹ˆë‹¤.\nWelcome to any comments or opinions on our content! You can leave messages using the direct Discussion or the Giscus window on each post.\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nğŸ‘©â€ğŸ’» Lab 2\n\n\n\n\n\n\nlab\n\n\nquantization\n\n\nlinear\n\n\nkmeans\n\n\n\nK-means & Linear Quantization\n\n\n\n\n\nMar 6, 2024\n\n\nJung Yeon Lee\n\n\n\n\n\n\n\n\n\n\n\n\nğŸ§‘â€ğŸ« Lecture 5-6\n\n\n\n\n\n\nlecture\n\n\nquantization\n\n\n\nQuantization\n\n\n\n\n\nMar 5, 2024\n\n\nSeunghyun Oh\n\n\n\n\n\n\n\n\n\n\n\n\nğŸ§‘â€ğŸ« Lecture 4\n\n\n\n\n\n\npruning\n\n\nlecture\n\n\n\nPruning and Sparsity (Part II)\n\n\n\n\n\nFeb 18, 2024\n\n\nJung Yeon Lee\n\n\n\n\n\n\n\n\n\n\n\n\nğŸ§‘â€ğŸ« Lecture 3\n\n\n\n\n\n\npruning\n\n\nlecture\n\n\n\nPruning and Sparsity (Part I)\n\n\n\n\n\nJan 28, 2024\n\n\nSeunghyun Oh\n\n\n\n\n\n\n\n\n\n\n\n\nğŸ‘©â€ğŸ’» Lab 0\n\n\n\n\n\n\nlab\n\n\npytorch\n\n\nbasic\n\n\n\nPyTorch Tutorial\n\n\n\n\n\nJan 11, 2024\n\n\nJung Yeon Lee\n\n\n\n\n\n\nNo matching items"
  },
  {
    "objectID": "posts/lecs/lec03.html",
    "href": "posts/lecs/lec03.html",
    "title": "ğŸ§‘â€ğŸ« Lecture 3",
    "section": "",
    "text": "ì•ìœ¼ë¡œ ì´ 5ì¥ì— ê±¸ì³ì„œ ë”¥ëŸ¬ë‹ ëª¨ë¸ ê²½ëŸ‰í™” ê¸°ë²•ë“¤ì— ëŒ€í•´ì„œ ì†Œê°œí•˜ë ¤ê³  í•œë‹¤. ê²½ëŸ‰í™” ê¸°ë²•ìœ¼ë¡œëŠ” Pruning, Quantization, Neural Network Architecture Search, Knowledge Distillation, ê·¸ë¦¬ê³  Tiny Engineì—ì„œ ëŒë¦¬ê¸° ìœ„í•œ ë°©ë²•ì„ ì§„í–‰í•  ì˜ˆì •ì¸ë° ë³¸ ë‚´ìš©ì€ MITì—ì„œ Song Han êµìˆ˜ë‹˜ì´ Fall 2022ì— í•œ ê°•ì˜ TinyML and Efficient Deep Learning Computing 6.S965ë¥¼ ë°”íƒ•ìœ¼ë¡œ ì¬ì •ë¦¬í•œ ë‚´ìš©ì´ë‹¤. ê°•ì˜ ìë£Œì™€ ì˜ìƒì€ ì´ ë§í¬ë¥¼ ì°¸ì¡°í•˜ì!\nì²« ë²ˆì§¸ ë‚´ìš©ìœ¼ë¡œ â€œê°€ì§€ì¹˜ê¸°â€ë¼ëŠ” ì˜ë¯¸ë¥¼ ê°€ì§„ Pruningì— ëŒ€í•´ì„œ ì´ì•¼ê¸°, ì‹œì‘!"
  },
  {
    "objectID": "posts/lecs/lec03.html#introduction-to-pruning",
    "href": "posts/lecs/lec03.html#introduction-to-pruning",
    "title": "ğŸ§‘â€ğŸ« Lecture 3",
    "section": "1. Introduction to Pruning",
    "text": "1. Introduction to Pruning\nPruningì´ë€ ì˜ë¯¸ì²˜ëŸ¼ Neural Networkì—ì„œ ë§¤ê°œë³€ìˆ˜(ë…¸ë“œ)ë¥¼ ì œê±°í•˜ëŠ” ë°©ë²•ì…ë‹ˆë‹¤. ì´ëŠ” Dropoutí•˜ê³  ë¹„ìŠ·í•œ ì˜ë¯¸ë¡œ ë³¼ ìˆ˜ ìˆëŠ”ë°, Dropoutì˜ ê²½ìš° ëª¨ë¸ í›ˆë ¨ ë„ì¤‘ ëœë¤ì ìœ¼ë¡œ íŠ¹ì • ë…¸ë“œë¥¼ ì œì™¸ì‹œí‚¤ê³  í›ˆë ¨ì‹œì¼œ ëª¨ë¸ì˜ Robustnessë¥¼ ë†’ì´ëŠ” ë°©ë²•ìœ¼ë¡œ í›ˆë ¨ì„ í•˜ê³ ë‚˜ì„œë„ ëª¨ë¸ì˜ ë…¸ë“œëŠ” ê·¸ëŒ€ë¡œ ìœ ì§€ê°€ ëœë‹¤. ë°˜ë©´ Pruningì˜ ê²½ìš° í›ˆë ¨ì„ ë§ˆì¹œ í›„ì—, íŠ¹ì • Threshold ì´í•˜ì˜ ë§¤ê°œë³€ìˆ˜(ë…¸ë“œ)ì˜ ê²½ìš° ì‹œ Neural Networkì—ì„œ ì œì™¸ì‹œì¼œ ëª¨ë¸ì˜ í¬ê¸°ë¥¼ ì¤„ì´ë©´ì„œ ë™ì‹œì— ì¶”ë¡  ì†ë„ ë˜í•œ ë†’ì¼ ìˆ˜ ìˆë‹¤.\n\\[\n\\underset{W_p}{argmin}\\ L(x;W_p), \\text{ subject to } \\lvert\\lvert W_p\\lvert\\lvert_0\\ &lt; N\n\\]\n\nL represents the objective function for neural network training\n\\(x\\) is input, \\(W\\) is original weights, \\(W_p\\) is pruned weights\n\\(\\lvert\\lvert W_p\\lvert\\lvert_0\\) calcuates the #nonzeros in \\(W_p\\) and \\(N\\) is the target #nonzeros\n\nì´ëŠ” ìœ„ì™€ ê°™ì€ ì‹ìœ¼ë¡œ í‘œí˜„í•  ìˆ˜ ìˆë‹¤. íŠ¹ì • W ì˜ ê²½ìš° 0 ìœ¼ë¡œ ë§Œë“¤ì–´ ë…¸ë“œë¥¼ ì—†ì• ëŠ” ê²½ìš°ë¼ê³  ë³¼ ìˆ˜ ìˆê² ìŠµë‹ˆë‹¤. ê·¸ë ‡ê²Œ Pruningí•œ Neural NetworkëŠ” ì•„ë˜ ê·¸ë¦¼ ì²˜ëŸ¼ ëœë‹¤.\n\n\n\nReference. MIT-TinyML-lecture03-Pruning-1\n\n\nê·¸ëŸ¼ ì™œ Pruningì„ í•˜ëŠ” ê±¸ê¹Œ? ê°•ì˜ì—ì„œ Pruningì„ ì‚¬ìš©í•˜ë©´ Latency, Memeoryì™€ ê°™ì€ ë¦¬ì†ŒìŠ¤ë¥¼ í™•ë³´í•  ìˆ˜ ìˆë‹¤ê³  ê´€ë ¨ëœ ì•„ë˜ê°™ì€ ì—°êµ¬ê²°ê³¼ë¥¼ ê°™ì´ ë³´ì—¬ì¤€ë‹¤.\n\n\n\nReference. MIT-TinyML-lecture03-Pruning-1\n\n\nSong Han êµìˆ˜ë‹˜ì€ Vision ë”¥ëŸ¬ë‹ ëª¨ë¸ ê²½ëŸ‰í™” ì—°êµ¬ë¥¼ ì£¼ë¡œí•˜ì…”ì„œ, CNNì„ ê¸°ë°˜ìœ¼ë¡œ í•œ ëª¨ë¸ì„ ì˜ˆì‹œë¡œ ë³´ì—¬ì£¼ì‹ ë‹¤. ëª¨ë‘ Pruningì´í›„ì— ëª¨ë¸ ì‚¬ì´ì¦ˆì˜ ê²½ìš° ìµœëŒ€ 12ë°° ì¤„ì–´ ë“¤ë©° ì—°ì‚°ì˜ ê²½ìš° 6.3ë°°ê¹Œì§€ ì¤„ì–´ ë“  ê²ƒì„ ë³¼ ìˆ˜ ë‹¤.\nê·¸ë ‡ë‹¤ë©´ ì €ë ‡ê²Œ â€œí¬ê¸°ê°€ ì¤„ì–´ë“  ëª¨ë¸ì´ ì„±ëŠ¥ì„ ìœ ì§€í•  ìˆ˜ ìˆì„ê¹Œ?â€œ\n\n\n\nReference. MIT-TinyML-lecture03-Pruning-1\n\n\nê·¸ë˜í”„ì—ì„œ ëª¨ë¸ì˜ Weight ë¶„í¬ë„ë¥¼ ìœ„ ê·¸ë¦¼ì—ì„œ ë³´ë©´, Pruningì„ í•˜ê³  ë‚œ ì´í›„ì— Weight ë¶„í¬ë„ì˜ ì¤‘ì‹¬ì— íŒŒë¼ë¯¸í„°ê°€ ì˜ë ¤ë‚˜ê°„ ê²Œ ë³´ì¸ë‹¤. ì´í›„ Fine Tuningì„ í•˜ê³  ë‚œ ë‹¤ìŒì˜ ë¶„í¬ê°€ ë‚˜ì™€ ìˆëŠ”ë°, ì–´ëŠ ì •ë„ ì •í™•ë„ëŠ” ë–¨ì–´ì§€ì§€ë§Œ ì„±ëŠ¥ì´ ìœ ì§€ë˜ëŠ” ê±¸ ê´€ì°°í•  ìˆ˜ ìˆë‹¤.\n\n\n\nReference. MIT-TinyML-lecture03-Pruning-1\n\n\nê·¸ëŸ° Fine tuningì„ ë°˜ë³µì ìœ¼ë¡œ í•˜ê²Œ ëœë‹¤ë©´(Iterative Pruning and Fine tuning) ê·¸ë˜í”„ì—ì„œëŠ” ìµœëŒ€ 90í”„ë¡œ ì´ìƒì˜ íŒŒë¼ë¯¸í„°ë¥¼ ëœì–´ë‚¼ ìˆ˜ ìˆë‹¤ê³  í•œë‹¤.\në¬¼ë¡  íŠ¹ì • ëª¨ë¸ì—ì„œ, íŠ¹ì • Taskë¥¼ ëŒ€ìƒìœ¼ë¡œ í•œ ê²ƒì´ë¼ ì¼ë°˜í™”í•  ìˆ˜ëŠ” ì—†ì§€ë§Œ ë¦¬ì†ŒìŠ¤ë¥¼ ê³ ë ¤í•˜ëŠ” ìƒí™©ì´ë¼ë©´ ì¶©ë¶„íˆ ì‹œë„í•´ë³¼ ë§Œí•œ ê°€ì¹˜ê°€ ìˆì–´ ë³´ì¸ë‹¤. ê·¸ëŸ¼ ì´ë ‡ê²Œ ì„±ëŠ¥ì„ ìœ ì§€í•˜ë©´ì„œ Pruningì„ í•˜ê¸° ìœ„í•´ì„œ ì–´ë–¤ ìš”ì†Œë¥¼ ê³ ë ¤í•´ì•¼ í• ì§€ ë” ìì„¸íˆ ì´ì•¼ê¸°í•´ë³´ì!\nì†Œê°œí•˜ëŠ” ê³ ë ¤ìš”ì†ŒëŠ” ì•„ë˜ì™€ ê°™ë‹¤. Pruning íŒ¨í„´ë¶€í„° ì°¨ë¡€ëŒ€ë¡œ ì‹œì‘!\n\nPruning Granularity â†’ Pruning íŒ¨í„´\nPruning Criterion â†’ ì–¼ë§ˆë§Œí¼ì— íŒŒë¼ë¯¸í„°ë¥¼ Pruning í•  ê±´ê°€?\nPruning Ratio â†’ ì „ì²´ íŒŒë¼ë¯¸í„°ì—ì„œ Pruningì„ ì–¼ë§ˆë§Œí¼ì˜ ë¹„ìœ¨ë¡œ?\nFine Turning â†’ Pruning ì´í›„ì— ì–´ë–»ê²Œ Fine-Tuning í•  ê±´ê°€?\nADMM â†’ Pruning ì´í›„, ì–´ë–»ê²Œ Convexê°€ ëœë‹¤ê³  í•  ìˆ˜ ìˆì§€?\nLottery Ticket Hypothesis â†’ Trainingë¶€í„° Pruningê¹Œì§€ ëª¨ë¸ì„ ë§Œë“¤ì–´ ë³´ì!\nSystem Support â†’ í•˜ë“œì›¨ì–´ë‚˜ ì†Œí”„íŠ¸ì›¨ì–´ì ìœ¼ë¡œ Pruningì„ ì§€ì›í•˜ëŠ” ê²½ìš°ëŠ”?"
  },
  {
    "objectID": "posts/lecs/lec03.html#determine-the-pruning-granularity",
    "href": "posts/lecs/lec03.html#determine-the-pruning-granularity",
    "title": "ğŸ§‘â€ğŸ« Lecture 3",
    "section": "2. Determine the Pruning Granularity",
    "text": "2. Determine the Pruning Granularity\n\n\n\nThe case of convolutional layers, red box is preserved and white one is pruned referred from MIT-TinyML-lecture03-Pruning-1\n\n\nì—¬ê¸°ì„œ ê³ ë ¤ìš”ì†ŒëŠ” â€œì–¼ë§ˆë§Œí¼ ë‰´ëŸ°ì„ ê·¸ë£¹í™”í•˜ì—¬ ê³ ë ¤í•  ê²ƒì¸ê°€?â€ ì…ë‹ˆë‹¤. Regularí•œ ì •ë¡œë„ ë¶„ë¥˜í•˜ë©´ì„œ Irregularí•œ ê²½ìš°ì™€ Regularí•œ ê²½ìš°ì˜ íŠ¹ì§•ì„ ì•„ë˜ì²˜ëŸ¼ ë§í•©ë‹ˆë‹¤.\n\nFine-grained/Unstructured\n\nMore flexible pruning index choice\nHard to accelerate (irregular data expression)\nCan deliver speed up on some custom hardware\n\nCoarse-grained/Structured\n\nLess flexible pruning index choice (a subset of the fine-grained case)\nEasy to accelerate\n\n\nPruningì„ í•œë‹¤ê³  ëª¨ë¸ ì¶œë ¥ì´ ë‚˜ì˜¤ëŠ” ì‹œê°„ì´ ì§§ì•„ì§€ëŠ” ê²ƒì´ ì•„ë‹˜ë„ ì–¸ê¸‰í•©ë‹ˆë‹¤. Hardware Accelerationì˜ ê°€ëŠ¥ë„ê°€ ìˆëŠ”ë°, ì´ íŠ¹ì§•ì„ ë³´ë©´ ì•Œ ìˆ˜ ìˆë“¯, Pruningì˜ ììœ ë„ì™€ Hardware Accelerationì´ trade-off, ì¦‰ ê²½ëŸ‰í™” ì •ë„ì™€ Latencyì‚¬ì´ì— trade-off ê°€ ìˆì„ ê²ƒì´ ì˜ˆì¸¡ë©ë‹ˆë‹¤. í•˜ë‚˜ì”©, ìë£Œë¥¼ ë³´ë©´ì„œ ì‚´í´ ë³´ê² ìŠµë‹ˆë‹¤.\n\n2.1 Pattern-based Pruning\nIrregularì—ì„œë„ Pattern-based Pruningì€ ì—°ì†ì ì¸ ë‰´ëŸ° Mê°œ ì¤‘ Nê°œë¥¼ Pruning í•˜ëŠ” ë°©ë²•ì´ë‹¤. ì¼ë°˜ì ìœ¼ë¡œëŠ” N:M = 2:4 ìœ¼ë¡œ í•œë‹¤ê³  ì†Œê°œí•œë‹¤.\n\n\n\nReference. Accelerating Inference with Sparsity Using the NVIDIA Ampere Architecture and NVIDIA TensorRT\n\n\nReference. Accelerating Inference with Sparsity Using the NVIDIA Ampere Architecture and NVIDIA TensorRT\nì˜ˆì‹œë¥¼ ë“¤ì–´ ë³´ë©´, ìœ„ì™€ ê°™ì€ Matrixì—ì„œ í–‰ì„ ë³´ì‹œë©´ 8ê°œì˜ Weightì¤‘ 4ê°œê°€ Non-zeroì¸ ê²ƒì„ ë³¼ ìˆ˜ ìˆìŠµë‹ˆë‹¤. ì—¬ê¸°ì„œ Zeroì¸ ë¶€ë¶„ì„ ì—†ì• ê³  2bit indexë¡œ í•˜ì—¬ Matrix ì—°ì‚°ì„ í•˜ë©´ Nvidiaâ€™s Ampere GPUì—ì„œ ì†ë„ë¥¼ 2ë°°ê¹Œì§€ ë†’ì¼ ìˆ˜ ìˆë‹¤ê³  í•œë‹¤. ì—¬ê¸°ì„œ SparsityëŠ” â€œì–¼ë§ˆë§Œí¼ ê²½ëŸ‰í™” ëëŠ”ì§€?â€ ì´ë¼ê³  ìƒê°í•˜ë©´ ëœë‹¤.\n\nN:M sparsity means that in each contiguous M elements, N of them is pruned\nA classic case is 2:4 sparsity (50% sparsity)\nIt is supported by Nvidiaâ€™s Ampere GPU Architecture, which delivers up to 2x speed up and usually maintains accuracy.\n\n\n\n\nReference. https://developer.nvidia.com/blog/accelerating-inference-with-sparsity-using-ampere-and-tensorrt/\n\n\n\n\n2.2 Channel-level Pruning\në°˜ëŒ€ë¡œ íŒ¨í„´ì´ ìƒëŒ€ì ìœ¼ë¡œ regular í•œ ìª½ì¸ Channel-level Pruningì€ ì¶”ë¡ ì‹œê°„ì„ ì¤„ì¼ ìˆ˜ ìˆëŠ” ë°˜ë©´ì— ê²½ëŸ‰í™” ë¹„ìœ¨ì´ ì ë‹¤ê³  ë§í•œë‹¤. ì•„ë˜ ê·¸ë¦¼ì„ ë³´ì‹œë©´ Layerë§ˆë‹¤ Sparsityê°€ ë‹¤ë¥¸ ê±¸ ë³´ì‹¤ ìˆ˜ ìˆë‹¤.\n\nPro: Direct speed up!\nCon: smaller compression ratio\n\n\n\n\nReference AMC: Automl for Model Compression and Acceleration on Mobile Devices [HeÂ et al., ECCV 2018]\n\n\nì•„ë˜ì— ìë£Œì—ì„œëŠ” Channel ë³„ë¡œ í•œ Pruningì˜ ê²½ìš° ì „ì²´ ë‰´ë ¨ì„ ê°€ì§€ê³  í•œ Pruningë³´ë‹¤ ì¶”ë¡  ì‹œê°„ì„ ë” ì¤„ì¼ ìˆ˜ ìˆë‹¤ê³  ë§í•œë‹¤.\n\n\n\nReference AMC: Automl for Model Compression and Acceleration on Mobile Devices [HeÂ et al., ECCV 2018]\n\n\nìë£Œë¥¼ ë³´ë©´ Sparsityì—ì„œëŠ” íŒ¨í„´í™” ë¼ ìˆìœ¼ë©´ ê°€ì†í™”ê°€ ìš©ì´í•´ Latency, ì¶”ë¡  ì‹œê°„ì„ ì¤„ì¼ ìˆ˜ ìˆì§€ë§Œ ê·¸ ë§Œí¼ Pruningí•˜ëŠ” ë‰´ëŸ°ì˜ ìˆ˜ê°€ ì ì–´ ê²½ëŸ‰í™” ë¹„ìœ¨ì´ ì¤„ ê²ƒìœ¼ë¡œ ë³´ì¸ë‹¤. í•˜ì§€ë§Œ ë¹„êµì  ë¶ˆê·œì¹™í•œ ìª½ì— ì†í•˜ëŠ” Pattern-based Pruningì˜ ê²½ìš°ê°€ í•˜ë“œì›¨ì–´ì—ì„œ ì§€ì›í•´ì£¼ëŠ” ê²½ìš°, ëª¨ë¸ í¬ê¸°ì™€ Latencyë¥¼ ë‘˜ ë‹¤ ìµœì ìœ¼ë¡œ ì¡ì„ ìˆ˜ ìˆì„ ê²ƒìœ¼ë¡œ ë³´ì¸ë‹¤."
  },
  {
    "objectID": "posts/lecs/lec03.html#determine-the-pruning-criterion",
    "href": "posts/lecs/lec03.html#determine-the-pruning-criterion",
    "title": "ğŸ§‘â€ğŸ« Lecture 3",
    "section": "3. Determine the Pruning Criterion",
    "text": "3. Determine the Pruning Criterion\nê·¸ë ‡ë‹¤ë©´ ì–´ë–¤ íŒŒë¼ë¯¸í„°ë¥¼ ê°€ì§€ëŠ” ë‰´ëŸ°ì„ ìš°ë¦¬ëŠ” ì˜ë¼ë‚´ì•¼ í• ê¹Œìš”? Synapseì™€ Neuronìœ¼ë¡œ ë‚˜ëˆ ì„œ ì‚´í´ë³´ì.\n\nWhich synapses? Which neurons? Which one is less important?\nHow to Select Synapses and Select Neurons to Prune\n\n\n3.1 Select of Synapses\ní¬ê²Œ ì„¸ ê°€ì§€ë¡œ ë¶„ë¥˜í•˜ëŠ”ë°, ê° ë‰´ëŸ°ì˜ í¬ê¸°, ê° ì±„ë„ì— ì „ì²´ ë‰´ëŸ°ì— ëŒ€í•œ í¬ê¸°, ê·¸ë¦¬ê³  í…Œì¼ëŸ¬ ê¸‰ìˆ˜ë¥¼ ì´ìš©í•˜ì—¬ gradientì™€ weightë¥¼ ëª¨ë‘ ê³ ë ¤í•œ í¬ê¸°ë¥¼ ì†Œê°œí•œë‹¤. Song han êµìˆ˜ë‹˜ì´ ë°©ë²•ë“¤ì„ ì†Œê°œí•˜ê¸°ì— ì•ì„œì„œ ìœ ìˆ˜ì˜ ê¸°ì—…ë“¤ë„ ì§€ë‚œ 5ë…„ ë™ì•ˆ ì£¼ë¡œ Magnitude-based Pruningë§Œì„ ì‚¬ìš©í•´ì™”ë‹¤ê³  í•˜ëŠ”ë°, 2023ë…„ì´ ë¼ì„œ On-device AIê°€ ê°ê´‘ë°›ê¸° ì‹œì‘í•´ì„œ ì ì°¨ì ìœ¼ë¡œ ê´€ì‹¬ì„ ë°›ê¸° ì‹œì‘í•œ ê±´ê°€ ì‹¶ê¸°ë„ í•˜ë‹¤.\n3.1.1 Magnitude-based Pruning\ní¬ê¸°ë¥¼ ê¸°ì¤€ìœ¼ë¡œ í•˜ëŠ” ê²½ìš°, â€œì–¼ë§ˆë§Œí¼ ë‰´ëŸ° ê·¸ë£¹ì—ì„œ ê³ ë ¤í•  ê²ƒì¸ê°€?â€ì™€ â€œê·¸ë£¹ë‚´ì—ì„œ ì–´ë–¤ ì •ê·œí™”ë¥¼ ì‚¬ìš©í•  ê²ƒì¸ê°€?ë¥¼ ê³ ë ¤í•œë‹¤.\n\nHeuristic pruning criterion, Element-wise Pruning\n\\[\nImportance = \\lvert W \\lvert\n\\]\n\n\n\nReference. MIT-TinyML-lecture03-Pruning-1\n\n\nHeuristic pruning criterion, Row-wise Pruning, L1-norm magnitude\n\\[\nImportance = \\sum_{i\\in S}\\lvert w_i \\lvert, \\\\where\\ W^{(S)}\\ is\\ the\\ structural\\ set\\ S\\ of\\ parameters\\ W\n\\]\n\n\n\nReference. MIT-TinyML-lecture03-Pruning-1\n\n\nHeuristic pruning criterion, Row-wise Pruning, L2-norm magnitude\n\\[\nImportance = \\sum_{i\\in S}\\lvert w_i \\lvert, \\\\where\\ W^{(S)}\\ is\\ the\\ structural\\ set\\ S\\ of\\ parameters\\ W\n\\]\n\n\n\nReference. MIT-TinyML-lecture03-Pruning-1\n\n\nHeuristic pruning criterion, \\(L_p\\)- norm\n\\[\n\\lvert\\lvert W^{(S)}\\lvert\\lvert=\\huge( \\large\n   \\sum_{i\\in S} \\lvert w_i \\lvert^p\n\\huge) \\large^{\\frac{1}{p}}\n\\]\n\n3.1.2 Scaling-based Pruning\në‘ ë²ˆì§¸ë¡œ Scalingì„ í•˜ëŠ” ê²½ìš° ì±„ë„ë§ˆë‹¤ Scaling Factorë¥¼ ë‘¬ì„œ Pruningì„ í•œë‹¤. ê·¸ëŸ¼ Scaling Factorë¥¼ ì–´ë–»ê²Œ ë‘¬ì•¼ í• ê¹Œ? ê°•ì˜ì—ì„œ ì†Œê°œí•˜ëŠ” ì´ ë…¼ë¬¸ì—ì„œëŠ” Scaling factor \\(\\gamma\\) íŒŒë¼ë¯¸í„°ë¥¼ trainable íŒŒë¼ë¯¸í„°ë¡œ ë‘ë©´ì„œ batch normalization layerì— ì‚¬ìš©í•œë‹¤.\n\nScale factor is associated with each filter(i.e.Â output channel) in convolution layers.\nThe filters or output channels with small scaling factor magnitude will be pruned\nThe scaling factors can be reused from batch normalization layer\n\\[\n  z_o = \\gamma\\dfrac{z_i-\\mu_{B}}{\\sqrt{\\sigma_B^2+\\epsilon}}+\\beta\n  \\]\n\n\n\n\nReference. MIT-TinyML-lecture03-Pruning-1\n\n\n3.1.3 Talyor Expansion Analysis on Pruning Error\nì„¸ ë²ˆì§¸ ë°©ë²•ì€ í…Œì¼ëŸ¬ ê¸‰ìˆ˜ë¥¼ ì´ìš©í•˜ì—¬ Objective functionì„ ìµœì†Œí™” í•˜ëŠ” ì§€ì ì„ ì°¾ëŠ” ë°©ë²•ì…ë‹ˆë‹¤. Talyor Seriesì— ëŒ€í•œ ìì„¸í•œ ë‚´ìš©ì€ ì—¬ê¸°ì„œ!\n\nEvaluate pruning error induced by pruning synapses.\nMinimize the objective function L(x; W)\nA Taylor series can approximate the induced error.\n\n\\[\n\\delta L = L(x;W)-L(x;W_p=W-\\delta W) \\\\ = \\sum_i g_i\\delta w_i + \\frac{1}{2} \\sum_i h_{ii}\\delta w_i^2 + \\frac{1}{2}\\sum_{i\\not=j}h_{ij}\\delta w_i \\delta w_j + O(\\lvert\\lvert \\delta W \\lvert\\lvert^3)\n\\] \\[\nwhere\\ g_i=\\dfrac{\\delta L}{\\delta w_i}, h_{i, j} = \\dfrac{\\delta^2 L}{\\delta w_i \\delta w_j}\n\\]\n\nSecond-Order-based Pruning\n\n\n\nReference. MIT-TinyML-lecture03-Pruning-1\n\n\n\n\n\nReference. MIT-TinyML-lecture03-Pruning-1\n\n\nOptimal Brain Damage[LeCunÂ et al.,Â NeurIPS 1989] ë…¼ë¬¸ì—ì„œëŠ” ì´ ë°©ë²•ì„ ì´ìš©í•˜ê¸° ìœ„í•´ ì„¸ ê°€ì§€ë¥¼ ê°€ì •í•œë‹¤.\n\nObjective function Lì´ quadratic ì´ê¸° ë•Œë¬¸ì— ë§ˆì§€ë§‰ í•­ì´ ë¬´ì‹œëœë‹¤(ì´ëŠ” Talyor Seriesì˜ Error í•­ì„ ì•Œë©´ ì´í•´ê°€ ë” ì‰½ë‹¤!)\në§Œì•½ ì‹ ê²½ë§ì´ ìˆ˜ë ´í•˜ê²Œë˜ë©´, ì²« ë²ˆì§¸í•­ë„ ë¬´ì‹œëœë‹¤.\nê° íŒŒë¼ë¯¸í„°ê°€ ë…ë¦½ì ì´ë¼ë©´ Cross-termë„ ë¬´ì‹œëœë‹¤.\n\nê·¸ëŸ¬ë©´ ì‹ì„ ì•„ë˜ì²˜ëŸ¼ ì •ë¦¬í•  ìˆ˜ ìˆëŠ”ë°, ì¤‘ìš”í•œ ë¶€ë¶„ì€ Hessian Matrix Hì— ì‚¬ìš©í•˜ëŠ” Computationì´ ì–´ë µë‹¤ëŠ” ì !\n\\[\n\\delta L_i = L(x;W)-L(x;W_p\\lvert w_i=0)\\approx \\dfrac{1}{2} h_{ii}w_i^2,\\ where\\ h_{ii}=\\dfrac{\\partial^2 L}{\\partial w_i \\partial w_j}\n\\]\n\\[\nimportance_{w_i} = \\lvert \\delta L_i\\lvert = \\frac{1}{2}h_{ii}w_i^2\n\\] \\[\n*\\ h_{ii} \\text{ is non-negative}\n\\]\nFirst-Order-based Pruning\n\nì°¸ê³ ë¡œ ì´ ë°©ë²•ì€ 2023ë…„ì—ëŠ” ì†Œê°œí•˜ì§€ ì•ŠëŠ”ë‹¤.\n\n\n\n\nReference. MIT-TinyML-lecture03-Pruning-1\n\n\n\nIf only first-order expansion is considered under an i.i.d(Independent and identically distributed) assumption,\n\n\\[\n\\delta L_i = L(x;W) - L(x; W_P\\lvert w_i=0) \\approx g_iw_i,\\ where\\ g_i=\\dfrac{\\partial L}{\\partial w_i}\n\\] \\[\nimportance_{w_i} = \\lvert \\delta L_i \\lvert = \\lvert g_i w_i \\lvert \\ or \\ importance_{w_i} = \\lvert \\delta L_i \\lvert^2 = (g_i w_i)^2\n\\]\n\nFor coarse-grained pruning, we have,\n\\[\n  importance_{\\ W^{(S)}} = \\sum_{i \\in S}\\lvert \\delta L_i \\lvert^2 = \\sum_{i \\in S} (g_i w_i)^2,\\ where \\ W^{(S)}is\\ the\\ structural\\ set\\ of\\ parameters\n  \\]\n\n\n\n\n3.2 Select of Neurons\nì–´ë–¤ Neuronì„ ì—†ì•¨ ì§€ë¥¼ ê³ ë ¤(Less useful â†’ Remove) í•œ ì´ ë°©ë²•ì€ Neuronì˜ ê²½ìš°ë„ ìˆì§€ë§Œ ì•„ë˜ ê·¸ë¦¼ì²˜ëŸ¼ Channelë¡œ ê³ ë ¤í•  ìˆ˜ë„ ìˆë‹¤. í™•ì‹¤íˆ ì „ì— ì†Œê°œí–ˆë˜ ë°©ë²•ë“¤ë³´ë‹¤ â€œCoarse-grained pruningâ€ì¸ ë°©ë²•ì´ë‹¤.\n\n\nPercentage-of-Zero-based Pruning\nì²«ë²ˆì§¸ëŠ” Channelë§ˆë‹¤ 0ì˜ ë¹„ìœ¨ì„ ë´ì„œ ë¹„ìœ¨ì´ ë†’ì€ Channel ì„ ì—†ë‚´ëŠ” ë°©ë²•ì´ë‹¤. ReLU activationì„ ì‚¬ìš©í•˜ë©´ Outputì´ 0ì´ ë‚˜ì˜¤ëŠ”ë°, ì—¬ê¸°ì„œ 0ì˜ ë¹„ìœ¨, Average Percentage of Zero activations(APoZ)ë¼ê³  ë¶€ë¥´ëŠ” ê²ƒì„ ë³´ê³  ê°€ì§€ì¹˜ê¸°í•  Channelì„ ì œê±°í•œë‹¤.\n\nReLU activation will generate zeros in the output activation\nSimilar to magnitude of weights, the Average Percentage of Zero activations(APoZ) can be exploited to measure the importance the neuron has\n\n\n\n\nReference. MIT-TinyML-lecture03-Pruning-1\n\n\nFirst-Order-based Pruning\n\nì°¸ê³ ë¡œ ì´ ë°©ë²•ì€ 2023ë…„ì—ëŠ” ì†Œê°œí•˜ì§€ ì•ŠëŠ” ë°©ë²•ì´ë‹¤.\nMinimize the error on loss function introduced by pruning neurons\nSimilar to previous Taylor expansion on weights, the induced error of the objective functionÂ L(x;Â W)Â can be approximated by a Taylor series expanded on activations.\n\\[\n  \\delta L_i = L(x; W) - L(x\\lvert x_i = 0; W) \\approx \\dfrac{\\partial L}{\\partial x_i}x_i\n  \\]\nFor a structural set of neuronsÂ \\(x^{(S)}\\)Â (e.g., a channel plane),\n\\[\n  \\lvert \\delta L_{x^{(S)}} \\lvert\\ = \\Large\\lvert \\small\\sum_{i\\in S}\\dfrac{\\partial L}{\\partial x_i}x_i\\Large\\lvert\n  \\]\n\nRegression-based Pruning\nì´ ë°©ë²•ì€ Quantizedí•œ ë ˆì´ì–´ì˜ output \\(\\hat Z\\)(construction error of the corresponding layerâ€™s outputs)ì™€ \\(Z\\)ë¥¼ Trainingì„ í†µí•´ ì°¨ì´ë¥¼ ì¤„ì´ëŠ” ë°©ë²•ì´ë‹¤. ì°¸ê³ ë¡œ ë¬¸ì œë¥¼ í‘¸ëŠ” ìì„¸í•œ ê³¼ì •ì€ 2022ë…„ ê°•ì˜ì—ë§Œ ë‚˜ì™€ ìˆë‹¤.\n\n\\[\nZ=XW^T=\\sum_{c=0}^{c_i-1}X_cW_c^T\n\\]\n\n\n\nReference. MIT-TinyML-lecture03-Pruning-1\n\n\në¬¸ì œë¥¼ ì‹ìœ¼ë¡œ ì •ì˜í•´ë³´ë©´ ì•„ë˜ì™€ ê°™ì€ë°,\n\n\\(\\beta\\) is the coefficient vector of length \\(c_i\\) for channel selection.\n\\(\\beta_c = 0\\) means channel \\(c\\) is pruned.\n\\(N_c\\) is the number of none zero channel\n\nìš°ì„  ë¬¸ì œë¥¼ í‘¸ëŠ” ë‹¨ê³„ëŠ” ë‘ ë‹¨ê³„ë¡œ ë‚˜ëˆˆë‹¤. Channelì˜ Scale \\(\\beta\\)ë¥¼ ìš°ì„  ê³„ì‚°í•œ í›„ì— \\(W\\)ë¥¼ Quantizedí•œ ë ˆì´ì–´ì˜ output \\(\\hat Z\\)(construction error of the corresponding layerâ€™s outputs)ì™€ \\(Z\\)ì˜ ì°¨ì´ê°€ ìµœì†Œí™”ë˜ëŠ” ì§€ì ê¹Œì§€ Trainingì‹œí‚¨ë‹¤.\nSolve the problem in two folds:\n\nFix W, solve \\(\\beta\\) for channel selection â†’ NP(Nondeterministic polynomial)-hard\nFix \\(\\beta\\), solve W to minimize reconstruction error(Weight Reconstruction)\n\nê° ë¬¸ì œë¥¼ í‘¸ëŠ” ê³¼ì •ì„ ì¡°ê¸ˆ ë” ìì„¸íˆ ì‚´í´ë´ë³´ì. ë³¸ ë‚´ìš©ì€ 2022ë…„ ê°•ì˜ì— ìˆìœ¼ë‹ˆ ì°¸ê³ !\nNP(Nondeterministic polynomial)-hardëŠ” ì•„ë˜ì™€ ê°™ì´ ì‹ìœ¼ë¡œ ì •ë¦¬í•  ìˆ˜ ìˆë‹¤.\n\\[\n\\underset{\\beta}{argmin} \\lvert\\lvert Z- \\sum_{c=0}^{c_i-1} \\beta_cX_cW_c^T \\lvert\\lvert_F^2 = \\lvert\\lvert \\sum_{c=0}^{c_i-1}X_cW_c^T - \\sum_{c=0}^{c_i-1} \\beta_cX_cW_c^T \\lvert\\lvert_F^2\n\\] \\[\n= \\lvert\\lvert\\sum_{c=0}^{c_i-1} (1-\\beta_c)X_cW_c^T \\lvert\\lvert_F^2, \\ s.t.\\ \\lvert\\lvert\\beta\\lvert\\lvert_0 \\ \\leq N_c\n\\]\nê°•ì˜ì—ì„œ ì†Œê°œí•˜ëŠ” ThiNetì´ë¼ëŠ” ë…¼ë¬¸ì—ì„œëŠ” greedy solutionì„ ì´ìš©í•´ì„œ ì±„ë„ í•˜ë‚˜í•˜ë‚˜ì”© Pruning í•´ë³´ë©° objective functionì˜ l2-norm ìµœì†Ÿê°’ì„ êµ¬í•œë‹¤.\n1: S = []\n2: while len(S) &lt; N:\n3:   min_norm, min_c = +inf, 0\n4:   for c in range(c_i):\n5:     tmpS=S+[c]\n6:     Z = X[:,tmpS] * W[:,tmpS].t()\n7:     norm = Z.norm(2)\n8:     if norm &lt; min_norm:\n9:       min_norm, min_c = norm, c\n10:   S.append(min_c)\n11:   c_i.pop(min_c)\nì—¬ê¸°ì„œ ë”í•´ì„œ \\(\\beta\\) ë¥¼ êµ¬í•˜ëŠ” ê³¼ì •ì—ì„œ ì¼ë°˜í™”ë¥¼ ìœ„í•´ LASSO ë°©ì‹ì„ ì‚¬ìš©í•œë‹¤(LASSOì— ëŒ€í•œ ìì„¸í•œ ë‚´ìš©ì€ ì—¬ê¸°ì„œ). Relax the \\(l_0\\) to \\(l_1\\) regularization (LASSO):\n\\[\n\\underset{\\beta}{argmin}\\ \\lvert\\lvert Z- \\sum_{c=0}^{c_i-1}\\beta_cX_cW_c^T\\lvert\\lvert^2_F+\\lambda\\lvert\\lvert \\beta \\lvert\\lvert_1\n\\]\n\n\\(\\lambda\\) is a penalty coefficient. By increasing \\(\\lambda\\), there will be more zeros in \\(\\beta\\).\nGradually increase \\(\\lambda\\) and solve the LASSO regression for \\(\\beta\\), until \\(\\lvert\\lvert \\beta \\lvert\\lvert_0==N_c\\) is met.\nWhy \\(\\lvert\\lvert \\beta \\lvert\\lvert_0==N_c\\)?\nì—¬ê¸°ì— ëŒ€í•´ì„œëŠ” ë”°ë¡œ ì–¸ê¸‰ë˜ì§€ ì•Šì•˜ì§€ë§Œ, ì˜ë¯¸ìƒ scale ì „ì²´ Nê°œ ì¤‘ì—ì„œ ìµœì ê°’ì„ ì°¾ì•„ì•¼í•œë‹¤ë©´ ì „ì²´ë¥¼ Nìœ¼ë¡œ ìœ ì§€í•˜ë©´ì„œ ìµœì ê°’ì„ ì°¾ê¸° ìœ„í•´ì„œê°€ ì•„ë‹ê¹Œ?\n\në‘ ë²ˆì§¸ëŠ” êµ¬í•œ \\(\\beta\\)ë¥¼ ê³ ì •í•œ ìƒíƒœë¡œ Weightë¥¼ Quantized ì „í›„ì˜ ì°¨ì´ë¥¼ ìµœì†Œí™” í•˜ê²Œ â€œWeight Reconstructionâ€ í•œë‹¤. êµ¬í•˜ëŠ” ê³¼ì •ì€ least square approachë¥¼ ì´ìš©í•œ unique closed-form solution ì´ë¯€ë¡œ ì•„ë˜ë¥¼ ì°¸ì¡°í•˜ì.\n\\[\n\\underset{\\beta}{argmin}\\ \\lvert\\lvert Z- \\sum_{c=0}^{c_i-1}\\beta_cX_cW_c^T\\lvert\\lvert^2_F\n\\]\n\n\\(\\beta\\) is a coefficient vector from the previous step\nThis is a classic linear regression problem, which has a unique closed-form solution using the least square approach.\n\\[\n  \\underset{W}{argmin} \\lvert\\lvert Z-\\hat{Z} \\lvert\\lvert^2_F = \\lvert\\lvert Z-UW^T \\lvert\\lvert_F^2\n  \\]\nwhere\n\\[\n  U= \\Large[ \\small\\beta_0X_0\\ \\beta_1X_1 \\ \\cdots \\beta_cX_c \\cdots \\beta_{c_i-1}X_{c_i-1} \\Large]\n  \\]\nand thus,\n\\[\n  W^T = (U^TU)^{-1}U^T Z\n  \\]\n\nQ. How \\((U^TU)^{-1}\\) exists?\nLeast Square method, ì„ì˜ì˜ ë²¡í„° \\(v = (v_0, v_1, \\dots, v_n)\\) ê°€ ìˆì„ ë•Œ \\(v^Tv\\) ì˜ ì—­í–‰ë ¬ì€ í•­ìƒ ìˆì„ê¹Œ? ê°€ì •ì—ì„œ â€œa unique closed-form solutionâ€ë¼ê³  í–ˆìœ¼ë¯€ë¡œ ì´ëŠ” ì¦‰ linearly independenë¡œ ê³ ë ¤í•  ìˆê³  ì—­í–‰ë ¬ì´ ìˆë‹¤(\\(v^Tv\\) is invertible)ëŠ” ì´ì•¼ê¸°ì´ë‹¤."
  },
  {
    "objectID": "posts/lecs/lec03.html#discussion",
    "href": "posts/lecs/lec03.html#discussion",
    "title": "ğŸ§‘â€ğŸ« Lecture 3",
    "section": "4. Discussion",
    "text": "4. Discussion\n\nPruningì„ Dropoutì´ë‘ ë¹„êµí•´ì„œ ì–´ë–¤ ì°¨ì´ì ì´ ìˆëŠ”ê°€?\në‘ ê°€ì§€ ë°©ë²•ì€ ë¶„ëª…íˆ Neuronê³¼ Synapseë¥¼ ì—†ëŒ„ë‹¤ëŠ” ì¸¡ë©´ì—ì„œëŠ” ë¹„ìŠ·í•˜ë‹¤. í•˜ì§€ë§Œ ë‘ ê°€ì§€ ì¸¡ë©´ì—ì„œ ì°¨ì´ì ì´ ìˆëŠ”ë°, í•œ ê°€ì§€ëŠ” ëª©ì í•˜ëŠ” ë°”ì´ê³ , ë‘ ë²ˆì§¸ëŠ” ì‹œì ì´ë‹¤. Dropoutì€ ëª©ì í•˜ëŠ” ë°”ê°€ í›ˆë ¨ì¤‘ì— overfittingì„ ë°©ì§€í•˜ê¸° ìœ„í•¨ì´ ìˆê³  Pruningì˜ ê²½ìš°ëŠ” í›ˆë ¨ì„ ë§ˆì¹œ ëª¨ë¸ì˜ í¬ê¸°ë¥¼ ì¤„ì´ëŠ” ê²ƒì— ìˆë‹¤. ê·¸ë¦¬ê³  ë‘ ë²ˆì§¸ ì‹œì ì˜ ê²½ìš° Dropoutì€ í›ˆë ¨ì¤‘ì— ì´ë¤„ì§€ëŠ” ë°˜ë©´ Pruningì€ í›ˆë ¨ì„ ë§ˆì¹˜ê³ , ê·¸ í¬ê¸°ë¥¼ ì¤„ì¸ í›„ì— ì„±ëŠ¥ì´ ë–¨ì–´ì§€ë©´ ê·¸ì— ë§ê²Œ Fine-tuningì„ í•œë‹¤.\nìŠ¤í„°ë””ì—ì„œëŠ” â€œì™œ dropoutì„ í†µí•´ ì‚¬ì´ì¦ˆë¥¼ ì¤„ì´ì§€ ì•Šì•˜ëŠ”ê°€? ê·¸ë¦¬ê³  êµ¬ì§€ í›ˆë ¨ì„ ë§ˆì¹œ ë‹¤ìŒì— í•  í•„ìš”ê°€ ìˆë‚˜?â€ ë¼ê³  ì§ˆë¬¸ì´ ë‚˜ì™”ì—ˆë‹¤. ë¬¼ë¡  í›ˆë ¨ ì¤‘ì— ëª¨ë¸ì˜ ì‚¬ì´ì¦ˆë¥¼ ì‘ê²Œ ë§Œë“¤ ìˆ˜ ìˆìœ¼ë©´, ê°€ëŠ¥í•œ ê·¸ë ‡ê²Œ í•˜ë©´ ë  ê²ƒì´ë‹¤. í•˜ì§€ë§Œ, ì´ ë˜í•œ ë‘ê°€ì§€ ì¸¡ë©´ì„ ê³ ë ¤í•  í•„ìš”ê°€ ìˆë‹¤. í•˜ë‚˜ëŠ” â€œê³¼ì—° ëª¨ë¸ì˜ ì‚¬ì´ì¦ˆë¥¼ í›ˆë ¨ ì¤‘ í˜¹ì€ ì „ì— ì¤„ì—¬ë‚˜ê°€ë©´ì„œ ì¶©ë¶„íˆ ì„±ëŠ¥ì„ ë‚¼ ìˆ˜ ìˆëŠ”ê°€?â€ì´ê³  ë‹¤ë¥¸ í•˜ë‚˜ëŠ” Pruningì´ë‚˜ ëª¨ë¸ ê²½ëŸ‰í™”ëŠ” ìµœì í™”ì— ì´ˆì ì„ ë§ì¶˜ë‹¤ê³  ìƒê°í•œë‹¤. ê·¸ë ‡ê¸° ë•Œë¬¸ì— í›ˆë ¨ ì¤‘ê°„ì— Channel pruningê³¼ ê°™ì€ ê¸°ë²•ì„ ì‚¬ìš©í•  ìˆ˜ ìˆì„ ì§€ëŠ” ë¯¸ì§€ìˆ˜ì´ê³ , ì„¤ë ¹ Fine-grained Pruningê³¼ ê°™ì€ ê¸°ë²•ì„ ì‚¬ìš©í•œë‹¤ í•˜ë”ë¼ë„ ì´ëŠ” ëª¨ë¸ì˜ ì‚¬ì´ì¦ˆë§Œ ì¤„ì–´ ë“¤ ë¿, ë‚˜ë¨¸ì§€ ë©”ëª¨ë¦¬(e.g.Â RAM)ì´ë‚˜ Latencyê°™ì€ ì„±ëŠ¥ì€ ì¢‹ê²Œ ê°€ì ¸ê°ˆ ìˆ˜ ìˆì„ì§€ë„ ë¯¸ì§€ìˆ˜ë¼ê³  ìƒê°í•œë‹¤.\ní•„ìëŠ” ìœ„ì™€ ê°™ì€ ìµœì í™”ë¥¼ í†µí•œ ì„±ëŠ¥ ê°œì„ ì„ ì´ ê¸€ì—ì„œì²˜ëŸ¼ 2022ë…„ TinyML ê°•ì˜ì—ì„œ ì œê³µí•˜ëŠ” ì‹¤ìŠµì„ í†µí•´ ê²½í—˜í–ˆì—ˆë‹¤. ì•ì„  ì˜ˆì‹œëŠ” OSë¥¼ ê°€ì§„ ë””ë°”ì´ìŠ¤ê°€ ì•„ë‹Œ Bare-metal firmwareë¡œ í™˜ê²½ì´ ì¡°ê¸ˆ íŠ¹ìˆ˜í•˜ê¸°ë„ í•˜ê³ , ì‹¤ì œë¡œ Torchë‚˜ Tensorflowliteì—ì„œ ì œê³µí•˜ëŠ” ëª¨ë¸ ê²½ëŸ‰í™”ë¥¼ ì§ì ‘ì ìœ¼ë¡œ ë¶„ì„í•´ë´ì•¼ ì‹¤ì§ˆì ì¸ ì˜ˆì‹œë¥¼ ì•Œ ìˆ˜ ìˆê² ì§€ë§Œ, í˜¹ì—¬ ì´í•´í•´ ì°¸ê³ ê°€ ë ê¹Œ ë§ë¶™ì—¬ ë†“ëŠ”ë‹¤."
  },
  {
    "objectID": "posts/lecs/lec03.html#reference",
    "href": "posts/lecs/lec03.html#reference",
    "title": "ğŸ§‘â€ğŸ« Lecture 3",
    "section": "5. Reference",
    "text": "5. Reference\n\nMIT-TinyML-lecture03-Pruning-1\nAMC: Automl for Model Compression and Acceleration on Mobile Devices, 2018\nLearning Efficient Convolutional Networks through Network Slimming, 2017\nThiNet: A Filter Level Pruning Method for Deep Neural Network Compression, 2017\nChannel Pruning for Accelerating Very Deep Neural Networks"
  },
  {
    "objectID": "posts/labs/lab00.html",
    "href": "posts/labs/lab00.html",
    "title": "ğŸ‘©â€ğŸ’» Lab 0",
    "section": "",
    "text": "Lab 0 PyTorch Tutorial\nIn this tutorial, we will explore how to train a neural network with PyTorch.\n\nSetup\nWe will first install a few packages that will be used in this tutorial:\n\n!pip install torchprofile 1&gt;/dev/null\n\nWe will then import a few libraries:\n\nimport random\nfrom collections import OrderedDict, defaultdict\n\nimport numpy as np\nimport torch\nfrom matplotlib import pyplot as plt\nfrom torch import nn\nfrom torch.optim import *\nfrom torch.optim.lr_scheduler import *\nfrom torch.utils.data import DataLoader\nfrom torchprofile import profile_macs\nfrom torchvision.datasets import *\nfrom torchvision.transforms import *\nfrom tqdm.auto import tqdm\n\nTo ensure the reproducibility, we will control the seed of random generators:\n\nrandom.seed(0)\nnp.random.seed(0)\ntorch.manual_seed(0)\n\n\n\nData\nIn this tutorial, we will use CIFAR-10 as our target dataset. This dataset contains images from 10 classes, where each image is of size 3x32x32, i.e.Â 3-channel color images of 32x32 pixels in size.\n\ntransforms = {\n  \"train\": Compose([\n    RandomCrop(32, padding=4),\n    RandomHorizontalFlip(),\n    ToTensor(),\n  ]),\n  \"test\": ToTensor(),\n}\n\ndataset = {}\nfor split in [\"train\", \"test\"]:\n  dataset[split] = CIFAR10(\n    root=\"data/cifar10\",\n    train=(split == \"train\"),\n    download=True,\n    transform=transforms[split],\n  )\n\nWe can visualize a few images in the dataset and their corresponding class labels:\n\nsamples = [[] for _ in range(10)]\nfor image, label in dataset[\"test\"]:\n  if len(samples[label]) &lt; 4:\n    samples[label].append(image)\n\nplt.figure(figsize=(20, 9))\nfor index in range(40):\n  label = index % 10\n  image = samples[label][index // 10]\n\n  # Convert from CHW to HWC for visualization\n  image = image.permute(1, 2, 0)\n\n  # Convert from class index to class name\n  label = dataset[\"test\"].classes[label]\n\n  # Visualize the image\n  plt.subplot(4, 10, index + 1)\n  plt.imshow(image)\n  plt.title(label)\n  plt.axis(\"off\")\nplt.show()\n\nTo train a neural network, we will need to feed data in batches. We create data loaders with batch size of 512:\n\ndataflow = {}\nfor split in ['train', 'test']:\n  dataflow[split] = DataLoader(\n    dataset[split],\n    batch_size=512,\n    shuffle=(split == 'train'),\n    num_workers=0,\n    pin_memory=True,\n  )\n\nWe can print the data type and shape from the training data loader:\n\nfor inputs, targets in dataflow[\"train\"]:\n  print(\"[inputs] dtype: {}, shape: {}\".format(inputs.dtype, inputs.shape))\n  print(\"[targets] dtype: {}, shape: {}\".format(targets.dtype, targets.shape))\n  break\n\n\n\nModel\nIn this tutorial, we will use a variant of VGG-11 (with fewer downsamples and a smaller classifier) as our model.\n\nclass VGG(nn.Module):\n  ARCH = [64, 128, 'M', 256, 256, 'M', 512, 512, 'M', 512, 512, 'M']\n\n  def __init__(self) -&gt; None:\n    super().__init__()\n\n    layers = []\n    counts = defaultdict(int)\n\n    def add(name: str, layer: nn.Module) -&gt; None:\n      layers.append((f\"{name}{counts[name]}\", layer))\n      counts[name] += 1\n\n    in_channels = 3\n    for x in self.ARCH:\n      if x != 'M':\n        # conv-bn-relu\n        add(\"conv\", nn.Conv2d(in_channels, x, 3, padding=1, bias=False))\n        add(\"bn\", nn.BatchNorm2d(x))\n        add(\"relu\", nn.ReLU(True))\n        in_channels = x\n      else:\n        # maxpool\n        add(\"pool\", nn.MaxPool2d(2))\n\n    self.backbone = nn.Sequential(OrderedDict(layers))\n    self.classifier = nn.Linear(512, 10)\n\n  def forward(self, x: torch.Tensor) -&gt; torch.Tensor:\n    # backbone: [N, 3, 32, 32] =&gt; [N, 512, 2, 2]\n    x = self.backbone(x)\n\n    # avgpool: [N, 512, 2, 2] =&gt; [N, 512]\n    x = x.mean([2, 3])\n\n    # classifier: [N, 512] =&gt; [N, 10]\n    x = self.classifier(x)\n    return x\n\nmodel = VGG().cuda()\n\nIts backbone is composed of eight conv-bn-relu blocks interleaved with four maxpoolâ€™s to downsample the feature map by 2^4 = 16 times:\n\nprint(model.backbone)\n\nAfter the feature map is pooled, its classifier predicts the final output with a linear layer:\n\nprint(model.classifier)\n\nAs this course focuses on efficiency, we will then inspect its model size and (theoretical) computation cost.\n\nThe model size can be estimated by the number of trainable parameters:\n\n\nnum_params = 0\nfor param in model.parameters():\n  if param.requires_grad:\n    num_params += param.numel()\nprint(\"#Params:\", num_params)\n\n\nThe computation cost can be estimated by the number of multiplyâ€“accumulate operations (MACs) using TorchProfile:\n\n\nnum_macs = profile_macs(model, torch.zeros(1, 3, 32, 32).cuda())\nprint(\"#MACs:\", num_macs)\n\nThis model has 9.2M parameters and requires 606M MACs for inference. We will work together in the next few labs to improve its efficiency.\n\n\nOptimization\nAs we are working on a classification problem, we will apply cross entropy as our loss function to optimize the model:\n\ncriterion = nn.CrossEntropyLoss()\n\nOptimization will be carried out using stochastic gradient descent (SGD) with momentum:\n\noptimizer = SGD(\n  model.parameters(),\n  lr=0.4,\n  momentum=0.9,\n  weight_decay=5e-4,\n)\n\nThe learning rate will be modulated using the following scheduler (which is adapted from this blog series):\n\nnum_epochs = 20\nsteps_per_epoch = len(dataflow[\"train\"])\n\n# Define the piecewise linear scheduler\nlr_lambda = lambda step: np.interp(\n  [step / steps_per_epoch],\n  [0, num_epochs * 0.3, num_epochs],\n  [0, 1, 0]\n)[0]\n\n# Visualize the learning rate schedule\nsteps = np.arange(steps_per_epoch * num_epochs)\nplt.plot(steps, [lr_lambda(step) * 0.4 for step in steps])\nplt.xlabel(\"Number of Steps\")\nplt.ylabel(\"Learning Rate\")\nplt.grid(\"on\")\nplt.show()\n\nscheduler = LambdaLR(optimizer, lr_lambda)\n\n\n\nTraining\nWe first define the training function that optimizes the model for one epoch (i.e., a pass over the training set):\n\ndef train(\n  model: nn.Module,\n  dataflow: DataLoader,\n  criterion: nn.Module,\n  optimizer: Optimizer,\n  scheduler: LambdaLR,\n) -&gt; None:\n  model.train()\n\n  for inputs, targets in tqdm(dataflow, desc='train', leave=False):\n    # Move the data from CPU to GPU\n    inputs = inputs.cuda()\n    targets = targets.cuda()\n\n    # Reset the gradients (from the last iteration)\n    optimizer.zero_grad()\n\n    # Forward inference\n    outputs = model(inputs)\n    loss = criterion(outputs, targets)\n\n    # Backward propagation\n    loss.backward()\n\n    # Update optimizer and LR scheduler\n    optimizer.step()\n    scheduler.step()\n\nWe then define the evaluation function that calculates the metric (i.e., accuracy in our case) on the test set:\n\n@torch.inference_mode()\ndef evaluate(\n  model: nn.Module,\n  dataflow: DataLoader\n) -&gt; float:\n  model.eval()\n\n  num_samples = 0\n  num_correct = 0\n\n  for inputs, targets in tqdm(dataflow, desc=\"eval\", leave=False):\n    # Move the data from CPU to GPU\n    inputs = inputs.cuda()\n    targets = targets.cuda()\n\n    # Inference\n    outputs = model(inputs)\n\n    # Convert logits to class indices\n    outputs = outputs.argmax(dim=1)\n\n    # Update metrics\n    num_samples += targets.size(0)\n    num_correct += (outputs == targets).sum()\n\n  return (num_correct / num_samples * 100).item()\n\nWith training and evaluation functions, we can finally start training the model! This will take around 10 minutes.\n\nfor epoch_num in tqdm(range(1, num_epochs + 1)):\n  train(model, dataflow[\"train\"], criterion, optimizer, scheduler)\n  metric = evaluate(model, dataflow[\"test\"])\n  print(f\"epoch {epoch_num}:\", metric)\n\nIf everything goes well, your trained model should be able to achieve &gt;92.5% of accuracy!\n\n\nVisualization\nWe can visualize the modelâ€™s prediction to see how the model truly performs:\n\nplt.figure(figsize=(20, 10))\nfor index in range(40):\n  image, label = dataset[\"test\"][index]\n\n  # Model inference\n  model.eval()\n  with torch.inference_mode():\n    pred = model(image.unsqueeze(dim=0).cuda())\n    pred = pred.argmax(dim=1)\n\n  # Convert from CHW to HWC for visualization\n  image = image.permute(1, 2, 0)\n\n  # Convert from class indices to class names\n  pred = dataset[\"test\"].classes[pred]\n  label = dataset[\"test\"].classes[label]\n\n  # Visualize the image\n  plt.subplot(4, 10, index + 1)\n  plt.imshow(image)\n  plt.title(f\"pred: {pred}\" + \"\\n\" + f\"label: {label}\")\n  plt.axis(\"off\")\nplt.show()"
  },
  {
    "objectID": "about.html",
    "href": "about.html",
    "title": "TinyML Study Group",
    "section": "",
    "text": "@Gueltto9th"
  },
  {
    "objectID": "about.html#lec-4lab",
    "href": "about.html#lec-4lab",
    "title": "TinyML Study Group",
    "section": "21Lec + 4Lab",
    "text": "21Lec + 4Lab\n\nLec1/2ì™€ Lab0ì€ ì œì™¸\nê°•ì˜ë¥¼ ë“£ê³  1ëª…ì”© ëŒì•„ê°€ë©´ì„œ ê°•ì˜ ë³µìŠµ recap ë°œí‘œ\në‹¤ë¥¸ ì‚¬ëŒë“¤ì€ ì§ˆë¬¸/ë””ìŠ¤ì»¤ì…˜ í† í”½ ê°€ì ¸ì˜¤ê¸°\nì£¼ 1íšŒ (ì•½ 16ì£¼ - 4ê°œì›” ì´ë‚´ ì™„ë£Œ ëª©í‘œ)\n\n\n\n[Chapter I: Efficient Inference]\n\nLecture 3: Pruning and Sparsity (Part I) @ooshyun\nLecture 4: Pruning and Sparsity (Part II) @curieuxjy\nLab 1 @CastleFlag\nLecture 5: Quantization (Part I) @ooshyun\nLecture 6: Quantization (Part II) @ooshyun\nLab 2 @curieuxjy\nLecture 7: Neural Architecture Search (Part I) @CastleFlag\nLecture 8: Neural Architecture Search (Part II) @CastleFlag\nLab 3 @ooshyun\nLecture 9: Knowledge Distillation @curieuxjy\nLecture 10: MCUNet: TinyML on Microcontrollers @curieuxjy\nLecture 11: TinyEngine and Parallel Processing @CastleFlag\n\n\n\n[Chapter II: Domain-Specific Optimization]\n\nLecture 12: Transformer and LLM (Part I) @ooshyun\nLecture 13: Transformer and LLM (Part II) @curieuxjy\nLecture 14: Vision Transformer @CastleFlag\nLab 4 @ooshyun\nLecture 15: GAN, Video, and Point Cloud @curieuxjy\nLecture 16: Diffusion Model @CastleFlag\nLecture 17: Distributed Training (Part I) @ooshyun\nLecture 18: Distributed Training (Part II) @curieuxjy\nLab 5 @CastleFlag\nLecture 19: On-Device Training and Transfer Learning @ooshyun\nLecture 20: Efficient Fine-tuning and Prompt Engineering @curieuxjy\n\n\n\n[Chapter IV: Advanced Topics]\n\nLecture 21: Basics of Quantum Computing @CastleFlag\nLecture 22: Quantum Machine Learning @ooshyun\nLecture 23: Noise Robust Quantum ML @curieuxjy"
  },
  {
    "objectID": "posts/labs/lab02.html",
    "href": "posts/labs/lab02.html",
    "title": "ğŸ‘©â€ğŸ’» Lab 2",
    "section": "",
    "text": "Lecture 5ì™€ 6ì„ í†µí•´ ë°°ìš´ Quantization ë‚´ìš© ì¤‘ì— K-means Quantizationê³¼ Linear Quantizationì— ëŒ€í•´ ì‹¤ìŠµí•˜ë©° ë°°ì›Œë³´ëŠ” Lab2ì— ëŒ€í•œ í’€ì´ì™€ ì„¤ëª…ì— ëŒ€í•œ í¬ìŠ¤íŒ…ì´ë‹¤. ê¸°ì¡´ì˜ ì‹¤ìŠµ ë…¸íŠ¸ëŠ” Original ê°•ì˜ì˜ ë§í¬ë¥¼, í•œêµ­ì–´ ë²ˆì—­ê³¼ Solutionì€ ì´ ë§í¬ë¥¼ ì°¸ê³ í•˜ë©´ ë©ë‹ˆë‹¤. ì•„ë˜ Colaboratory ë²„íŠ¼ì„ ëˆ„ë¥´ë©´ ì‹¤ìŠµë…¸íŠ¸ë¥¼ ë°”ë¡œ ì‹¤í–‰ì‹œí‚¤ëŠ” Colab Notebookì„ ì‹¤í–‰ì‹œí‚¬ ìˆ˜ ìˆìŠµë‹ˆë‹¤."
  },
  {
    "objectID": "posts/labs/lab02.html#goals",
    "href": "posts/labs/lab02.html#goals",
    "title": "ğŸ‘©â€ğŸ’» Lab 2",
    "section": "Goals",
    "text": "Goals\nì´ë²ˆ ì‹¤ìŠµì—ì„œëŠ” ëª¨ë¸ í¬ê¸°ì™€ ì§€ì—° ì‹œê°„ì„ ì¤„ì´ê¸° ìœ„í•´ í´ë˜ì‹í•œ neural network modelì„ quantizingí•˜ëŠ” ì—°ìŠµì„ í•  ê²ƒì…ë‹ˆë‹¤. ì´ ì‹¤ìŠµì˜ ëª©í‘œëŠ” ë‹¤ìŒê³¼ ê°™ìŠµë‹ˆë‹¤:\n\nQuantizationì˜ ê¸°ë³¸ ê°œë…ì„ ì´í•´í•©ë‹ˆë‹¤.\nk-means quantizationì„ êµ¬í˜„í•˜ê³  ì ìš©í•©ë‹ˆë‹¤.\nk-means quantizationì— ëŒ€í•´ quantization-aware trainingì„ êµ¬í˜„í•˜ê³  ì ìš©í•©ë‹ˆë‹¤.\nlinear quantizationì„ êµ¬í˜„í•˜ê³  ì ìš©í•©ë‹ˆë‹¤.\nlinear quantizationì— ëŒ€í•´ integer-only inferenceë¥¼ êµ¬í˜„í•˜ê³  ì ìš©í•©ë‹ˆë‹¤.\nQuantizationì—ì„œì˜ ì„±ëŠ¥ ê°œì„ (ì˜ˆ: ì†ë„ í–¥ìƒ)ì— ëŒ€í•œ ê¸°ë³¸ì ì¸ ì´í•´ë¥¼ ì–»ìŠµë‹ˆë‹¤.\nì´ëŸ¬í•œ quantization ì ‘ê·¼ ë°©ì‹ ì‚¬ì´ì˜ ì°¨ì´ì ê³¼ íŠ¸ë ˆì´ë“œì˜¤í”„ë¥¼ ì´í•´í•©ë‹ˆë‹¤."
  },
  {
    "objectID": "posts/labs/lab02.html#contents",
    "href": "posts/labs/lab02.html#contents",
    "title": "ğŸ‘©â€ğŸ’» Lab 2",
    "section": "Contents",
    "text": "Contents\nì£¼ìš” ì„¹ì…˜ì€ K-Means Quantization ê³¼ Linear Quantization 2ê°€ì§€ë¡œ êµ¬ì„±ë˜ì–´ ìˆìŠµë‹ˆë‹¤.\nì´ë²ˆ ì‹¤ìŠµ ë…¸íŠ¸ì—ì„œ ì´ 10ê°œì˜ ì§ˆë¬¸ì„ í†µí•´ í•™ìŠµí•˜ê²Œ ë©ë‹ˆë‹¤.:\n\nK-Means Quantizationì— ëŒ€í•´ì„œëŠ” 3ê°œì˜ ì§ˆë¬¸ì´ ìˆìŠµë‹ˆë‹¤ (Question 1-3).\nLinear Quantizationì— ëŒ€í•´ì„œëŠ” 6ê°œì˜ ì§ˆë¬¸ì´ ìˆìŠµë‹ˆë‹¤ (Question 4-9).\nQuestion 10ì€ k-means quantizationê³¼ linear quantizationì„ ë¹„êµí•©ë‹ˆë‹¤.\n\n\nì‹¤ìŠµë…¸íŠ¸ì— ëŒ€í•œ ì„¤ì • ë¶€ë¶„(Setup)ì€ Colaboratory Noteë¥¼ ì—´ë©´ í™•ì¸í•˜ì‹¤ ìˆ˜ ìˆìŠµë‹ˆë‹¤. í¬ìŠ¤íŒ…ì—ì„œëŠ” ë³´ë‹¤ ì‹¤ìŠµë‚´ìš©ì— ì§‘ì¤‘í•  ìˆ˜ ìˆë„ë¡ ìƒëµë˜ì–´ ìˆìŠµë‹ˆë‹¤.\n\n\në¨¼ì € FP32 Modelì˜ ì •í™•ë„ì™€ ëª¨ë¸ í¬ê¸°ë¥¼ í‰ê°€í•´ë´…ì‹œë‹¤\n\nfp32_model_accuracy = evaluate(model, dataloader['test'])\nfp32_model_size = get_model_size(model)\nprint(f\"fp32 model has accuracy={fp32_model_accuracy:.2f}%\")\nprint(f\"fp32 model has size={fp32_model_size/MiB:.2f} MiB\")\n\n\n\n\nfp32 model has accuracy=92.95%\nfp32 model has size=35.20 MiB"
  },
  {
    "objectID": "posts/labs/lab02.html#question-1-10-pts",
    "href": "posts/labs/lab02.html#question-1-10-pts",
    "title": "ğŸ‘©â€ğŸ’» Lab 2",
    "section": "Question 1 (10 pts)",
    "text": "Question 1 (10 pts)\nì•„ë˜ì˜ K-Means quantization functionì„ ì™„ì„±í•˜ì„¸ìš”.\n\nfrom fast_pytorch_kmeans import KMeans\n\ndef k_means_quantize(fp32_tensor: torch.Tensor, bitwidth=4, codebook=None):\n    \"\"\"\n    quantize tensor using k-means clustering\n    :param fp32_tensor:\n    :param bitwidth: [int] quantization bit width, default=4\n    :param codebook: [Codebook] (the cluster centroids, the cluster label tensor)\n    :return:\n        [Codebook = (centroids, labels)]\n            centroids: [torch.(cuda.)FloatTensor] the cluster centroids\n            labels: [torch.(cuda.)LongTensor] cluster label tensor\n    \"\"\"\n    if codebook is None:\n        ############### YOUR CODE STARTS HERE ###############\n        # get number of clusters based on the quantization precision\n        n_clusters = 2 ** bitwidth  # Calculate number of clusters as 2^bitwidth\n        ############### YOUR CODE ENDS HERE #################\n        # use k-means to get the quantization centroids\n        kmeans = KMeans(n_clusters=n_clusters, mode='euclidean', verbose=0)\n        labels = kmeans.fit_predict(fp32_tensor.view(-1, 1)).to(torch.long)\n        centroids = kmeans.centroids.to(torch.float).view(-1)\n        codebook = Codebook(centroids, labels)\n\n    ############### YOUR CODE STARTS HERE ###############\n    # decode the codebook into k-means quantized tensor for inference\n    # hint: one line of code\n    quantized_tensor = codebook.centroids[codebook.labels].view_as(fp32_tensor)\n    ############### YOUR CODE ENDS HERE #################\n    fp32_tensor.set_(quantized_tensor.view_as(fp32_tensor))\n    return codebook\n\nìœ„ì—ì„œ ì‘ì„±í•œ k-means quantization functionì„ ë”ë¯¸ í…ì„œì— ì ìš©í•˜ì—¬ í™•ì¸í•´ë´…ì‹œë‹¤.\n\ntest_k_means_quantize()\n\ntensor([[-0.3747,  0.0874,  0.3200, -0.4868,  0.4404],\n        [-0.0402,  0.2322, -0.2024, -0.4986,  0.1814],\n        [ 0.3102, -0.3942, -0.2030,  0.0883, -0.4741],\n        [-0.1592, -0.0777, -0.3946, -0.2128,  0.2675],\n        [ 0.0611, -0.1933, -0.4350,  0.2928, -0.1087]])\n* Test k_means_quantize()\n    target bitwidth: 2 bits\n        num unique values before k-means quantization: 25\n        num unique values after  k-means quantization: 4\n* Test passed."
  },
  {
    "objectID": "posts/labs/lab02.html#question-2-10-pts",
    "href": "posts/labs/lab02.html#question-2-10-pts",
    "title": "ğŸ‘©â€ğŸ’» Lab 2",
    "section": "Question 2 (10 pts)",
    "text": "Question 2 (10 pts)\në§ˆì§€ë§‰ ì½”ë“œ ì…€ì€ 2ë¹„íŠ¸ k-means quantizationì„ ìˆ˜í–‰í•˜ê³  quantization ì „í›„ì˜ í…ì„œë¥¼ í”Œë¡¯í•©ë‹ˆë‹¤. ê° í´ëŸ¬ìŠ¤í„°ëŠ” ê³ ìœ í•œ ìƒ‰ìƒìœ¼ë¡œ ë Œë”ë§ë˜ë©°, quantized í…ì„œë“¤ì´ 4(\\(2^2\\))ê°€ì§€ ê³ ìœ í•œ ìƒ‰ìƒìœ¼ë¡œ í‘œì‹œë©ë‹ˆë‹¤.\nì´ëŸ¬í•œ í˜„ìƒì„ ê´€ì°°í•œ ê²ƒì„ ë°”íƒ•ìœ¼ë¡œ ì§ˆë¬¸ë“¤ì— ë‹µí•˜ì„¸ìš”.\n\nQuestion 2.1 (5 pts)\n4ë¹„íŠ¸ë¡œ k-means quantizationì´ ìˆ˜í–‰ë˜ë©´, quantized í…ì„œì—ëŠ” ëª‡ ê°œì˜ ê³ ìœ í•œ ìƒ‰ìƒì´ ë Œë”ë§ë ê¹Œìš”?\nYour Answer:\n4ë¹„íŠ¸ k-means quantizationì´ ìˆ˜í–‰ë˜ë©´, quantized í…ì„œì— \\((2^4 = 16)\\)ê°œì˜ ê³ ìœ í•œ ìƒ‰ìƒì´ ë Œë”ë§ë©ë‹ˆë‹¤. ì´ëŠ” 4ë¹„íŠ¸ë¡œ 0000ë¶€í„° 1111ê¹Œì§€ì˜ 16ê°€ì§€ ë‹¤ë¥¸ ìƒíƒœë‚˜ ì¡°í•©ì„ ë‚˜íƒ€ë‚¼ ìˆ˜ ìˆìœ¼ë©°, ì´ëŠ” í…ì„œ ê°’ì´ ê·¸ë£¹í™”ë  ìˆ˜ ìˆëŠ” 16ê°œì˜ ê³ ìœ í•œ í´ëŸ¬ìŠ¤í„°ì— í•´ë‹¹í•©ë‹ˆë‹¤.\n\n\nQuestion 2.2 (5 pts)\nn-ë¹„íŠ¸ k-means quantizationì´ ìˆ˜í–‰ë˜ë©´, quantized í…ì„œì— ëª‡ ê°œì˜ ê³ ìœ í•œ ìƒ‰ìƒì´ ë Œë”ë§ë ê¹Œìš”?\nYour Answer:\nn-ë¹„íŠ¸ k-means quantizationì´ ìˆ˜í–‰ë˜ë©´, quantized í…ì„œì—ëŠ” \\((2^n)\\)ê°œì˜ ê³ ìœ í•œ ìƒ‰ìƒì´ ë Œë”ë§ ë©ë‹ˆë‹¤. ì´ëŠ” në¹„íŠ¸ë¥¼ ì‚¬ìš©í•˜ì—¬ \\((2^n)\\)ê°œì˜ ë‹¤ë¥¸ ìƒíƒœë‚˜ ì¡°í•©ì„ ë‚˜íƒ€ë‚¼ ìˆ˜ ìˆìœ¼ë©°, ì´ëŠ” í…ì„œ ê°’ì´ ê·¸ë£¹í™”ë  ìˆ˜ ìˆëŠ” \\((2^n)\\)ê°œì˜ ê³ ìœ í•œ í´ëŸ¬ìŠ¤í„°ì— í•´ë‹¹í•©ë‹ˆë‹¤."
  },
  {
    "objectID": "posts/labs/lab02.html#k-means-quantization-on-whole-model",
    "href": "posts/labs/lab02.html#k-means-quantization-on-whole-model",
    "title": "ğŸ‘©â€ğŸ’» Lab 2",
    "section": "K-Means Quantization on Whole Model",
    "text": "K-Means Quantization on Whole Model\nlab 1ì—ì„œ í–ˆë˜ ê²ƒê³¼ ìœ ì‚¬í•˜ê²Œ, ì´ì œ ì „ì²´ ëª¨ë¸ì„ quantizingí•˜ê¸° ìœ„í•´ k-means quantization í•¨ìˆ˜ë¥¼ í´ë˜ìŠ¤ë¡œ ë˜í•‘í•©ë‹ˆë‹¤. KMeansQuantizer í´ë˜ìŠ¤ì—ì„œëŠ” ëª¨ë¸ ê°€ì¤‘ì¹˜ê°€ ë³€ê²½ë  ë•Œë§ˆë‹¤ codebooks(i.e., centroidsì™€ labels)ì„ ì ìš©í•˜ê±°ë‚˜ ì—…ë°ì´íŠ¸í•  ìˆ˜ ìˆë„ë¡ codebooksì˜ ë³€í™”ë¥¼ ê¸°ë¡í•´ì•¼ í•©ë‹ˆë‹¤.\n\nfrom torch.nn import parameter\nclass KMeansQuantizer:\n    def __init__(self, model : nn.Module, bitwidth=4):\n        self.codebook = KMeansQuantizer.quantize(model, bitwidth)\n\n    @torch.no_grad()\n    def apply(self, model, update_centroids):\n        for name, param in model.named_parameters():\n            if name in self.codebook:\n                if update_centroids:\n                    update_codebook(param, codebook=self.codebook[name])\n                self.codebook[name] = k_means_quantize(\n                    param, codebook=self.codebook[name])\n\n    @staticmethod\n    @torch.no_grad()\n    def quantize(model: nn.Module, bitwidth=4):\n        codebook = dict()\n        if isinstance(bitwidth, dict):\n            for name, param in model.named_parameters():\n                if name in bitwidth:\n                    codebook[name] = k_means_quantize(param, bitwidth=bitwidth[name])\n        else:\n            for name, param in model.named_parameters():\n                if param.dim() &gt; 1:\n                    codebook[name] = k_means_quantize(param, bitwidth=bitwidth)\n        return codebook\n\nì´ì œ K-Means Quantizationì„ ì‚¬ìš©í•˜ì—¬ ëª¨ë¸ì„ 8ë¹„íŠ¸, 4ë¹„íŠ¸, 2ë¹„íŠ¸ë¡œ quantizeí•´ë´…ì‹œë‹¤. ëª¨ë¸ í¬ê¸°ë¥¼ ê³„ì‚°í•  ë•Œ codebooksì˜ ì €ì¥ ê³µê°„ì€ ë¬´ì‹œí•œë‹¤ëŠ” ì ì„ ìœ ì˜í•˜ì„¸ìš”.\n\nprint('Note that the storage for codebooks is ignored when calculating the model size.')\nquantizers = dict()\nfor bitwidth in [8, 4, 2]:\n    recover_model()\n    print(f'k-means quantizing model into {bitwidth} bits')\n    quantizer = KMeansQuantizer(model, bitwidth)\n    quantized_model_size = get_model_size(model, bitwidth)\n    print(f\"    {bitwidth}-bit k-means quantized model has size={quantized_model_size/MiB:.2f} MiB\")\n    quantized_model_accuracy = evaluate(model, dataloader['test'])\n    print(f\"    {bitwidth}-bit k-means quantized model has accuracy={quantized_model_accuracy:.2f}%\")\n    quantizers[bitwidth] = quantizer\n\nNote that the storage for codebooks is ignored when calculating the model size.\nk-means quantizing model into 8 bits\n    8-bit k-means quantized model has size=8.80 MiB\n    8-bit k-means quantized model has accuracy=92.76%\nk-means quantizing model into 4 bits\n    4-bit k-means quantized model has size=4.40 MiB\n    4-bit k-means quantized model has accuracy=79.07%\nk-means quantizing model into 2 bits\n    2-bit k-means quantized model has size=2.20 MiB\n    2-bit k-means quantized model has accuracy=10.00%"
  },
  {
    "objectID": "posts/labs/lab02.html#trained-k-means-quantization",
    "href": "posts/labs/lab02.html#trained-k-means-quantization",
    "title": "ğŸ‘©â€ğŸ’» Lab 2",
    "section": "Trained K-Means Quantization",
    "text": "Trained K-Means Quantization\në§ˆì§€ë§‰ ì…€ì˜ ê²°ê³¼ì—ì„œ ë³¼ ìˆ˜ ìˆë“¯ì´, ëª¨ë¸ì„ ì ì€ ë¹„íŠ¸ë¡œ quantizeí•  ë•Œ ì •í™•ë„ê°€ í¬ê²Œ ë–¨ì–´ì§‘ë‹ˆë‹¤. ë”°ë¼ì„œ, ì •í™•ë„ë¥¼ íšŒë³µí•˜ê¸° ìœ„í•´ quantization-aware trainingì„ í•´ì•¼ í•©ë‹ˆë‹¤.\nk-means quantization-aware í›ˆë ¨ ë™ì•ˆ, centroidsë„ ì—…ë°ì´íŠ¸ë©ë‹ˆë‹¤. ì´ëŠ” Deep Compression: Compressing Deep Neural Networks With Pruning, Trained Quantization And Huffman Codingì—ì„œ ì œì•ˆë˜ì—ˆìŠµë‹ˆë‹¤.\ncentroidsì— ëŒ€í•œ ê·¸ë˜ë””ì–¸íŠ¸ëŠ” ë‹¤ìŒê³¼ ê°™ì´ ê³„ì‚°ë©ë‹ˆë‹¤,\n\n\\(\\frac{\\partial \\mathcal{L} }{\\partial C_k} = \\sum_{j} \\frac{\\partial \\mathcal{L} }{\\partial W_{j}} \\frac{\\partial W_{j} }{\\partial C_k} = \\sum_{j} \\frac{\\partial \\mathcal{L} }{\\partial W_{j}} \\mathbf{1}(I_{j}=k)\\)\n\nì—¬ê¸°ì„œ \\(\\mathcal{L}\\)ì€ ì†ì‹¤, \\(C_k\\)ëŠ” k-ë²ˆì§¸ centroid, \\(I_{j}\\)ëŠ” ê°€ì¤‘ì¹˜ \\(W_{j}\\)ì˜ ë¼ë²¨ì…ë‹ˆë‹¤.\n\\(\\mathbf{1}()\\)ì€ ì§€ì‹œ í•¨ìˆ˜ì´ë©°, \\(\\mathbf{1}(I_{j}=k)\\)ëŠ” \\(1\\;\\mathrm{if}\\;I_{j}=k\\;\\mathrm{else}\\;0\\), ì¦‰, \\(I_{j}==k\\)ë¥¼ ì˜ë¯¸í•©ë‹ˆë‹¤.\nlabì—ì„œëŠ” ê°„ë‹¨íˆ ìµœì‹  ê°€ì¤‘ì¹˜ì— ë”°ë¼ centroidsë¥¼ ì§ì ‘ ì—…ë°ì´íŠ¸í•©ë‹ˆë‹¤:\n\n\\(C_k = \\frac{\\sum_{j}W_{j}\\mathbf{1}(I_{j}=k)}{\\sum_{j}\\mathbf{1}(I_{j}=k)}\\)\n\n\nQuestion 3 (10 pts)\nì•„ë˜ì˜ codebook update functionì„ ì™„ì„±í•˜ì„¸ìš”.\nHint:\nìœ„ì˜ centroidsë¥¼ ì—…ë°ì´íŠ¸í•˜ëŠ” ë°©ì •ì‹ì€ ì‹¤ì œë¡œ ë™ì¼í•œ í´ëŸ¬ìŠ¤í„°ì— ìˆëŠ” ê°€ì¤‘ì¹˜ì˜ í‰ê· (mean)ì„ ì—…ë°ì´íŠ¸ëœ centroid ê°’ìœ¼ë¡œ ì‚¬ìš©í•˜ê³  ìˆìŠµë‹ˆë‹¤.\n\ndef update_codebook(fp32_tensor: torch.Tensor, codebook: Codebook):\n    \"\"\"\n    update the centroids in the codebook using updated fp32_tensor\n    :param fp32_tensor: [torch.(cuda.)Tensor]\n    :param codebook: [Codebook] (the cluster centroids, the cluster label tensor)\n    \"\"\"\n    n_clusters = codebook.centroids.numel()\n    fp32_tensor = fp32_tensor.view(-1)\n    for k in range(n_clusters):\n    ############### YOUR CODE STARTS HERE ###############\n        codebook.centroids[k] = fp32_tensor[codebook.labels == k].mean()\n    ############### YOUR CODE ENDS HERE #################\n\nì´ì œ ë‹¤ìŒ ì½”ë“œ ì…€ì„ ì‹¤í–‰í•˜ì—¬ k-means quantized ëª¨ë¸ì„ finetuningí•˜ì—¬ ì •í™•ë„ë¥¼ íšŒë³µí•´ë´…ì‹œë‹¤. ì •í™•ë„ í•˜ë½ì´ 0.5ë³´ë‹¤ ì‘ìœ¼ë©´ finetuningì„ ì¤‘ë‹¨í•©ë‹ˆë‹¤.\n\naccuracy_drop_threshold = 0.5\nquantizers_before_finetune = copy.deepcopy(quantizers)\nquantizers_after_finetune = quantizers\n\nfor bitwidth in [8, 4, 2]:\n    recover_model()\n    quantizer = quantizers[bitwidth]\n    print(f'k-means quantizing model into {bitwidth} bits')\n    quantizer.apply(model, update_centroids=False)\n    quantized_model_size = get_model_size(model, bitwidth)\n    print(f\"    {bitwidth}-bit k-means quantized model has size={quantized_model_size/MiB:.2f} MiB\")\n    quantized_model_accuracy = evaluate(model, dataloader['test'])\n    print(f\"    {bitwidth}-bit k-means quantized model has accuracy={quantized_model_accuracy:.2f}% before quantization-aware training \")\n    accuracy_drop = fp32_model_accuracy - quantized_model_accuracy\n    if accuracy_drop &gt; accuracy_drop_threshold:\n        print(f\"        Quantization-aware training due to accuracy drop={accuracy_drop:.2f}% is larger than threshold={accuracy_drop_threshold:.2f}%\")\n        num_finetune_epochs = 5\n        optimizer = torch.optim.SGD(model.parameters(), lr=0.01, momentum=0.9)\n        scheduler = torch.optim.lr_scheduler.CosineAnnealingLR(optimizer, num_finetune_epochs)\n        criterion = nn.CrossEntropyLoss()\n        best_accuracy = 0\n        epoch = num_finetune_epochs\n        while accuracy_drop &gt; accuracy_drop_threshold and epoch &gt; 0:\n            train(model, dataloader['train'], criterion, optimizer, scheduler,\n                  callbacks=[lambda: quantizer.apply(model, update_centroids=True)])\n            model_accuracy = evaluate(model, dataloader['test'])\n            is_best = model_accuracy &gt; best_accuracy\n            best_accuracy = max(model_accuracy, best_accuracy)\n            print(f'        Epoch {num_finetune_epochs-epoch} Accuracy {model_accuracy:.2f}% / Best Accuracy: {best_accuracy:.2f}%')\n            accuracy_drop = fp32_model_accuracy - best_accuracy\n            epoch -= 1\n    else:\n        print(f\"        No need for quantization-aware training since accuracy drop={accuracy_drop:.2f}% is smaller than threshold={accuracy_drop_threshold:.2f}%\")\n\nk-means quantizing model into 8 bits\n    8-bit k-means quantized model has size=8.80 MiB\n    8-bit k-means quantized model has accuracy=92.76% before quantization-aware training \n        No need for quantization-aware training since accuracy drop=0.19% is smaller than threshold=0.50%\nk-means quantizing model into 4 bits\n    4-bit k-means quantized model has size=4.40 MiB\n    4-bit k-means quantized model has accuracy=79.07% before quantization-aware training \n        Quantization-aware training due to accuracy drop=13.88% is larger than threshold=0.50%\n        Epoch 0 Accuracy 92.47% / Best Accuracy: 92.47%\nk-means quantizing model into 2 bits\n    2-bit k-means quantized model has size=2.20 MiB\n    2-bit k-means quantized model has accuracy=10.00% before quantization-aware training \n        Quantization-aware training due to accuracy drop=82.95% is larger than threshold=0.50%\n        Epoch 0 Accuracy 90.21% / Best Accuracy: 90.21%\n        Epoch 1 Accuracy 90.82% / Best Accuracy: 90.82%\n        Epoch 2 Accuracy 91.00% / Best Accuracy: 91.00%\n        Epoch 3 Accuracy 91.12% / Best Accuracy: 91.12%\n        Epoch 4 Accuracy 91.20% / Best Accuracy: 91.20%"
  },
  {
    "objectID": "posts/labs/lab02.html#n-bit-integer",
    "href": "posts/labs/lab02.html#n-bit-integer",
    "title": "ğŸ‘©â€ğŸ’» Lab 2",
    "section": "n-bit Integer",
    "text": "n-bit Integer\nn-ë¹„íŠ¸ signed integerëŠ” ë³´í†µ twoâ€™s complement í‘œê¸°ë²•ìœ¼ë¡œ í‘œí˜„ë©ë‹ˆë‹¤.\nn-ë¹„íŠ¸ signed integerëŠ” ë²”ìœ„ \\([-2^{n-1}, 2^{n-1}-1]\\) ë‚´ì˜ ì •ìˆ˜ë¥¼ ì¸ì½”ë”©í•  ìˆ˜ ìˆìŠµë‹ˆë‹¤. ì˜ˆë¥¼ ë“¤ì–´, 8ë¹„íŠ¸ ì •ìˆ˜ëŠ” [-128, 127] ë²”ìœ„ì— ì†í•©ë‹ˆë‹¤.\n\ndef get_quantized_range(bitwidth):\n    quantized_max = (1 &lt;&lt; (bitwidth - 1)) - 1\n    quantized_min = -(1 &lt;&lt; (bitwidth - 1))\n    return quantized_min, quantized_max"
  },
  {
    "objectID": "posts/labs/lab02.html#question-4-15-pts",
    "href": "posts/labs/lab02.html#question-4-15-pts",
    "title": "ğŸ‘©â€ğŸ’» Lab 2",
    "section": "Question 4 (15 pts)",
    "text": "Question 4 (15 pts)\nì•„ë˜ì˜ linear quantization functionì„ ì™„ì„±í•˜ì„¸ìš”.\nHint:\n\n\\(r=S(q-Z)\\)ì—ì„œ, \\(q = r/S + Z\\)ìœ¼ë¡œ ë°”ê¿”ì„œ ë³¼ ìˆ˜ ìˆìŠµë‹ˆë‹¤.\n\\(r\\)ê³¼ \\(S\\) ëª¨ë‘ ë¶€ë™ ì†Œìˆ˜ì  ìˆ«ì(floating number)ì´ë¯€ë¡œ, ì •ìˆ˜ \\(Z\\)ë¥¼ ì§ì ‘ \\(r/S\\)ì— ë”í•  ìˆ˜ ì—†ìŠµë‹ˆë‹¤. ë”°ë¼ì„œ \\(q = \\mathrm{int}(\\mathrm{round}(r/S)) + Z\\)ì…ë‹ˆë‹¤.\ntorch.FloatTensorë¥¼ torch.IntTensorë¡œ ë³€í™˜í•˜ê¸° ìœ„í•´ì„œ, torch.round(), torch.Tensor.round(), torch.Tensor.round_()ì„ ì‚¬ìš©í•˜ì—¬ ëª¨ë“  ê°’ì„ ë¶€ë™ ì†Œìˆ˜ì  ì •ìˆ˜ë¡œ ë¨¼ì € ë³€í™˜í•©ë‹ˆë‹¤.\nê·¸ ë‹¤ìŒ torch.Tensor.to(torch.int8)ë¥¼ ì‚¬ìš©í•˜ì—¬ ë°ì´í„° íƒ€ì…ì„ torch.floatì—ì„œ torch.int8ë¡œ ë³€í™˜í•  ìˆ˜ ìˆìŠµë‹ˆë‹¤.\n\n\ndef linear_quantize(fp_tensor, bitwidth, scale, zero_point, dtype=torch.int8) -&gt; torch.Tensor:\n    \"\"\"\n    linear quantization for single fp_tensor\n      from\n        fp_tensor = (quantized_tensor - zero_point) * scale\n      we have,\n        quantized_tensor = int(round(fp_tensor / scale)) + zero_point\n    :param tensor: [torch.(cuda.)FloatTensor] floating tensor to be quantized\n    :param bitwidth: [int] quantization bit width\n    :param scale: [torch.(cuda.)FloatTensor] scaling factor\n    :param zero_point: [torch.(cuda.)IntTensor] the desired centroid of tensor values\n    :return:\n        [torch.(cuda.)FloatTensor] quantized tensor whose values are integers\n    \"\"\"\n    assert(fp_tensor.dtype == torch.float)\n    assert(isinstance(scale, float) or\n           (scale.dtype == torch.float and scale.dim() == fp_tensor.dim()))\n    assert(isinstance(zero_point, int) or\n           (zero_point.dtype == dtype and zero_point.dim() == fp_tensor.dim()))\n\n    ############### YOUR CODE STARTS HERE ###############\n    # Step 1: scale the fp_tensor\n    scaled_tensor = fp_tensor / scale\n    # Step 2: round the floating value to integer value\n    rounded_tensor = torch.round(scaled_tensor)\n    ############### YOUR CODE ENDS HERE #################\n\n    rounded_tensor = rounded_tensor.to(dtype)\n\n    ############### YOUR CODE STARTS HERE ###############\n    # Step 3: shift the rounded_tensor to make zero_point 0\n    shifted_tensor = rounded_tensor + zero_point\n    ############### YOUR CODE ENDS HERE #################\n\n    # Step 4: clamp the shifted_tensor to lie in bitwidth-bit range\n    quantized_min, quantized_max = get_quantized_range(bitwidth)\n    quantized_tensor = shifted_tensor.clamp_(quantized_min, quantized_max)\n    return quantized_tensor\n\nìœ„ì—ì„œ ì‘ì„±í•œ linear quantization ê¸°ëŠ¥ì„ ë”ë¯¸ í…ì„œì— ì ìš©í•˜ì—¬ ê¸°ëŠ¥ì„ ê²€ì¦í•´ë´…ì‹œë‹¤.\n\ntest_linear_quantize()\n\n* Test linear_quantize()\n    target bitwidth: 2 bits\n        scale: 0.3333333333333333\n        zero point: -1\n* Test passed."
  },
  {
    "objectID": "posts/labs/lab02.html#question-5-10-pts",
    "href": "posts/labs/lab02.html#question-5-10-pts",
    "title": "ğŸ‘©â€ğŸ’» Lab 2",
    "section": "Question 5 (10 pts)",
    "text": "Question 5 (10 pts)\nì´ì œ linear quantizationì„ ìœ„í•œ ìŠ¤ì¼€ì¼ë§ ì¸ì \\(S\\)ì™€ ì œë¡œ í¬ì¸íŠ¸ \\(Z\\)ë¥¼ ê²°ì •í•´ì•¼ í•©ë‹ˆë‹¤.\nlinear quantizationì€ \\(r = S(q-Z)\\) ë¡œ í‘œí˜„ë  ìˆ˜ ìˆë‹¤ëŠ” ê²ƒì„ ê¸°ì–µí•˜ì„¸ìš”.\n\nScale\nLinear quantizationì€ ë¶€ë™ ì†Œìˆ˜ì  ë²”ìœ„ [fp_min, fp_max]ë¥¼ ì–‘ìí™”ëœ ë²”ìœ„ [quantized_min, quantized_max]ë¡œ íˆ¬ì˜(projection)í•©ë‹ˆë‹¤. ì¦‰,\n\n\\(r_{\\mathrm{max}} = S(q_{\\mathrm{max}}-Z)\\)\n\\(r_{\\mathrm{min}} = S(q_{\\mathrm{min}}-Z)\\)\n\nì´ ë‘ ë°©ì •ì‹ì„ ë¹¼ë©´, ìš°ë¦¬ëŠ” ë‹¤ìŒì„ ì–»ìŠµë‹ˆë‹¤,\n\nQuestion 5.1 (1 pts)\në‹¤ìŒ í…ìŠ¤íŠ¸ ì…€ì—ì„œ ì˜¬ë°”ë¥¸ ë‹µì„ ì„ íƒí•˜ê³  ì˜ëª»ëœ ë‹µì„ ì‚­ì œí•´ì£¼ì„¸ìš”.\n\n\\(S=r_{\\mathrm{max}} / q_{\\mathrm{max}}\\)\n\n\n\\(S=(r_{\\mathrm{max}} + r_{\\mathrm{min}}) / (q_{\\mathrm{max}} + q_{\\mathrm{min}})\\)\n\n\nâœ…\\(S=(r_{\\mathrm{max}} - r_{\\mathrm{min}}) / (q_{\\mathrm{max}} - q_{\\mathrm{min}})\\)\n\n\n\\(S=r_{\\mathrm{max}} / q_{\\mathrm{max}} - r_{\\mathrm{min}} / q_{\\mathrm{min}}\\)\n\nfp_tensorì˜ \\(r_{\\mathrm{min}}\\)ê³¼ \\(r_{\\mathrm{max}}\\)ë¥¼ ê²°ì •í•˜ëŠ” ë‹¤ì–‘í•œ ë°©ë²•ì´ ìˆìŠµë‹ˆë‹¤.\n\nê°€ì¥ í”í•œ ë°©ë²•ì€ fp_tensorì˜ ìµœì†Œê°’ê³¼ ìµœëŒ€ê°’ì„ ì‚¬ìš©í•˜ëŠ” ê²ƒì…ë‹ˆë‹¤.\në˜ ë‹¤ë¥¸ ë„ë¦¬ ì‚¬ìš©ë˜ëŠ” ë°©ë²•ì€ Kullback-Leibler-J ë°œì‚°ì„ ìµœì†Œí™”í•˜ì—¬ fp_maxë¥¼ ê²°ì •í•˜ëŠ” ê²ƒì…ë‹ˆë‹¤.\n\n\n\n\nzero point\nìŠ¤ì¼€ì¼ë§ ì¸ì \\(S\\)ë¥¼ ê²°ì •í•˜ë©´, \\(r_{\\mathrm{min}}\\)ê³¼ \\(q_{\\mathrm{min}}\\) ì‚¬ì´ì˜ ê´€ê³„ë¥¼ ì‚¬ìš©í•˜ì—¬ ì œë¡œ í¬ì¸íŠ¸ \\(Z\\)ë¥¼ ê³„ì‚°í•  ìˆ˜ ìˆìŠµë‹ˆë‹¤.\n\nQuestion 5.2 (1 pts)\në‹¤ìŒ í…ìŠ¤íŠ¸ ì…€ì—ì„œ ì˜¬ë°”ë¥¸ ë‹µì„ ì„ íƒí•˜ê³  ì˜ëª»ëœ ë‹µì„ ì‚­ì œí•´ì£¼ì„¸ìš”.\n\n\\(Z = \\mathrm{int}(\\mathrm{round}(r_{\\mathrm{min}} / S - q_{\\mathrm{min}})\\)\n\n\n\\(Z = \\mathrm{int}(\\mathrm{round}(q_{\\mathrm{min}} - r_{\\mathrm{min}} / S))\\)\n\n\nâœ…\\(Z = q_{\\mathrm{min}} - r_{\\mathrm{min}} / S\\)\n\n\n\\(Z = r_{\\mathrm{min}} / S - q_{\\mathrm{min}}\\)\n\n\n\n\nQuestion 5.3 (8 pts)\nfloating point tensor \\(r\\)ë¡œë¶€í„° scale \\(S\\)ì™€ zero point \\(Z\\)ë¥¼ ê³„ì‚°í•˜ëŠ” ì•„ë˜ì˜ í•¨ìˆ˜ë¥¼ ì™„ì„±í•˜ì„¸ìš”.\n\ndef get_quantization_scale_and_zero_point(fp_tensor, bitwidth):\n    \"\"\"\n    get quantization scale for single tensor\n    :param fp_tensor: [torch.(cuda.)Tensor] floating tensor to be quantized\n    :param bitwidth: [int] quantization bit width\n    :return:\n        [float] scale\n        [int] zero_point\n    \"\"\"\n    quantized_min, quantized_max = get_quantized_range(bitwidth)\n    fp_max = fp_tensor.max().item()\n    fp_min = fp_tensor.min().item()\n\n    ############### YOUR CODE STARTS HERE ###############\n    # Calculate scale\n    scale = (fp_max - fp_min) / (quantized_max - quantized_min)\n    # Calculate zero_point\n    zero_point = quantized_min - round(fp_min / scale)\n    ############### YOUR CODE ENDS HERE #################\n\n    # clip the zero_point to fall in [quantized_min, quantized_max]\n    if zero_point &lt; quantized_min:\n        zero_point = quantized_min\n    elif zero_point &gt; quantized_max:\n        zero_point = quantized_max\n    else: # convert from float to int using round()\n        zero_point = round(zero_point)\n    return scale, int(zero_point)\n\nì´ì œ Question 4ì˜ linear_quantize()ì™€ Question 5ì˜ get_quantization_scale_and_zero_point()ì„ í•˜ë‚˜ì˜ í•¨ìˆ˜ë¡œ ë˜í•‘í•©ë‹ˆë‹¤.\n\ndef linear_quantize_feature(fp_tensor, bitwidth):\n    \"\"\"\n    linear quantization for feature tensor\n    :param fp_tensor: [torch.(cuda.)Tensor] floating feature to be quantized\n    :param bitwidth: [int] quantization bit width\n    :return:\n        [torch.(cuda.)Tensor] quantized tensor\n        [float] scale tensor\n        [int] zero point\n    \"\"\"\n    scale, zero_point = get_quantization_scale_and_zero_point(fp_tensor, bitwidth)\n    quantized_tensor = linear_quantize(fp_tensor, bitwidth, scale, zero_point)\n    return quantized_tensor, scale, zero_point"
  },
  {
    "objectID": "posts/labs/lab02.html#special-case-linear-quantization-on-weight-tensor",
    "href": "posts/labs/lab02.html#special-case-linear-quantization-on-weight-tensor",
    "title": "ğŸ‘©â€ğŸ’» Lab 2",
    "section": "Special case: linear quantization on weight tensor",
    "text": "Special case: linear quantization on weight tensor\në¨¼ì € ê°€ì¤‘ì¹˜ ê°’ì˜ ë¶„í¬ë¥¼ ì‚´í´ë´…ì‹œë‹¤.\n\ndef plot_weight_distribution(model, bitwidth=32):\n    # bins = (1 &lt;&lt; bitwidth) if bitwidth &lt;= 8 else 256\n    if bitwidth &lt;= 8:\n        qmin, qmax = get_quantized_range(bitwidth)\n        bins = np.arange(qmin, qmax + 2)\n        align = 'left'\n    else:\n        bins = 256\n        align = 'mid'\n    fig, axes = plt.subplots(3,3, figsize=(10, 6))\n    axes = axes.ravel()\n    plot_index = 0\n    for name, param in model.named_parameters():\n        if param.dim() &gt; 1:\n            ax = axes[plot_index]\n            ax.hist(param.detach().view(-1).cpu(), bins=bins, density=True,\n                    align=align, color = 'blue', alpha = 0.5,\n                    edgecolor='black' if bitwidth &lt;= 4 else None)\n            if bitwidth &lt;= 4:\n                quantized_min, quantized_max = get_quantized_range(bitwidth)\n                ax.set_xticks(np.arange(start=quantized_min, stop=quantized_max+1))\n            ax.set_xlabel(name)\n            ax.set_ylabel('density')\n            plot_index += 1\n    fig.suptitle(f'Histogram of Weights (bitwidth={bitwidth} bits)')\n    fig.tight_layout()\n    fig.subplots_adjust(top=0.925)\n    plt.show()\n\nrecover_model()\nplot_weight_distribution(model)\n\n\n\n\n\n\n\n\nìœ„ì˜ íˆìŠ¤í† ê·¸ë¨ì—ì„œ ë³¼ ìˆ˜ ìˆë“¯ì´, ê°€ì¤‘ì¹˜ ê°’ì˜ ë¶„í¬ëŠ” (ì´ ê²½ìš°ì—ëŠ” classifierë¥¼ ì œì™¸í•˜ê³ ) ê±°ì˜ 0ì„ ì¤‘ì‹¬ìœ¼ë¡œ ëŒ€ì¹­ì ì…ë‹ˆë‹¤ . ë”°ë¼ì„œ ê°€ì¤‘ì¹˜ë¥¼ ì–‘ìí™”í•  ë•Œ ë³´í†µ ì œë¡œ í¬ì¸íŠ¸ \\(Z=0\\)ìœ¼ë¡œ ì„¤ì •í•©ë‹ˆë‹¤.\n\\(r = S(q-Z)\\)ì—ì„œ,\n\n\\(r_{\\mathrm{max}} = S \\cdot q_{\\mathrm{max}}\\)\n\n\n\\(S = r_{\\mathrm{max}} / q_{\\mathrm{max}}\\)\n\nê°€ì¤‘ì¹˜ ê°’ì˜ ìµœëŒ€ ì ˆëŒ“ê°’ì„ \\(r_{\\mathrm{max}}\\)ë¡œ ì´ìš©í•©ë‹ˆë‹¤.\n\ndef get_quantization_scale_for_weight(weight, bitwidth):\n    \"\"\"\n    get quantization scale for single tensor of weight\n    :param weight: [torch.(cuda.)Tensor] floating weight to be quantized\n    :param bitwidth: [integer] quantization bit width\n    :return:\n        [floating scalar] scale\n    \"\"\"\n    # we just assume values in weight are symmetric\n    # we also always make zero_point 0 for weight\n    fp_max = max(weight.abs().max().item(), 5e-7)\n    _, quantized_max = get_quantized_range(bitwidth)\n    return fp_max / quantized_max\n\n\nPer-channel Linear Quantization\n2D convolutionì˜ ê²½ìš°, ê°€ì¤‘ì¹˜ í…ì„œëŠ” (num_output_channels, num_input_channels, kernel_height, kernel_width) ëª¨ì–‘ì˜ 4ì°¨ì› í…ì„œì…ë‹ˆë‹¤.\në§ì€ ì‹¤í—˜ë“¤ì„ í†µí•´, ì„œë¡œ ë‹¤ë¥¸ ì¶œë ¥ ì±„ë„ì— ëŒ€í•´ ì„œë¡œ ë‹¤ë¥¸ ìŠ¤ì¼€ì¼ë§ ì¸ì \\(S\\)ì™€ ì œë¡œ í¬ì¸íŠ¸ \\(Z\\)ë¥¼ ì‚¬ìš©í•˜ëŠ” ê²ƒì´ ë” ë‚˜ì€ ì„±ëŠ¥ì„ ë°œíœ˜í•œë‹¤ëŠ” ê²ƒì„ ì•Œ ìˆ˜ ìˆì—ˆìŠµë‹ˆë‹¤. ë”°ë¼ì„œ ê° ì¶œë ¥ ì±„ë„ì˜ ì„œë¸Œí…ì„œì— ëŒ€í•œ ìŠ¤ì¼€ì¼ë§ ì¸ì \\(S\\)ì™€ ì œë¡œ í¬ì¸íŠ¸ \\(Z\\)ë¥¼ ë…ë¦½ì ìœ¼ë¡œ ì •í•´ì•¼ í•©ë‹ˆë‹¤.\n\ndef linear_quantize_weight_per_channel(tensor, bitwidth):\n    \"\"\"\n    linear quantization for weight tensor\n        using different scales and zero_points for different output channels\n    :param tensor: [torch.(cuda.)Tensor] floating weight to be quantized\n    :param bitwidth: [int] quantization bit width\n    :return:\n        [torch.(cuda.)Tensor] quantized tensor\n        [torch.(cuda.)Tensor] scale tensor\n        [int] zero point (which is always 0)\n    \"\"\"\n    dim_output_channels = 0\n    num_output_channels = tensor.shape[dim_output_channels]\n    scale = torch.zeros(num_output_channels, device=tensor.device)\n    for oc in range(num_output_channels):\n        _subtensor = tensor.select(dim_output_channels, oc)\n        _scale = get_quantization_scale_for_weight(_subtensor, bitwidth)\n        scale[oc] = _scale\n    scale_shape = [1] * tensor.dim()\n    scale_shape[dim_output_channels] = -1\n    scale = scale.view(scale_shape)\n    quantized_tensor = linear_quantize(tensor, bitwidth, scale, zero_point=0)\n    return quantized_tensor, scale, 0\n\n\n\nA Quick Peek at Linear Quantization on Weights\nì´ì œ ê°€ì¤‘ì¹˜ì— ëŒ€í•´ linear quantizationë¥¼ ì ìš©í•  ë•Œ ê°€ì¤‘ì¹˜ ë¶„í¬ì™€ ëª¨ë¸ í¬ê¸°ë¥¼ ì„œë¡œ ë‹¤ë¥¸ bitwidthsë¡œ ì‚´í´ë³´ê² ìŠµë‹ˆë‹¤.\n\n@torch.no_grad()\ndef peek_linear_quantization():\n    for bitwidth in [4, 2]:\n        for name, param in model.named_parameters():\n            if param.dim() &gt; 1:\n                quantized_param, scale, zero_point = \\\n                    linear_quantize_weight_per_channel(param, bitwidth)\n                param.copy_(quantized_param)\n        plot_weight_distribution(model, bitwidth)\n        recover_model()\n\npeek_linear_quantization()"
  },
  {
    "objectID": "posts/labs/lab02.html#quantized-inference",
    "href": "posts/labs/lab02.html#quantized-inference",
    "title": "ğŸ‘©â€ğŸ’» Lab 2",
    "section": "Quantized Inference",
    "text": "Quantized Inference\nì–‘ìí™” í›„, convolution ë° fully-connected layerì˜ ì¶”ë¡ ë„ ë³€ê²½ë©ë‹ˆë‹¤.\n\\(r = S(q-Z)\\)ë¥¼ ìƒê¸°í•´ ë³´ë©´, ë‹¤ìŒê³¼ ê°™ìŠµë‹ˆë‹¤.\n\n\\(r_{\\mathrm{input}} = S_{\\mathrm{input}}(q_{\\mathrm{input}}-Z_{\\mathrm{input}})\\)\n\\(r_{\\mathrm{weight}} = S_{\\mathrm{weight}}(q_{\\mathrm{weight}}-Z_{\\mathrm{weight}})\\)\n\\(r_{\\mathrm{bias}} = S_{\\mathrm{bias}}(q_{\\mathrm{bias}}-Z_{\\mathrm{bias}})\\)\n\n\\(Z_{\\mathrm{weight}}=0\\)ì´ë¯€ë¡œ, \\(r_{\\mathrm{weight}} = S_{\\mathrm{weight}}q_{\\mathrm{weight}}\\)ì…ë‹ˆë‹¤.\në¶€ë™ ì†Œìˆ˜ì  convolutionì€ ë‹¤ìŒê³¼ ê°™ì´ ì‘ì„±í•  ìˆ˜ ìˆìŠµë‹ˆë‹¤.\n\n\\(r_{\\mathrm{output}} = \\mathrm{CONV}[r_{\\mathrm{input}}, r_{\\mathrm{weight}}] + r_{\\mathrm{bias}}\\)\\ \\(\\;\\;\\;\\;\\;\\;\\;\\;= \\mathrm{CONV}[S_{\\mathrm{input}}(q_{\\mathrm{input}}-Z_{\\mathrm{input}}), S_{\\mathrm{weight}}q_{\\mathrm{weight}}] + S_{\\mathrm{bias}}(q_{\\mathrm{bias}}-Z_{\\mathrm{bias}})\\)\\ \\(\\;\\;\\;\\;\\;\\;\\;\\;= \\mathrm{CONV}[q_{\\mathrm{input}}-Z_{\\mathrm{input}}, q_{\\mathrm{weight}}]\\cdot (S_{\\mathrm{input}} \\cdot S_{\\mathrm{weight}}) + S_{\\mathrm{bias}}(q_{\\mathrm{bias}}-Z_{\\mathrm{bias}})\\)\n\nê³„ì‚°ì„ ë” ê°„ë‹¨í•˜ê²Œ í•˜ê¸° ìœ„í•´\n\n\\(Z_{\\mathrm{bias}} = 0\\)\n\\(S_{\\mathrm{bias}} = S_{\\mathrm{input}} \\cdot S_{\\mathrm{weight}}\\)\n\në¡œ ì„¤ì •í•˜ì—¬,\n\n\\(r_{\\mathrm{output}} = (\\mathrm{CONV}[q_{\\mathrm{input}}-Z_{\\mathrm{input}}, q_{\\mathrm{weight}}] + q_{\\mathrm{bias}})\\cdot (S_{\\mathrm{input}} \\cdot S_{\\mathrm{weight}})\\) \\ \\(\\;\\;\\;\\;\\;\\;\\;\\;\\;= (\\mathrm{CONV}[q_{\\mathrm{input}}, q_{\\mathrm{weight}}] - \\mathrm{CONV}[Z_{\\mathrm{input}}, q_{\\mathrm{weight}}] + q_{\\mathrm{bias}})\\cdot (S_{\\mathrm{input}}S_{\\mathrm{weight}})\\)\n\nì´ë©°,\n\n\\(r_{\\mathrm{output}} = S_{\\mathrm{output}}(q_{\\mathrm{output}}-Z_{\\mathrm{output}})\\)\n\nì´ë¯€ë¡œ\n\n\\(S_{\\mathrm{output}}(q_{\\mathrm{output}}-Z_{\\mathrm{output}}) = (\\mathrm{CONV}[q_{\\mathrm{input}}, q_{\\mathrm{weight}}] - \\mathrm{CONV}[Z_{\\mathrm{input}}, q_{\\mathrm{weight}}] + q_{\\mathrm{bias}})\\cdot (S_{\\mathrm{input}} S_{\\mathrm{weight}})\\)\n\në”°ë¼ì„œ\n\n\\(q_{\\mathrm{output}} = (\\mathrm{CONV}[q_{\\mathrm{input}}, q_{\\mathrm{weight}}] - \\mathrm{CONV}[Z_{\\mathrm{input}}, q_{\\mathrm{weight}}] + q_{\\mathrm{bias}})\\cdot (S_{\\mathrm{input}}S_{\\mathrm{weight}} / S_{\\mathrm{output}}) + Z_{\\mathrm{output}}\\)\n\n\\(Z_{\\mathrm{input}}\\), \\(q_{\\mathrm{weight}}\\), \\(q_{\\mathrm{bias}}\\)ëŠ” ì¶”ë¡  ì „ì— ê²°ì •ë˜ë¯€ë¡œ,\n\n\\(Q_{\\mathrm{bias}} = q_{\\mathrm{bias}} - \\mathrm{CONV}[Z_{\\mathrm{input}}, q_{\\mathrm{weight}}]\\)\n\në¡œ ì„¤ì •í•˜ë©´,\n\n\\(q_{\\mathrm{output}} = (\\mathrm{Linear}[q_{\\mathrm{input}}, q_{\\mathrm{weight}}] + Q_{\\mathrm{bias}})\\cdot (S_{\\mathrm{input}} \\cdot S_{\\mathrm{weight}} / S_{\\mathrm{output}}) + Z_{\\mathrm{output}}\\)\n\n\nQuestion 6 (5 pts)\nbiasë¥¼ linear quantizingí•˜ëŠ” í•¨ìˆ˜ë¥¼ ì™„ì„±í•˜ì„¸ìš”.\nHint:\nìœ„ì˜ ì¶”ë¡ ê³¼ì •ì—ì„œ ì•„ë˜ì™€ ê°™ì€ ìˆ˜ì‹ì„ ì–»ì—ˆìŠµë‹ˆë‹¤.\n\n\\(Z_{\\mathrm{bias}} = 0\\)\n\\(S_{\\mathrm{bias}} = S_{\\mathrm{input}} \\cdot S_{\\mathrm{weight}}\\)\n\n\ndef linear_quantize_bias_per_output_channel(bias, weight_scale, input_scale):\n    \"\"\"\n    linear quantization for single bias tensor\n        quantized_bias = fp_bias / bias_scale\n    :param bias: [torch.FloatTensor] bias weight to be quantized\n    :param weight_scale: [float or torch.FloatTensor] weight scale tensor\n    :param input_scale: [float] input scale\n    :return:\n        [torch.IntTensor] quantized bias tensor\n    \"\"\"\n    assert(bias.dim() == 1)\n    assert(bias.dtype == torch.float)\n    assert(isinstance(input_scale, float))\n    if isinstance(weight_scale, torch.Tensor):\n        assert(weight_scale.dtype == torch.float)\n        weight_scale = weight_scale.view(-1)\n        assert(bias.numel() == weight_scale.numel())\n\n    ############### YOUR CODE STARTS HERE ###############\n    bias_scale = weight_scale * input_scale\n    ############### YOUR CODE ENDS HERE #################\n\n    quantized_bias = linear_quantize(bias, 32, bias_scale,\n                                     zero_point=0, dtype=torch.int32)\n    return quantized_bias, bias_scale, 0\n\n\n\nQuantized Fully-Connected Layer\nì–‘ìí™”ëœ fully-connected layerì˜ ê²½ìš°, \\(Q_{\\mathrm{bias}}\\)ë¥¼ ë¨¼ì € ê³„ì‚°í•©ë‹ˆë‹¤. \\(Q_{\\mathrm{bias}} = q_{\\mathrm{bias}} - \\mathrm{Linear}[Z_{\\mathrm{input}}, q_{\\mathrm{weight}}]\\)ë¥¼ ê¸°ì–µí•˜ì„¸ìš”.\n\ndef shift_quantized_linear_bias(quantized_bias, quantized_weight, input_zero_point):\n    \"\"\"\n    shift quantized bias to incorporate input_zero_point for nn.Linear\n        shifted_quantized_bias = quantized_bias - Linear(input_zero_point, quantized_weight)\n    :param quantized_bias: [torch.IntTensor] quantized bias (torch.int32)\n    :param quantized_weight: [torch.CharTensor] quantized weight (torch.int8)\n    :param input_zero_point: [int] input zero point\n    :return:\n        [torch.IntTensor] shifted quantized bias tensor\n    \"\"\"\n    assert(quantized_bias.dtype == torch.int32)\n    assert(isinstance(input_zero_point, int))\n    return quantized_bias - quantized_weight.sum(1).to(torch.int32) * input_zero_point\n\n\nQuestion 7 (15 pts)\nì•„ë˜ì˜ ì–‘ìí™”ëœ fully-connected layer inference functionë¥¼ ì™„ì„±í•˜ì„¸ìš”.\nHint:\n\n\\(q_{\\mathrm{output}} = (\\mathrm{Linear}[q_{\\mathrm{input}}, q_{\\mathrm{weight}}] + Q_{\\mathrm{bias}})\\cdot (S_{\\mathrm{input}} S_{\\mathrm{weight}} / S_{\\mathrm{output}}) + Z_{\\mathrm{output}}\\)\n\n\ndef quantized_linear(input, weight, bias, feature_bitwidth, weight_bitwidth,\n                     input_zero_point, output_zero_point,\n                     input_scale, weight_scale, output_scale):\n    \"\"\"\n    quantized fully-connected layer\n    :param input: [torch.CharTensor] quantized input (torch.int8)\n    :param weight: [torch.CharTensor] quantized weight (torch.int8)\n    :param bias: [torch.IntTensor] shifted quantized bias or None (torch.int32)\n    :param feature_bitwidth: [int] quantization bit width of input and output\n    :param weight_bitwidth: [int] quantization bit width of weight\n    :param input_zero_point: [int] input zero point\n    :param output_zero_point: [int] output zero point\n    :param input_scale: [float] input feature scale\n    :param weight_scale: [torch.FloatTensor] weight per-channel scale\n    :param output_scale: [float] output feature scale\n    :return:\n        [torch.CharIntTensor] quantized output feature (torch.int8)\n    \"\"\"\n    assert(input.dtype == torch.int8)\n    assert(weight.dtype == input.dtype)\n    assert(bias is None or bias.dtype == torch.int32)\n    assert(isinstance(input_zero_point, int))\n    assert(isinstance(output_zero_point, int))\n    assert(isinstance(input_scale, float))\n    assert(isinstance(output_scale, float))\n    assert(weight_scale.dtype == torch.float)\n\n    # Step 1: integer-based fully-connected (8-bit multiplication with 32-bit accumulation)\n    if 'cpu' in input.device.type:\n        # use 32-b MAC for simplicity\n        output = torch.nn.functional.linear(input.to(torch.int32), weight.to(torch.int32), bias)\n    else:\n        # current version pytorch does not yet support integer-based linear() on GPUs\n        output = torch.nn.functional.linear(input.float(), weight.float(), bias.float())\n\n    ############### YOUR CODE STARTS HERE ###############\n    # Step 2: scale the output\n    #         hint: 1. scales are floating numbers, we need to convert output to float as well\n    #               2. the shape of weight scale is [oc, 1, 1, 1] while the shape of output is [batch_size, oc]\n    real_scale = input_scale * weight_scale.view(-1) / output_scale\n    output = output.float() * real_scale\n\n    # Step 3: Shift output by output_zero_point\n    output += output_zero_point\n    ############### YOUR CODE STARTS HERE ###############\n\n    # Make sure all value lies in the bitwidth-bit range\n    output = output.round().clamp(*get_quantized_range(feature_bitwidth)).to(torch.int8)\n    return output\n\nLetâ€™s verify the functionality of defined quantized fully connected layer.\n\ntest_quantized_fc()\n\n* Test quantized_fc()\n    target bitwidth: 2 bits\n      batch size: 4\n      input channels: 8\n      output channels: 8\n* Test passed.\n\n\n\n\n\n\n\n\n\n\n\n\nQuantized Convolution\nì–‘ìí™”ëœ ì»¨ë³¼ë£¨ì…˜ ë ˆì´ì–´ì˜ ê²½ìš°, ë¨¼ì € \\(Q_{\\mathrm{bias}}\\)ë¥¼ ê³„ì‚°í•©ë‹ˆë‹¤. \\(Q_{\\mathrm{bias}} = q_{\\mathrm{bias}} - \\mathrm{CONV}[Z_{\\mathrm{input}}, q_{\\mathrm{weight}}]\\)ë¥¼ ê¸°ì–µí•˜ì„¸ìš”.\n\ndef shift_quantized_conv2d_bias(quantized_bias, quantized_weight, input_zero_point):\n    \"\"\"\n    shift quantized bias to incorporate input_zero_point for nn.Conv2d\n        shifted_quantized_bias = quantized_bias - Conv(input_zero_point, quantized_weight)\n    :param quantized_bias: [torch.IntTensor] quantized bias (torch.int32)\n    :param quantized_weight: [torch.CharTensor] quantized weight (torch.int8)\n    :param input_zero_point: [int] input zero point\n    :return:\n        [torch.IntTensor] shifted quantized bias tensor\n    \"\"\"\n    assert(quantized_bias.dtype == torch.int32)\n    assert(isinstance(input_zero_point, int))\n    return quantized_bias - quantized_weight.sum((1,2,3)).to(torch.int32) * input_zero_point\n\n\nQuestion 8 (15 pts)\nì•„ë˜ì˜ quantized convolution functionì„ ì™„ì„±í•˜ì„¸ìš”.\nHint: &gt; \\(q_{\\mathrm{output}} = (\\mathrm{CONV}[q_{\\mathrm{input}}, q_{\\mathrm{weight}}] + Q_{\\mathrm{bias}}) \\cdot (S_{\\mathrm{input}}S_{\\mathrm{weight}} / S_{\\mathrm{output}}) + Z_{\\mathrm{output}}\\)\n\ndef quantized_conv2d(input, weight, bias, feature_bitwidth, weight_bitwidth,\n                     input_zero_point, output_zero_point,\n                     input_scale, weight_scale, output_scale,\n                     stride, padding, dilation, groups):\n    \"\"\"\n    quantized 2d convolution\n    :param input: [torch.CharTensor] quantized input (torch.int8)\n    :param weight: [torch.CharTensor] quantized weight (torch.int8)\n    :param bias: [torch.IntTensor] shifted quantized bias or None (torch.int32)\n    :param feature_bitwidth: [int] quantization bit width of input and output\n    :param weight_bitwidth: [int] quantization bit width of weight\n    :param input_zero_point: [int] input zero point\n    :param output_zero_point: [int] output zero point\n    :param input_scale: [float] input feature scale\n    :param weight_scale: [torch.FloatTensor] weight per-channel scale\n    :param output_scale: [float] output feature scale\n    :return:\n        [torch.(cuda.)CharTensor] quantized output feature\n    \"\"\"\n    assert(len(padding) == 4)\n    assert(input.dtype == torch.int8)\n    assert(weight.dtype == input.dtype)\n    assert(bias is None or bias.dtype == torch.int32)\n    assert(isinstance(input_zero_point, int))\n    assert(isinstance(output_zero_point, int))\n    assert(isinstance(input_scale, float))\n    assert(isinstance(output_scale, float))\n    assert(weight_scale.dtype == torch.float)\n\n    # Step 1: calculate integer-based 2d convolution (8-bit multiplication with 32-bit accumulation)\n    input = torch.nn.functional.pad(input, padding, 'constant', input_zero_point)\n    if 'cpu' in input.device.type:\n        # use 32-b MAC for simplicity\n        output = torch.nn.functional.conv2d(input.to(torch.int32), weight.to(torch.int32), None, stride, 0, dilation, groups)\n    else:\n        # current version pytorch does not yet support integer-based conv2d() on GPUs\n        output = torch.nn.functional.conv2d(input.float(), weight.float(), None, stride, 0, dilation, groups)\n        output = output.round().to(torch.int32)\n    if bias is not None:\n        output = output + bias.view(1, -1, 1, 1)\n\n    ############### YOUR CODE STARTS HERE ###############\n    # hint: this code block should be the very similar to quantized_linear()\n\n    # Step 2: scale the output\n    #         hint: 1. scales are floating numbers, we need to convert output to float as well\n    #               2. the shape of weight scale is [oc, 1, 1, 1] while the shape of output is [batch_size, oc, height, width]\n    real_scale = input_scale * weight_scale.view(-1) / output_scale\n    output = output.float() * real_scale.unsqueeze(1).unsqueeze(2)\n\n    # Step 3: shift output by output_zero_point\n    #         hint: one line of code\n    output += output_zero_point\n    ############### YOUR CODE STARTS HERE ###############\n\n    # Make sure all value lies in the bitwidth-bit range\n    output = output.round().clamp(*get_quantized_range(feature_bitwidth)).to(torch.int8)\n    return output"
  },
  {
    "objectID": "posts/labs/lab02.html#question-9-10-pts",
    "href": "posts/labs/lab02.html#question-9-10-pts",
    "title": "ğŸ‘©â€ğŸ’» Lab 2",
    "section": "Question 9 (10 pts)",
    "text": "Question 9 (10 pts)\në§ˆì§€ë§‰ìœ¼ë¡œ ëª¨ë“  ê²ƒì„ ì¢…í•©í•˜ì—¬ ëª¨ë¸ì— ëŒ€í•œ í›ˆë ¨ í›„ int8 ì–‘ìí™”ë¥¼ ìˆ˜í–‰í•©ë‹ˆë‹¤. ëª¨ë¸ì˜ ì»¨ë³¼ë£¨ì…˜ ë ˆì´ì–´ì™€ ì„ í˜• ë ˆì´ì–´ë¥¼ í•˜ë‚˜ì”© ì–‘ìí™”ëœ ë²„ì „ìœ¼ë¡œ ë³€í™˜í•©ë‹ˆë‹¤.\n\në¨¼ì €, BatchNorm ê³„ì¸µì„ ì´ì „ convolutional layerì— ìœµí•©í•  ê²ƒì´ë©°, ì´ëŠ” ì–‘ìí™” ì „ì— í•˜ëŠ” í‘œì¤€ ê´€í–‰ì…ë‹ˆë‹¤. BatchNormì„ ìœµí•©í•˜ë©´ ì¶”ë¡  ì¤‘ì— ì¶”ê°€ ê³±ì…ˆì´ ì¤„ì–´ë“­ë‹ˆë‹¤.\n\nìœµí•© ëª¨ë¸ì¸ model_fusedê°€ ì›ë˜ ëª¨ë¸ê³¼ ë™ì¼í•œ ì •í™•ë„ë¥¼ ê°–ëŠ”ì§€ë„ ê²€ì¦í•  ì˜ˆì •ì…ë‹ˆë‹¤(BN fusionì€ ë„¤íŠ¸ì›Œí¬ ê¸°ëŠ¥ì„ ë³€ê²½í•˜ì§€ ì•ŠëŠ” ë™ë“±í•œ ë³€í™˜ì…ë‹ˆë‹¤).\n\ndef fuse_conv_bn(conv, bn):\n    # modified from https://mmcv.readthedocs.io/en/latest/_modules/mmcv/cnn/utils/fuse_conv_bn.html\n    assert conv.bias is None\n\n    factor = bn.weight.data / torch.sqrt(bn.running_var.data + bn.eps)\n    conv.weight.data = conv.weight.data * factor.reshape(-1, 1, 1, 1)\n    conv.bias = nn.Parameter(- bn.running_mean.data * factor + bn.bias.data)\n\n    return conv\n\nprint('Before conv-bn fusion: backbone length', len(model.backbone))\n#  fuse the batchnorm into conv layers\nrecover_model()\nmodel_fused = copy.deepcopy(model)\nfused_backbone = []\nptr = 0\nwhile ptr &lt; len(model_fused.backbone):\n    if isinstance(model_fused.backbone[ptr], nn.Conv2d) and \\\n        isinstance(model_fused.backbone[ptr + 1], nn.BatchNorm2d):\n        fused_backbone.append(fuse_conv_bn(\n            model_fused.backbone[ptr], model_fused.backbone[ptr+ 1]))\n        ptr += 2\n    else:\n        fused_backbone.append(model_fused.backbone[ptr])\n        ptr += 1\nmodel_fused.backbone = nn.Sequential(*fused_backbone)\n\nprint('After conv-bn fusion: backbone length', len(model_fused.backbone))\n# sanity check, no BN anymore\nfor m in model_fused.modules():\n    assert not isinstance(m, nn.BatchNorm2d)\n\n#  the accuracy will remain the same after fusion\nfused_acc = evaluate(model_fused, dataloader['test'])\nprint(f'Accuracy of the fused model={fused_acc:.2f}%')\n\nBefore conv-bn fusion: backbone length 29\nAfter conv-bn fusion: backbone length 21\nAccuracy of the fused model=92.95%\n\n\n\n\n\n\nê° íŠ¹ì§• ë§µì˜ ë²”ìœ„ë¥¼ ì–»ê¸° ìœ„í•´ ì¼ë¶€ ìƒ˜í”Œ ë°ì´í„°ë¡œ ëª¨ë¸ì„ ì‹¤í–‰í•˜ì—¬ íŠ¹ì§• ë§µì˜ ë²”ìœ„ë¥¼ ì–»ê³ , í•´ë‹¹ ìŠ¤ì¼€ì¼ë§ íŒ©í„°ì™€ ì œë¡œ í¬ì¸íŠ¸ë¥¼ ê³„ì‚°í•  ìˆ˜ ìˆìŠµë‹ˆë‹¤.\n\n\n# add hook to record the min max value of the activation\ninput_activation = {}\noutput_activation = {}\n\ndef add_range_recoder_hook(model):\n    import functools\n    def _record_range(self, x, y, module_name):\n        x = x[0]\n        input_activation[module_name] = x.detach()\n        output_activation[module_name] = y.detach()\n\n    all_hooks = []\n    for name, m in model.named_modules():\n        if isinstance(m, (nn.Conv2d, nn.Linear, nn.ReLU)):\n            all_hooks.append(m.register_forward_hook(\n                functools.partial(_record_range, module_name=name)))\n    return all_hooks\n\nhooks = add_range_recoder_hook(model_fused)\nsample_data = iter(dataloader['train']).__next__()[0]\nmodel_fused(sample_data.cuda())\n\n# remove hooks\nfor h in hooks:\n    h.remove()\n\n\në§ˆì§€ë§‰ìœ¼ë¡œ ëª¨ë¸ ì–‘ìí™”ë¥¼ í•´ë³´ê² ìŠµë‹ˆë‹¤. ë‹¤ìŒê³¼ ê°™ì€ ë§¤í•‘ìœ¼ë¡œ ëª¨ë¸ì„ ë³€í™˜í•©ë‹ˆë‹¤.\n\nnn.Conv2d: QuantizedConv2d,\nnn.Linear: QuantizedLinear,\n# the following twos are just wrappers, as current\n# torch modules do not support int8 data format;\n# we will temporarily convert them to fp32 for computation\nnn.MaxPool2d: QuantizedMaxPool2d,\nnn.AvgPool2d: QuantizedAvgPool2d,\n\nclass QuantizedConv2d(nn.Module):\n    def __init__(self, weight, bias,\n                 input_zero_point, output_zero_point,\n                 input_scale, weight_scale, output_scale,\n                 stride, padding, dilation, groups,\n                 feature_bitwidth=8, weight_bitwidth=8):\n        super().__init__()\n        # current version Pytorch does not support IntTensor as nn.Parameter\n        self.register_buffer('weight', weight)\n        self.register_buffer('bias', bias)\n\n        self.input_zero_point = input_zero_point\n        self.output_zero_point = output_zero_point\n\n        self.input_scale = input_scale\n        self.register_buffer('weight_scale', weight_scale)\n        self.output_scale = output_scale\n\n        self.stride = stride\n        self.padding = (padding[1], padding[1], padding[0], padding[0])\n        self.dilation = dilation\n        self.groups = groups\n\n        self.feature_bitwidth = feature_bitwidth\n        self.weight_bitwidth = weight_bitwidth\n\n\n    def forward(self, x):\n        return quantized_conv2d(\n            x, self.weight, self.bias,\n            self.feature_bitwidth, self.weight_bitwidth,\n            self.input_zero_point, self.output_zero_point,\n            self.input_scale, self.weight_scale, self.output_scale,\n            self.stride, self.padding, self.dilation, self.groups\n            )\n\nclass QuantizedLinear(nn.Module):\n    def __init__(self, weight, bias,\n                 input_zero_point, output_zero_point,\n                 input_scale, weight_scale, output_scale,\n                 feature_bitwidth=8, weight_bitwidth=8):\n        super().__init__()\n        # current version Pytorch does not support IntTensor as nn.Parameter\n        self.register_buffer('weight', weight)\n        self.register_buffer('bias', bias)\n\n        self.input_zero_point = input_zero_point\n        self.output_zero_point = output_zero_point\n\n        self.input_scale = input_scale\n        self.register_buffer('weight_scale', weight_scale)\n        self.output_scale = output_scale\n\n        self.feature_bitwidth = feature_bitwidth\n        self.weight_bitwidth = weight_bitwidth\n\n    def forward(self, x):\n        return quantized_linear(\n            x, self.weight, self.bias,\n            self.feature_bitwidth, self.weight_bitwidth,\n            self.input_zero_point, self.output_zero_point,\n            self.input_scale, self.weight_scale, self.output_scale\n            )\n\nclass QuantizedMaxPool2d(nn.MaxPool2d):\n    def forward(self, x):\n        # current version PyTorch does not support integer-based MaxPool\n        return super().forward(x.float()).to(torch.int8)\n\nclass QuantizedAvgPool2d(nn.AvgPool2d):\n    def forward(self, x):\n        # current version PyTorch does not support integer-based AvgPool\n        return super().forward(x.float()).to(torch.int8)\n\n# we use int8 quantization, which is quite popular\nfeature_bitwidth = weight_bitwidth = 8\nquantized_model = copy.deepcopy(model_fused)\nquantized_backbone = []\nptr = 0\nwhile ptr &lt; len(quantized_model.backbone):\n    if isinstance(quantized_model.backbone[ptr], nn.Conv2d) and \\\n        isinstance(quantized_model.backbone[ptr + 1], nn.ReLU):\n        conv = quantized_model.backbone[ptr]\n        conv_name = f'backbone.{ptr}'\n        relu = quantized_model.backbone[ptr + 1]\n        relu_name = f'backbone.{ptr + 1}'\n\n        input_scale, input_zero_point = \\\n            get_quantization_scale_and_zero_point(\n                input_activation[conv_name], feature_bitwidth)\n\n        output_scale, output_zero_point = \\\n            get_quantization_scale_and_zero_point(\n                output_activation[relu_name], feature_bitwidth)\n\n        quantized_weight, weight_scale, weight_zero_point = \\\n            linear_quantize_weight_per_channel(conv.weight.data, weight_bitwidth)\n        quantized_bias, bias_scale, bias_zero_point = \\\n            linear_quantize_bias_per_output_channel(\n                conv.bias.data, weight_scale, input_scale)\n        shifted_quantized_bias = \\\n            shift_quantized_conv2d_bias(quantized_bias, quantized_weight,\n                                        input_zero_point)\n\n        quantized_conv = QuantizedConv2d(\n            quantized_weight, shifted_quantized_bias,\n            input_zero_point, output_zero_point,\n            input_scale, weight_scale, output_scale,\n            conv.stride, conv.padding, conv.dilation, conv.groups,\n            feature_bitwidth=feature_bitwidth, weight_bitwidth=weight_bitwidth\n        )\n\n        quantized_backbone.append(quantized_conv)\n        ptr += 2\n    elif isinstance(quantized_model.backbone[ptr], nn.MaxPool2d):\n        quantized_backbone.append(QuantizedMaxPool2d(\n            kernel_size=quantized_model.backbone[ptr].kernel_size,\n            stride=quantized_model.backbone[ptr].stride\n            ))\n        ptr += 1\n    elif isinstance(quantized_model.backbone[ptr], nn.AvgPool2d):\n        quantized_backbone.append(QuantizedAvgPool2d(\n            kernel_size=quantized_model.backbone[ptr].kernel_size,\n            stride=quantized_model.backbone[ptr].stride\n            ))\n        ptr += 1\n    else:\n        raise NotImplementedError(type(quantized_model.backbone[ptr]))  # should not happen\nquantized_model.backbone = nn.Sequential(*quantized_backbone)\n\n# finally, quantized the classifier\nfc_name = 'classifier'\nfc = model.classifier\ninput_scale, input_zero_point = \\\n    get_quantization_scale_and_zero_point(\n        input_activation[fc_name], feature_bitwidth)\n\noutput_scale, output_zero_point = \\\n    get_quantization_scale_and_zero_point(\n        output_activation[fc_name], feature_bitwidth)\n\nquantized_weight, weight_scale, weight_zero_point = \\\n    linear_quantize_weight_per_channel(fc.weight.data, weight_bitwidth)\nquantized_bias, bias_scale, bias_zero_point = \\\n    linear_quantize_bias_per_output_channel(\n        fc.bias.data, weight_scale, input_scale)\nshifted_quantized_bias = \\\n    shift_quantized_linear_bias(quantized_bias, quantized_weight,\n                                input_zero_point)\n\nquantized_model.classifier = QuantizedLinear(\n    quantized_weight, shifted_quantized_bias,\n    input_zero_point, output_zero_point,\n    input_scale, weight_scale, output_scale,\n    feature_bitwidth=feature_bitwidth, weight_bitwidth=weight_bitwidth\n)\n\nì–‘ìí™” ê³¼ì •ì´ ì™„ë£Œë˜ì—ˆìŠµë‹ˆë‹¤! ëª¨ë¸ ì•„í‚¤í…ì²˜ë¥¼ ì¸ì‡„í•˜ê³  ì‹œê°í™”í•˜ë©° ì–‘ìí™”ëœ ëª¨ë¸ì˜ ì •í™•ì„±ë„ ê²€ì¦í•´ ë³´ê² ìŠµë‹ˆë‹¤.\n\nQuestion 9.1 (5 pts)\nì–‘ìí™”ëœ ëª¨ë¸ì„ ì‹¤í–‰í•˜ê¸° ìœ„í•´ì„œëŠ” (0, 1) ë²”ìœ„ì˜ ì…ë ¥ ë°ì´í„°ë¥¼ (-128, 127) ë²”ìœ„ì˜ int8 ë²”ìœ„ë¡œ ë§¤í•‘í•˜ëŠ” ì¶”ê°€ì ì¸ ì „ì²˜ë¦¬ê°€ í•„ìš”í•©ë‹ˆë‹¤. ì´ ì „ì²˜ë¦¬ë¥¼ ì§„í–‰í•˜ëŠ” ì•„ë˜ ì½”ë“œë¥¼ ì™„ì„±í•˜ì„¸ìš”.\nHint: ì–‘ìí™”ëœ ëª¨ë¸ì€ fp32 ëª¨ë¸ê³¼ ê±°ì˜ ë™ì¼í•œ ì •í™•ë„ë¥¼ ê°€ì§€ê³  ìˆìŠµë‹ˆë‹¤.\n\nprint(quantized_model)\n\ndef extra_preprocess(x):\n    # hint: you need to convert the original fp32 input of range (0, 1)\n    #  into int8 format of range (-128, 127)\n    ############### YOUR CODE STARTS HERE ###############\n    x_scaled = x * 255\n    x_shifted = x_scaled - 128\n    return x_shifted.clamp(-128, 127).to(torch.int8)\n    ############### YOUR CODE ENDS HERE #################\n\nint8_model_accuracy = evaluate(quantized_model, dataloader['test'],\n                               extra_preprocess=[extra_preprocess])\nprint(f\"int8 model has accuracy={int8_model_accuracy:.2f}%\")\n\nVGG(\n  (backbone): Sequential(\n    (0): QuantizedConv2d()\n    (1): QuantizedConv2d()\n    (2): QuantizedMaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n    (3): QuantizedConv2d()\n    (4): QuantizedConv2d()\n    (5): QuantizedMaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n    (6): QuantizedConv2d()\n    (7): QuantizedConv2d()\n    (8): QuantizedMaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n    (9): QuantizedConv2d()\n    (10): QuantizedConv2d()\n    (11): QuantizedMaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n    (12): QuantizedAvgPool2d(kernel_size=2, stride=2, padding=0)\n  )\n  (classifier): QuantizedLinear()\n)\nint8 model has accuracy=10.00%"
  },
  {
    "objectID": "posts/labs/lab02.html#question-9.2-bonus-question-5-pts",
    "href": "posts/labs/lab02.html#question-9.2-bonus-question-5-pts",
    "title": "ğŸ‘©â€ğŸ’» Lab 2",
    "section": "Question 9.2 (Bonus Question; 5 pts)",
    "text": "Question 9.2 (Bonus Question; 5 pts)\nlinear quantized modelì— ReLU ì¸µì´ ì—†ëŠ” ì´ìœ ë¥¼ ì„¤ëª…í•˜ì„¸ìš”.\nYour Answer:\nì„ í˜•(Linear) ì–‘ìí™” ëª¨ë¸ì—ì„œ ReLU(Rectified Linear Unit) ì¸µì´ ì—†ëŠ” ì´ìœ ëŠ” ì£¼ë¡œ ì–‘ìí™” ê³¼ì •ì—ì„œì˜ ë°ì´í„° í‘œí˜„ ë°©ì‹ê³¼ ì—°ì‚°ì˜ íš¨ìœ¨ì„±ê³¼ ê´€ë ¨ì´ ìˆìŠµë‹ˆë‹¤. ì–‘ìí™”ëŠ” ëª¨ë¸ì˜ ê°€ì¤‘ì¹˜ë‚˜ í™œì„±í™”ë¥¼ ê³ ì •ëœ ë¹„íŠ¸ ë„ˆë¹„(ì˜ˆ: 8ë¹„íŠ¸)ì˜ ì •ìˆ˜ë¡œ ì œí•œí•˜ì—¬ ì €ì¥í•˜ê³  ê³„ì‚°í•˜ëŠ” ê¸°ìˆ ì…ë‹ˆë‹¤. ì´ëŸ¬í•œ ì œí•œì€ ëª¨ë¸ì˜ í¬ê¸°ë¥¼ ì¤„ì´ê³ , ê³„ì‚° ì†ë„ë¥¼ í–¥ìƒì‹œí‚¤ë©°, ì €ì „ë ¥ ì¥ì¹˜ì—ì„œì˜ ì‹¤í–‰ì„ ìš©ì´í•˜ê²Œ í•©ë‹ˆë‹¤. ê·¸ëŸ¬ë‚˜ ì´ ê³¼ì •ì—ì„œ ë°ì´í„°ì˜ ì •ë°€ë„ê°€ ì†ì‹¤ë  ìˆ˜ ìˆìœ¼ë©°, ì´ëŠ” ëª¨ë¸ ì„±ëŠ¥ì— ì˜í–¥ì„ ë¯¸ì¹  ìˆ˜ ìˆìŠµë‹ˆë‹¤.\nReLU í™œì„±í™” í•¨ìˆ˜ëŠ” ì…ë ¥ì´ ì–‘ìˆ˜ì¼ ê²½ìš° ê·¸ëŒ€ë¡œ ì¶œë ¥í•˜ê³ , ìŒìˆ˜ì¼ ê²½ìš° 0ìœ¼ë¡œ ë§Œë“œëŠ” ê°„ë‹¨í•˜ê³  íš¨ìœ¨ì ì¸ ë¹„ì„ í˜• í•¨ìˆ˜ì…ë‹ˆë‹¤. ReLUëŠ” ë”¥ëŸ¬ë‹ ëª¨ë¸ì—ì„œ ë„ë¦¬ ì‚¬ìš©ë˜ë©°, íŠ¹íˆ ì€ë‹‰ì¸µì—ì„œ ë¹„ì„ í˜•ì„±ì„ ì¶”ê°€í•˜ì—¬ ëª¨ë¸ì˜ í‘œí˜„ë ¥ì„ í–¥ìƒì‹œí‚¤ëŠ” ë° ì¤‘ìš”í•œ ì—­í• ì„ í•©ë‹ˆë‹¤.\nì„ í˜• ì–‘ìí™” ëª¨ë¸ì—ì„œ ReLU ì¸µì´ ì—†ëŠ” ì£¼ëœ ì´ìœ ëŠ” ë‹¤ìŒê³¼ ê°™ìŠµë‹ˆë‹¤:\n\nì–‘ìí™”ëœ ë°ì´í„°ì˜ ë²”ìœ„ ì œí•œ: ì •ìˆ˜ ì–‘ìí™” ê³¼ì •ì—ì„œëŠ” ë°ì´í„°ê°€ íŠ¹ì • ë²”ìœ„ ë‚´ì˜ ê°’ìœ¼ë¡œ ì œí•œë©ë‹ˆë‹¤. ì˜ˆë¥¼ ë“¤ì–´, 8ë¹„íŠ¸ ì–‘ìí™”ì—ì„œëŠ” ê°’ì´ -128ë¶€í„° 127ê¹Œì§€ì˜ ì •ìˆ˜ ë²”ìœ„ë¥¼ ê°€ì§‘ë‹ˆë‹¤. ì´ëŸ¬í•œ ë²”ìœ„ ë‚´ì—ì„œ ReLUë¥¼ ì ìš©í•˜ë©´ ìŒìˆ˜ ê°’ì´ ëª¨ë‘ 0ìœ¼ë¡œ ë³€í™˜ë˜ì–´, ì–‘ìˆ˜ ê°’ë§Œ ë‚¨ê²Œ ë©ë‹ˆë‹¤. ì´ ê³¼ì •ì—ì„œ ë°ì´í„°ì˜ ë²”ìœ„ê°€ ë”ìš± ì œí•œë˜ì–´, ì–‘ìí™”ëœ ëª¨ë¸ì˜ í‘œí˜„ë ¥ì´ ë”ìš± ê°ì†Œí•  ìˆ˜ ìˆìŠµë‹ˆë‹¤.\níš¨ìœ¨ì„±: ì–‘ìí™”ëœ ëª¨ë¸ì€ ê°€ëŠ¥í•œ í•œ ê³„ì‚°ì„ ê°„ë‹¨í•˜ê²Œ ìœ ì§€í•˜ì—¬ ë¹ ë¥¸ ì¶”ë¡  ì†ë„ì™€ ë‚®ì€ ì „ë ¥ ì†Œëª¨ë¥¼ ë‹¬ì„±í•˜ë ¤ê³  í•©ë‹ˆë‹¤. ReLUì™€ ê°™ì€ ë¹„ì„ í˜• í•¨ìˆ˜ë¥¼ ì¶”ê°€í•˜ë©´, ì¶”ë¡  ê³¼ì •ì—ì„œ ì¶”ê°€ì ì¸ ê³„ì‚°ì´ í•„ìš”í•˜ê²Œ ë©ë‹ˆë‹¤. ì–´ë–¤ ê²½ìš°ì—ëŠ” ëª¨ë¸ì˜ êµ¬ì¡°ë‚˜ ëª©ì ì— ë”°ë¼ ì´ëŸ¬í•œ ì¶”ê°€ ê³„ì‚° ì—†ì´ë„ ì¶©ë¶„í•œ ì„±ëŠ¥ì„ ë‹¬ì„±í•  ìˆ˜ ìˆìœ¼ë¯€ë¡œ, ReLU ì¸µì„ ìƒëµí•  ìˆ˜ ìˆìŠµë‹ˆë‹¤.\nëª¨ë¸ ì„¤ê³„ì™€ ëª©ì : íŠ¹ì • ì–‘ìí™” ëª¨ë¸ì—ì„œëŠ” ì„±ëŠ¥ ìœ ì§€ë¥¼ ìœ„í•´ ReLU ëŒ€ì‹  ë‹¤ë¥¸ ê¸°ë²•ì´ë‚˜ í™œì„±í™” í•¨ìˆ˜ë¥¼ ì‚¬ìš©í•  ìˆ˜ ìˆìŠµë‹ˆë‹¤. ì˜ˆë¥¼ ë“¤ì–´, ì–‘ìí™” ì „ ëª¨ë¸ì—ì„œ ReLUë¥¼ ì‚¬ìš©í•˜ëŠ” ëŒ€ì‹ , ì–‘ìí™” ê³¼ì •ì—ì„œ ìµœì í™”ëœ í™œì„±í™” í•¨ìˆ˜ë¥¼ ì„ íƒí•˜ê±°ë‚˜, ReLUì˜ íš¨ê³¼ë¥¼ ëª¨ë°©í•  ìˆ˜ ìˆëŠ” ë‹¤ë¥¸ ë°©ë²•ì„ ëª¨ìƒ‰í•  ìˆ˜ ìˆìŠµë‹ˆë‹¤.\n\nê²°ë¡ ì ìœ¼ë¡œ, ì„ í˜• ì–‘ìí™” ëª¨ë¸ì—ì„œ ReLU ì¸µì˜ ë¶€ì¬ëŠ” ë°ì´í„°ì˜ ë²”ìœ„ ì œí•œ, ê³„ì‚° íš¨ìœ¨ì„±, ê·¸ë¦¬ê³  íŠ¹ì • ëª¨ë¸ ì„¤ê³„ì™€ ëª©ì ì— ê¸°ì¸í•  ìˆ˜ ìˆìŠµë‹ˆë‹¤. ëª¨ë¸ì˜ ì„¤ê³„ìëŠ” ì„±ëŠ¥, ì†ë„, í¬ê¸° ë“±ì˜ ìš”êµ¬ ì‚¬í•­ì„ ê· í˜• ìˆê²Œ ê³ ë ¤í•˜ì—¬ ìµœì ì˜ ëª¨ë¸ êµ¬ì¡°ë¥¼ ê²°ì •í•´ì•¼ í•©ë‹ˆë‹¤."
  },
  {
    "objectID": "posts/lecs/lec04.html",
    "href": "posts/lecs/lec04.html",
    "title": "ğŸ§‘â€ğŸ« Lecture 4",
    "section": "",
    "text": "ì´ì „ í¬ìŠ¤íŒ…ì—ì„œ Pruningì— ëŒ€í•´ì„œ ë°°ì› ì—ˆë‹¤. ì´ë²ˆì—ëŠ” Pruningì— ëŒ€í•œ ë‚¨ì€ ì´ì•¼ê¸°ì¸ Pruning Ratioë¥¼ ì •í•˜ëŠ” ë°©ë²•, Fine-tuning ê³¼ì •ì— ëŒ€í•´ ì•Œì•„ë³´ê³ , ë§ˆì§€ë§‰ìœ¼ë¡œ Sparsityë¥¼ ìœ„í•œ System Supportì— ëŒ€í•´ ì•Œì•„ë³´ê³ ì í•œë‹¤."
  },
  {
    "objectID": "posts/lecs/lec04.html#pruning-ratio",
    "href": "posts/lecs/lec04.html#pruning-ratio",
    "title": "ğŸ§‘â€ğŸ« Lecture 4",
    "section": "1. Pruning Ratio",
    "text": "1. Pruning Ratio\nPruningì„ í•˜ê¸° ìœ„í•´ì„œ ì–´ëŠ ì •ë„ Pruningì„ í•´ì•¼ í• ì§€ ì–´ë–»ê²Œ ì •í•´ì•¼ í• ê¹Œ?\nì¦‰, ë‹¤ì‹œ ë§í•´ì„œ ëª‡ % ì •ë„ ê·¸ë¦¬ê³  ì–´ë–»ê²Œ Pruningì„ í•´ì•¼ ì¢‹ì„ê¹Œ?\n\n\n\nPruning ë°©ì‹ ë¹„êµ\n\n\nìš°ì„  Channel ë³„ Pruningì„ í•  ë•Œ, Channel êµ¬ë¶„ ì—†ì´ ë™ì¼í•œ Pruning ë¹„ìœ¨(Uniform)ì„ ì ìš©í•˜ë©´ ì„±ëŠ¥ì´ ì¢‹ì§€ ì•Šë‹¤. ì˜¤ë¥¸ìª½ ê·¸ë˜í”„ì—ì„œ ì§€í–¥í•´ì•¼ í•˜ëŠ” ë°©í–¥ì€ LatencyëŠ” ì ê²Œ, AccuracyëŠ” ë†’ê²Œì´ë¯€ë¡œ ì™¼ìª½ ìƒë‹¨ì˜ ì˜ì—­ì´ ë˜ë„ë¡ Pruningì„ ì§„í–‰í•´ì•¼ í•œë‹¤. ê·¸ë ‡ë‹¤ë©´ ê²°ë¡ ì€ Channel ë³„ êµ¬ë¶„ì„ í•´ì„œ ì–´ë–¤ Channelì€ Pruning ë¹„ìœ¨ì„ ë†’ê²Œ, ì–´ë–¤ Channelì€ Pruning ë¹„ìœ¨ì„ ë‚®ê²Œ í•´ì•¼ í•œë‹¤ëŠ” ì´ì•¼ê¸°ê°€ ëœë‹¤.\n\n1.1 Sensitiviy Analysis\nChannel ë³„ êµ¬ë¶„ì„ í•´ì„œ Pruningì„ í•œë‹¤ëŠ” ê¸°ë³¸ ì•„ì´ë””ì–´ëŠ” ì•„ë˜ì™€ ê°™ë‹¤.\n\nAccuracyì— ì˜í–¥ì„ ë§ì´ ì£¼ëŠ” LayerëŠ” Pruningì„ ì ê²Œ í•´ì•¼ í•œë‹¤.\nAccuracyì— ì˜í–¥ì„ ì ê²Œ ì£¼ëŠ” LayerëŠ” Pruningì„ ë§ì´ í•´ì•¼ í•œë‹¤.\n\nAccuracyë¥¼ ë˜ë„ë¡ì´ë©´ ì›ë˜ì˜ ëª¨ë¸ë³´ë‹¤ ëœ ë–¨ì–´ì§€ê²Œ ë§Œë“¤ë©´ì„œ Pruningì„ í•´ì„œ ëª¨ë¸ì„ ê°€ë³ê²Œ ë§Œë“œëŠ” ê²ƒì´ ëª©í‘œì´ê¸° ë•Œë¬¸ì— ë‹¹ì—°í•œ ì•„ì´ë””ì–´ì¼ ê²ƒì´ë‹¤. Accuracyì— ì˜í–¥ì„ ë§ì´ ì¤€ë‹¤ëŠ” ë§ì€ Sensitiveí•œ Layerì´ë‹¤ë¼ëŠ” í‘œí˜„ìœ¼ë¡œ ë‹¤ë¥´ê²Œ ë§í•  ìˆ˜ ìˆë‹¤. ë”°ë¼ì„œ ê° Layerì˜ Senstivityë¥¼ ì¸¡ì •í•´ì„œ Sensitiveí•œ LayerëŠ” Pruning Ratioë¥¼ ë‚®ê²Œ ì„¤ê³„í•˜ë©´ ëœë‹¤.\nLayerì˜ Sensitivityë¥¼ ì¸¡ì •í•˜ê¸° ìœ„í•´ Sensitivity Analysisë¥¼ ì§„í–‰í•´ë³´ì. ë‹¹ì—°íˆ íŠ¹ì • Layerì˜ Pruning Ratioê°€ ë†’ì„ ìˆ˜ë¡ weightê°€ ë§ì´ ê°€ì§€ì¹˜ê¸° ëœ ê²ƒì´ë¯€ë¡œ AccuracyëŠ” ë–¨ì–´ì§€ê²Œ ëœë‹¤.\n\n\n\nL0 Pruning Rate ê·¸ë˜í”„\n\n\n\n\n\n\n\n\nPruningì„ í•˜ëŠ” WeightëŠ” ì–´ë–»ê²Œ ê²°ì •í•˜ë‚˜ìš”?\n\n\n\n\n\nPruning Ratioì— ì˜í•´ Pruned ë˜ëŠ” weightëŠ” ì´ì „ ê°•ì˜ì—ì„œ ë°°ìš´ â€œImportance(weightì˜ ì ˆëŒ“ê°’ í¬ê¸°)â€ì— ë”°ë¼ ì„ íƒëœë‹¤.\n\n\n\nìœ„ì˜ ê·¸ë¦¼ì—ì„œ ì²˜ëŸ¼ Layer 0(L0)ë§Œì„ ê°€ì§€ê³  Pruning Ratioë¥¼ ë†’ì—¬ê°€ë©´ì„œ ê´€ì°°í•´ë³´ë©´, ì•½ 70% ì´í›„ë¶€í„°ëŠ” Accuracyê°€ ê¸‰ê²©í•˜ê²Œ ë–¨ì–´ì§€ëŠ” ê²ƒì„ ë³¼ ìˆ˜ ìˆë‹¤. L0ì—ì„œ Ratioë¥¼ ë†’ì—¬ê°€ë©° Accuracyì˜ ë³€í™”ë¥¼ ê´€ì°°í•œ ê²ƒì²˜ëŸ¼ ë‹¤ë¥¸ Layerë“¤ë„ ê´€ì°°í•´ë³´ì.\n\n\n\nLayerë³„ Sensitivity ë¹„êµ\n\n\nL1ì€ ë‹¤ë¥¸ Layerë“¤ì— ë¹„í•´ ìƒëŒ€ì ìœ¼ë¡œ Pruning Ratioë¥¼ ë†’ì—¬ê°€ë„ Accuracyì˜ ë–¨ì–´ì§€ëŠ” ì •ë„ê°€ ì•½í•œ ë°˜ë©´, L0ëŠ” ë‹¤ë¥¸ Layerë“¤ì— ë¹„í•´ ìƒëŒ€ì ìœ¼ë¡œ Pruning Ratioë¥¼ ë†’ì—¬ê°€ë©´ Accuracyì˜ ë–¨ì–´ì§€ëŠ” ì •ë„ê°€ ì‹¬í•œ ê²ƒì„ í™•ì¸í•  ìˆ˜ ìˆë‹¤. ë”°ë¼ì„œ L1ì€ Sensitivityê°€ ë†’ë‹¤ê³  ë³¼ ìˆ˜ ìˆìœ¼ë©° Pruningì„ ì ê²Œí•´ì•¼ í•˜ê³ , L0ì€ Sensitivityê°€ ë‚®ë‹¤ê³  ë³¼ ìˆ˜ ìˆìœ¼ë©° Pruningì„ ë§ê²Œí•´ì•¼ í•¨ì„ ì•Œ ìˆ˜ ìˆë‹¤.\nì—¬ê¸°ì„œ Sensitivity Analysisì—ì„œ ê³ ë ¤í•´ì•¼í•  ëª‡ê°€ì§€ ì‚¬í•­ë“¤ì— ëŒ€í•´ì„œ ì§šê³  ë„˜ì–´ê°€ì.\n\nSensitivity Analysisì—ì„œ ëª¨ë“  Layerë“¤ì´ ë…ë¦½ì ìœ¼ë¡œ ì‘ë™í•œë‹¤ëŠ” ê²ƒì„ ì „ì œë¡œ í•œë‹¤. ì¦‰, L0ì˜ Pruningì´ L1ì˜ íš¨ê³¼ì— ì˜í–¥ì„ ì£¼ì§€ ì•ŠëŠ” ë…ë¦½ì„±ì„ ê°€ì§„ë‹¤ëŠ” ê²ƒì„ ì „ì œë¡œ í•œë‹¤.\nLayerì˜ Pruning Ratioê°€ ë™ì¼í•˜ë‹¤ê³  í•´ì„œ Pruned Weightìˆ˜ê°€ ê°™ìŒì„ ì˜ë¯¸í•˜ì§€ ì•ŠëŠ”ë‹¤.\n\n100ê°œì˜ weightê°€ ìˆëŠ” layerì˜ 10% Pruning Ratio ì ìš©ì€ 10ê°œì˜ weightê°€ pruned ë˜ì—ˆìŒì„ ì˜ë¯¸í•˜ê³ , 500ê°œì˜ weightê°€ ìˆëŠ” layerì˜ 10% Pruning Ratio ì ìš©ì€ 50ê°œì˜ weightê°€ pruned ë˜ì—ˆìŒì„ ì˜ë¯¸í•œë‹¤.\nLayerì˜ ì „ì²´ í¬ê¸°ì— ë”°ë¼ Pruning Ratioì˜ ì ìš© íš¨ê³¼ëŠ” ë‹¤ë¥¼ ìˆ˜ ìˆë‹¤.\n\n\nSensitivity Analysisê¹Œì§€ ì§„í–‰í•œ í›„ì—ëŠ” ë³´í†µ ì‚¬ëŒì´ Accuracyê°€ ë–¨ì–´ì§€ëŠ” ì •ë„, thresholdë¥¼ ì •í•´ì„œ Pruning Ratioë¥¼ ì •í•œë‹¤.\n\n\n\nThreshold ì •í•˜ê¸°\n\n\nìœ„ ê·¸ë˜í”„ì—ì„œëŠ” Accuracyê°€ ì•½ 75%ìˆ˜ì¤€ìœ¼ë¡œ ìœ ì§€ë˜ëŠ” threhsold \\(T\\) ìˆ˜í‰ì„ ì„ ê¸°ì¤€ìœ¼ë¡œ L0ëŠ” ì•½ 74%, L4ëŠ” ì•½ 80%, L3ëŠ” ì•½ 82%, L2ëŠ” 90%ê¹Œì§€ Pruningì„ ì§„í–‰í•´ì•¼ ê² ë‹¤ê³  ì •í•œ ì˜ˆì‹œë¥¼ ë³´ì—¬ì¤€ë‹¤. ë¯¼ê°í•œ layerì¸ L0ëŠ” ìƒëŒ€ì ìœ¼ë¡œ Pruningì„ ì ê²Œ, ëœ ë¯¼ê°í•œ layerì¸ L2ëŠ” Pruningì„ ë§ê²Œ í•˜ëŠ” ê²ƒì„ í™•ì¸í•  ìˆ˜ ìˆë‹¤.\në¬¼ë¡  ì‚¬ëŒì´ ì •í•˜ëŠ” thresholdëŠ” ê°œì„ ì˜ ì—¬ì§€ê°€ ë¬¼ë¡  ìˆë‹¤. Pruning Ratioë¥¼ ì¢€ ë” Automaticí•˜ê²Œ ì°¾ëŠ” ë°©ë²•ì— ëŒ€í•´ ì•Œì•„ë³´ì.\n\n\n1.2 AMC\nAMCëŠ” AutoML for Model Compressionì˜ ì•½ìë¡œ, ê°•í™”í•™ìŠµ(Reinforcement Learning) ë°©ë²•ìœ¼ë¡œ ìµœì ì˜ Pruning Ratioë¥¼ ì°¾ë„ë¡ í•˜ëŠ” ë°©ë²•ì´ë‹¤.\n\n\n\nAMC ì „ì²´ êµ¬ì¡°\n\n\nAMCì˜ êµ¬ì¡°ëŠ” ìœ„ ê·¸ë¦¼ê³¼ ê°™ë‹¤. ê°•í™”í•™ìŠµ ì•Œê³ ë¦¬ì¦˜ ê³„ì—´ ì¤‘, Actor-Critic ê³„ì—´ì˜ ì•Œê³ ë¦¬ì¦˜ì¸ Deep Deterministic Policy Gradient(DDPG)ì„ í™œìš©í•˜ì—¬ Pruning Ratioë¥¼ ì •í•˜ëŠ” Actionì„ ì„ íƒí•˜ë„ë¡ í•™ìŠµí•œë‹¤. ìì„¸í•œ MDP(Markov Decision Process) ì„¤ê³„ëŠ” ì•„ë˜ì™€ ê°™ë‹¤.\n\n\n\nAMCì˜ MDP\n\n\nê°•í™”í•™ìŠµ Agentì˜ í•™ìŠµ ë°©í–¥ì„ ê²°ì •í•˜ëŠ” ì¤‘ìš”í•œ Reward Functionì€ ëª¨ë¸ì˜ Accuracyë¥¼ ê³ ë ¤í•´ì„œ Errorë¥¼ ì¤„ì´ë„ë¡ ìœ ë„í•  ë¿ë§Œ ì•„ë‹ˆë¼ Latencyë¥¼ ê°„ì ‘ì ìœ¼ë¡œ ê³ ë ¤í•  ìˆ˜ ìˆë„ë¡ ëª¨ë¸ì˜ ì—°ì‚°ëŸ‰ì„ ë‚˜íƒ€ë‚´ëŠ” FLOPë¥¼ ì ê²Œ í•˜ë„ë¡ ìœ ë„í•˜ë„ë¡ ì„¤ê³„í•œë‹¤. ì˜¤ë¥¸ìª½ì— ëª¨ë¸ë“¤ì˜ ì—°ì‚°ëŸ‰ ë³„(Operations) Top-1 Accuracy ê·¸ë˜í”„ë¥¼ ë³´ë©´ ì—°ì‚°ëŸ‰ì´ ë§ì„ìˆ˜ë¡ ë¡œê·¸í•¨ìˆ˜ì²˜ëŸ¼ Accuracyê°€ ì¦ê°€í•˜ëŠ” ê²ƒì„ ë³´ê³  ì´ë¥¼ ë³´ê³  ë°˜ì˜í•œ ë¶€ë¶„ì´ë¼ê³  ë³¼ ìˆ˜ ìˆë‹¤.\n\n\n\nAMCì˜ Reward Function\n\n\nì´ë ‡ê²Œ AMCë¡œ Pruningì„ ì§„í–‰í–ˆì„ ë•Œ, Human Expertê°€ Pruning í•œ ê²ƒê³¼ ë¹„êµí•´ë³´ì. ì•„ë˜ ëª¨ë¸ ì„¹ì…˜ë³„ Density íˆìŠ¤í† ê·¸ë¨ ê·¸ë˜í”„ì—ì„œ Totalì„ ë³´ë©´, ë™ì¼ Accuracyê°€ ë‚˜ì˜¤ë„ë¡ Pruningì„ ì§„í–‰í–ˆì„ ë•Œ AMCë¡œ Pruningì„ ì§„í–‰í•œ ê²ƒ(ì£¼í™©ìƒ‰)ì´ Human Expert Pruning ëª¨ë¸(íŒŒë€ìƒ‰)ë³´ë‹¤ Densityê°€ ë‚®ì€ ê²ƒì„ í™•ì¸í•  ìˆ˜ ìˆë‹¤. ì¦‰, AMCë¡œ Pruning ì§„í–‰í–ˆì„ ë•Œ ë” ë§ì€ weightë¥¼ Pruning ë” ê°€ë²¼ìš´ ëª¨ë¸ì„ ê°€ì§€ê³ ë„ Accuracyë¥¼ ìœ ì§€í–ˆë‹¤ê³  ë³¼ ìˆ˜ ìˆë‹¤.\n\n\n\nAMCì˜ Density Graph\n\n\në‘ë²ˆì§¸ êº¾ì€ ì„  ê·¸ë˜í”„ì—ì„œ AMCë¥¼ ê°€ì§€ê³  Pruningê³¼ Fine-tuningì„ ë²ˆê°ˆì•„ ê°€ë©° ì—¬ëŸ¬ ìŠ¤í…ìœ¼ë¡œ ì§„í–‰í•´ê°€ë©´ì„œ ê´€ì°°í•œ ê²ƒì„ ì¡°ê¸ˆ ë” ìì„¸íˆ ì‚´í´ë³´ì. ê° Iteration(Pruning+Fine-tuning)ì„ stage1, 2, 3, 4ë¡œ ë‚˜íƒ€ë‚´ì–´ plotí•œ ê²ƒì„ ë³´ë©´, 1x1 convë³´ë‹¤ 3x3 convì—ì„œ Densityê°€ ë” ë‚®ì€ ê²ƒì„ í™•ì¸í•  ìˆ˜ ìˆë‹¤. ì¦‰, 3x3 convì—ì„œ 1x1 convë³´ë‹¤ Pruningì„ ë§ì´ í•œ ê²ƒì„ ë³¼ ìˆ˜ ìˆë‹¤. ì´ë¥¼ í•´ì„í•´ë³´ìë©´, AMCê°€ 3x3 convì„ Pruningí•˜ë©´ 9ê°œì˜ weightë¥¼ pruningí•˜ê³  ì´ëŠ” 1x1 conv pruningí•´ì„œ 1ê°œì˜ weightë¥¼ ì—†ì• ëŠ” ê²ƒë³´ë‹¤ í•œë²ˆì— ë” ë§ì€ weight ìˆ˜ë¥¼ ì¤„ì¼ ìˆ˜ ìˆê¸° ë•Œë¬¸ì— 3x3 conv pruningì„ ì ê·¹ í™œìš©í–ˆì„ ê²ƒìœ¼ë¡œ ë³¼ ìˆ˜ ìˆë‹¤.\n\n\n\nAMC Result\n\n\nì´ AMC ì‹¤í—˜ ê²°ê³¼í‘œì—ì„œ ë³´ë©´, FLOPì™€ Time ê°ê° 50%ë¡œ ì¤„ì¸ AMC ëª¨ë¸ ë‘˜ë‹¤ Top-1 Accuracyê°€ ê¸°ì¡´ì˜ 1.0 MobileNetì˜ Accuracyë³´ë‹¤ ì•½ 0.1~0.4% ì •ë„ë§Œ ì¤„ê³  Latencyë‚˜ SpeedUpì´ íš¨ìœ¨ì ìœ¼ë¡œ ì¡°ì •ëœ ê²ƒì„ í™•ì¸í•  ìˆ˜ ìˆë‹¤.\n\n\n\n\n\n\nAMC ì‹¤í—˜ ê²°ê³¼í‘œì—ì„œ 0.75 MobileNetì˜ SpeedUpì´ ì™œ 1.7x ì¸ê°€ìš”?\n\n\n\n\n\nê²°ê³¼í‘œì—ì„œ 0.75 MobileNetì€ 25%ì˜ weightë¥¼ ê°ì†Œì‹œí‚¨ ê²ƒì´ê¸° ë•Œë¬¸ì— SpeedUpì´ \\(\\frac{4}{3} \\simeq 1.3\\)xì´ì–´ì•¼ í•œë‹¤ê³  ìƒê°í•  ìˆ˜ ìˆë‹¤. í•˜ì§€ë§Œ ì—°ì‚°ëŸ‰ì€ quadraticí•˜ê²Œ ê°ì†Œí•˜ê²Œ ë˜ê¸° ë•Œë¬¸ì— \\(\\frac{4}{3} \\cdot \\frac{4}{3} \\simeq 1.7\\)xë¡œ SpeedUpì´ ëœë‹¤.\n\n\n\n\n\n1.3 NetAdapt\në˜ ë‹¤ë¥¸ Pruning Ratioë¥¼ ì •í•˜ëŠ” ê¸°ë²•ìœ¼ë¡œ NetAdaptì´ ìˆë‹¤. Latency Constraintë¥¼ ê°€ì§€ê³  layerë§ˆë‹¤ pruningì„ ì ìš©í•´ë³¸ë‹¤. ì˜ˆë¥¼ ë“¤ì–´, ì¤„ì¼ ëª©í‘œ latency ëŸ‰ì„ lmsë¡œ ì •í•˜ë©´, 10ms â†’ 9msë¡œ ì¤„ ë•Œê¹Œì§€ layerì˜ pruning ratioë¥¼ ë†’ì—¬ê°€ëŠ” ë°©ë²•ì´ë‹¤.\n\n\n\nNetAdapt\n\n\nNetAdaptì˜ ì „ì²´ì ì¸ ê³¼ì •ì€ ì•„ë˜ì™€ ê°™ì´ ì§„í–‰ëœë‹¤. ê¸°ì¡´ ëª¨ë¸ì—ì„œ ê° layerë¥¼ Latency Constraintì— ë„ë‹¬í•˜ë„ë¡ Pruningí•˜ë©´ì„œ Accuracy(\\(Acc_A\\)ë“±)ì„ ë°˜ë³µì ìœ¼ë¡œ ì¸¡ì •í•œë‹¤.\n\nê° layerì˜ pruning ratioë¥¼ ì¡°ì ˆí•œë‹¤.\nShort-term fine tuningì„ ì§„í–‰í•œë‹¤.\nLatency Constraintì— ë„ë‹¬í–ˆëŠ”ì§€ í™•ì¸í•œë‹¤.\nLatency Constraint ë„ë‹¬í•˜ë©´ í•´ë‹¹ layerì˜ ìµœì ì˜ Pruning ratioë¡œ íŒë‹¨í•œë‹¤.\nê° layerì˜ ìµœì  Pruning ratioê°€ ì •í•´ì¡Œë‹¤ë©´ ë§ˆì§€ë§‰ìœ¼ë¡œ Long-term fine tuningì„ ì§„í–‰í•œë‹¤.\n\n\n\n\nNetAdapt ê³¼ì •\n\n\nì´ì™€ ê°™ì´ NetAdaptì˜ ê³¼ì •ì„ ì§„í–‰í•˜ë©´ ì•„ë˜ì™€ ê°™ì€ ì‹¤í—˜ ê²°ê³¼ë¥¼ ë³¼ ìˆ˜ ìˆë‹¤. Uniformí•˜ê²Œ Pruningì„ ì§„í–‰í•œ Multipilersë³´ë‹¤ NetAdaptê°€ 1.7x ë” ë¹ ë¥´ê³  ì˜¤íˆë ¤ AccuracyëŠ” ì•½ 0.3% ì •ë„ ë†’ì€ ê²ƒì„ ì•Œ ìˆ˜ ìˆë‹¤.\n\n\n\nNetAdaptì˜ Latency / Top-1 Accuracy ê·¸ë˜í”„"
  },
  {
    "objectID": "posts/lecs/lec04.html#fine-tuningtrain",
    "href": "posts/lecs/lec04.html#fine-tuningtrain",
    "title": "ğŸ§‘â€ğŸ« Lecture 4",
    "section": "2. Fine-tuning/Train",
    "text": "2. Fine-tuning/Train\nPrunned ëª¨ë¸ì˜ í¼í¬ë¨¼ìŠ¤ë¥¼ í–¥ìƒí•˜ê¸° ìœ„í•´ì„œëŠ” Pruningë¥¼ ì§„í–‰í•˜ê³  ë‚˜ì„œ Fine-tuning ê³¼ì •ì´ í•„ìš”í•˜ë‹¤.\n\n2.1 Iterative Pruning\në³´í†µ Pruned ëª¨ë¸ì˜ Fine-tuning ê³¼ì •ì—ì„œëŠ” ê¸°ì¡´ì— í•™ìŠµí–ˆë˜ learning rateë³´ë‹¤ ì‘ì€ rateë¥¼ ì‚¬ìš©í•œë‹¤. ì˜ˆë¥¼ë“¤ì–´ ê¸°ì¡´ì˜ ëª¨ë¸ì„ í•™ìŠµí•  ë•Œ ì‚¬ìš©í•œ learning rateì˜ \\(1/100\\) ë˜ëŠ” \\(1/10\\)ì„ ì‚¬ìš©í•œë‹¤. ë˜í•œ Pruning ê³¼ì •ê³¼ Fine-tuning ê³¼ì •ì€ 1ë²ˆë§Œ ì§„í–‰í•˜ê¸°ë³´ë‹¤ ì ì°¨ì ìœ¼ë¡œ pruning ratioë¥¼ ëŠ˜ë ¤ê°€ë©° Pruning, Fine-tuningì„ ë²ˆê°ˆì•„ê°€ë©° ì—¬ëŸ¬ë²ˆ ì§„í–‰í•˜ëŠ”ê²Œ ë” ì¢‹ë‹¤.\n\n\n\nIterative Pruning + Fine-tuning ë¹„êµ ê·¸ë˜í”„\n\n\n\n\n2.2 Regularization\nTinyMLì˜ ëª©í‘œëŠ” ê°€ëŠ¥í•œ ë§ì€ weightë“¤ì„ 0ìœ¼ë¡œ ë§Œë“œëŠ” ê²ƒìœ¼ë¡œ ìƒê°í•  ìˆ˜ ìˆë‹¤. ê·¸ë˜ì•¼ ëª¨ë¸ì„ ê°€ë³ê²Œ ë§Œë“¤ ìˆ˜ ìˆê¸° ë•Œë¬¸ì´ë‹¤. ê·¸ë˜ì„œ Regularization ê¸°ë²•ì„ ì´ìš©í•´ì„œ ëª¨ë¸ì˜ weightë“¤ì„ 0ìœ¼ë¡œ, í˜¹ì€ 0ê³¼ ê°€ê¹ê²Œ ì‘ì€ ê°’ì„ ê°€ì§€ë„ë¡ ë§Œë“ ë‹¤. ì‘ì€ ê°’ì˜ weightê°€ ë˜ë„ë¡ í•˜ëŠ” ì´ìœ ëŠ” 0ê³¼ ê°€ê¹Œìš´ ì‘ì€ ê°’ë“¤ì€ ë‹¤ìŒ layerë“¤ë¡œ ë„˜ì–´ê°€ë©´ì„œ 0ì´ ë  ê°€ëŠ¥ì„±ì´ ë†’ì•„ì§€ê¸° ë•Œë¬¸ì´ë‹¤. ê¸°ì¡´ì˜ ë”¥ëŸ¬ë‹ ëª¨ë¸ë“¤ì˜ ê³¼ì í•©(Overfitting)ì„ ë§‰ê¸° ìœ„í•œ Regularization ê¸°ë²•ë“¤ê³¼ ë‹¤ë¥´ì§€ ì•Šìœ¼ë‚˜ ì˜ë„ì™€ ëª©ì ì€ ë‹¤ë¥¸ ê²ƒì„ ì§šì–´ë³¼ ìˆ˜ ìˆë‹¤.\n\n\n\nPruningì„ ìœ„í•œ Regularization\n\n\n\n\n2.3 The Lottery Ticket Hypothesis\n2019ë…„ ICLRì—ì„œ ë°œí‘œëœ ë…¼ë¬¸ì—ì„œ Jonathan Frankleê³¼ Michael Carbinì´ ì†Œê°œí•œ The Lottery Ticket Hypothesis(LTH)ì€ ì‹¬ì¸µ ì‹ ê²½ë§(DNN) í›ˆë ¨ì— ëŒ€í•œ í¥ë¯¸ë¡œìš´ ì•„ì´ë””ì–´ë¥¼ ì œì•ˆí•œë‹¤. ë¬´ì‘ìœ„ë¡œ ì´ˆê¸°í™”ëœ ëŒ€ê·œëª¨ ì‹ ê²½ë§ ë‚´ì— ë” ì‘ì€ í•˜ìœ„ ë„¤íŠ¸ì›Œí¬(Winning Ticket)ê°€ ì¡´ì¬í•œë‹¤ëŠ” ê²ƒì„ ë§í•œë‹¤. ì´ í•˜ìœ„ ë„¤íŠ¸ì›Œí¬ëŠ” ì²˜ìŒë¶€í„° ë³„ë„ë¡œ í›ˆë ¨í•  ë•Œ ì›ë˜ ë„¤íŠ¸ì›Œí¬ì˜ ì„±ëŠ¥ì— ë„ë‹¬í•˜ê±°ë‚˜ ëŠ¥ê°€í•  ìˆ˜ ìˆë‹¤. ì´ ê°€ì„¤ì€ ì´ëŸ¬í•œ Winning Ticketì´ í•™ìŠµí•˜ëŠ” ë° ì í•©í•œ ì´ˆê¸° ê°€ì¤‘ì¹˜ë¥¼ ê°–ëŠ”ë‹¤ê³  ê°€ì •í•œë‹¤.\n\n\n\nLTH ì„¤ëª… ê·¸ë¦¼"
  },
  {
    "objectID": "posts/lecs/lec04.html#system-support-for-sparsity",
    "href": "posts/lecs/lec04.html#system-support-for-sparsity",
    "title": "ğŸ§‘â€ğŸ« Lecture 4",
    "section": "3. System Support for Sparsity",
    "text": "3. System Support for Sparsity\nDNNì„ ê°€ì†í™” ì‹œí‚¤ëŠ” ë°©ë²•ì€ í¬ê²Œ 3ê°€ì§€, Sparse Weight, Sparse Activation, Weight Sharingì´ ìˆë‹¤. Sparse Weight, Sparse Activationì€ Pruningì´ê³  Weight Sharingì€ Quantizationì˜ ë°©ë²•ì´ë‹¤.\n\n\n\nDNNì„ ê°€ì†í™” ì‹œí‚¤ëŠ” ë°©ë²•\n\n\n\nSparse Weight: Weightë¥¼ Pruningí•˜ì—¬ Computationì€ Pruning Ratioì— ëŒ€ì‘í•˜ì—¬ ë¹¨ë¼ì§„ë‹¤. í•˜ì§€ë§Œ MemoryëŠ” Pruningëœ weightì˜ ìœ„ì¹˜ë¥¼ ê¸°ì–µí•˜ê¸° ìœ„í•œ memory ìš©ëŸ‰ì´ í•„ìš”í•˜ë¯€ë¡œ Pruning Ratioì— ë¹„ë¡€í•˜ì—¬ ì¤„ì§„ ì•ŠëŠ”ë‹¤.\nSparse Activation: Weightë¥¼ Pruningí•˜ëŠ” ê²ƒê³¼ ë‹¤ë¥´ê²Œ Activationì€ Test Inputì— ë”°ë¼ dynamic í•˜ë¯€ë¡œ Weightë¥¼ Pruningí•˜ëŠ” ê²ƒë³´ë‹¤ Computationì´ ëœ ì¤„ì–´ë“ ë‹¤.\nWeight Sharing: Quantization ë°©ë²•ìœ¼ë¡œ 32-bit dataë¥¼ 4-bit dataë¡œ ë³€ê²½í•¨ìœ¼ë¡œì¨ 8ë°°ì˜ memory ì ˆì•½ì„ í•  ìˆ˜ ìˆë‹¤.\n\n\n3.1 EIE\nEfficient Inference Engineì€ ê¸°ê³„ í•™ìŠµ ëª¨ë¸ì„ ì‹¤ì‹œê°„ìœ¼ë¡œ ì‹¤í–‰í•˜ê¸° ìœ„í•´ ìµœì í™”ëœ ì†Œí”„íŠ¸ì›¨ì–´ ë¼ì´ë¸ŒëŸ¬ë¦¬ë‚˜ í”„ë ˆì„ì›Œí¬ë¥¼ ë§í•œë‹¤. Processing Elements(PE)ì˜ êµ¬ì¡°\n\n\n\nPE ì—°ì‚° Logically / Physically ë¶„ì„\n\n\nì•„ë˜ ê·¸ë¦¼ì—ì„œ Inputë³„(\\(\\vec{a}\\)) ì—°ì‚°ì€ ì•„ë˜ì™€ ê°™ì´ Inputì´ 0ì¼ ë•ŒëŠ” skipë˜ê³  0ì´ ì•„ë‹ ë•ŒëŠ” prunning ë˜ì§€ ì•Šì€ weightì™€ ì—°ì‚°ì´ ì§„í–‰ëœë‹¤.\n\n\n\nInputë³„ ì—°ì‚° ê³¼ì •\n\n\nEIE ì‹¤í—˜ì€ ê°€ì¥ lossê°€ ì ì€ data ìë£Œí˜•ì¸ 16 bit Intí˜•ì„ ì‚¬ìš©í–ˆë‹¤.(0.5% loss) AlexNetì´ë‚˜ VGGì™€ ê°™ì´ ReLU Activationì´ ë§ì´ ì‚¬ìš©ë˜ëŠ” ëª¨ë¸ë“¤ì€ ê²½ëŸ‰í™”ê°€ ë§ì´ ëœ ë°˜ë©´, RNNì™€ LSTMì´ ì‚¬ìš©ëœ NeuralTalk ëª¨ë¸ë“¤ ê°™ì€ ê²½ìš°ì—ëŠ” ReLUë¥¼ ì‚¬ìš©í•˜ì§€ ì•Šì•„ ê²½ëŸ‰í™”ë  ìˆ˜ ìˆëŠ” ë¶€ë¶„ì´ ì—†ì–´ Activation Densityê°€ 100%ì¸ ê²ƒì„ í™•ì¸í•  ìˆ˜ ìˆë‹¤.\n\n\n\nEIE ì‹¤í—˜ ê²°ê³¼\n\n\n\n\n3.2 M:N Weight Sparsity\nì´ ë°©ë²•ì€ Nvidia í•˜ë“œì›¨ì–´ì˜ ì§€ì›ì´ í•„ìš”í•œ ë°©ë²•ìœ¼ë¡œ ë³´í†µ 2:4 Weight Sparsityë¥¼ ì‚¬ìš©í•œë‹¤. ì™¼ìª½ì˜ Sparse Matrixë¥¼ ì¬ë°°ì¹˜í•´ì„œ Non-zero data matrixì™€ ì¸ë±ìŠ¤ë¥¼ ì €ì¥í•˜ëŠ” Index matrixë¥¼ ë”°ë¡œ ë§Œë“¤ì–´ì„œ ì €ì¥í•œë‹¤.\n\n\n\n2:4 Weight Sparsity\n\n\nM:N Weight Sparsity ì ìš©í•˜ì§€ ì•Šì€ Dense GEMMê³¼ ì ìš©í•œ Sparse GEMMì„ ê³„ì‚°í•  ë•ŒëŠ” ì•„ë˜ì˜ ê·¸ë¦¼ê³¼ ê°™ì€ ê³¼ì •ìœ¼ë¡œ ì—°ì‚°ì´ ì§„í–‰ëœë‹¤.\n\n\n\nDense VS. Sparse GEMM\n\n\n\n\n3.3 Sparse Convolution\nSubmanifold Sparse Convolutional Networks (SSCN)ì€ ê³ ì°¨ì› ë°ì´í„°ì—ì„œ íš¨ìœ¨ì ì¸ ê³„ì‚°ì„ ê°€ëŠ¥í•˜ê²Œ í•˜ëŠ” ì‹ ê²½ë§ ì•„í‚¤í…ì²˜ì˜ í•œ í˜•íƒœì´ë‹¤. ì´ ê¸°ìˆ ì€ íŠ¹íˆ 3D í¬ì¸íŠ¸ í´ë¼ìš°ë“œ ë˜ëŠ” ê³ í•´ìƒë„ ì´ë¯¸ì§€ì™€ ê°™ì´ ëŒ€ê·œëª¨ ë° ê³ ì°¨ì› ë°ì´í„°ë¥¼ ì²˜ë¦¬í•  ë•Œ ì¤‘ìš”í•˜ë‹¤. SSCNì˜ í•µì‹¬ ì•„ì´ë””ì–´ëŠ” ë°ì´í„°ì˜ Sparcityì„ í™œìš©í•˜ì—¬ ê³„ì‚°ê³¼ ë©”ëª¨ë¦¬ ì‚¬ìš©ëŸ‰ì„ í¬ê²Œ ì¤„ì´ëŠ” ê²ƒì´ë‹¤.\n\n\n\nì¶œì²˜: Submanifold Sparse Convolutional Networks\n\n\nì´ëŸ¬í•œ Sparse Convolutionì€ ê¸°ë³¸ Convolutionê³¼ ë¹„êµí–ˆì„ ë•Œ ì•„ë˜ ê·¸ë¦¼ê³¼ ê°™ì´ ë‚˜íƒ€ë‚´ë³¼ ìˆ˜ ìˆë‹¤.\n\n\n\nConventional VS. Sparse Convolution\n\n\nì—°ì‚° ê³¼ì •ì„ ë¹„êµí•´ë³´ê¸° ìœ„í•´ Input Point Cloud(\\(P\\)), Feature Map(\\(W\\)), Ouput Point Cloud(\\(Q\\))ë¥¼ ì•„ë˜ì™€ ê°™ì´ ìˆë‹¤ê³  í•˜ì. ê¸°ì¡´ì˜ Convolutionê³¼ Sparse Convolutionì„ ë¹„êµí•´ë³´ë©´ ì—°ì‚°ëŸ‰ì´ 9:2ë¡œ ë§¤ìš° ì ì€ ì—°ì‚°ë§Œ í•„ìš”í•œ ê²ƒì„ ì•Œ ìˆ˜ ìˆë‹¤.\n\n\n\nConventional VS. Sparse ì—°ì‚°ëŸ‰ ë¹„êµ\n\n\nFeature Map(\\(W\\))ì„ ê¸°ì¤€ìœ¼ë¡œ ê° weight ë§ˆë‹¤ í•„ìš”í•œ Input dataì˜ í¬ê¸°ê°€ ë‹¤ë¥´ë‹¤. ì˜ˆë¥¼ ë“¤ì–´ \\(W_{-1, 0}\\)ì€ \\(P1\\)ê³¼ ë§Œì˜ ì—°ì‚°ì´ ì§„í–‰ë˜ë¯€ë¡œ \\(P1\\)ë§Œ ì—°ì‚°ì‹œ ë¶ˆëŸ¬ë‚´ê²Œ ëœë‹¤.\n\n\n\nSparse Convolution ê³„ì‚° ê³¼ì •\n\n\në”°ë¼ì„œ Feature Mapì˜ \\(W\\)ì— ë”°ë¼ í•„ìš”í•œ Input dataë¥¼ í‘œí˜„í•˜ê³  ë”°ë¡œ computationì„ ì§„í–‰í•˜ë©´ ì•„ë˜ì™€ ê°™ì´ ê³ ë¥´ì§€ ëª»í•œ ì—°ì‚°ëŸ‰ ë¶„ë°°ê°€ ì§„í–‰ë˜ëŠ”ë°(ì™¼ìª½ ê·¸ë¦¼) ì´ëŠ” computationì— overheadëŠ” ì—†ì§€ë§Œ regularityê°€ ì¢‹ì§€ ì•Šë‹¤. ë˜ëŠ” ê°€ì¥ computationì´ ë§ì€ ê²ƒì„ ê¸°ì¤€ìœ¼ë¡œ Batch ë‹¨ìœ„ë¡œ ê³„ì‚°í•˜ê²Œ ëœë‹¤ë©´(ê°€ìš´ë° ê·¸ë¦¼) ì ì€ computation weightì—ì„œì˜ ë¹„íš¨ìœ¨ì ì¸ ê³„ì‚° ëŒ€ê¸°ì‹œê°„ì´ ìƒê¸°ë¯€ë¡œ overheadê°€ ìƒê¸´ë‹¤. ë”°ë¼ì„œ ì ì ˆíˆ ë¹„ìŠ·í•œ ì—°ì‚°ëŸ‰ì„ ê°€ì§€ëŠ” groupingì„ ì§„í–‰í•œ ë’¤ batchë¡œ ë¬¶ìœ¼ë©´ ì ì ˆíˆ computationì„ ì§„í–‰í•  ìˆ˜ ìˆë‹¤.(ì˜¤ë¥¸ìª½ ê·¸ë¦¼)\n\n\n\nGrouping Computation\n\n\nì´ëŸ° Groupingì„ ì ìš©í•œ í›„ Sparse Convolutionì„ ì§„í–‰í•˜ë©´ Adaptive Groupingì´ ì ìš©ë˜ì–´ ì•„ë˜ì™€ ê°™ì´ ì§„í–‰ëœë‹¤.\n\n\n\nSparse Convolution ì˜ˆì‹œ\n\n\nì—¬ê¸°ê¹Œì§€ê°€ 2023ë…„ë„ ê°•ì˜ì—ì„œ ë§ˆì§€ë§‰ Sparse Convolutionì— ëŒ€í•´ ì„¤ëª…í•œ ë¶€ë¶„ì„ ì •ë¦¬í•œ ë¶€ë¶„ì´ë‹¤. í•˜ì§€ë§Œ ê°•ì˜ì—ì„œ ì„¤ëª…ì´ ë§ì´ ìƒëµë˜ì–´ ìˆìœ¼ë¯€ë¡œ ì¢€ ë” ìì„¸í•œ ë‚´ìš©ì€ Youtube ë°œí‘œ ì˜ìƒì´ë‚˜ 2022ë…„ë„ ê°•ì˜ë¥¼ ì°¸ê³ í•˜ëŠ” ê²ƒì„ ê¶Œì¥í•œë‹¤."
  },
  {
    "objectID": "posts/lecs/lec04.html#reference",
    "href": "posts/lecs/lec04.html#reference",
    "title": "ğŸ§‘â€ğŸ« Lecture 4",
    "section": "4. Reference",
    "text": "4. Reference\n\nMIT-TinyML-lecture04-Pruning-2\nAMC: Automl for Model Compression and Acceleration on Mobile Devices, 2018\nContinuous control with deep reinforcement learning\nFLOPsë€? ë”¥ëŸ¬ë‹ ì—°ì‚°ëŸ‰ì— ëŒ€í•´ì„œ\nNetAdapt: Platform-Aware Neural Network Adaptation for Mobile Applications\nThe Lottery Ticket Hypothesis: Finding Sparse, Trainable Neural Networks\nThe Lottery Ticket Hypothesis Finding Sparse, Trainable Neural Networks ë…¼ë¬¸ ë¦¬ë·°\nLLM Inference - HW/SW Optimizations\nAccelerating Sparse Deep Neural Networks\nSubmanifold Sparse Convolutional Networks\nTorchSparse: Efficient Point Cloud Inference Engine\nTorchSparse++: Efficient Training and Inference Framework for Sparse Convolution on GPUs\nmit-han-lab/torchsparse\nMLSysâ€™22 TorchSparse: Efficient Point Cloud Inference Engine"
  },
  {
    "objectID": "posts/lecs/lec05.html",
    "href": "posts/lecs/lec05.html",
    "title": "ğŸ§‘â€ğŸ« Lecture 5-6",
    "section": "",
    "text": "ì´ë²ˆ ê¸€ì—ì„œëŠ” MIT HAN LABì—ì„œ ê°•ì˜í•˜ëŠ” TinyML and Efficient Deep Learning Computingì— ë‚˜ì˜¤ëŠ” Quantization ë°©ë²•ì„ ì†Œê°œí•˜ë ¤ í•œë‹¤. Quantization(ì–‘ìí™”) ì‹ í˜¸ì™€ ì´ë¯¸ì§€ì—ì„œ ì•„ë‚ ë¡œê·¸ë¥¼ ë””ì§€í„¸ë¡œ ë³€í™˜í•˜ëŠ” ê³¼ì •ì—ì„œ ì‚¬ìš©í•˜ëŠ” ê°œë…ì´ë‹¤. ì•„ë˜ ê·¸ë¦¼ê³¼ ê°™ì´ ì—°ì†ì ì¸ ì„¼ì„œë¡œ ë¶€í„° ë“¤ì–´ì˜¤ëŠ” ì•„ë‚ ë¡œê·¸ ë°ì´í„° ë‚˜ ì´ë¯¸ì§€ë¥¼ í‘œí˜„í•˜ê¸° ìœ„í•´ ë‹¨ìœ„ ì‹œê°„ì— ëŒ€í•´ì„œ ë°ì´í„°ë¥¼ ìƒ˜í”Œë§í•˜ì—¬ ë°ì´í„°ë¥¼ ìˆ˜ì§‘í•œë‹¤.\në””ì§€í„¸ë¡œ ë°ì´í„°ë¥¼ ë³€í™˜í•˜ê¸° ìœ„í•´ ë°ì´í„° íƒ€ì…ì„ ì •í•˜ë©´ì„œ ì´ë¥¼ í•˜ë‚˜ì”© ì–‘ìí™”í•œë‹¤. ì–‘ìˆ˜ì™€ ìŒìˆ˜ë¥¼ í‘œí˜„í•˜ê¸° ìœ„í•´ Unsigned Integer ì—ì„œ Signed Integer, Signedì—ì„œë„ Sign-Magnitude ë°©ì‹ê³¼ Twoâ€™s Complementë°©ì‹ìœ¼ë¡œ, ê·¸ë¦¬ê³  ë” ë§ì€ ì†Œìˆ«ì  ìë¦¬ë¥¼ í‘œí˜„í•˜ê¸° ìœ„í•´ Fixed-pointì—ì„œ Floating pointë¡œ ë°ì´í„° íƒ€ì…ì—ì„œ ìˆ˜ì˜ ë²”ì£¼ë¥¼ í™•ì¥ì‹œí‚¨ë‹¤. ì°¸ê³ ë¡œ Deviceì˜ Computationalityì™€ ML ëª¨ë¸ì˜ ì„±ëŠ¥ì§€í‘œì¤‘ í•˜ë‚˜ì¸ FLOPì´ ë°”ë¡œ floating point operations per secondì´ë‹¤.\nì´ ê¸€ì—ì„œ floating pointë¥¼ ì´í•´í•˜ë©´, fixed pointë¥¼ ì‚¬ìš©í•˜ëŠ” ê²ƒì´ ë§¤ëª¨ë¦¬ì—ì„œ, ê·¸ë¦¬ê³  ì—°ì‚°ì—ì„œ ë” íš¨ìœ¨ì ì¼ ê²ƒì´ë¼ê³  ì˜ˆìƒí•´ë³¼ ìˆ ìˆ˜ ìˆë‹¤. MLëª¨ë¸ì„ í´ë¼ìš°ë“œ ì„œë²„ì—ì„œ ëŒë¦´ ë•ŒëŠ” í¬ê²Œ ë¬¸ì œë˜ì§€ ì•Šì•˜ì§€ë§Œ ì•„ë˜ ë‘ ê°€ì§€ í‘œë¥¼ ë³´ë©´ ì—ë„ˆì§€ì†Œëª¨, ì¦‰ ë°°í„°ë¦¬ íš¨ìœ¨ì—ì„œ í¬ê²Œ ì°¨ì´ê°€ ë³´ì¸ë‹¤. ê·¸ë ‡ê¸° ë•Œë¬¸ì— ëª¨ë¸ì—ì„œ Floating pointë¥¼ fixed pointë¡œ ë” ë§ì´ ë°”ê¾¸ë ¤ê³  í•˜ëŠ”ë° ì´ ë°©ë²•ìœ¼ë¡œ ë‚˜ì˜¨ ê²ƒì´ ë°”ë¡œ Quatizationì´ë‹¤.\nì´ë²ˆ ê¸€ì—ì„œëŠ” Quntization ì¤‘ì—ì„œ Quantization ë°©ë²•ê³¼ ê·¸ ì¤‘ Linearí•œ ë°©ë²•ì— ëŒ€í•´ ë” ìì„¸í•˜ê²Œ, ê·¸ë¦¬ê³  Post-training Quantizationê¹Œì§€ ë‹¤ë£¨ê³ , ë‹¤ìŒ ê¸€ì—ì„œëŠ” Quantization-Aware Training, Binary/Tenary Quantization, Mixed Precision Quantizationê¹Œì§€ ë‹¤ë£¨ë ¤ê³  í•œë‹¤."
  },
  {
    "objectID": "posts/lecs/lec05.html#common-network-quantization",
    "href": "posts/lecs/lec05.html#common-network-quantization",
    "title": "ğŸ§‘â€ğŸ« Lecture 5-6",
    "section": "1. Common Network Quantization",
    "text": "1. Common Network Quantization\nì•ì„œì„œ ì†Œê°œí•œ ê²ƒì²˜ëŸ¼ Neural Netoworkë¥¼ ìœ„í•œ Quantizationì€ ë‹¤ìŒê³¼ ê°™ì´ ë‚˜ëˆŒ ìˆ˜ ìˆë‹¤. Quantization ë°©ë²•ì„ í•˜ë‚˜ì”© ì•Œì•„ë³´ì.\n\n\n\nReference. MIT-TinyML-lecture5-Quantization-1\n\n\n\n\n\n Reference. MIT-TinyML-lecture5-Quantization-1 in https://efficientml.ai \n\n\n\n1.1 K-Means-based Quantization\nê·¸ ì¤‘ ì²« ë²ˆì§¸ë¡œ K-means-based Quantizationì´ ìˆë‹¤. Deep Compression [HanÂ et al., ICLR 2016] ë…¼ë¬¸ì— ì†Œê°œí–ˆë‹¤ëŠ” ì´ ë°©ë²•ì€ ì¤‘ì‹¬ê°’ì„ ê¸°ì¤€ìœ¼ë¡œ clusteringì„ í•˜ëŠ” ë°©ë²•ì´ë‹¤. ì˜ˆì œë¥¼ ë´ë³´ì.\n\n\n\nReference. MIT-TinyML-lecture5-Quantization-1\n\n\nìœ„ ì˜ˆì œëŠ” weightë¥¼ codebookì—ì„œ -1, 0, 1.5, 2ë¡œ ë‚˜ëˆ  ê°ê°ì— ë§ëŠ” ì¸ë±ìŠ¤ë¡œ í‘œê¸°í•œë‹¤. ì´ë ‡ê²Œ ì—°ì‚°ì„ í•˜ë©´ ê¸°ì¡´ì— 64bytesë¥¼ ì‚¬ìš©í–ˆë˜ weightê°€ 20bytesë¡œ ì¤„ì–´ë“ ë‹¤. codebookìœ¼ë¡œ ì˜ˆì œëŠ” 2bitë¡œ ë‚˜ëˆ´ì§€ë§Œ, ì´ë¥¼ N-bitë§Œí¼ ì¤„ì¸ë‹¤ë©´ ìš°ë¦¬ëŠ” ì´ 32/Në°°ì˜ ë©”ëª¨ë¦¬ë¥¼ ì¤„ì¼ ìˆ˜ ìˆë‹¤. í•˜ì§€ë§Œ ì´ ê³¼ì •ì—ì„œ quantizatio error, ì¦‰ quantizationì„ í•˜ê¸° ì „ê³¼ í•œ í›„ì— ì˜¤ì°¨ê°€ ìƒê¸°ëŠ” ê²ƒì„ ìœ„ ì˜ˆì œì—ì„œ ë³¼ ìˆ˜ ìˆë‹¤. ë©”ëª¨ë¦¬ ì‚¬ìš©ëŸ‰ì„ ì¤„ì´ëŠ” ê²ƒë„ ì¢‹ì§€ë§Œ, ì´ ë•Œë¬¸ì— ì„±ëŠ¥ì— ì˜¤ì°¨ê°€ ìƒê¸°ì§€ ì•Šê²Œ í•˜ê¸°ìœ„í•´ ì´ ì˜¤ì°¨ë¥¼ ì¤„ì´ëŠ” ê²ƒ ë˜í•œ ì¤‘ìš”í•˜ë‹¤.\n\n\n\nReference. MIT-TinyML-lecture5-Quantization-1\n\n\nì´ë¥¼ ë³´ì™„í•˜ê¸° ìœ„í•´ Quantizedí•œ Weightë¥¼ ìœ„ì— ê·¸ë¦¼ì²˜ëŸ¼ Fine-tuningí•˜ê¸°ë„ í•œë‹¤. centroidë¥¼ fine-tuningí•œë‹¤ê³  ìƒê°í•˜ë©´ ë˜ëŠ”ë°, ê° centroidì—ì„œ ìƒê¸°ëŠ” ì˜¤ì°¨ë¥¼ í‰ê· ë‚´ tuningí•˜ëŠ” ë°©ë²•ì´ë‹¤. ì´ ë°©ë²•ì„ ì œì•ˆí•œ ë…¼ë¬¸ ì—ì„œëŠ” Convolution ë ˆì´ì–´ì—ì„œëŠ” 4bitê¹Œì§€ centroidë¥¼ ê°€ì¡Œì„ ë•Œ, Full-Connected layerì—ì„œëŠ” 2 bitê¹Œì§€ centroidë¥¼ ê°€ì¡Œì„ ë•Œ ì„±ëŠ¥ì— í•˜ë½ì´ ì—†ë‹¤ê³  ë§í•˜ê³  ìˆì—ˆë‹¤.\n\n\n\nReference. Deep Compression [Han et al., ICLR 2016]\n\n\nì´ë ‡ê²Œ Quantization ëœ WeightëŠ” ìœ„ì²˜ëŸ¼ ì—°ì†ì ì¸ ê°’ì—ì„œ ì•„ë˜ì²˜ëŸ¼ Discreteí•œ ê°’ìœ¼ë¡œ ë°”ë€ë‹¤.\n\n\n\nReference. Deep Compression [Han et al., ICLR 2016]\n\n\në…¼ë¬¸ì€ ì´ë ‡ê²Œ Quantizationí•œ weightë¥¼ í•œ ë²ˆ ë” Huffman codingë¥¼ ì´ìš©í•´ ìµœì í™”ì‹œí‚¨ë‹¤. ì§§ê²Œ ì„¤ëª…í•˜ìë©´, ë¹ˆë„ìˆ˜ê°€ ë†’ì€ ë¬¸ìëŠ” ì§§ì€ ì´ì§„ì½”ë“œë¥¼, ë¹ˆë„ ìˆ˜ê°€ ë‚®ì€ ë¬¸ìì—ëŠ” ê¸´ ì´ì§„ì½”ë“œë¥¼ ì“°ëŠ” ë°©ë²•ì´ë‹¤. ì••ì¶• ê²°ê³¼ë¡œ Generalí•œ ëª¨ë¸ê³¼ ì••ì¶• ë¹„ìœ¨ì´ ê½¤ í° SqueezeNetì„ ì˜ˆë¡œ ë“ ë‹¤. ìì„¸í•œ ë‚´ìš©ì€ ë…¼ë¬¸ì„ ì°¸ê³ í•˜ëŠ” ê±¸ë¡œ.\n\n\n\nReference. Deep Compression [Han et al., ICLR 2016]\n\n\n\n\n\nReference. Deep Compression [Han et al., ICLR 2016]\n\n\ninferenceë¥¼ ìœ„í•´ weightë¥¼ Decodingí•˜ëŠ” ê³¼ì •ì€ inferenceê³¼ì •ì—ì„œ ì €ì¥í•œ clusterì˜ ì¸ë±ìŠ¤ë¥¼ ì´ìš©í•´ codebookì—ì„œ í•´ë‹¹í•˜ëŠ” ê°’ì„ ì°¾ì•„ë‚´ëŠ” ê²ƒì´ë‹¤. ì´ ë°©ë²•ì€ ì €ì¥ ê³µê°„ì„ ì¤„ì¼ ìˆ˜ëŠ” ìˆì§€ë§Œ, floating point Computationì´ë‚˜ ë©”ëª¨ë¦¬ ì ‘ê·¼í•˜ëŠ” ë°©ì‹ìœ¼ë¡œ centroidë¥¼ ì“°ëŠ” í•œê³„ê°€ ìˆì„ ìˆ˜ ë°–ì— ì—†ë‹¤.\n\n\n\nReference. MIT-TinyML-lecture5-Quantization-1\n\n\n\n\n\n Reference. Deep Compression [Han et al., ICLR 2016] \n\n\n\n\n1.2 Linear Quantization\në‘ ë²ˆì§¸ ë°©ë²•ì€ Linear Quatizationì´ë‹¤. floating-pointì¸ weightë¥¼ N-bitì˜ ì •ìˆ˜ë¡œ affine mappingì„ ì‹œí‚¤ëŠ” ë°©ë²•ì´ë‹¤. ê°„ë‹¨í•˜ê²Œ ì‹ìœ¼ë¡œ ë³´ëŠ” ê²Œ ë” ì´í•´ê°€ ì‰½ë‹¤.\n\n\n\nReference. MIT-TinyML-lecture5-Quantization-1\n\n\nì—¬ê¸°ì„œ S(Scale of Linear Quantization)ì™€ Z(Zero point of Linear Quantization)ê°€ ìˆëŠ”ë° ì´ ë‘˜ì´ quantization parameter ë¡œì¨ tuningì„ í•  ìˆ˜ ìˆëŠ” ê°’ì¸ ê²ƒì´ë‹¤.\n\n\n\nReference. MIT-TinyML-lecture5-Quantization-1\n\n\n\n\n1.3 Scale and Zero point\n\n\n\nReference. MIT-TinyML-lecture5-Quantization-1\n\n\nì´ Scaleê³¼ Zero point ë‘ íŒŒë¼ë¯¸í„°ë¥¼ ì´ìš©í•´ì„œ affine mappingì€ ìœ„ ê·¸ë¦¼ê³¼ ê°™ë‹¤. Bit ìˆ˜(Bit Width)ê°€ ë‚®ì•„ì§€ë©´ ë‚®ì•„ì§ˆ ìˆ˜ë¡, floating pointì—ì„œ í‘œí˜„í•  ìˆëŠ” ìˆ˜ ë˜í•œ ì¤„ì–´ë“¤ ê²ƒì´ë‹¤. ê·¸ë ‡ë‹¤ë©´ Scaleì™€ Zero pointëŠ” ê°ê° ì–´ë–»ê²Œ ê³„ì‚°í• ê¹Œ?\nìš°ì„  floating-point ì¸ ìˆ«ìì˜ ë²”ìœ„ ì¤‘ ìµœëŒ€ê°’ê³¼ ìµœì†Ÿê°’ì— ë§ê²Œ ë‘ ì‹ì„ ì„¸ìš°ê³  ì´ë¥¼ ì—°ë¦½ë°©ì •ì‹ìœ¼ë¡œ Scaleê³¼ Zero pointì„ êµ¬í•  ìˆ˜ ìˆë‹¤.\n\nScale point \\[\n  r_{max} = S(q_{max}-Z)\n  \\] \\[\n  r_{min} = S(q_{min}-Z)\n  \\]\n\\[\n  r_{max} - r_{min} = S(q_{max} - q_{min})\n  \\]\n\\[\n  S = \\dfrac{r_{max}-r_{min}}{q_{max}-q_{min}}\n  \\]\nZero point \\[\n  r_{min} = S(q_{min}-Z)\n  \\]\n\\[\n  Z=q_{min}-\\dfrac{r_{min}}{S}\n  \\]\n\\[\n  Z = round\\Big(q_{min}-\\dfrac{r_{min}}{S}\\Big)\n  \\]\n\nì˜ˆë¥¼ ë“¤ì–´, ì•„ë˜ì™€ ê°™ì€ ì˜ˆì œì—ì„œ \\(r_{max}\\) ëŠ”\\(2.12\\) ì´ê³  \\(r_{min}\\) ì€ \\(-1.08\\) ë¡œ Scaleì„ ê³„ì‚°í•˜ë©´ ì•„ë˜ ê·¸ë¦¼ì²˜ëŸ¼ ëœë‹¤. Zero pointëŠ” \\(-1\\) ë¡œ ê³„ì‚°í•  ìˆ˜ ìˆë‹¤.\n\n\n\n\nReference. MIT-TinyML-lecture5-Quantization-1\n\n\nê·¸ëŸ¼ Symmetricí•˜ê²Œ rì˜ ë²”ìœ„ë¥¼ ì œí•œí•˜ëŠ” ê²ƒê³¼ ê°™ì€ ë‹¤ë¥¸ Linear Quantizationì€ ì—†ì„ê¹Œ? ì´ë¥¼ ì•ì„œ, Quatizedëœ ê°’ë“¤ì´ Matrix Multiplicationì„ í•˜ë©´ì„œ ë¯¸ë¦¬ ê³„ì‚°ë  ìˆ˜ ìˆëŠ” ìˆ˜ (Quantized Weight, Scale, Zero point)ê°€ ìˆìœ¼ë‹ˆ inferenceì‹œ ì—°ì‚°ëŸ‰ì„ ì¤„ì´ê¸° ìœ„í•´ ë¯¸ë¦¬ ê³„ì‚°í•  ìˆ˜ ìˆëŠ” íŒŒë¼ë¯¸í„°ëŠ” ì—†ì„ê¹Œ?\n\n\n1.4 Quantized Matrix Multiplication\nì…ë ¥ X, Weight W, ê²°ê³¼ Yê°€ Matrix Multiplicationì„ í–ˆë‹¤ê³  í•  ë•Œ ì‹ì„ ê³„ì‚°í•´ë³´ì.\n\\[\nY=WX\n\\]\n\\[\nS_Y(q_Y-Z_Y) = S_W(q_W-Z_W) \\cdot S_X(q_X-Z_X\n\\]\n\\[\n\\vdots\n\\]\n\n\n\nReference. MIT-TinyML-lecture5-Quantization-1\n\n\nì—¬ê¸°ì„œ ë§ˆì§€ë§‰ ì •ë¦¬í•œ ì‹ì„ ì‚´í´ë³´ë©´,\n\\(Z_x\\) ì™€ \\(q_w, Z_w, Z_X\\) ì˜ ê²½ìš°ëŠ” ë¯¸ë¦¬ ì—°ì‚°ì´ ê°€ëŠ¥í•˜ë‹¤. ë˜ \\(S_wS_X/S_Y\\) ì˜ ê²½ìš° í•­ìƒ ìˆ˜ì˜ ë²”ìœ„ê°€ \\((0, 1)\\) ë¡œ \\(2^{-n}M_0\\) , \\(M_0 \\in [0.5, 1)\\) ë¡œ ë³€í˜•í•˜ë©´ N-bit Integerë¡œ Fixed-point í˜•íƒœë¡œ í‘œí˜„ ê°€ëŠ¥í•˜ë‹¤. ì—¬ê¸°ì— \\(Z_w\\)ê°€ 0ì´ë©´ ì–´ë–¨ê¹Œ? ë˜ ë¯¸ë¦¬ ê³„ì‚°í•  ìˆ˜ ìˆëŠ” í•­ì´ ë³´ì¸ë‹¤.\n\n\n1.5 Symmetric Linear Quantization\n\n\n\nReference. MIT-TinyML-lecture5-Quantization-1\n\n\n\\(Z_w = 0\\) ì´ë¼ê³  í•¨ì€ ë°”ë¡œ ìœ„ì™€ ê°™ì€ Weight ë¶„í¬ì¸ë°, ë°”ë¡œ Symmetricí•œ Linear Quantizationìœ¼ë¡œ \\(Z_w\\)ë¥¼ 0ìœ¼ë¡œ ë§Œë“¤ì–´ \\(Z_w q_x\\)í•­ì„ 0ìœ¼ë¡œ ë‘˜ ìˆ˜ ìˆì–´ ì—°ì‚°ì„ ë˜ ì¤„ì¼ ìˆ˜ ìˆì„ ê²ƒì´ë‹¤.\nSymmetric Linear Quantizationì€ ì£¼ì–´ì§„ ë°ì´í„°ì—ì„œ Full range modeì™€ Restrict range modeë¡œ ë‚˜ë‰œë‹¤.\nì²« ë²ˆì§¸ Full range mode ëŠ” Scaleì„ real number(ë°ì´í„°, weight)ì—ì„œ ë²”ìœ„ê°€ ë„“ì€ ìª½ì— ë§ì¶”ëŠ” ê²ƒì´ë‹¤. ì˜ˆë¥¼ ë“¤ì–´ ì•„ë˜ì˜ ê²½ìš°, r_minì´ r_maxë³´ë‹¤ ì ˆëŒ“ê°’ì´ ë” í¬ê¸° ë•Œë¬¸ì— r_minì— ë§ì¶° q_minì„ ê°€ì§€ê³  Scaleì„ êµ¬í•œë‹¤. ì´ ë°©ë²•ì€ Pytorch native quantizationê³¼ ONNXì—ì„œ ì‚¬ìš©ëœë‹¤ê³  ê°•ì˜ì—ì„œ ì†Œê°œí•œë‹¤.\n\n\n\nReference. MIT-TinyML-lecture5-Quantization-1\n\n\në‘ ë²ˆì§¸ Restrict range modeëŠ” Scaleì„ real number(ë°ì´í„°, weight)ì—ì„œ ë²”ìœ„ê°€ ì¢ì€ ìª½ì— ë§ì¶”ëŠ” ê²ƒì´ë‹¤. ì˜ˆë¥¼ ë“¤ì–´ ì•„ë˜ì˜ ê²½ìš°, r_minê°€ r_maxë³´ë‹¤ ì ˆëŒ“ê°’ì´ ë” í¬ê¸° ë•Œë¬¸ì— r_minì— ë§ì¶”ë©´ì„œ q_maxì— ë§ë„ë¡ Scaleì„ êµ¬í•œë‹¤. ì´ ë°©ë²•ì€ TensorFlow, NVIDIA TensorRT, Intel DNNLì—ì„œ ì‚¬ìš©ëœë‹¤ê³  ê°•ì˜ì—ì„œ ì†Œê°œí•œë‹¤.\n\n\n\nReference. MIT-TinyML-lecture5-Quantization-1\n\n\nê·¸ë ‡ë‹¤ë©´ ì™œ Symmetric ì¨ì•¼í• ê¹Œ? Asymmetric ë°©ë²•ê³¼ Symmetric ë°©ë²•ì˜ ì°¨ì´ëŠ” ë­˜ê¹Œ? (feat. Neural Network Distiller) ì•„ë˜ ê·¸ë¦¼ì„ ì°¸ê³ í•˜ë©´ ë˜ì§€ë§Œ, ê°€ì¥ í° ì°¨ì´ë¡œ ë³´ì´ëŠ” ê²ƒì€ Computation vs Compactful quantized rangeë¡œ ì´í•´ê°„ë‹¤.\n\n\n\nReference. MIT-TinyML-lecture5-Quantization-1\n\n\n\n\n1.6 Linear Quantization examples\nê·¸ëŸ¼ Quatization ë°©ë²•ì— ëŒ€í•´ ì•Œì•„ë´¤ìœ¼ë‹ˆ ì´ë¥¼ Full-Connected Layer, Convolution Layerì— ì ìš©í•´ë³´ê³  ì–´ë–¤ íš¨ê³¼ê°€ ìˆëŠ”ì§€ ì•Œì•„ë³´ì.\n\n1.6.1 Full-Connected Layer\nì•„ë˜ì²˜ëŸ¼ ì‹ì„ ì „ê°œí•´ë³´ë©´ ë¯¸ë¦¬ ì—°ì‚°í•  ê³„ì‚°í•  ìˆ˜ ìˆëŠ” í•­ê³¼ N-bit integerë¡œ í‘œí˜„í•  ìˆëŠ” í•­ìœ¼ë¡œ ë‚˜ëˆŒ ìˆ˜ ìˆë‹¤(ì „ê°œí•˜ëŠ” ì´ìœ ëŠ” ì•„ë§ˆ ë¯¸ë¦¬ ê³„ì‚°í•  ìˆ˜ ìˆëŠ” í•­ì„ ì•Œì•„ë³´ê¸° ìœ„í•¨ì´ ì•„ë‹ê¹Œ ì‹¶ë‹¤).\n\\[\nY=WX+b\n\\]\n\\[\n\\downarrow\n\\]\n\\[\nS_Y(q_Y - Z_Y) = S_W(q_W - Z_W) \\cdot S_X(q_X - Z_X) + S_b(q_b - Z_b)\n\\]\n\\[\n\\downarrow \\ Z_w=0\n\\]\n\\[\nS_Y(q_Y - Z_Y) = S_WS_X(q_Wq_X - Z_xq_W) + S_b(q_b - Z_b)\n\\]\n\\[\n\\downarrow \\ Z_b=0, S_b=S_WS_X\n\\]\n\\[\nS_Y(q_Y - Z_Y) = S_WS_X(q_Wq_X - Z_xq_W+q_b)\n\\]\n\\[\n\\downarrow\n\\]\n\\[\nq_Y = \\dfrac{S_WS_X}{S_Y}(q_Wq_X+q_b - Z_Xq_W) + Z_Y\n\\]\n\\[\n\\downarrow \\ q_{bias}=q_b-Z_xq_W\\\\\n\\]\n\\[\nq_Y = \\dfrac{S_WS_X}{S_Y}(q_Wq_X+ q_{bias}) + Z_Y\\\\\n\\]\nê°„ë‹¨íˆ í‘œê¸°í•˜ê¸° ìœ„í•´ \\(Z_W=0, Z_b=0, S_b = S_W S_X\\) ì´ë¼ê³  ê°€ì •í•œë‹¤.\n\n\n\nReference. MIT-TinyML-lecture5-Quantization-1\n\n\n\n\n1.6.2 Convolutional Layer\nConvolution Layerì˜ ê²½ìš°ëŠ” Weightì™€ Xì˜ ê³±ì˜ ê²½ìš°ë¥¼ Convolutionìœ¼ë¡œ ë°”ê¿”ì„œ ìƒê°í•´ë³´ë©´ ëœë‹¤. ê·¸ë„ ê·¸ëŸ´ ê²ƒì´ Convolutionì€ Kernelê³¼ Inputì˜ ê³±ì˜ í•©ìœ¼ë¡œ ì´ë£¨ì–´ì ¸ ìˆê¸° ë•Œë¬¸ì— Full-Connectedì™€ ê±°ì˜ ìœ ì‚¬í•˜ê²Œ ì „ê°œë  ìˆ˜ ìˆì„ ê²ƒì´ë‹¤.\n\n\n\nReference. MIT-TinyML-lecture5-Quantization-1"
  },
  {
    "objectID": "posts/lecs/lec05.html#post-training-quantization-ptq",
    "href": "posts/lecs/lec05.html#post-training-quantization-ptq",
    "title": "ğŸ§‘â€ğŸ« Lecture 5-6",
    "section": "2. Post-training Quantization (PTQ)",
    "text": "2. Post-training Quantization (PTQ)\nê·¸ëŸ¼ ì•ì„œì„œ Quantizaedí•œ Layerë¥¼ Fine tuningí•  ì—†ì„ê¹Œ? â€œHow should we get the optimal linear quantization parameters (S, Z)?â€ ì´ ì§ˆë¬¸ì— ëŒ€í•´ì„œ Weight, Activation, Bias ì„¸ ê°€ì§€ì™€ ê·¸ì— ëŒ€í•˜ì—¬ ë…¼ë¬¸ì—ì„œ ë³´ì—¬ì£¼ëŠ” ê²°ê³¼ê¹Œì§€ ì•Œì•„ë³´ì.\n\n2.1 Weight quantization\nTL;DR. ì´ ê°•ì˜ì—ì„œ ì†Œê°œí•˜ëŠ” Weight quantizationì€ Grandularityì— ë”°ë¼ Whole(Per-Tensor), Channel, ê·¸ë¦¬ê³  Layerë¡œ ë“¤ì–´ê°„ë‹¤.\n\n2.1.1 Granularity\nWeight quantizationì—ì„œ Granularityì— ë”°ë¼ì„œ Per-Tensor, Per-Channel, Group, ê·¸ë¦¬ê³  Generalized í•˜ëŠ” ë°©ë²•ìœ¼ë¡œ í™•ì¥ì‹œì¼œ Shared Micro-exponent(MX) data typeì„ ì°¨ë¡€ë¡œ ë³´ì—¬ì¤€ë‹¤. Scaleì„ ëª‡ ê°œë‚˜ ë‘˜ ê²ƒì´ëƒ, ê·¸ Scaleì„ ì ìš©í•˜ëŠ” ë²”ìœ„ë¥¼ ì–´ë–»ê²Œ ë‘˜ ê²ƒì´ëƒ, ê·¸ë¦¬ê³  Scaleì„ ì–¼ë§ˆë‚˜ ë””í…Œì¼í•˜ê²Œ(e.g.Â floating-point)í•  ê²ƒì´ëƒì— ì´ˆì ì„ ë‘”ë‹¤.\n\n\n\nReference. MIT-TinyML-lecture5-Quantization-2\n\n\nì²« ë²ˆì§¸ëŠ” Per-Tensor Quantization íŠ¹ë³„í•˜ê²Œ ì„¤ëª…í•  ê²ƒ ì—†ì´ ì´ì „ê¹Œì§€ ì„¤ëª…í–ˆë˜ í•˜ë‚˜ì˜ Scaleì„ ì‚¬ìš©í•˜ëŠ” Linear Quantizationì´ë¼ê³  ìƒê°í•˜ë©´ ë˜ê² ë‹¤. íŠ¹ì§•ìœ¼ë¡œëŠ” Large modelì— ëŒ€í•´ì„œëŠ” ì„±ëŠ¥ì´ ê´œì°®ì§€ë§Œ ì‘ì€ ëª¨ë¸ë¡œ ë–¨ì–´ì§€ë©´ ì„±ëŠ¥ì´ ê¸‰ê²©í•˜ê²Œ ë–¨ì–´ì§„ë‹¤ê³  ì„¤ëª…í•œë‹¤. Channelë³„ë¡œ weight ë²”ì£¼ê°€ ë„“ì€ ê²½ìš°ë‚˜ outlier weightê°€ ìˆëŠ” ê²½ìš° quantization ì´í›„ì— ì„±ëŠ¥ì´ í•˜ë½í–ˆë‹¤ê³  ë§í•œë‹¤.\n\n\n\nReference. MIT-TinyML-lecture5-Quantization-2\n\n\nê·¸ë˜ì„œ ê·¸ í•´ê²°ë°©ì•ˆìœ¼ë¡œ ë‚˜ì˜¤ëŠ” ê²ƒì´ ë‘ ë²ˆì§¸ ë°©ë²•ì¸ Per-Channel Quantizationì´ë‹¤. ìœ„ ì˜ˆì œì—ì„œ ë³´ë©´ Channel ë§ˆë‹¤ ìµœëŒ€ê°’ê³¼ ê°ê°ì— ë§ëŠ” Scaleì„ ë”°ë¡œ ê°€ì§€ëŠ” ê²ƒì„ ë³¼ ìˆ˜ ìˆë‹¤. ê·¸ë¦¬ê³  ì ìš©í•œ ê²°ê³¼ì¸ ì•„ë˜ ê·¸ë¦¼ì„ ë³´ë©´ Per-Channelê³¼ Per-Tensorë¥¼ ë¹„êµí•´ë³´ë©´ Per-Channelì´ ê¸°ì¡´ì— floating point weightì™€ì˜ ì°¨ì´ê°€ ë” ì ë‹¤. í•˜ì§€ë§Œ, ë§Œì•½ í•˜ë“œì›¨ì–´ì—ì„œ Per-Channel Quantizationì„ ì§€ì›í•˜ì§€ ì•ŠëŠ”ë‹¤ë©´ ë¶ˆí•„ìš”í•œ ì—°ì‚°ì„ ì¶”ê°€ë¡œ í•´ì•¼í•˜ê¸° ë•Œë¬¸ì— ì´ëŠ” ì í•©í•œ ë°©ë²•ì´ ë  ìˆ˜ ì—†ë‹¤ëŠ” ì ë„ ê³ ë ¤í•´ì•¼í•  ê²ƒì´ë‹¤(ì´ëŠ” ì´ì „ Tiny Engineì— ëŒ€í•œ ê¸€ì—ì„œ Channelë‚´ì— ìºì‹±ì„ ì´ìš©í•œ ìµœì í™”ì™€ ì—°ê´€ì´ ìˆë‹¤). ê·¸ëŸ¼ ë˜ ë‹¤ë¥¸ ë°©ë²•ì€ ì—†ì„ê¹Œ?\n\n\n\nReference. MIT-TinyML-lecture5-Quantization-2\n\n\nì„¸ ë²ˆì§¸ ë°©ë²•ì€ Group Quantizationìœ¼ë¡œ ì†Œê°œí•˜ëŠ” Per-vector Scaled Quantizationì™€ Shared Micro-exponent(MX) data type ì´ë‹¤. Per-vector Scaled Quantizationì€ 2023ë…„ë„ ê°•ì˜ë¶€í„° ì†Œê°œí•˜ëŠ”ë°, ì´ ë°©ë²•ì€ Scale factorë¥¼ ê·¸ë£¹ë³„ë¡œ í•˜ë‚˜, Per-Tensorë¡œ í•˜ë‚˜ë¡œ ë‘ê°œë¥¼ ë‘ëŠ” ë°©ë²•ì´ë‹¤. ì•„ë˜ì˜ ê·¸ë¦¼ì„ ë³´ë©´,\n\n\n\nReference. MIT-TinyML-lecture5-Quantization-2\n\n\n\\[\nr=S(q-Z) \\rightarrow r=\\gamma \\cdot S_{q}(q-Z)\n\\]\n\\(S_q\\) ë¡œ vectorë³„ ìŠ¤ì¼€ì¼ë§ì„ í•˜ë‚˜, \\(\\gamma\\) ë¡œ Tensorì— ìŠ¤ì¼€ì¼ë§ì„ í•˜ë©° ê°ë§ˆëŠ” floating pointë¡œ í•˜ëŠ” ê²ƒì„ ë³¼ ìˆ˜ìˆë‹¤. ì•„ë¬´ë˜ë„ vectorë‹¨ìœ„ë¡œ ìŠ¤ì¼€ì¼ë§ì„ í•˜ê²Œë˜ë©´ channelê³¼ ë¹„êµí•´ì„œ í•˜ë“œì›¨ì–´ í”Œë«í¼ì— ë§ê²Œ accuracyì˜ trade-offë¥¼ ì¡°ì ˆí•˜ê¸° ë” ìˆ˜ì›”í•  ê²ƒìœ¼ë¡œ ë³´ì¸ë‹¤.\nì—¬ê¸°ì„œ ê°•ì˜ëŠ” ì§€í‘œì¸ Memory Overheadë¡œ â€œEffective Bit Widthâ€ë¥¼ ì†Œê°œí•œë‹¤. ì´ëŠ” Microsoftì—ì„œ ì œê³µí•˜ëŠ” Quantization Approach MX4, MX6, MX9ê³¼ ì—°ê²°ë¼ ìˆëŠ”ë°, ì´ ë°ì´í„°íƒ€ì…ì€ ì¡°ê¸ˆ ì´í›„ì— ë” ìì„¸íˆ ì„¤ëª…í•  ê²ƒì´ë‹¤. Effective Bit Width? ì˜ˆì‹œ í•˜ë‚˜ë¥¼ ë“¤ì–´ ì´í•´í•´ë³´ì. ë§Œì•½ 4-bit Quatizationì„ 4-bit per-vector scaleì„ 16 elements(4ê°œì˜ weightê°€ ê°ê° 4bitë¥¼ ê°€ì§„ë‹¤ê³  ìƒê°í•˜ë©´ 16 elementë¡œ ê³„ì‚°ëœë‹¤ ìœ ì¶”í•  ìˆë‹¤) ë¼ë©´, Effective Bit WidthëŠ” 4(Scale bit) + 4(Vector Scale bit) / 16(Vector Size) = 4.25ê°€ ëœë‹¤. Elementë‹¹ Scale bitë¼ê³  ê°„ë‹¨í•˜ê²Œ ìƒê°í•  ìˆ˜ë„ ìˆì„ ë“¯ ì‹¶ë‹¤.\në§ˆì§€ë§‰ Per-vector Scaled Quantizationì„ ì´í•´í•˜ë‹¤ë³´ë©´ ì´ì „ì— Per-Tensor, Per-Channelë„ ê·¸ë£¹ìœ¼ë¡œ ì–¼ë§ˆë§Œí¼ ë¬¶ëŠ” ì°¨ì´ê°€ ìˆê³ , ì´ëŠ” ì´ë“¤ì„ ì¼ë°˜í™”í•  ìˆ˜ ìˆì–´ ë³´ì¸ë‹¤. ê°•ì˜ì—ì„œ ë°”ë¡œ ë‹¤ìŒì— ì†Œê°œí•˜ëŠ” ë°©ë²•ì´ ë°”ë¡œ Multi-level scaling schemeì´ë‹¤. Per-Channel Quantizationì™€ Per-Vector Quantization(VSQ, Vector-Scale Quantization)ë¶€í„° ë´ë³´ì.\n\n\n\nReference. With Shared Microexponents, A Little Shifting Goes a Long Way [Bita Rouhani et al.]\n\n\nPer-Channel QuantizationëŠ” Scale factorê°€ í•˜ë‚˜ë¡œ Effective Bit WidthëŠ” 4ê°€ ëœë‹¤. ê·¸ë¦¬ê³  VSQëŠ” ì´ì „ì— ê³„ì‚°í–ˆ ë“¯ 4.25ê°€ ë  ê²ƒì´ë‹¤(ì°¸ê³ ë¡œ Per Channelë¡œ ì ìš©ë˜ëŠ” Scaleì˜ ê²½ìš° elementì˜ ìˆ˜ê°€ ë§ì•„ì„œ ê·¸ëŸ°ì§€ ë”°ë¡œ Effective Bit Widthë¡œ ê³„ì‚°í•˜ì§€ëŠ” ì•ŠëŠ”ë‹¤). VSQê¹Œì§€ ë³´ë©´ì„œ Effective Bit WidthëŠ”,\nEffective Bit Width = Scale bit + Group 0 Scale bit / Group 0 Size +...\ne.g. VSQ Data type int4 = Scale bit (4) + Group 0 Scale bit(4) / Group 0 Size(16) = 4.25\nì´ë ‡ê²Œ ê³„ì‚°í•  ìˆ˜ ìˆë‹¤. ê·¸ë¦¬ê³ , MX4, MX6, MX9ê°€ ë‚˜ì˜¨ë‹¤. ì°¸ê³ ë¡œ SëŠ” Sign bit, Mì€ Mantissa bit, EëŠ” Exponent bitë¥¼ ì˜ë¯¸í•œë‹¤(Mantissaë‚˜ Exponentì— ëŒ€í•œ ìì„¸í•œ ë‚´ìš©ì€ floating point vs fixed point ê¸€ì„ ì°¸ê³ í•˜ì). ì•„ë˜ëŠ” Microsoftì—ì„œ ì œê³µí•˜ëŠ” Quantization Approach MX4, MX6, MX9ì— ëŒ€í•œ í‘œì´ë‹¤.\n\n\n\nReference. MIT-TinyML-lecture5-Quantization-1\n\n\n\n\n2.1.2 Weight Equalization\nì—¬ê¸°ê¹Œì§€ Weight Quatizationì—ì„œ ê·¸ë£¹ìœ¼ë¡œ ì–¼ë§ˆë§Œí¼ ë¬¶ëŠ”ì§€ì— ë”°ë¼(ê°•ì˜ì—ì„œëŠ” Granularity) Quatizationì„ í•˜ëŠ” ì—¬ëŸ¬ ë°©ë²•ì„ ì†Œê°œí–ˆë‹¤. ë‹¤ìŒìœ¼ë¡œ ì†Œê°œ í•  ë°©ë²•ì€ Weight Equalizationì´ë‹¤. 2022ë…„ì— ì†Œê°œí•´ì¤€ ë‚´ìš©ì¸ë°, ì´ëŠ” ië²ˆì§¸ layerì˜ output channelë¥¼ scaling down í•˜ë©´ì„œ i+1ë²ˆì§¸ layerì˜ input channelì„ scaling up í•´ì„œ Scaleë¡œ ì¸í•´ Quantization ì „í›„ë¡œ ìƒê¸°ëŠ” Layerê°„ ì°¨ì´ë¥¼ ì¤„ì´ëŠ” ë°©ë²•ì´ë‹¤.\n\n\n\nReference. Data-Free Quantization Through Weight Equalization and Bias Correction [Markus et al., ICCV 2019]\n\n\nì˜ˆë¥¼ ë“¤ì–´ ìœ„ì— ê·¸ë¦¼ì²˜ëŸ¼ Layer iì˜ output channelê³¼ Layer i+1ì˜ input channelì´ ìˆë‹¤. ì—¬ê¸°ì„œ ì‹ì„ ì „ê°œí•˜ë©´ ì•„ë˜ì™€ ê°™ì€ë°,\n\\[\n\\begin{aligned}\ny^{(i+1)}&=f(W^{(i+1)}x^{(i+1)}+b^{(i+1)}) \\\\\n         &=f(W^{(i+1)} \\cdot f(W^{(i)}x^{(i)}+b^{(i)}) +b^{(i+1)}) \\\\\n         &=f(W^{(i+1)}S \\cdot f(S^{-1}(W^{(i)}x^{(i)}+S^{-1}b^{(i)})) +b^{(i+1)})\n\\end{aligned}\n\\]\nwhere \\(S = diag(s)\\) , \\(s_j\\) is the weight equalization scale factor of output channel \\(j\\)\nì—¬ê¸°ì„œ Scale(S)ê°€ i+1ë²ˆì§¸ layerì˜ weightì—, ië²ˆì§¸ weightì— 1/S ë¡œ Scaleë  ë–„ ê¸°ì¡´ì— Scale í•˜ì§€ ì•Šì€ ì‹ê³¼ ìœ ì‚¬í•˜ê²Œ ìœ ì§€í•  ìˆëŠ” ê²ƒì„ ë³¼ ìˆ˜ ìˆë‹¤. ì¦‰,\n\\[\nr^{(i)}_{oc_j} / s = r^{(i+1)}_{ic_j} \\cdot s\n\\]\n\\[\ns_j = \\dfrac{1}{r^{(i+1)}_{ic=j}}\\sqrt{r_{oc=j}^{(i)}\\cdot r_{ic=j}^{(i+1)}}\n\\]\n\\[\nr^{(i)}_{oc_j} = r^{(i)}_{oc_j} / s = \\sqrt{r_{oc=j}^{(i)}\\cdot r_{ic=j}^{(i+1)}}\n\\]\n\\[\nr^{(i)}_{ic_j} =r^{(i)}_{ic_j} \\cdot s = \\sqrt{r_{oc=j}^{(i)}\\cdot r_{ic=j}^{(i+1)}}\n\\]\nì´ë ‡ê²Œ í•˜ë©´ ië²ˆì§¸ layerì˜ output channelê³¼ i+1ë²ˆì§¸ layerì˜ input channelì˜ Scaleì„ ê°ê° \\(S\\) ì™€ \\(1/S\\) ë¡œí•˜ë©° weightê°„ì˜ ê²©ì°¨ë¥¼ ì¤„ì¼ ìˆ˜ ìˆë‹¤.\n\n\n2.1.3 Adaptive rounding\n\n\n\nReference. MIT-TinyML-lecture5-Quantization-1\n\n\në§ˆì§€ë§‰ ì†Œê°œí•˜ëŠ” ë°©ë²•ì€ Adaptive rounding ì´ë‹¤. ë°˜ì˜¬ë¦¼ì€ Round-to-nearestìœ¼ë¡œ ë¶ˆë¦¬ëŠ” ì¼ë°˜ì ì¸ ë°˜ì˜¬ë¦¼ì„ ìƒê°í•  ìˆ˜ ìˆê³ , í•˜ë‚˜ì˜ ê¸°ì¤€ì„ ê°€ì§€ê³  ë°˜ì˜¬ë¦¼ì„ í•˜ëŠ” Adaptive Roundë¥¼ ìƒê°í•  í•  ìˆ˜ ìˆë‹¤. ê°•ì˜ì—ì„œëŠ” Round-to-nearestê°€ ìµœì ì˜ ë°©ë²•ì´ ë˜ì§€ ì•ŠëŠ”ë‹¤ê³  ë§í•˜ë©°, Adaptive roundë¡œ weightì— 0ë¶€í„° 1 ì‚¬ì´ì˜ ê°’ì„ ë”í•´ ìˆ˜ì‹ì²˜ëŸ¼ \\(\\tilde{w} = \\lfloor\\lfloor  w\\rfloor + \\delta\\rceil, \\delta \\in [0, 1]\\) ìµœì ì˜ Optimalí•œ ë°˜ì˜¬ë¦¼ ê°’ì„ êµ¬í•œë‹¤. $$\n\\[\\begin{aligned}\n&argmin_V\\lvert\\lvert Wx-\\tilde Wx\\lvert\\lvert ^2_F + \\lambda f_{reg}(V) \\\\\n\n\\rightarrow & argmin_V\\lvert\\lvert Wx-\\lfloor\\lfloor W \\rfloor + h(V) \\rceil x\\lvert\\lvert ^2_F + \\lambda f_{reg}(V)\n\\end{aligned}\\]\n$$ ### 2.2 Activation quantization ë‘ ë²ˆì§¸ë¡œ Activation quantizationì´ ìˆë‹¤. ëª¨ë¸ê²°ê³¼ë¡œ ë‚˜ì˜¤ëŠ” ê²°ê³¼ë¥¼ ì§ì ‘ì ìœ¼ë¡œ ê²°ì •í•˜ëŠ” Activation Quatizationì—ì„œëŠ” ë‘ ê°€ì§€ë¥¼ ê³ ë ¤í•œ ë°©ë²•ì„ ì†Œê°œí•œë‹¤. í•˜ë‚˜ëŠ” Activation ë ˆì´ì–´ì—ì„œ ê²°ê³¼ê°’ì„ Smoothingí•œ ë¶„í¬ë¥¼ ê°€ì§€ê²Œ í•˜ê¸° ìœ„í•´ Exponential Moving Average(EMA)ë¥¼ ì‚¬ìš©í•˜ëŠ” ë°©ë²•ì´ê³ , ë‹¤ë¥¸ í•˜ë‚˜ëŠ” ë‹¤ì–‘í•œ ì…ë ¥ê°’ì„ ê³ ë ¤í•´ batch samplesì„ FP32 ëª¨ë¸ê³¼ calibrationí•˜ëŠ” ë°©ë²•ì´ë‹¤.\nExponential Moving Average (EMA)ì€ ì•„ë˜ ì‹ì—ì„œ \\(\\alpha\\) ë¥¼ êµ¬í•˜ëŠ” ë°©ë²•ì´ë‹¤. \\[\n\\hat r^{(t)}_{max, min} = \\alpha r^{(t)}_{max, min} + (1-\\alpha) \\hat r^{(t)}_{max, min}  \n\\] Calibrationì˜ ì»¨ì…‰ì€ ë§ì€ inputì˜ min/max í‰ê· ì„ ì´ìš©í•˜ìëŠ” ê²ƒì´ë‹¤. ê·¸ë˜ì„œ trained FP32 modelê³¼ sample batchë¥¼ ê°€ì§€ê³  quantizedí•œ ëª¨ë¸ì˜ ê²°ê³¼ì™€ calibrationì„ ëŒë¦¬ë©´ì„œ ê·¸ ì°¨ì´ë¥¼ ìµœì†Œí™” ì‹œí‚¤ëŠ”ë°, ì—¬ê¸°ì— ì´ìš©í•˜ëŠ” ì§€í‘œëŠ” loss of informationì™€ Newton-Raphson methodë¥¼ ì‚¬ìš©í•œ Mean Square Error(MSE)ê°€ ìˆë‹¤. \\[\nMSE = \\underset{\\lvert r \\lvert_{max}}{min}\\ \\mathbb{E}[(X-Q(X))^2]\n\\] \\[\nKL\\ divergence=D_{KL}(P\\lvert\\lvert Q) = \\sum_i^N P(x_i)log\\dfrac{P(x_i)}{Q(x_i)}\n\\] ### 2.3 Quanization Bias Correction\në§ˆì§€ë§‰ìœ¼ë¡œ Quatizationìœ¼ë¡œ biased errorë¥¼ ì¡ëŠ”ë‹¤ëŠ” ê²ƒì„ ì†Œê°œí•œë‹¤. \\(\\epsilon = Q(W)-W\\) ì´ë¼ê³  ë‘ê³  ì•„ë˜ì²˜ëŸ¼ ì‹ì´ ì „ê°œì‹œí‚¤ë©´ ë§ˆì§€ë§‰ í•­ì—ì„œ ë³´ì´ëŠ” \\(-\\epsilon\\mathbb{E}[x]\\) ë¶€ë¶„ì´ biasë¥¼ quatizationì„ í•  ë•Œ ì œê±° ëœë‹¤ê³  í•œë‹¤(ì´ ë¶€ë¶„ì€ 2023ë…„ì—ëŠ” ì†Œê°œí•˜ì§„ ì•ŠëŠ”ë°, ë‹¹ì—°í•œ ê²ƒì´ì–´ì„œ ì•ˆí•˜ëŠ”ì§€, í˜¹ì€ ì˜í–¥ì´ í¬ì§€ ì•Šì•„ì„œ ê·¸ëŸ°ì§€ëŠ” ëª¨ë¥´ê² ë‹¤. Bias Quatizationì´í›„ì— MobileNetV2ì—ì„œ í•œ ë ˆì´ì–´ì˜ outputì„ ë³´ë©´ ì–´ëŠì •ë„ ì œê±°ë˜ëŠ” ê²ƒì²˜ëŸ¼ ë³´ì¸ë‹¤). \\[\n\\begin{aligned}\n\\mathbb{E}[y] &= \\mathbb{E}[Wx] + \\mathbb{E}[\\epsilon x] - \\mathbb{E}[\\epsilon x],\\ \\mathbb{E}[Q(W)x] = \\mathbb{E}[Wx] + \\mathbb{E}[\\epsilon x] \\\\\n\\mathbb{E}[y] &= \\mathbb{E}[Q(W)x] - \\epsilon\\mathbb{E}[x]\n\\end{aligned}\n\\]\n\n\n\nReference. MIT-TinyML-lecture5-Quantization-2\n\n\n\n\n\n2.4 Post-Training INT8 Linear Quantization Result\nì•ì„  Post-Training Quantizationì„ ì ìš©í•œ ê²°ê³¼ë¥¼ ë³´ì—¬ì¤€ë‹¤. ì´ë¯¸ì§€ê³„ì—´ ëª¨ë¸ì„ ëª¨ë‘ ì‚¬ìš©í–ˆìœ¼ë©°, ì„±ëŠ¥í•˜ë½í­ì€ ì§€í‘œë¡œ ë³´ì—¬ì¤€ë‹¤. ë¹„êµì  í° ëª¨ë¸ë“¤ì˜ ê²½ìš° ì¤€ìˆ˜í•œ ì„±ëŠ¥ì„ ë³´ì—¬ì£¼ì§€ë§Œ MobileNetV1, V2ì™€ ê°™ì€ ì‘ì€ ëª¨ë¸ì€ ìƒê°ë³´ë‹¤ Quantizationìœ¼ë¡œ ë–¨ì–´ì§€ëŠ” ì„±ëŠ¥í­(-11.8%, -2.1%) ì´ í° ê²ƒì„ ë³¼ ìˆ˜ ìˆë‹¤. ê·¸ëŸ¼ ì‘ì€ í¬ê¸°ì˜ ëª¨ë¸ë“¤ì€ ì–´ë–»ê²Œ Training í•´ì•¼í• ê¹Œ?\n\n\n\nReference. MIT-TinyML-lecture5-Quantization-2"
  },
  {
    "objectID": "posts/lecs/lec05.html#quantization-aware-trainingqat",
    "href": "posts/lecs/lec05.html#quantization-aware-trainingqat",
    "title": "ğŸ§‘â€ğŸ« Lecture 5-6",
    "section": "3. Quantization-Aware Training(QAT)",
    "text": "3. Quantization-Aware Training(QAT)\n\n3.1 Quantization-Aware Training\n\n\n\nReference. MIT-TinyML-lecture06-Quantization-2\n\n\n\nUsually, fine-tuning a pre-trained floating point model provides better accuracy than training from scratch.\n\nì´ì „ì— K-mean Quantizationì—ì„œ Fine-tuningë•Œ Centroidì— gradientë¥¼ ë°˜ì˜í–ˆì—ˆë‹¤. Quantization-Aware Trainingì€ ì´ì™€ ìœ ì‚¬í•˜ê²Œ Quantization - Reconstructionì„ í†µí•´ ë§Œë“¤ì–´ì§„ Weightë¡œ Trainingì„ í•˜ëŠ” ë°©ë²•ì„ ë§í•œë‹¤. ì˜ˆì‹œë¥¼ ë“¤ì–´ì„œ ìì„¸íˆ ì‚´í´ë³´ì.\n\n\n\nReference. MIT-TinyML-lecture06-Quantization-2\n\n\n\nA full precision copy of the weights W is maintained throughout the training.\nThe small gradients are accumulated without loss of precision\nOnce the model is trained, only the quantized weights are used for inference\n\nìœ„ ê·¸ë¦¼ì—ì„œ Layer Nì´ ë³´ì¸ë‹¤. ì´ Layer Nì€ weightsë¥¼ íŒŒë¼ë¯¸í„°ë¡œ ê°€ì§€ì§€ë§Œ, ì‹¤ì œë¡œ Training ê³¼ì •ì—ì„œ ì“°ì´ëŠ” weightëŠ” â€œweight quantizationâ€ì„ í†µí•´ Quantization - Reconstructionì„ í†µí•´ ë§Œë“¤ì–´ì§„ Weightë¥¼ ê°€ì§€ê³  í›ˆë ¨ì„ í•  ê²ƒì´ë‹¤.\n\n\n3.2 Straight-Through Estimator(STE)\n\n\n\nReference. MIT-TinyML-lecture06-Quantization-2\n\n\nê·¸ëŸ¼ í›ˆë ¨ì—ì„œ gradientëŠ” ì–´ë–»ê²Œ ì „ë‹¬í•  ìˆ˜ ìˆì„ê¹Œ? Quantizationì˜ ê°œë…ìƒ, weight quantizationì—ì„œ weightë¡œ ë„˜ì–´ê°€ëŠ” gradientëŠ” ì—†ì„ ìˆ˜ ë°–ì— ì—†ë‹¤. ê·¸ë ‡ê²Œ ë˜ë©´ ì‚¬ì‹¤ìƒ weightë¡œ back propagationì´ ë  ìˆ˜ ì—†ê²Œ ë˜ê³ , ê·¸ë˜ì„œ ì†Œê°œí•˜ëŠ” ê°œë…ì´ Straight-Through Estimator(STE) ì…ë‹ˆë‹¤. ë§ì´ ê±°ì°½í•´ì„œ ê·¸ë ‡ì§€, Q(W)ì—ì„œ ë°›ì€ gradientë¥¼ ê·¸ëŒ€ë¡œ weights ë¡œ ë„˜ê²¨ì£¼ëŠ” ë°©ì‹ì´ë‹¤.\n\nQuantization is discrete-valued, and thus the derivative is 0 almost everywhere â†’ NN will learn nothing!\nStraight-Through Estimator(STE) simply passes the gradients through the quantization as if it had been the identity function.\n\\[\n  g_W = \\dfrac{\\partial L}{\\partial  W} = \\dfrac{\\partial L}{\\partial  Q(W)}\n  \\]\n\n\n\n\nReference. MIT-TinyML-lecture06-Quantization-2\n\n\n\nReference\n\nNeural Networks for Machine Learning [HintonÂ et al., Coursera Video Lecture, 2012]\nEstimating or Propagating Gradients Through Stochastic Neurons for Conditional Computation [Bengio, arXiv 2013]\n\n\nì´ í›ˆë ¨ì˜ ê²°ê³¼ê°€ ê¶ê¸ˆí•˜ì‹œë‹¤ë©´ ì´ ë…¼ë¬¸ì„ ì°¸ê³ í•˜ì. ì°¸ê³ ë¡œ ë…¼ë¬¸ì—ì„œëŠ” MobileNetV1, V2 ê·¸ë¦¬ê³  NASNet-Mobileì„ ì´ìš©í•´ Post-Training Quantizationê³¼ Quantization-Aware Trainingì„ ë¹„êµí•˜ê³  ìˆë‹¤."
  },
  {
    "objectID": "posts/lecs/lec05.html#binary-and-ternary-quantization",
    "href": "posts/lecs/lec05.html#binary-and-ternary-quantization",
    "title": "ğŸ§‘â€ğŸ« Lecture 5-6",
    "section": "4. Binary and Ternary Quantization",
    "text": "4. Binary and Ternary Quantization\nì, ê·¸ëŸ¼ Quantizationì„ ê¶ê·¹ì ìœ¼ë¡œ 2bitë¡œ í•  ìˆ˜ëŠ” ì—†ì„ê¹Œ? ë°”ë¡œ Binary(1, -1)ê³¼ Tenary(1, 0, -1) ì´ë‹¤.\n\nCan we push the quantization precision to 1 bit?\n\n\n\nReference. MIT-TinyML-lecture06-Quantization-2\n\n\nReference\n\nBinaryConnect: Training Deep Neural Networks with Binary Weights during Propagations [CourbariauxÂ et al., NeurIPS 2015]\nXNOR-Net: ImageNet Classification using Binary Convolutional Neural Networks [RastegariÂ et al., ECCV 2016]\n\n\në¨¼ì € Weightë¥¼ 2bitë¡œ Quantizationì„ í•˜ê²Œ ë˜ë©´, ë©”ëª¨ë¦¬ì—ì„œëŠ” 32bitë¥¼ 1bitë¡œ ì¤„ì´ë‹ˆ 32ë°°ë‚˜ ì¤„ì¼ ìˆ˜ ìˆê³ , Computationë„ (8x5)+(-3x2)+(5x0)+(-1x1)ì—ì„œ 5-2+0-1 ë¡œ ì ˆë°˜ì„ ì¤„ì¼ ìˆ˜ ìˆë‹¤.\n\n4.1 Binarization: Deterministic Binarization\nê·¸ëŸ¼ Binarizationì—ì„œ +1ê³¼ -1ì„ ì–´ë–¤ ê¸°ì¤€ìœ¼ë¡œ í•´ì•¼í• ê¹Œ? ê°€ì¥ ì‰¬ìš´ ë°©ë²•ì€ threholdë¥¼ ê¸°ì¤€ìœ¼ë¡œ +-1ë¡œ ë‚˜ëˆ„ëŠ” ê²ƒì´ë‹¤.\nDirectly computes the bit value base on a threshold, usually 0 resulting in a sign function.\n\\[\nq = sign(r) = \\begin{dcases}\n+1, &r \\geq 0 \\\\\n-1, &r &lt; 0\n\\end{dcases}\n\\]\n\n\n4.2 Binarization: Stochastic Binarization\në‹¤ë¥¸ ë°©ë²•ìœ¼ë¡œëŠ” outputì—ì„œ hard-sigmoid functionì„ ê±°ì³ì„œ ë‚˜ì˜¨ ê°’ë§Œí¼ í™•ë¥ ì ìœ¼ë¡œ +-1ì´ ë‚˜ì˜¤ë„ë¡ í•˜ëŠ” ê²ƒì´ë‹¤. í•˜ì§€ë§Œ ì´ ë°©ë²•ì€ ë¬´ì‘ìœ„ë¡œ ë¹„íŠ¸ë¥¼ ìƒì„±í•˜ëŠ” í•˜ë“œì›¨ì–´ë¥¼ í•˜ëŠ” ê²ƒì´ ì–´ë µê¸° ë•Œë¬¸ì— ì‚¬ìš©í•˜ì§„ ì•ŠëŠ”ë‹¤ê³  ì–¸ê¸‰í•œë‹¤.\n\nUse global statistics or the value of input data to determine the probability of being -1 or +1\nIn Binary Connect(BC), probability is determined by hard sigmoid function \\(\\sigma(r)\\)\n\\[\n  q=\\begin{dcases}\n  +1, &\\text{with probability } p=\\sigma(r)\\\\\n  -1, & 1-p\n  \\end{dcases}\n  \\\\\n  where\\ \\sigma(r)=min(max(\\dfrac{r+1}{2}, 0), 1)\n  \\]\n\n\n\nReference. MIT-TinyML-lecture06-Quantization-2\n\n\nHarder to implement as it requires the hardware to generate random bits when quantizing.\n\n\n\n4.3 Binarization: Use Scale\nì•ì„  ë°©ë²•ì„ ì´ìš©í•´ì„œ ImageNet Top-1 ì„ í‰ê°€í•´ë³´ë©´ Quantizationì´í›„ -21.2%ë‚˜ ì„±ëŠ¥ì´ í•˜ë½í•˜ëŠ” ê±¸ ë³¼ ìˆ˜ ìˆë‹¤. â€œì–´ë–»ê²Œ ë³´ì™„í•  ìˆ˜ ìˆì„ê¹Œ?â€ í•œ ê²ƒì´ linear qunatizationì—ì„œ ì‚¬ìš©í–ˆë˜ Scale ê°œë…ì´ë‹¤.\n\nUsing Scale, Minimizing Quantization Error in Binarization\n\n\n\nReference. MIT-TinyML-lecture06-Quantization-2\n\n\n\nì—¬ê¸°ì„œ Scaleì€ \\(\\dfrac{1}{n}\\lvert\\lvert W \\lvert\\lvert_1\\) ë¡œ ê³„ì‚°í•  ìˆ˜ ìˆê³ , ì„±ëŠ¥ì€ í•˜ë½ì´ ê±°ì˜ ì—†ëŠ” ê²ƒë„ í™•ì¸í•  ìˆ˜ ìˆë‹¤. ì™œ \\(\\dfrac{1}{n}\\lvert\\lvert W \\lvert\\lvert_1\\)ì¸ì§€ëŠ” ì•„ë˜ ì¦ëª…ê³¼ì •ì„ ì°¸ê³ í•˜ì!\n\nWhy \\(\\alpha\\) is \\(\\dfrac{1}{n}\\lvert\\lvert W \\lvert\\lvert_1\\)?\n\\[\n  \\begin{aligned}\n  &J(B, \\alpha)=\\lvert\\lvert W-\\alpha B\\lvert\\lvert^2 \\\\\n  &\\alpha^*, B^*= \\underset{\\alpha, B}{argmin}\\ J(B, \\alpha) \\\\\n  &J(B,\\alpha) = \\alpha^2B^TB-2\\alpha W^T B + W^TW\\ since\\ B \\in \\{+1, -1\\}^n \\\\\n  &B^TB=n(constant), W^TW= constant(a \\ known\\ variable) \\\\\n  &J(B,\\alpha) = \\alpha^2n-2\\alpha W^T B + C \\\\\n  &B^* = \\underset{B}{argmax} \\{W^T B\\}\\ s.t.\\ B\\in \\{+1,-1 \\}^n \\\\\n  &\\alpha^*=\\dfrac{W^TB^*}{n} \\\\\n  &\\alpha^*=\\dfrac{W^Tsign(W)}{n} = \\dfrac{\\lvert W_i \\lvert}{n} = \\dfrac{1}{n}\\lvert\\lvert W\\lvert\\lvert_{l1}\n  \\end{aligned}\n  \\]\n\nReference. XNOR-Net: ImageNet Classification using Binary Convolutional Neural Networks [RastegariÂ et al., ECCV 2016]\nB*ëŠ” J(B,\\(\\alpha\\))ì—ì„œ ìµœì†Ÿê°’ì„ êµ¬í•´ì•¼í•˜ë¯€ë¡œ \\(W^T\\)B ê°€ ìµœëŒ€ì—¬ì•¼í•˜ê³  ê·¸ëŸ¬ê¸° ìœ„í•´ì„œëŠ” Wê°€ ì–‘ìˆ˜ì¼ë•ŒëŠ” Bë„ ì–‘ìˆ˜, Wê°€ ìŒìˆ˜ì¼ ë•ŒëŠ” Bë„ ìŒìˆ˜ì—¬ì•¼ \\(W^TB=\\sum\\lvert W \\lvert\\) ì´ ë˜ë©´ì„œ ìµœëŒ“ê°’ì´ ë  ìˆ˜ ìˆë‹¤.\n\n\n\n\n4.4 Binarization: Activation\nê·¸ëŸ¼ Activationê¹Œì§€ Quantizationì„ í•´ë´…ì‹œë‹¤.\n4.4.1 Activation\n\n\n\nUntitled\n\n\nì—¬ê¸°ì„œ ì¡°ê¸ˆ ë” ì—°ì‚°ì„ ìµœì í™” í•  ìˆ˜ ìˆì–´ë³´ì´ëŠ” ê²ƒì´ Matrix Muliplicationì´ XOR ì—°ì‚°ê³¼ ë¹„ìŠ·í•˜ê²Œ ë³´ì¸ë‹¤.\n4.4.2 XNOR bit count\n\n\n\nReference. MIT-TinyML-lecture06-Quantization-2\n\n\n\n\\(y_i=-n+ popcount(W_i\\ xnor\\ x) &lt;&lt; 1\\) â†’ popcount returns the number of 1\n\nê·¸ë˜ì„œ popcountê³¼ XNORì„ ì´ìš©í•´ì„œ Computationì—ì„œ ì¢€ ë” ìµœì í™”ë¥¼ ì§„í–‰í•©ë‹ˆë‹¤. ì´ë ‡ê²Œ ìµœì í™”ë¥¼ ì§„í–‰í•˜ê²Œ ë˜ë©´, ë©”ëª¨ë¦¬ëŠ” 32ë°°, Computationì€ 58ë°°ê°€ëŸ‰ ì¤„ì–´ë“¤ ìˆ˜ ìˆë‹¤ê³  ë§í•œë‹¤.\n\n\n\nReference. MIT-TinyML-lecture06-Quantization-2\n\n\nì´ë ‡ê²Œ Weight, Scale factor, Activation, ê·¸ë¦¬ê³  XNOR-Bitcout ê¹Œì§€. ì´ ë„¤ ê°€ì§€ ë‹¨ê³„ë¡œ Binary Quantizationì„ ë‚˜ëˆˆë‹¤. ë‹¤ìŒìœ¼ë¡œëŠ” Ternary Quantizationì€ ì•Œì•„ë³´ì.\n\n\n\nReference. XNOR-Net: ImageNet Classification using Binary Convolutional Neural Networks [RastegariÂ et al., ECCV 2016]\n\n\n\n\nBinarizing Input ì˜ ê²½ìš°ëŠ” averageë¥¼ ëª¨ë“  channelì— ê°™ì´ ì ìš©í•  ê²ƒì´ê¸° ë•Œë¬¸ì— ê·¸ cë§Œí¼ì„ average filterë¡œ í•œ ë²ˆì— ì ìš©í•œë‹¤ëŠ” ë§ì´ë‹¤.\n\n\n\n\n4.5 Ternary Weight Networks(TWN)\nTernaryëŠ” Binary Quantizationê³¼ ë‹¨ê³„ëŠ” ëª¨ë‘ ê°™ì§€ë§Œ, ê°€ì§ˆ ìˆ˜ ìˆëŠ” ê°’ìœ¼ë¡œ 0 ì„ ì¶”ê°€í•œë‹¤. ì•„ë˜ ê·¸ë¦¼ì€ Scaleì„ ì´ìš©í•´ì„œ Quantization Errorë¥¼ ì¤„ì´ëŠ” ë°©ë²•ì„ ë§í•˜ê³  ìˆë‹¤. \\[\nq = \\begin{dcases}\nr_t, &r &gt; \\Delta \\\\\n0, &\\lvert r\\lvert \\leq \\Delta \\\\\n-r_t, &r &lt; -\\Delta\n\\end{dcases} \\\\\nwhere\\ \\Delta = 0.7\\times \\mathbb{E}(\\lvert r \\lvert), r_t = \\mathbb{E}_{\\lvert r \\lvert &gt; \\Delta}(\\lvert r \\lvert )\n\\]  ### 4.6 Trained Ternary Quantization(TTQ)\nTenary Quantizationì—ì„œ ë˜ í•œê°€ì§€ ë‹¤ë¥´ê²Œ ì„¤ëª…í•˜ëŠ” ê²ƒì€ 1ê³¼ -1ë¡œë§Œ ì •í•´ì ¸ ìˆë˜ Binary Quantizationê³¼ ë‹¤ë¥´ê²Œ TenaryëŠ” 1, 0, -1ë¡œ Quantizationì„ í•œ í›„, ì¶”ê°€ì ì¸ í›ˆë ¨ì„ í†µí•´ \\(w_t\\)ì™€ \\(-w_t\\)ë¡œ fine-tuningì„ í•˜ëŠ” ë°©ë²•ë„ ì œì•ˆí•œë‹¤(í•´ë‹¹ ë…¼ë¬¸ì—ì„œëŠ” ì´ëŸ¬í•œ ê¸°ë²•ì„ ì´ìš©í•´ì„œ í•œ ê²°ê³¼ë¥¼ CIFAR-10 ì´ë¯¸ì§€ ë°ì´í„°ë¥¼ ê°€ì§€ê³  ResNets, AlexNet, ImageNetì—ì„œ ë³´ì—¬ì¤€ë‹¤). \\[\nq = \\begin{dcases}\nw_t, &r &gt; \\Delta \\\\\n0, &\\lvert r\\lvert \\leq \\Delta \\\\\n-w_t, &r &lt; -\\Delta\n\\end{dcases}\n\\] \n\n\n4.7 Accuracy Degradation\nBinary, Ternary Quantizationì„ ì‚¬ìš©í•œ ê²°ê³¼ë¥¼ ë³´ì—¬ì¤€ë‹¤(Resnet-18 ê²½ìš°ì—ëŠ” Ternary ê°€ ì˜¤íˆë ¤ Binaryë³´ë‹¤ ì„±ëŠ¥ì´ ë” ë–¨ì–´ì§„ë‹¤!)\n\nBinarization\n\n\n\nReference. Binarized Neural Networks: Training Deep Neural Networks with Weights and Activations Constrained to +1 orÂ âˆ’1. [CourbariauxÂ et al., Arxiv 2016], XNOR-Net: ImageNet Classification using Binary Convolutional Neural Networks [RastegariÂ et al., ECCV 2016]\n\n\nTernary Weight Networks (TWN)\n\n\n\nReference. Ternary Weight Networks [LiÂ et al., Arxiv 2016]\n\n\nTrained Ternary Quantization (TTQ)\n\n\n\nReference. Trained Ternary Quantization [ZhuÂ et al., ICLR 2017]"
  },
  {
    "objectID": "posts/lecs/lec05.html#low-bit-width-quantization",
    "href": "posts/lecs/lec05.html#low-bit-width-quantization",
    "title": "ğŸ§‘â€ğŸ« Lecture 5-6",
    "section": "5. Low Bit-Width Quantization",
    "text": "5. Low Bit-Width Quantization\në‚¨ì€ ë¶€ë¶„ë“¤ì€ ì—¬ëŸ¬ê°€ì§€ ì‹¤í—˜ / ì—°êµ¬ë“¤ì„ ì†Œê°œí•˜ê³  ìˆë‹¤.\n\nBinary Quantizationì€ Quantization Aware Trainingì„ í•  ìˆ˜ ìˆì„ê¹Œ?\n2,3 bitê³¼ 8bit ê·¸ ì¤‘ê°„ìœ¼ë¡œëŠ” Quantizationì„ í•  ìˆ˜ ì—†ì„ê¹Œ?\në ˆì´ì–´ì—ì„œ Quantizationì„ í•˜ì§€ ì•ŠëŠ” ë ˆì´ì–´, ì˜ˆë¥¼ ë“¤ì–´ ê²°ê³¼ì— ì˜í–¥ì„ ì˜ˆë¯¼í•˜ê²Œ ë¯¸ì¹˜ëŠ” ì²« ë²ˆì§¸ ë ˆì´ì–´ê°€ ê°™ì€ ê²½ìš° Quantizationì„ í•˜ì§€ ì•Šìœ¼ë©´ ì–´ë–»ê²Œ ë ê¹Œ?\nActivation í•¨ìˆ˜ë¥¼ ë°”ê¾¸ë©´ ì–´ë–¨ê¹Œ?\nì˜ˆë¥¼ ë“¤ì–´ ì²«ë²ˆì§¸ ë ˆì´ì–´ì˜ Në°° ë„“ê²Œ í•˜ëŠ” ê²ƒê³¼ ê°™ì´ ëª¨ë¸ êµ¬ì¡°ë¥¼ ë°”ê¾¸ë©´ ì–´ë–»ê²Œ ë ê¹Œ?\nì¡°ê¸ˆì”© Quantizationì„ í•  ìˆ˜ ì—†ì„ê¹Œ? (20% â†’ 40% â†’ â€¦ â†’ 100%)\n\nê°•ì˜ì—ì„œëŠ” í¬ê²Œ ì–¸ê¸‰í•˜ì§€ ì•Šê³  ê°„ ë‚´ìš©ë“¤ì´ë¼ ì„¤ëª…ì„ í•˜ì§€ëŠ” ì•Šê² ë‹¤. í•´ë‹¹ ë‚´ìš©ë“¤ì€ ìì„¸í•œ ë‚´ìš©ì„ ì•Œê³ ì‹¶ìœ¼ë©´ ê° íŒŒíŠ¸ì— ì–¸ê¸‰ëœ ë…¼ë¬¸ì„ ì°¸ì¡°í•˜ê¸¸!\n\n5.1 Train Binarized Neural Networks From Scratch\n\nStraight-Through Estimator(STE)\n\n\n\n\nReference. MIT-TinyML-lecture06-Quantization-2\n\n\n\nGradient pass straight to floating-point weights\nFloating-point weight with in [-1, 1]\nReference. Binarized Neural Networks: Training Deep Neural Networks with Weights and Activations Constrained to +1 orÂ âˆ’1. [Courbariaux et al., Arxiv 2016]\n\n\n\n5.2 Quantization-Aware Training: DoReFa-Net With Low Bit-Width Gradients\n\n\n\nReference. MIT-TinyML-lecture06-Quantization-2\n\n\n\nGradient Quantization\n\\[\n  Q(g) = 2 \\cdot max(\\lvert G \\lvert) \\cdot \\Large[ \\small quantize_k \\Large( \\small \\dfrac{g}{2\\cdot max(\\lvert G \\lvert)} + \\dfrac{1}{2} + N(k) \\Large ) \\small -\\dfrac{1}{2} \\Large]\\small\n  \\] \\[\n  where\\ N(k)=\\dfrac{\\sigma}{2^k-1} and\\ \\sigma \\thicksim Uniform(-0.5, 0.5)\n  \\]\n\nNoise function \\(N(k)\\) is added to compensate the potential bias introduced by gradient quantization.\n\nResult\n\n\n\nReference. DoReFa-Net: Training Low Bitwidth Convolutional Neural Networks with Low Bitwidth Gradients [ZhouÂ et al., arXiv 2016]\n\n\n\n\n\n5.3 Replace the Activation Function: Parameterized Clipping Activation Function\n\nThe most common activation function ReLU is unbounded. The dynamic range of inputs becomes problematic for low bit-width quantization due to very limited range and resolution.\nReLU is replaced with hard-coded bounded activation functions: ReLU6, ReLU1, etc\nThe clipping value per layer can be learned as well: PACT(Parametrized Clipping Activation Function)\n \\[\n  y=PACT(x;\\alpha) = 0.5(\\lvert x \\lvert - \\lvert x -\\alpha \\lvert + \\alpha ) = \\begin{dcases}\n  0, & x \\in [-\\infty, 0) \\\\\n  x, & x \\in [0, \\alpha) \\\\\n  \\alpha, & x \\in [\\alpha, +\\infty)\n  \\end{dcases}\n  \\]\nThe upper clipping value of the activation function is a trainable. With STE, the gradient is computed as\n\\[\n  \\dfrac{\\partial Q(y)}{\\partial \\alpha} = \\dfrac{\\partial Q(y)}{\\partial y} \\cdot \\dfrac{\\partial y}{\\partial \\alpha} = \\begin{dcases}\n  0 & x \\in (-\\infty, \\alpha)\\\\\n  1 & x \\in [\\alpha, +\\infty)\\\\\n  \\end{dcases}\n  \\]\n\\[\n  \\rightarrow\n  \\dfrac{\\partial L}{\\partial \\alpha} = \\dfrac{\\partial L}{\\partial Q(y)} \\cdot \\dfrac{\\partial Q(y)}{\\partial \\alpha} = \\begin{dcases}\n  0 & x \\in (-\\infty, \\alpha)\\\\\n  \\frac{\\partial L}{\\partial Q(y)} & x \\in [\\alpha, +\\infty)\\\\\n  \\end{dcases}\n  \\]\nThe larger \\(\\alpha\\), the more the parameterized clipping function resembles a ReLU function\n\nTo avoid large quantization errors due to a wide dynamic range \\([0, \\alpha]\\), L2-regularizer for \\(\\alpha\\) is included in the training loss function.\n\nResult\n\n\n\nReference. PACT: Parameterized Clipping Activation for Quantized Neural Networks [ChoiÂ et al., arXiv 2018]\n\n\n\n\n\n5.4 Modify the Neural Network Architecture\n\nWiden the neural network to compensate for the loss of information due to quantization\nex. Double the channels, reduce the quantization precision\n\n\n\nReference. WRPN: Wide Reduced-Precision Networks [MishraÂ et al., ICLR 2018]\n\n\nReplace a single floating-point convolution with multiple binary convolutions.\n\nTowards Accurate Binary Convolutional Neural Network [LinÂ et al., NeurIPS 2017]\nQuantization [Neural Network Distiller]\n\n\n\n\n5.5 No Quantization on First and Last Layer\n\nBecause it is more sensitive to quantization and small portion of the overall computation\nQuantizing these layers to 8-bit integer does not reduce accuracy\n\n\n\n5.6 Iterative Quantization: Incremental Network Quantization\n\nReference. Incremental Network Quantization: Towards Lossless CNNs with Low-precision Weights [ZhouÂ et al., ICLR 2017]\n\n\n\n\nReference. Incremental Network Quantization: Towards Lossless CNNs with Low-precision Weights [ZhouÂ et al., ICLR 2017]\n\n\n\nSetting\n\nWeight quantization only\nQuantize weights to \\(2^n\\) for faster computation (bit shift instead of multiply)\n\nAlgorithm\n\nStart from a pre-trained fp32 model\nFor the remaining fp32 weights\n\nPartition into two disjoint groups(e.g., according to magnitude)\nQuantize the first group (higher magnitude), and re-train the other group to recover accuracy\n\nRepeat until all the weights are quantized (a popular stride is {50%, 75%, 87.5%, 100%})\n\n\n\n\nReference. MIT-TinyML-lecture06-Quantization-2"
  },
  {
    "objectID": "posts/lecs/lec05.html#mixed-precision-quantization",
    "href": "posts/lecs/lec05.html#mixed-precision-quantization",
    "title": "ğŸ§‘â€ğŸ« Lecture 5-6",
    "section": "6. Mixed-precision quantization",
    "text": "6. Mixed-precision quantization\në§ˆì§€ë§‰ìœ¼ë¡œ ë ˆì´ì–´ë§ˆë‹¤ Quantization bitë¥¼ ë‹¤ë¥´ê²Œ ê°€ì ¸ê°€ë©´ ì–´ë–¨ì§€ì— ëŒ€í•´ì„œ ì´ì•¼ê¸°í•œë‹¤. í•˜ì§€ë§Œ ê²½ìš°ì˜ ìˆ˜ê°€ 8bit ë³´ë‹¤ ì‘ê±°ë‚˜ ê°™ê²Œ Quantizationì„ í•  ì‹œ, weightì™€ activationë¡œ ê²½ìš°ì˜ ìˆ˜ë¥¼ ê³ ë ¤ë¥¼ í•œë‹¤ë©´ Nê°œ ë ˆì´ì–´ì— ëŒ€í•´ì„œ \\((8 \\times 8)^N\\)ë¼ëŠ” ì–´ë§ˆì–´ë§ˆí•œ ê²½ìš°ì˜ ìˆ˜ê°€ ë‚˜ì˜¨ë‹¤. ê·¸ë¦¬ê³  ì´ì— ëŒ€í•´ì„œëŠ” ë‹¤ìŒ íŒŒíŠ¸ì— ë‚˜ê°ˆ Neural Architecture Search(NAS) ì—ì„œ ë‹¤ë£° ë“¯ ì‹¶ë‹¤.\n\n6.1 Uniform Quantization\n\n\n\n6.2 Mixed-precision Quantization\n\n\n\n6.3 Huge Design Space and Solution: Design Automation\n\n\nDesign Space: Each of Choices(8x8=64) â†’ \\(64^n\\)\n\n\n\nReference. HAQ: Hardware-Aware Automated Quantization with Mixed Precision [WangÂ et al., CVPR 2019]\n\n\nResult in Mixed-Precision Quantized MobileNetV1\n\n\n\nReference. HAQ: Hardware-Aware Automated Quantization with Mixed Precision [WangÂ et al., CVPR 2019]\n\n\n\nThis paper compares with Model size, Latency and Energy\n\n\nê°€ì¥ ë§ˆì§€ë§‰ì— ì–¸ê¸‰í•˜ëŠ” Edgeì™€ í´ë¼ìš°ë“œì—ì„œëŠ” Convolution ë ˆì´ì–´ì˜ ì¢…ë¥˜ ì¤‘ ë”í•˜ê³  ëœ Quantizationí•˜ëŠ” ë ˆì´ì–´ê°€ ê°ê° depthwiseì™€ pointwiseë¡œ ë‹¤ë¥´ë‹¤ê³  ì´ì•¼ê¸°í•œë‹¤. ì´ ë‚´ìš©ì— ëŒ€í•´ì„œ ë” ìì„¸íˆ ì´í•´í•˜ê¸° ìœ„í•´ì„œëŠ” ì•„ë§ˆë„ NASë¡œ ë„˜ì–´ê°€ë´ì•¼ ì•Œ ìˆ˜ ìˆì§€ ì•Šì„ê¹Œ ì‹¶ë‹¤.\n\nQuantization Policy for Edge and Cloud\n\n\n\nReference. HAQ: Hardware-Aware Automated Quantization with Mixed Precision [WangÂ et al., CVPR 2019]"
  },
  {
    "objectID": "posts/lecs/lec05.html#reference",
    "href": "posts/lecs/lec05.html#reference",
    "title": "ğŸ§‘â€ğŸ« Lecture 5-6",
    "section": "7. Reference",
    "text": "7. Reference\n\nTinyML and Efficient Deep Learning Computing on MIT HAN LAB\nYoutube for TinyML and Efficient Deep Learning Computing on MIT HAN LAB\nDeep Compression [HanÂ et al., ICLR 2016]\nQuantization and Training of Neural Networks for Efficient Integer-Arithmetic-Only Inference [JacobÂ et al., CVPR 2018]\nWith Shared Microexponents, A Little Shifting Goes a Long Way [Bita Rouhani et al.]\nData-Free Quantization Through Weight Equalization and Bias Correction [Markus et al., ICCV 2019]\nEstimating or Propagating Gradients Through Stochastic Neurons for Conditional Computation [Bengio, arXiv 2013]\nBinarized Neural Networks: Training Deep Neural Networks with Weights and Activations Constrained to +1 orÂ âˆ’1. [CourbariauxÂ et al., Arxiv 2016]\nXNOR-Net: ImageNet Classification using Binary Convolutional Neural Networks [RastegariÂ et al., ECCV 2016]\nTernary Weight Networks [LiÂ et al., Arxiv 2016]\nTrained Ternary Quantization [ZhuÂ et al., ICLR 2017]\nDoReFa-Net: Training Low Bitwidth Convolutional Neural Networks with Low Bitwidth Gradients [ZhouÂ et al., arXiv 2016]\nWRPN: Wide Reduced-Precision Networks [MishraÂ et al., ICLR 2018]\nPACT: Parameterized Clipping Activation for Quantized Neural Networks [ChoiÂ et al., arXiv 2018]\nHAQ: Hardware-Aware Automated Quantization with Mixed Precision [WangÂ et al., CVPR 2019]"
  }
]