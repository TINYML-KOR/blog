[
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "Study Archive",
    "section": "",
    "text": "ìŠ¤í„°ë”” ìë£Œì™€ ê´€ë ¨í•´ì„œ ì–´ë–¤ í† ì˜ë‚˜ ì˜ê²¬ ëª¨ë‘ ê°ì‚¬í•©ë‹ˆë‹¤! Github Discussionì— ê¸€ì„ ë‚¨ê²¨ì£¼ì…”ë„ ì¢‹ê³  ê° í¬ìŠ¤íŒ… í•˜ë‹¨ì— ìˆëŠ” Giscus ëŒ“ê¸€ì°½ì— ì½”ë©˜íŠ¸ë“¤ì„ ë‚¨ê²¨ì£¼ì‹œë©´ ë©ë‹ˆë‹¤.\nWelcome to any comments or opinions on our content! You can leave messages using the direct Discussion or the Giscus window on each post.\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nğŸ§‘â€ğŸ« Lecture 4\n\n\n\n\n\n\npruning\n\n\nlecture\n\n\n\nPruning and Sparsity (Part II)\n\n\n\n\n\nFeb 18, 2024\n\n\nJung Yeon Lee\n\n\n\n\n\n\n\n\n\n\n\n\nğŸ§‘â€ğŸ« Lecture 3\n\n\n\n\n\n\npruning\n\n\nlecture\n\n\n\nPruning and Sparsity (Part I)\n\n\n\n\n\nJan 28, 2024\n\n\nSeunghyun Oh\n\n\n\n\n\n\n\n\n\n\n\n\nğŸ‘©â€ğŸ’» Lab 0\n\n\n\n\n\n\nlab\n\n\npytorch\n\n\nbasic\n\n\n\nPyTorch Tutorial\n\n\n\n\n\nJan 11, 2024\n\n\nJung Yeon Lee\n\n\n\n\n\n\nNo matching items"
  },
  {
    "objectID": "posts/lecs/lec04.html",
    "href": "posts/lecs/lec04.html",
    "title": "ğŸ§‘â€ğŸ« Lecture 4",
    "section": "",
    "text": "ì´ì „ í¬ìŠ¤íŒ…ì—ì„œ Pruningì— ëŒ€í•´ì„œ ë°°ì› ì—ˆë‹¤. ì´ë²ˆì—ëŠ” Pruningì— ëŒ€í•œ ë‚¨ì€ ì´ì•¼ê¸°ì¸ Pruning Ratioë¥¼ ì •í•˜ëŠ” ë°©ë²•, Fine-tuning ê³¼ì •ì— ëŒ€í•´ ì•Œì•„ë³´ê³ , ë§ˆì§€ë§‰ìœ¼ë¡œ Sparsityë¥¼ ìœ„í•œ System Supportì— ëŒ€í•´ ì•Œì•„ë³´ê³ ì í•œë‹¤."
  },
  {
    "objectID": "posts/lecs/lec04.html#pruning-ratio",
    "href": "posts/lecs/lec04.html#pruning-ratio",
    "title": "ğŸ§‘â€ğŸ« Lecture 4",
    "section": "1. Pruning Ratio",
    "text": "1. Pruning Ratio\nPruningì„ í•˜ê¸° ìœ„í•´ì„œ ì–´ëŠ ì •ë„ Pruningì„ í•´ì•¼ í• ì§€ ì–´ë–»ê²Œ ì •í•´ì•¼ í• ê¹Œ?\nì¦‰, ë‹¤ì‹œ ë§í•´ì„œ ëª‡ % ì •ë„ ê·¸ë¦¬ê³  ì–´ë–»ê²Œ Pruningì„ í•´ì•¼ ì¢‹ì„ê¹Œ?\n\n\n\nPruning ë°©ì‹ ë¹„êµ\n\n\nìš°ì„  Channel ë³„ Pruningì„ í•  ë•Œ, Channel êµ¬ë¶„ ì—†ì´ ë™ì¼í•œ Pruning ë¹„ìœ¨(Uniform)ì„ ì ìš©í•˜ë©´ ì„±ëŠ¥ì´ ì¢‹ì§€ ì•Šë‹¤. ì˜¤ë¥¸ìª½ ê·¸ë˜í”„ì—ì„œ ì§€í–¥í•´ì•¼ í•˜ëŠ” ë°©í–¥ì€ LatencyëŠ” ì ê²Œ, AccuracyëŠ” ë†’ê²Œì´ë¯€ë¡œ ì™¼ìª½ ìƒë‹¨ì˜ ì˜ì—­ì´ ë˜ë„ë¡ Pruningì„ ì§„í–‰í•´ì•¼ í•œë‹¤. ê·¸ë ‡ë‹¤ë©´ ê²°ë¡ ì€ Channel ë³„ êµ¬ë¶„ì„ í•´ì„œ ì–´ë–¤ Channelì€ Pruning ë¹„ìœ¨ì„ ë†’ê²Œ, ì–´ë–¤ Channelì€ Pruning ë¹„ìœ¨ì„ ë‚®ê²Œ í•´ì•¼ í•œë‹¤ëŠ” ì´ì•¼ê¸°ê°€ ëœë‹¤.\n\n1.1 Sensitiviy Analysis\nChannel ë³„ êµ¬ë¶„ì„ í•´ì„œ Pruningì„ í•œë‹¤ëŠ” ê¸°ë³¸ ì•„ì´ë””ì–´ëŠ” ì•„ë˜ì™€ ê°™ë‹¤.\n\nAccuracyì— ì˜í–¥ì„ ë§ì´ ì£¼ëŠ” LayerëŠ” Pruningì„ ì ê²Œ í•´ì•¼ í•œë‹¤.\nAccuracyì— ì˜í–¥ì„ ì ê²Œ ì£¼ëŠ” LayerëŠ” Pruningì„ ë§ì´ í•´ì•¼ í•œë‹¤.\n\nAccuracyë¥¼ ë˜ë„ë¡ì´ë©´ ì›ë˜ì˜ ëª¨ë¸ë³´ë‹¤ ëœ ë–¨ì–´ì§€ê²Œ ë§Œë“¤ë©´ì„œ Pruningì„ í•´ì„œ ëª¨ë¸ì„ ê°€ë³ê²Œ ë§Œë“œëŠ” ê²ƒì´ ëª©í‘œì´ê¸° ë•Œë¬¸ì— ë‹¹ì—°í•œ ì•„ì´ë””ì–´ì¼ ê²ƒì´ë‹¤. Accuracyì— ì˜í–¥ì„ ë§ì´ ì¤€ë‹¤ëŠ” ë§ì€ Sensitiveí•œ Layerì´ë‹¤ë¼ëŠ” í‘œí˜„ìœ¼ë¡œ ë‹¤ë¥´ê²Œ ë§í•  ìˆ˜ ìˆë‹¤. ë”°ë¼ì„œ ê° Layerì˜ Senstivityë¥¼ ì¸¡ì •í•´ì„œ Sensitiveí•œ LayerëŠ” Pruning Ratioë¥¼ ë‚®ê²Œ ì„¤ê³„í•˜ë©´ ëœë‹¤.\nLayerì˜ Sensitivityë¥¼ ì¸¡ì •í•˜ê¸° ìœ„í•´ Sensitivity Analysisë¥¼ ì§„í–‰í•´ë³´ì. ë‹¹ì—°íˆ íŠ¹ì • Layerì˜ Pruning Ratioê°€ ë†’ì„ ìˆ˜ë¡ weightê°€ ë§ì´ ê°€ì§€ì¹˜ê¸° ëœ ê²ƒì´ë¯€ë¡œ AccuracyëŠ” ë–¨ì–´ì§€ê²Œ ëœë‹¤.\n\n\n\nL0 Pruning Rate ê·¸ë˜í”„\n\n\n\n\n\n\n\n\nPruningì„ í•˜ëŠ” WeightëŠ” ì–´ë–»ê²Œ ê²°ì •í•˜ë‚˜ìš”?\n\n\n\n\n\nPruning Ratioì— ì˜í•´ Pruned ë˜ëŠ” weightëŠ” ì´ì „ ê°•ì˜ì—ì„œ ë°°ìš´ â€œImportance(weightì˜ ì ˆëŒ“ê°’ í¬ê¸°)â€ì— ë”°ë¼ ì„ íƒëœë‹¤.\n\n\n\nìœ„ì˜ ê·¸ë¦¼ì—ì„œ ì²˜ëŸ¼ Layer 0(L0)ë§Œì„ ê°€ì§€ê³  Pruning Ratioë¥¼ ë†’ì—¬ê°€ë©´ì„œ ê´€ì°°í•´ë³´ë©´, ì•½ 70% ì´í›„ë¶€í„°ëŠ” Accuracyê°€ ê¸‰ê²©í•˜ê²Œ ë–¨ì–´ì§€ëŠ” ê²ƒì„ ë³¼ ìˆ˜ ìˆë‹¤. L0ì—ì„œ Ratioë¥¼ ë†’ì—¬ê°€ë©° Accuracyì˜ ë³€í™”ë¥¼ ê´€ì°°í•œ ê²ƒì²˜ëŸ¼ ë‹¤ë¥¸ Layerë“¤ë„ ê´€ì°°í•´ë³´ì.\n\n\n\nLayerë³„ Sensitivity ë¹„êµ\n\n\nL1ì€ ë‹¤ë¥¸ Layerë“¤ì— ë¹„í•´ ìƒëŒ€ì ìœ¼ë¡œ Pruning Ratioë¥¼ ë†’ì—¬ê°€ë„ Accuracyì˜ ë–¨ì–´ì§€ëŠ” ì •ë„ê°€ ì•½í•œ ë°˜ë©´, L0ëŠ” ë‹¤ë¥¸ Layerë“¤ì— ë¹„í•´ ìƒëŒ€ì ìœ¼ë¡œ Pruning Ratioë¥¼ ë†’ì—¬ê°€ë©´ Accuracyì˜ ë–¨ì–´ì§€ëŠ” ì •ë„ê°€ ì‹¬í•œ ê²ƒì„ í™•ì¸í•  ìˆ˜ ìˆë‹¤. ë”°ë¼ì„œ L1ì€ Sensitivityê°€ ë†’ë‹¤ê³  ë³¼ ìˆ˜ ìˆìœ¼ë©° Pruningì„ ì ê²Œí•´ì•¼ í•˜ê³ , L0ì€ Sensitivityê°€ ë‚®ë‹¤ê³  ë³¼ ìˆ˜ ìˆìœ¼ë©° Pruningì„ ë§ê²Œí•´ì•¼ í•¨ì„ ì•Œ ìˆ˜ ìˆë‹¤.\nì—¬ê¸°ì„œ Sensitivity Analysisì—ì„œ ê³ ë ¤í•´ì•¼í•  ëª‡ê°€ì§€ ì‚¬í•­ë“¤ì— ëŒ€í•´ì„œ ì§šê³  ë„˜ì–´ê°€ì.\n\nSensitivity Analysisì—ì„œ ëª¨ë“  Layerë“¤ì´ ë…ë¦½ì ìœ¼ë¡œ ì‘ë™í•œë‹¤ëŠ” ê²ƒì„ ì „ì œë¡œ í•œë‹¤. ì¦‰, L0ì˜ Pruningì´ L1ì˜ íš¨ê³¼ì— ì˜í–¥ì„ ì£¼ì§€ ì•ŠëŠ” ë…ë¦½ì„±ì„ ê°€ì§„ë‹¤ëŠ” ê²ƒì„ ì „ì œë¡œ í•œë‹¤.\nLayerì˜ Pruning Ratioê°€ ë™ì¼í•˜ë‹¤ê³  í•´ì„œ Pruned Weightìˆ˜ê°€ ê°™ìŒì„ ì˜ë¯¸í•˜ì§€ ì•ŠëŠ”ë‹¤.\n\n100ê°œì˜ weightê°€ ìˆëŠ” layerì˜ 10% Pruning Ratio ì ìš©ì€ 10ê°œì˜ weightê°€ pruned ë˜ì—ˆìŒì„ ì˜ë¯¸í•˜ê³ , 500ê°œì˜ weightê°€ ìˆëŠ” layerì˜ 10% Pruning Ratio ì ìš©ì€ 50ê°œì˜ weightê°€ pruned ë˜ì—ˆìŒì„ ì˜ë¯¸í•œë‹¤.\nLayerì˜ ì „ì²´ í¬ê¸°ì— ë”°ë¼ Pruning Ratioì˜ ì ìš© íš¨ê³¼ëŠ” ë‹¤ë¥¼ ìˆ˜ ìˆë‹¤.\n\n\nSensitivity Analysisê¹Œì§€ ì§„í–‰í•œ í›„ì—ëŠ” ë³´í†µ ì‚¬ëŒì´ Accuracyê°€ ë–¨ì–´ì§€ëŠ” ì •ë„, thresholdë¥¼ ì •í•´ì„œ Pruning Ratioë¥¼ ì •í•œë‹¤.\n\n\n\nThreshold ì •í•˜ê¸°\n\n\nìœ„ ê·¸ë˜í”„ì—ì„œëŠ” Accuracyê°€ ì•½ 75%ìˆ˜ì¤€ìœ¼ë¡œ ìœ ì§€ë˜ëŠ” threhsold \\(T\\) ìˆ˜í‰ì„ ì„ ê¸°ì¤€ìœ¼ë¡œ L0ëŠ” ì•½ 74%, L4ëŠ” ì•½ 80%, L3ëŠ” ì•½ 82%, L2ëŠ” 90%ê¹Œì§€ Pruningì„ ì§„í–‰í•´ì•¼ ê² ë‹¤ê³  ì •í•œ ì˜ˆì‹œë¥¼ ë³´ì—¬ì¤€ë‹¤. ë¯¼ê°í•œ layerì¸ L0ëŠ” ìƒëŒ€ì ìœ¼ë¡œ Pruningì„ ì ê²Œ, ëœ ë¯¼ê°í•œ layerì¸ L2ëŠ” Pruningì„ ë§ê²Œ í•˜ëŠ” ê²ƒì„ í™•ì¸í•  ìˆ˜ ìˆë‹¤.\në¬¼ë¡  ì‚¬ëŒì´ ì •í•˜ëŠ” thresholdëŠ” ê°œì„ ì˜ ì—¬ì§€ê°€ ë¬¼ë¡  ìˆë‹¤. Pruning Ratioë¥¼ ì¢€ ë” Automaticí•˜ê²Œ ì°¾ëŠ” ë°©ë²•ì— ëŒ€í•´ ì•Œì•„ë³´ì.\n\n\n1.2 AMC\nAMCëŠ” AutoML for Model Compressionì˜ ì•½ìë¡œ, ê°•í™”í•™ìŠµ(Reinforcement Learning) ë°©ë²•ìœ¼ë¡œ ìµœì ì˜ Pruning Ratioë¥¼ ì°¾ë„ë¡ í•˜ëŠ” ë°©ë²•ì´ë‹¤.\n\n\n\nAMC ì „ì²´ êµ¬ì¡°\n\n\nAMCì˜ êµ¬ì¡°ëŠ” ìœ„ ê·¸ë¦¼ê³¼ ê°™ë‹¤. ê°•í™”í•™ìŠµ ì•Œê³ ë¦¬ì¦˜ ê³„ì—´ ì¤‘, Actor-Critic ê³„ì—´ì˜ ì•Œê³ ë¦¬ì¦˜ì¸ Deep Deterministic Policy Gradient(DDPG)ì„ í™œìš©í•˜ì—¬ Pruning Ratioë¥¼ ì •í•˜ëŠ” Actionì„ ì„ íƒí•˜ë„ë¡ í•™ìŠµí•œë‹¤. ìì„¸í•œ MDP(Markov Decision Process) ì„¤ê³„ëŠ” ì•„ë˜ì™€ ê°™ë‹¤.\n\n\n\nAMCì˜ MDP\n\n\nê°•í™”í•™ìŠµ Agentì˜ í•™ìŠµ ë°©í–¥ì„ ê²°ì •í•˜ëŠ” ì¤‘ìš”í•œ Reward Functionì€ ëª¨ë¸ì˜ Accuracyë¥¼ ê³ ë ¤í•´ì„œ Errorë¥¼ ì¤„ì´ë„ë¡ ìœ ë„í•  ë¿ë§Œ ì•„ë‹ˆë¼ Latencyë¥¼ ê°„ì ‘ì ìœ¼ë¡œ ê³ ë ¤í•  ìˆ˜ ìˆë„ë¡ ëª¨ë¸ì˜ ì—°ì‚°ëŸ‰ì„ ë‚˜íƒ€ë‚´ëŠ” FLOPë¥¼ ì ê²Œ í•˜ë„ë¡ ìœ ë„í•˜ë„ë¡ ì„¤ê³„í•œë‹¤. ì˜¤ë¥¸ìª½ì— ëª¨ë¸ë“¤ì˜ ì—°ì‚°ëŸ‰ ë³„(Operations) Top-1 Accuracy ê·¸ë˜í”„ë¥¼ ë³´ë©´ ì—°ì‚°ëŸ‰ì´ ë§ì„ìˆ˜ë¡ ë¡œê·¸í•¨ìˆ˜ì²˜ëŸ¼ Accuracyê°€ ì¦ê°€í•˜ëŠ” ê²ƒì„ ë³´ê³  ì´ë¥¼ ë³´ê³  ë°˜ì˜í•œ ë¶€ë¶„ì´ë¼ê³  ë³¼ ìˆ˜ ìˆë‹¤.\n\n\n\nAMCì˜ Reward Function\n\n\nì´ë ‡ê²Œ AMCë¡œ Pruningì„ ì§„í–‰í–ˆì„ ë•Œ, Human Expertê°€ Pruning í•œ ê²ƒê³¼ ë¹„êµí•´ë³´ì. ì•„ë˜ ëª¨ë¸ ì„¹ì…˜ë³„ Density íˆìŠ¤í† ê·¸ë¨ ê·¸ë˜í”„ì—ì„œ Totalì„ ë³´ë©´, ë™ì¼ Accuracyê°€ ë‚˜ì˜¤ë„ë¡ Pruningì„ ì§„í–‰í–ˆì„ ë•Œ AMCë¡œ Pruningì„ ì§„í–‰í•œ ê²ƒ(ì£¼í™©ìƒ‰)ì´ Human Expert Pruning ëª¨ë¸(íŒŒë€ìƒ‰)ë³´ë‹¤ Densityê°€ ë‚®ì€ ê²ƒì„ í™•ì¸í•  ìˆ˜ ìˆë‹¤. ì¦‰, AMCë¡œ Pruning ì§„í–‰í–ˆì„ ë•Œ ë” ë§ì€ weightë¥¼ Pruning ë” ê°€ë²¼ìš´ ëª¨ë¸ì„ ê°€ì§€ê³ ë„ Accuracyë¥¼ ìœ ì§€í–ˆë‹¤ê³  ë³¼ ìˆ˜ ìˆë‹¤.\n\n\n\nAMCì˜ Density Graph\n\n\në‘ë²ˆì§¸ êº¾ì€ ì„  ê·¸ë˜í”„ì—ì„œ AMCë¥¼ ê°€ì§€ê³  Pruningê³¼ Fine-tuningì„ ë²ˆê°ˆì•„ ê°€ë©° ì—¬ëŸ¬ ìŠ¤í…ìœ¼ë¡œ ì§„í–‰í•´ê°€ë©´ì„œ ê´€ì°°í•œ ê²ƒì„ ì¡°ê¸ˆ ë” ìì„¸íˆ ì‚´í´ë³´ì. ê° Iteration(Pruning+Fine-tuning)ì„ stage1, 2, 3, 4ë¡œ ë‚˜íƒ€ë‚´ì–´ plotí•œ ê²ƒì„ ë³´ë©´, 1x1 convë³´ë‹¤ 3x3 convì—ì„œ Densityê°€ ë” ë‚®ì€ ê²ƒì„ í™•ì¸í•  ìˆ˜ ìˆë‹¤. ì¦‰, 3x3 convì—ì„œ 1x1 convë³´ë‹¤ Pruningì„ ë§ì´ í•œ ê²ƒì„ ë³¼ ìˆ˜ ìˆë‹¤. ì´ë¥¼ í•´ì„í•´ë³´ìë©´, AMCê°€ 3x3 convì„ Pruningí•˜ë©´ 9ê°œì˜ weightë¥¼ pruningí•˜ê³  ì´ëŠ” 1x1 conv pruningí•´ì„œ 1ê°œì˜ weightë¥¼ ì—†ì• ëŠ” ê²ƒë³´ë‹¤ í•œë²ˆì— ë” ë§ì€ weight ìˆ˜ë¥¼ ì¤„ì¼ ìˆ˜ ìˆê¸° ë•Œë¬¸ì— 3x3 conv pruningì„ ì ê·¹ í™œìš©í–ˆì„ ê²ƒìœ¼ë¡œ ë³¼ ìˆ˜ ìˆë‹¤.\n\n\n\nAMC Result\n\n\nì´ AMC ì‹¤í—˜ ê²°ê³¼í‘œì—ì„œ ë³´ë©´, FLOPì™€ Time ê°ê° 50%ë¡œ ì¤„ì¸ AMC ëª¨ë¸ ë‘˜ë‹¤ Top-1 Accuracyê°€ ê¸°ì¡´ì˜ 1.0 MobileNetì˜ Accuracyë³´ë‹¤ ì•½ 0.1~0.4% ì •ë„ë§Œ ì¤„ê³  Latencyë‚˜ SpeedUpì´ íš¨ìœ¨ì ìœ¼ë¡œ ì¡°ì •ëœ ê²ƒì„ í™•ì¸í•  ìˆ˜ ìˆë‹¤.\n\n\n\n\n\n\nAMC ì‹¤í—˜ ê²°ê³¼í‘œì—ì„œ 0.75 MobileNetì˜ SpeedUpì´ ì™œ 1.7x ì¸ê°€ìš”?\n\n\n\n\n\nê²°ê³¼í‘œì—ì„œ 0.75 MobileNetì€ 25%ì˜ weightë¥¼ ê°ì†Œì‹œí‚¨ ê²ƒì´ê¸° ë•Œë¬¸ì— SpeedUpì´ \\(\\frac{4}{3} \\simeq 1.3\\)xì´ì–´ì•¼ í•œë‹¤ê³  ìƒê°í•  ìˆ˜ ìˆë‹¤. í•˜ì§€ë§Œ ì—°ì‚°ëŸ‰ì€ quadraticí•˜ê²Œ ê°ì†Œí•˜ê²Œ ë˜ê¸° ë•Œë¬¸ì— \\(\\frac{4}{3} \\cdot \\frac{4}{3} \\simeq 1.7\\)xë¡œ SpeedUpì´ ëœë‹¤.\n\n\n\n\n\n1.3 NetAdapt\në˜ ë‹¤ë¥¸ Pruning Ratioë¥¼ ì •í•˜ëŠ” ê¸°ë²•ìœ¼ë¡œ NetAdaptì´ ìˆë‹¤. Latency Constraintë¥¼ ê°€ì§€ê³  layerë§ˆë‹¤ pruningì„ ì ìš©í•´ë³¸ë‹¤. ì˜ˆë¥¼ ë“¤ì–´, ì¤„ì¼ ëª©í‘œ latency ëŸ‰ì„ lmsë¡œ ì •í•˜ë©´, 10ms â†’ 9msë¡œ ì¤„ ë•Œê¹Œì§€ layerì˜ pruning ratioë¥¼ ë†’ì—¬ê°€ëŠ” ë°©ë²•ì´ë‹¤.\n\n\n\nNetAdapt\n\n\nNetAdaptì˜ ì „ì²´ì ì¸ ê³¼ì •ì€ ì•„ë˜ì™€ ê°™ì´ ì§„í–‰ëœë‹¤. ê¸°ì¡´ ëª¨ë¸ì—ì„œ ê° layerë¥¼ Latency Constraintì— ë„ë‹¬í•˜ë„ë¡ Pruningí•˜ë©´ì„œ Accuracy(\\(Acc_A\\)ë“±)ì„ ë°˜ë³µì ìœ¼ë¡œ ì¸¡ì •í•œë‹¤.\n\nê° layerì˜ pruning ratioë¥¼ ì¡°ì ˆí•œë‹¤.\nShort-term fine tuningì„ ì§„í–‰í•œë‹¤.\nLatency Constraintì— ë„ë‹¬í–ˆëŠ”ì§€ í™•ì¸í•œë‹¤.\nLatency Constraint ë„ë‹¬í•˜ë©´ í•´ë‹¹ layerì˜ ìµœì ì˜ Pruning ratioë¡œ íŒë‹¨í•œë‹¤.\nê° layerì˜ ìµœì  Pruning ratioê°€ ì •í•´ì¡Œë‹¤ë©´ ë§ˆì§€ë§‰ìœ¼ë¡œ Long-term fine tuningì„ ì§„í–‰í•œë‹¤.\n\n\n\n\nNetAdapt ê³¼ì •\n\n\nì´ì™€ ê°™ì´ NetAdaptì˜ ê³¼ì •ì„ ì§„í–‰í•˜ë©´ ì•„ë˜ì™€ ê°™ì€ ì‹¤í—˜ ê²°ê³¼ë¥¼ ë³¼ ìˆ˜ ìˆë‹¤. Uniformí•˜ê²Œ Pruningì„ ì§„í–‰í•œ Multipilersë³´ë‹¤ NetAdaptê°€ 1.7x ë” ë¹ ë¥´ê³  ì˜¤íˆë ¤ AccuracyëŠ” ì•½ 0.3% ì •ë„ ë†’ì€ ê²ƒì„ ì•Œ ìˆ˜ ìˆë‹¤.\n\n\n\nNetAdaptì˜ Latency / Top-1 Accuracy ê·¸ë˜í”„"
  },
  {
    "objectID": "posts/lecs/lec04.html#fine-tuningtrain",
    "href": "posts/lecs/lec04.html#fine-tuningtrain",
    "title": "ğŸ§‘â€ğŸ« Lecture 4",
    "section": "2. Fine-tuning/Train",
    "text": "2. Fine-tuning/Train\nPrunned ëª¨ë¸ì˜ í¼í¬ë¨¼ìŠ¤ë¥¼ í–¥ìƒí•˜ê¸° ìœ„í•´ì„œëŠ” Pruningë¥¼ ì§„í–‰í•˜ê³  ë‚˜ì„œ Fine-tuning ê³¼ì •ì´ í•„ìš”í•˜ë‹¤.\n\n2.1 Iterative Pruning\në³´í†µ Pruned ëª¨ë¸ì˜ Fine-tuning ê³¼ì •ì—ì„œëŠ” ê¸°ì¡´ì— í•™ìŠµí–ˆë˜ learning rateë³´ë‹¤ ì‘ì€ rateë¥¼ ì‚¬ìš©í•œë‹¤. ì˜ˆë¥¼ë“¤ì–´ ê¸°ì¡´ì˜ ëª¨ë¸ì„ í•™ìŠµí•  ë•Œ ì‚¬ìš©í•œ learning rateì˜ \\(1/100\\) ë˜ëŠ” \\(1/10\\)ì„ ì‚¬ìš©í•œë‹¤. ë˜í•œ Pruning ê³¼ì •ê³¼ Fine-tuning ê³¼ì •ì€ 1ë²ˆë§Œ ì§„í–‰í•˜ê¸°ë³´ë‹¤ ì ì°¨ì ìœ¼ë¡œ pruning ratioë¥¼ ëŠ˜ë ¤ê°€ë©° Pruning, Fine-tuningì„ ë²ˆê°ˆì•„ê°€ë©° ì—¬ëŸ¬ë²ˆ ì§„í–‰í•˜ëŠ”ê²Œ ë” ì¢‹ë‹¤.\n\n\n\nIterative Pruning + Fine-tuning ë¹„êµ ê·¸ë˜í”„\n\n\n\n\n2.2 Regularization\nTinyMLì˜ ëª©í‘œëŠ” ê°€ëŠ¥í•œ ë§ì€ weightë“¤ì„ 0ìœ¼ë¡œ ë§Œë“œëŠ” ê²ƒìœ¼ë¡œ ìƒê°í•  ìˆ˜ ìˆë‹¤. ê·¸ë˜ì•¼ ëª¨ë¸ì„ ê°€ë³ê²Œ ë§Œë“¤ ìˆ˜ ìˆê¸° ë•Œë¬¸ì´ë‹¤. ê·¸ë˜ì„œ Regularization ê¸°ë²•ì„ ì´ìš©í•´ì„œ ëª¨ë¸ì˜ weightë“¤ì„ 0ìœ¼ë¡œ, í˜¹ì€ 0ê³¼ ê°€ê¹ê²Œ ì‘ì€ ê°’ì„ ê°€ì§€ë„ë¡ ë§Œë“ ë‹¤. ì‘ì€ ê°’ì˜ weightê°€ ë˜ë„ë¡ í•˜ëŠ” ì´ìœ ëŠ” 0ê³¼ ê°€ê¹Œìš´ ì‘ì€ ê°’ë“¤ì€ ë‹¤ìŒ layerë“¤ë¡œ ë„˜ì–´ê°€ë©´ì„œ 0ì´ ë  ê°€ëŠ¥ì„±ì´ ë†’ì•„ì§€ê¸° ë•Œë¬¸ì´ë‹¤. ê¸°ì¡´ì˜ ë”¥ëŸ¬ë‹ ëª¨ë¸ë“¤ì˜ ê³¼ì í•©(Overfitting)ì„ ë§‰ê¸° ìœ„í•œ Regularization ê¸°ë²•ë“¤ê³¼ ë‹¤ë¥´ì§€ ì•Šìœ¼ë‚˜ ì˜ë„ì™€ ëª©ì ì€ ë‹¤ë¥¸ ê²ƒì„ ì§šì–´ë³¼ ìˆ˜ ìˆë‹¤.\n\n\n\nPruningì„ ìœ„í•œ Regularization\n\n\n\n\n2.3 The Lottery Ticket Hypothesis\n2019ë…„ ICLRì—ì„œ ë°œí‘œëœ ë…¼ë¬¸ì—ì„œ Jonathan Frankleê³¼ Michael Carbinì´ ì†Œê°œí•œ The Lottery Ticket Hypothesis(LTH)ì€ ì‹¬ì¸µ ì‹ ê²½ë§(DNN) í›ˆë ¨ì— ëŒ€í•œ í¥ë¯¸ë¡œìš´ ì•„ì´ë””ì–´ë¥¼ ì œì•ˆí•œë‹¤. ë¬´ì‘ìœ„ë¡œ ì´ˆê¸°í™”ëœ ëŒ€ê·œëª¨ ì‹ ê²½ë§ ë‚´ì— ë” ì‘ì€ í•˜ìœ„ ë„¤íŠ¸ì›Œí¬(Winning Ticket)ê°€ ì¡´ì¬í•œë‹¤ëŠ” ê²ƒì„ ë§í•œë‹¤. ì´ í•˜ìœ„ ë„¤íŠ¸ì›Œí¬ëŠ” ì²˜ìŒë¶€í„° ë³„ë„ë¡œ í›ˆë ¨í•  ë•Œ ì›ë˜ ë„¤íŠ¸ì›Œí¬ì˜ ì„±ëŠ¥ì— ë„ë‹¬í•˜ê±°ë‚˜ ëŠ¥ê°€í•  ìˆ˜ ìˆë‹¤. ì´ ê°€ì„¤ì€ ì´ëŸ¬í•œ Winning Ticketì´ í•™ìŠµí•˜ëŠ” ë° ì í•©í•œ ì´ˆê¸° ê°€ì¤‘ì¹˜ë¥¼ ê°–ëŠ”ë‹¤ê³  ê°€ì •í•œë‹¤.\n\n\n\nLTH ì„¤ëª… ê·¸ë¦¼"
  },
  {
    "objectID": "posts/lecs/lec04.html#system-support-for-sparsity",
    "href": "posts/lecs/lec04.html#system-support-for-sparsity",
    "title": "ğŸ§‘â€ğŸ« Lecture 4",
    "section": "3. System Support for Sparsity",
    "text": "3. System Support for Sparsity\nDNNì„ ê°€ì†í™” ì‹œí‚¤ëŠ” ë°©ë²•ì€ í¬ê²Œ 3ê°€ì§€, Sparse Weight, Sparse Activation, Weight Sharingì´ ìˆë‹¤. Sparse Weight, Sparse Activationì€ Pruningì´ê³  Weight Sharingì€ Quantizationì˜ ë°©ë²•ì´ë‹¤.\n\n\n\nDNNì„ ê°€ì†í™” ì‹œí‚¤ëŠ” ë°©ë²•\n\n\n\nSparse Weight: Weightë¥¼ Pruningí•˜ì—¬ Computationì€ Pruning Ratioì— ëŒ€ì‘í•˜ì—¬ ë¹¨ë¼ì§„ë‹¤. í•˜ì§€ë§Œ MemoryëŠ” Pruningëœ weightì˜ ìœ„ì¹˜ë¥¼ ê¸°ì–µí•˜ê¸° ìœ„í•œ memory ìš©ëŸ‰ì´ í•„ìš”í•˜ë¯€ë¡œ Pruning Ratioì— ë¹„ë¡€í•˜ì—¬ ì¤„ì§„ ì•ŠëŠ”ë‹¤.\nSparse Activation: Weightë¥¼ Pruningí•˜ëŠ” ê²ƒê³¼ ë‹¤ë¥´ê²Œ Activationì€ Test Inputì— ë”°ë¼ dynamic í•˜ë¯€ë¡œ Weightë¥¼ Pruningí•˜ëŠ” ê²ƒë³´ë‹¤ Computationì´ ëœ ì¤„ì–´ë“ ë‹¤.\nWeight Sharing: Quantization ë°©ë²•ìœ¼ë¡œ 32-bit dataë¥¼ 4-bit dataë¡œ ë³€ê²½í•¨ìœ¼ë¡œì¨ 8ë°°ì˜ memory ì ˆì•½ì„ í•  ìˆ˜ ìˆë‹¤.\n\n\n3.1 EIE\nEfficient Inference Engineì€ ê¸°ê³„ í•™ìŠµ ëª¨ë¸ì„ ì‹¤ì‹œê°„ìœ¼ë¡œ ì‹¤í–‰í•˜ê¸° ìœ„í•´ ìµœì í™”ëœ ì†Œí”„íŠ¸ì›¨ì–´ ë¼ì´ë¸ŒëŸ¬ë¦¬ë‚˜ í”„ë ˆì„ì›Œí¬ë¥¼ ë§í•œë‹¤. Processing Elements(PE)ì˜ êµ¬ì¡°\n\n\n\nPE ì—°ì‚° Logically / Physically ë¶„ì„\n\n\nì•„ë˜ ê·¸ë¦¼ì—ì„œ Inputë³„(\\(\\vec{a}\\)) ì—°ì‚°ì€ ì•„ë˜ì™€ ê°™ì´ Inputì´ 0ì¼ ë•ŒëŠ” skipë˜ê³  0ì´ ì•„ë‹ ë•ŒëŠ” prunning ë˜ì§€ ì•Šì€ weightì™€ ì—°ì‚°ì´ ì§„í–‰ëœë‹¤.\n\n\n\nInputë³„ ì—°ì‚° ê³¼ì •\n\n\nEIE ì‹¤í—˜ì€ ê°€ì¥ lossê°€ ì ì€ data ìë£Œí˜•ì¸ 16 bit Intí˜•ì„ ì‚¬ìš©í–ˆë‹¤.(0.5% loss) AlexNetì´ë‚˜ VGGì™€ ê°™ì´ ReLU Activationì´ ë§ì´ ì‚¬ìš©ë˜ëŠ” ëª¨ë¸ë“¤ì€ ê²½ëŸ‰í™”ê°€ ë§ì´ ëœ ë°˜ë©´, RNNì™€ LSTMì´ ì‚¬ìš©ëœ NeuralTalk ëª¨ë¸ë“¤ ê°™ì€ ê²½ìš°ì—ëŠ” ReLUë¥¼ ì‚¬ìš©í•˜ì§€ ì•Šì•„ ê²½ëŸ‰í™”ë  ìˆ˜ ìˆëŠ” ë¶€ë¶„ì´ ì—†ì–´ Activation Densityê°€ 100%ì¸ ê²ƒì„ í™•ì¸í•  ìˆ˜ ìˆë‹¤.\n\n\n\nEIE ì‹¤í—˜ ê²°ê³¼\n\n\n\n\n3.2 M:N Weight Sparsity\nì´ ë°©ë²•ì€ Nvidia í•˜ë“œì›¨ì–´ì˜ ì§€ì›ì´ í•„ìš”í•œ ë°©ë²•ìœ¼ë¡œ ë³´í†µ 2:4 Weight Sparsityë¥¼ ì‚¬ìš©í•œë‹¤. ì™¼ìª½ì˜ Sparse Matrixë¥¼ ì¬ë°°ì¹˜í•´ì„œ Non-zero data matrixì™€ ì¸ë±ìŠ¤ë¥¼ ì €ì¥í•˜ëŠ” Index matrixë¥¼ ë”°ë¡œ ë§Œë“¤ì–´ì„œ ì €ì¥í•œë‹¤.\n\n\n\n2:4 Weight Sparsity\n\n\nM:N Weight Sparsity ì ìš©í•˜ì§€ ì•Šì€ Dense GEMMê³¼ ì ìš©í•œ Sparse GEMMì„ ê³„ì‚°í•  ë•ŒëŠ” ì•„ë˜ì˜ ê·¸ë¦¼ê³¼ ê°™ì€ ê³¼ì •ìœ¼ë¡œ ì—°ì‚°ì´ ì§„í–‰ëœë‹¤.\n\n\n\nDense VS. Sparse GEMM\n\n\n\n\n3.3 Sparse Convolution\nSubmanifold Sparse Convolutional Networks (SSCN)ì€ ê³ ì°¨ì› ë°ì´í„°ì—ì„œ íš¨ìœ¨ì ì¸ ê³„ì‚°ì„ ê°€ëŠ¥í•˜ê²Œ í•˜ëŠ” ì‹ ê²½ë§ ì•„í‚¤í…ì²˜ì˜ í•œ í˜•íƒœì´ë‹¤. ì´ ê¸°ìˆ ì€ íŠ¹íˆ 3D í¬ì¸íŠ¸ í´ë¼ìš°ë“œ ë˜ëŠ” ê³ í•´ìƒë„ ì´ë¯¸ì§€ì™€ ê°™ì´ ëŒ€ê·œëª¨ ë° ê³ ì°¨ì› ë°ì´í„°ë¥¼ ì²˜ë¦¬í•  ë•Œ ì¤‘ìš”í•˜ë‹¤. SSCNì˜ í•µì‹¬ ì•„ì´ë””ì–´ëŠ” ë°ì´í„°ì˜ Sparcityì„ í™œìš©í•˜ì—¬ ê³„ì‚°ê³¼ ë©”ëª¨ë¦¬ ì‚¬ìš©ëŸ‰ì„ í¬ê²Œ ì¤„ì´ëŠ” ê²ƒì´ë‹¤.\n\n\n\nì¶œì²˜: Submanifold Sparse Convolutional Networks\n\n\nì´ëŸ¬í•œ Sparse Convolutionì€ ê¸°ë³¸ Convolutionê³¼ ë¹„êµí–ˆì„ ë•Œ ì•„ë˜ ê·¸ë¦¼ê³¼ ê°™ì´ ë‚˜íƒ€ë‚´ë³¼ ìˆ˜ ìˆë‹¤.\n\n\n\nConventional VS. Sparse Convolution\n\n\nì—°ì‚° ê³¼ì •ì„ ë¹„êµí•´ë³´ê¸° ìœ„í•´ Input Point Cloud(\\(P\\)), Feature Map(\\(W\\)), Ouput Point Cloud(\\(Q\\))ë¥¼ ì•„ë˜ì™€ ê°™ì´ ìˆë‹¤ê³  í•˜ì. ê¸°ì¡´ì˜ Convolutionê³¼ Sparse Convolutionì„ ë¹„êµí•´ë³´ë©´ ì—°ì‚°ëŸ‰ì´ 9:2ë¡œ ë§¤ìš° ì ì€ ì—°ì‚°ë§Œ í•„ìš”í•œ ê²ƒì„ ì•Œ ìˆ˜ ìˆë‹¤.\n\n\n\nConventional VS. Sparse ì—°ì‚°ëŸ‰ ë¹„êµ\n\n\nFeature Map(\\(W\\))ì„ ê¸°ì¤€ìœ¼ë¡œ ê° weight ë§ˆë‹¤ í•„ìš”í•œ Input dataì˜ í¬ê¸°ê°€ ë‹¤ë¥´ë‹¤. ì˜ˆë¥¼ ë“¤ì–´ \\(W_{-1, 0}\\)ì€ \\(P1\\)ê³¼ ë§Œì˜ ì—°ì‚°ì´ ì§„í–‰ë˜ë¯€ë¡œ \\(P1\\)ë§Œ ì—°ì‚°ì‹œ ë¶ˆëŸ¬ë‚´ê²Œ ëœë‹¤.\n\n\n\nSparse Convolution ê³„ì‚° ê³¼ì •\n\n\në”°ë¼ì„œ Feature Mapì˜ \\(W\\)ì— ë”°ë¼ í•„ìš”í•œ Input dataë¥¼ í‘œí˜„í•˜ê³  ë”°ë¡œ computationì„ ì§„í–‰í•˜ë©´ ì•„ë˜ì™€ ê°™ì´ ê³ ë¥´ì§€ ëª»í•œ ì—°ì‚°ëŸ‰ ë¶„ë°°ê°€ ì§„í–‰ë˜ëŠ”ë°(ì™¼ìª½ ê·¸ë¦¼) ì´ëŠ” computationì— overheadëŠ” ì—†ì§€ë§Œ regularityê°€ ì¢‹ì§€ ì•Šë‹¤. ë˜ëŠ” ê°€ì¥ computationì´ ë§ì€ ê²ƒì„ ê¸°ì¤€ìœ¼ë¡œ Batch ë‹¨ìœ„ë¡œ ê³„ì‚°í•˜ê²Œ ëœë‹¤ë©´(ê°€ìš´ë° ê·¸ë¦¼) ì ì€ computation weightì—ì„œì˜ ë¹„íš¨ìœ¨ì ì¸ ê³„ì‚° ëŒ€ê¸°ì‹œê°„ì´ ìƒê¸°ë¯€ë¡œ overheadê°€ ìƒê¸´ë‹¤. ë”°ë¼ì„œ ì ì ˆíˆ ë¹„ìŠ·í•œ ì—°ì‚°ëŸ‰ì„ ê°€ì§€ëŠ” groupingì„ ì§„í–‰í•œ ë’¤ batchë¡œ ë¬¶ìœ¼ë©´ ì ì ˆíˆ computationì„ ì§„í–‰í•  ìˆ˜ ìˆë‹¤.(ì˜¤ë¥¸ìª½ ê·¸ë¦¼)\n\n\n\nGrouping Computation\n\n\nì´ëŸ° Groupingì„ ì ìš©í•œ í›„ Sparse Convolutionì„ ì§„í–‰í•˜ë©´ Adaptive Groupingì´ ì ìš©ë˜ì–´ ì•„ë˜ì™€ ê°™ì´ ì§„í–‰ëœë‹¤.\n\n\n\nSparse Convolution ì˜ˆì‹œ\n\n\nì—¬ê¸°ê¹Œì§€ê°€ 2023ë…„ë„ ê°•ì˜ì—ì„œ ë§ˆì§€ë§‰ Sparse Convolutionì— ëŒ€í•´ ì„¤ëª…í•œ ë¶€ë¶„ì„ ì •ë¦¬í•œ ë¶€ë¶„ì´ë‹¤. í•˜ì§€ë§Œ ê°•ì˜ì—ì„œ ì„¤ëª…ì´ ë§ì´ ìƒëµë˜ì–´ ìˆìœ¼ë¯€ë¡œ ì¢€ ë” ìì„¸í•œ ë‚´ìš©ì€ Youtube ë°œí‘œ ì˜ìƒì´ë‚˜ 2022ë…„ë„ ê°•ì˜ë¥¼ ì°¸ê³ í•˜ëŠ” ê²ƒì„ ê¶Œì¥í•œë‹¤."
  },
  {
    "objectID": "posts/lecs/lec04.html#reference",
    "href": "posts/lecs/lec04.html#reference",
    "title": "ğŸ§‘â€ğŸ« Lecture 4",
    "section": "4. Reference",
    "text": "4. Reference\n\nMIT-TinyML-lecture04-Pruning-2\nAMC: Automl for Model Compression and Acceleration on Mobile Devices, 2018\nContinuous control with deep reinforcement learning\nFLOPsë€? ë”¥ëŸ¬ë‹ ì—°ì‚°ëŸ‰ì— ëŒ€í•´ì„œ\nNetAdapt: Platform-Aware Neural Network Adaptation for Mobile Applications\nThe Lottery Ticket Hypothesis: Finding Sparse, Trainable Neural Networks\nThe Lottery Ticket Hypothesis Finding Sparse, Trainable Neural Networks ë…¼ë¬¸ ë¦¬ë·°\nLLM Inference - HW/SW Optimizations\nAccelerating Sparse Deep Neural Networks\nSubmanifold Sparse Convolutional Networks\nTorchSparse: Efficient Point Cloud Inference Engine\nTorchSparse++: Efficient Training and Inference Framework for Sparse Convolution on GPUs\nmit-han-lab/torchsparse\nMLSysâ€™22 TorchSparse: Efficient Point Cloud Inference Engine"
  },
  {
    "objectID": "about.html",
    "href": "about.html",
    "title": "TinyML Study Group",
    "section": "",
    "text": "@Gueltto9th"
  },
  {
    "objectID": "about.html#lec-4lab",
    "href": "about.html#lec-4lab",
    "title": "TinyML Study Group",
    "section": "21Lec + 4Lab",
    "text": "21Lec + 4Lab\n\nLec1/2ì™€ Lab0ì€ ì œì™¸\nê°•ì˜ë¥¼ ë“£ê³  1ëª…ì”© ëŒì•„ê°€ë©´ì„œ ê°•ì˜ ë³µìŠµ recap ë°œí‘œ\në‹¤ë¥¸ ì‚¬ëŒë“¤ì€ ì§ˆë¬¸/ë””ìŠ¤ì»¤ì…˜ í† í”½ ê°€ì ¸ì˜¤ê¸°\nì£¼ 1íšŒ (ì•½ 16ì£¼ - 4ê°œì›” ì´ë‚´ ì™„ë£Œ ëª©í‘œ)\n\n\n\n[Chapter I: Efficient Inference]\n\nLecture 3: Pruning and Sparsity (Part I) @ooshyun\nLecture 4: Pruning and Sparsity (Part II) @curieuxjy\nLab 1 @CastleFlag\nLecture 5: Quantization (Part I) @ooshyun\nLecture 6: Quantization (Part II) @curieuxjy\nLab 2 @CastleFlag\nLecture 7: Neural Architecture Search (Part I) @ooshyun\nLecture 8: Neural Architecture Search (Part II) @curieuxjy\nLab 3 @CastleFlag\nLecture 9: Knowledge Distillation @ooshyun\nLecture 10: MCUNet: TinyML on Microcontrollers @curieuxjy\nLecture 11: TinyEngine and Parallel Processing @CastleFlag\n\n\n\n[Chapter II: Domain-Specific Optimization]\n\nLecture 12: Transformer and LLM (Part I) @ooshyun\nLecture 13: Transformer and LLM (Part II) @curieuxjy\nLecture 14: Vision Transformer @CastleFlag\nLab 4 @ooshyun\nLecture 15: GAN, Video, and Point Cloud @curieuxjy\nLecture 16: Diffusion Model @CastleFlag\nLecture 17: Distributed Training (Part I) @ooshyun\nLecture 18: Distributed Training (Part II) @curieuxjy\nLab 5 @CastleFlag\nLecture 19: On-Device Training and Transfer Learning @ooshyun\nLecture 20: Efficient Fine-tuning and Prompt Engineering @curieuxjy\n\n\n\n[Chapter IV: Advanced Topics]\n\nLecture 21: Basics of Quantum Computing @CastleFlag\nLecture 22: Quantum Machine Learning @ooshyun\nLecture 23: Noise Robust Quantum ML @curieuxjy"
  },
  {
    "objectID": "posts/labs/lab00.html",
    "href": "posts/labs/lab00.html",
    "title": "ğŸ‘©â€ğŸ’» Lab 0",
    "section": "",
    "text": "Lab 0 PyTorch Tutorial\nIn this tutorial, we will explore how to train a neural network with PyTorch.\n\nSetup\nWe will first install a few packages that will be used in this tutorial:\n\n!pip install torchprofile 1&gt;/dev/null\n\nWe will then import a few libraries:\n\nimport random\nfrom collections import OrderedDict, defaultdict\n\nimport numpy as np\nimport torch\nfrom matplotlib import pyplot as plt\nfrom torch import nn\nfrom torch.optim import *\nfrom torch.optim.lr_scheduler import *\nfrom torch.utils.data import DataLoader\nfrom torchprofile import profile_macs\nfrom torchvision.datasets import *\nfrom torchvision.transforms import *\nfrom tqdm.auto import tqdm\n\nTo ensure the reproducibility, we will control the seed of random generators:\n\nrandom.seed(0)\nnp.random.seed(0)\ntorch.manual_seed(0)\n\n\n\nData\nIn this tutorial, we will use CIFAR-10 as our target dataset. This dataset contains images from 10 classes, where each image is of size 3x32x32, i.e.Â 3-channel color images of 32x32 pixels in size.\n\ntransforms = {\n  \"train\": Compose([\n    RandomCrop(32, padding=4),\n    RandomHorizontalFlip(),\n    ToTensor(),\n  ]),\n  \"test\": ToTensor(),\n}\n\ndataset = {}\nfor split in [\"train\", \"test\"]:\n  dataset[split] = CIFAR10(\n    root=\"data/cifar10\",\n    train=(split == \"train\"),\n    download=True,\n    transform=transforms[split],\n  )\n\nWe can visualize a few images in the dataset and their corresponding class labels:\n\nsamples = [[] for _ in range(10)]\nfor image, label in dataset[\"test\"]:\n  if len(samples[label]) &lt; 4:\n    samples[label].append(image)\n\nplt.figure(figsize=(20, 9))\nfor index in range(40):\n  label = index % 10\n  image = samples[label][index // 10]\n\n  # Convert from CHW to HWC for visualization\n  image = image.permute(1, 2, 0)\n\n  # Convert from class index to class name\n  label = dataset[\"test\"].classes[label]\n\n  # Visualize the image\n  plt.subplot(4, 10, index + 1)\n  plt.imshow(image)\n  plt.title(label)\n  plt.axis(\"off\")\nplt.show()\n\nTo train a neural network, we will need to feed data in batches. We create data loaders with batch size of 512:\n\ndataflow = {}\nfor split in ['train', 'test']:\n  dataflow[split] = DataLoader(\n    dataset[split],\n    batch_size=512,\n    shuffle=(split == 'train'),\n    num_workers=0,\n    pin_memory=True,\n  )\n\nWe can print the data type and shape from the training data loader:\n\nfor inputs, targets in dataflow[\"train\"]:\n  print(\"[inputs] dtype: {}, shape: {}\".format(inputs.dtype, inputs.shape))\n  print(\"[targets] dtype: {}, shape: {}\".format(targets.dtype, targets.shape))\n  break\n\n\n\nModel\nIn this tutorial, we will use a variant of VGG-11 (with fewer downsamples and a smaller classifier) as our model.\n\nclass VGG(nn.Module):\n  ARCH = [64, 128, 'M', 256, 256, 'M', 512, 512, 'M', 512, 512, 'M']\n\n  def __init__(self) -&gt; None:\n    super().__init__()\n\n    layers = []\n    counts = defaultdict(int)\n\n    def add(name: str, layer: nn.Module) -&gt; None:\n      layers.append((f\"{name}{counts[name]}\", layer))\n      counts[name] += 1\n\n    in_channels = 3\n    for x in self.ARCH:\n      if x != 'M':\n        # conv-bn-relu\n        add(\"conv\", nn.Conv2d(in_channels, x, 3, padding=1, bias=False))\n        add(\"bn\", nn.BatchNorm2d(x))\n        add(\"relu\", nn.ReLU(True))\n        in_channels = x\n      else:\n        # maxpool\n        add(\"pool\", nn.MaxPool2d(2))\n\n    self.backbone = nn.Sequential(OrderedDict(layers))\n    self.classifier = nn.Linear(512, 10)\n\n  def forward(self, x: torch.Tensor) -&gt; torch.Tensor:\n    # backbone: [N, 3, 32, 32] =&gt; [N, 512, 2, 2]\n    x = self.backbone(x)\n\n    # avgpool: [N, 512, 2, 2] =&gt; [N, 512]\n    x = x.mean([2, 3])\n\n    # classifier: [N, 512] =&gt; [N, 10]\n    x = self.classifier(x)\n    return x\n\nmodel = VGG().cuda()\n\nIts backbone is composed of eight conv-bn-relu blocks interleaved with four maxpoolâ€™s to downsample the feature map by 2^4 = 16 times:\n\nprint(model.backbone)\n\nAfter the feature map is pooled, its classifier predicts the final output with a linear layer:\n\nprint(model.classifier)\n\nAs this course focuses on efficiency, we will then inspect its model size and (theoretical) computation cost.\n\nThe model size can be estimated by the number of trainable parameters:\n\n\nnum_params = 0\nfor param in model.parameters():\n  if param.requires_grad:\n    num_params += param.numel()\nprint(\"#Params:\", num_params)\n\n\nThe computation cost can be estimated by the number of multiplyâ€“accumulate operations (MACs) using TorchProfile:\n\n\nnum_macs = profile_macs(model, torch.zeros(1, 3, 32, 32).cuda())\nprint(\"#MACs:\", num_macs)\n\nThis model has 9.2M parameters and requires 606M MACs for inference. We will work together in the next few labs to improve its efficiency.\n\n\nOptimization\nAs we are working on a classification problem, we will apply cross entropy as our loss function to optimize the model:\n\ncriterion = nn.CrossEntropyLoss()\n\nOptimization will be carried out using stochastic gradient descent (SGD) with momentum:\n\noptimizer = SGD(\n  model.parameters(),\n  lr=0.4,\n  momentum=0.9,\n  weight_decay=5e-4,\n)\n\nThe learning rate will be modulated using the following scheduler (which is adapted from this blog series):\n\nnum_epochs = 20\nsteps_per_epoch = len(dataflow[\"train\"])\n\n# Define the piecewise linear scheduler\nlr_lambda = lambda step: np.interp(\n  [step / steps_per_epoch],\n  [0, num_epochs * 0.3, num_epochs],\n  [0, 1, 0]\n)[0]\n\n# Visualize the learning rate schedule\nsteps = np.arange(steps_per_epoch * num_epochs)\nplt.plot(steps, [lr_lambda(step) * 0.4 for step in steps])\nplt.xlabel(\"Number of Steps\")\nplt.ylabel(\"Learning Rate\")\nplt.grid(\"on\")\nplt.show()\n\nscheduler = LambdaLR(optimizer, lr_lambda)\n\n\n\nTraining\nWe first define the training function that optimizes the model for one epoch (i.e., a pass over the training set):\n\ndef train(\n  model: nn.Module,\n  dataflow: DataLoader,\n  criterion: nn.Module,\n  optimizer: Optimizer,\n  scheduler: LambdaLR,\n) -&gt; None:\n  model.train()\n\n  for inputs, targets in tqdm(dataflow, desc='train', leave=False):\n    # Move the data from CPU to GPU\n    inputs = inputs.cuda()\n    targets = targets.cuda()\n\n    # Reset the gradients (from the last iteration)\n    optimizer.zero_grad()\n\n    # Forward inference\n    outputs = model(inputs)\n    loss = criterion(outputs, targets)\n\n    # Backward propagation\n    loss.backward()\n\n    # Update optimizer and LR scheduler\n    optimizer.step()\n    scheduler.step()\n\nWe then define the evaluation function that calculates the metric (i.e., accuracy in our case) on the test set:\n\n@torch.inference_mode()\ndef evaluate(\n  model: nn.Module,\n  dataflow: DataLoader\n) -&gt; float:\n  model.eval()\n\n  num_samples = 0\n  num_correct = 0\n\n  for inputs, targets in tqdm(dataflow, desc=\"eval\", leave=False):\n    # Move the data from CPU to GPU\n    inputs = inputs.cuda()\n    targets = targets.cuda()\n\n    # Inference\n    outputs = model(inputs)\n\n    # Convert logits to class indices\n    outputs = outputs.argmax(dim=1)\n\n    # Update metrics\n    num_samples += targets.size(0)\n    num_correct += (outputs == targets).sum()\n\n  return (num_correct / num_samples * 100).item()\n\nWith training and evaluation functions, we can finally start training the model! This will take around 10 minutes.\n\nfor epoch_num in tqdm(range(1, num_epochs + 1)):\n  train(model, dataflow[\"train\"], criterion, optimizer, scheduler)\n  metric = evaluate(model, dataflow[\"test\"])\n  print(f\"epoch {epoch_num}:\", metric)\n\nIf everything goes well, your trained model should be able to achieve &gt;92.5% of accuracy!\n\n\nVisualization\nWe can visualize the modelâ€™s prediction to see how the model truly performs:\n\nplt.figure(figsize=(20, 10))\nfor index in range(40):\n  image, label = dataset[\"test\"][index]\n\n  # Model inference\n  model.eval()\n  with torch.inference_mode():\n    pred = model(image.unsqueeze(dim=0).cuda())\n    pred = pred.argmax(dim=1)\n\n  # Convert from CHW to HWC for visualization\n  image = image.permute(1, 2, 0)\n\n  # Convert from class indices to class names\n  pred = dataset[\"test\"].classes[pred]\n  label = dataset[\"test\"].classes[label]\n\n  # Visualize the image\n  plt.subplot(4, 10, index + 1)\n  plt.imshow(image)\n  plt.title(f\"pred: {pred}\" + \"\\n\" + f\"label: {label}\")\n  plt.axis(\"off\")\nplt.show()"
  },
  {
    "objectID": "posts/lecs/lec03.html",
    "href": "posts/lecs/lec03.html",
    "title": "ğŸ§‘â€ğŸ« Lecture 3",
    "section": "",
    "text": "ì•ìœ¼ë¡œ ì´ 5ì¥ì— ê±¸ì³ì„œ ë”¥ëŸ¬ë‹ ëª¨ë¸ ê²½ëŸ‰í™” ê¸°ë²•ë“¤ì— ëŒ€í•´ì„œ ì†Œê°œí•˜ë ¤ê³  í•œë‹¤. ê²½ëŸ‰í™” ê¸°ë²•ìœ¼ë¡œëŠ” Pruning, Quantization, Neural Network Architecture Search, Knowledge Distillation, ê·¸ë¦¬ê³  Tiny Engineì—ì„œ ëŒë¦¬ê¸° ìœ„í•œ ë°©ë²•ì„ ì§„í–‰í•  ì˜ˆì •ì¸ë° ë³¸ ë‚´ìš©ì€ MITì—ì„œ Song Han êµìˆ˜ë‹˜ì´ Fall 2022ì— í•œ ê°•ì˜ TinyML and Efficient Deep Learning Computing 6.S965ë¥¼ ë°”íƒ•ìœ¼ë¡œ ì¬ì •ë¦¬í•œ ë‚´ìš©ì´ë‹¤. ê°•ì˜ ìë£Œì™€ ì˜ìƒì€ ì´ ë§í¬ë¥¼ ì°¸ì¡°í•˜ì!\nì²« ë²ˆì§¸ ë‚´ìš©ìœ¼ë¡œ â€œê°€ì§€ì¹˜ê¸°â€ë¼ëŠ” ì˜ë¯¸ë¥¼ ê°€ì§„ Pruningì— ëŒ€í•´ì„œ ì´ì•¼ê¸°, ì‹œì‘!"
  },
  {
    "objectID": "posts/lecs/lec03.html#introduction-to-pruning",
    "href": "posts/lecs/lec03.html#introduction-to-pruning",
    "title": "ğŸ§‘â€ğŸ« Lecture 3",
    "section": "1. Introduction to Pruning",
    "text": "1. Introduction to Pruning\nPruningì´ë€ ì˜ë¯¸ì²˜ëŸ¼ Neural Networkì—ì„œ ë§¤ê°œë³€ìˆ˜(ë…¸ë“œ)ë¥¼ ì œê±°í•˜ëŠ” ë°©ë²•ì…ë‹ˆë‹¤. ì´ëŠ” Dropoutí•˜ê³  ë¹„ìŠ·í•œ ì˜ë¯¸ë¡œ ë³¼ ìˆ˜ ìˆëŠ”ë°, Dropoutì˜ ê²½ìš° ëª¨ë¸ í›ˆë ¨ ë„ì¤‘ ëœë¤ì ìœ¼ë¡œ íŠ¹ì • ë…¸ë“œë¥¼ ì œì™¸ì‹œí‚¤ê³  í›ˆë ¨ì‹œì¼œ ëª¨ë¸ì˜ Robustnessë¥¼ ë†’ì´ëŠ” ë°©ë²•ìœ¼ë¡œ í›ˆë ¨ì„ í•˜ê³ ë‚˜ì„œë„ ëª¨ë¸ì˜ ë…¸ë“œëŠ” ê·¸ëŒ€ë¡œ ìœ ì§€ê°€ ëœë‹¤. ë°˜ë©´ Pruningì˜ ê²½ìš° í›ˆë ¨ì„ ë§ˆì¹œ í›„ì—, íŠ¹ì • Threshold ì´í•˜ì˜ ë§¤ê°œë³€ìˆ˜(ë…¸ë“œ)ì˜ ê²½ìš° ì‹œ Neural Networkì—ì„œ ì œì™¸ì‹œì¼œ ëª¨ë¸ì˜ í¬ê¸°ë¥¼ ì¤„ì´ë©´ì„œ ë™ì‹œì— ì¶”ë¡  ì†ë„ ë˜í•œ ë†’ì¼ ìˆ˜ ìˆë‹¤.\n\\[\n\\underset{W_p}{argmin}\\ L(x;W_p), \\text{ subject to } \\lvert\\lvert W_p\\lvert\\lvert_0\\ &lt; N\n\\]\n\nL represents the objective function for neural network training\n\\(x\\) is input, \\(W\\) is original weights, \\(W_p\\) is pruned weights\n\\(\\lvert\\lvert W_p\\lvert\\lvert_0\\) calcuates the #nonzeros in \\(W_p\\) and \\(N\\) is the target #nonzeros\n\nì´ëŠ” ìœ„ì™€ ê°™ì€ ì‹ìœ¼ë¡œ í‘œí˜„í•  ìˆ˜ ìˆë‹¤. íŠ¹ì • W ì˜ ê²½ìš° 0 ìœ¼ë¡œ ë§Œë“¤ì–´ ë…¸ë“œë¥¼ ì—†ì• ëŠ” ê²½ìš°ë¼ê³  ë³¼ ìˆ˜ ìˆê² ìŠµë‹ˆë‹¤. ê·¸ë ‡ê²Œ Pruningí•œ Neural NetworkëŠ” ì•„ë˜ ê·¸ë¦¼ ì²˜ëŸ¼ ëœë‹¤.\n\n\n\nReference. MIT-TinyML-lecture03-Pruning-1\n\n\nê·¸ëŸ¼ ì™œ Pruningì„ í•˜ëŠ” ê±¸ê¹Œ? ê°•ì˜ì—ì„œ Pruningì„ ì‚¬ìš©í•˜ë©´ Latency, Memeoryì™€ ê°™ì€ ë¦¬ì†ŒìŠ¤ë¥¼ í™•ë³´í•  ìˆ˜ ìˆë‹¤ê³  ê´€ë ¨ëœ ì•„ë˜ê°™ì€ ì—°êµ¬ê²°ê³¼ë¥¼ ê°™ì´ ë³´ì—¬ì¤€ë‹¤.\n\n\n\nReference. MIT-TinyML-lecture03-Pruning-1\n\n\nSong Han êµìˆ˜ë‹˜ì€ Vision ë”¥ëŸ¬ë‹ ëª¨ë¸ ê²½ëŸ‰í™” ì—°êµ¬ë¥¼ ì£¼ë¡œí•˜ì…”ì„œ, CNNì„ ê¸°ë°˜ìœ¼ë¡œ í•œ ëª¨ë¸ì„ ì˜ˆì‹œë¡œ ë³´ì—¬ì£¼ì‹ ë‹¤. ëª¨ë‘ Pruningì´í›„ì— ëª¨ë¸ ì‚¬ì´ì¦ˆì˜ ê²½ìš° ìµœëŒ€ 12ë°° ì¤„ì–´ ë“¤ë©° ì—°ì‚°ì˜ ê²½ìš° 6.3ë°°ê¹Œì§€ ì¤„ì–´ ë“  ê²ƒì„ ë³¼ ìˆ˜ ë‹¤.\nê·¸ë ‡ë‹¤ë©´ ì €ë ‡ê²Œ â€œí¬ê¸°ê°€ ì¤„ì–´ë“  ëª¨ë¸ì´ ì„±ëŠ¥ì„ ìœ ì§€í•  ìˆ˜ ìˆì„ê¹Œ?â€œ\n\n\n\nReference. MIT-TinyML-lecture03-Pruning-1\n\n\nê·¸ë˜í”„ì—ì„œ ëª¨ë¸ì˜ Weight ë¶„í¬ë„ë¥¼ ìœ„ ê·¸ë¦¼ì—ì„œ ë³´ë©´, Pruningì„ í•˜ê³  ë‚œ ì´í›„ì— Weight ë¶„í¬ë„ì˜ ì¤‘ì‹¬ì— íŒŒë¼ë¯¸í„°ê°€ ì˜ë ¤ë‚˜ê°„ ê²Œ ë³´ì¸ë‹¤. ì´í›„ Fine Tuningì„ í•˜ê³  ë‚œ ë‹¤ìŒì˜ ë¶„í¬ê°€ ë‚˜ì™€ ìˆëŠ”ë°, ì–´ëŠ ì •ë„ ì •í™•ë„ëŠ” ë–¨ì–´ì§€ì§€ë§Œ ì„±ëŠ¥ì´ ìœ ì§€ë˜ëŠ” ê±¸ ê´€ì°°í•  ìˆ˜ ìˆë‹¤.\n\n\n\nReference. MIT-TinyML-lecture03-Pruning-1\n\n\nê·¸ëŸ° Fine tuningì„ ë°˜ë³µì ìœ¼ë¡œ í•˜ê²Œ ëœë‹¤ë©´(Iterative Pruning and Fine tuning) ê·¸ë˜í”„ì—ì„œëŠ” ìµœëŒ€ 90í”„ë¡œ ì´ìƒì˜ íŒŒë¼ë¯¸í„°ë¥¼ ëœì–´ë‚¼ ìˆ˜ ìˆë‹¤ê³  í•œë‹¤.\në¬¼ë¡  íŠ¹ì • ëª¨ë¸ì—ì„œ, íŠ¹ì • Taskë¥¼ ëŒ€ìƒìœ¼ë¡œ í•œ ê²ƒì´ë¼ ì¼ë°˜í™”í•  ìˆ˜ëŠ” ì—†ì§€ë§Œ ë¦¬ì†ŒìŠ¤ë¥¼ ê³ ë ¤í•˜ëŠ” ìƒí™©ì´ë¼ë©´ ì¶©ë¶„íˆ ì‹œë„í•´ë³¼ ë§Œí•œ ê°€ì¹˜ê°€ ìˆì–´ ë³´ì¸ë‹¤. ê·¸ëŸ¼ ì´ë ‡ê²Œ ì„±ëŠ¥ì„ ìœ ì§€í•˜ë©´ì„œ Pruningì„ í•˜ê¸° ìœ„í•´ì„œ ì–´ë–¤ ìš”ì†Œë¥¼ ê³ ë ¤í•´ì•¼ í• ì§€ ë” ìì„¸íˆ ì´ì•¼ê¸°í•´ë³´ì!\nì†Œê°œí•˜ëŠ” ê³ ë ¤ìš”ì†ŒëŠ” ì•„ë˜ì™€ ê°™ë‹¤. Pruning íŒ¨í„´ë¶€í„° ì°¨ë¡€ëŒ€ë¡œ ì‹œì‘!\n\nPruning Granularity â†’ Pruning íŒ¨í„´\nPruning Criterion â†’ ì–¼ë§ˆë§Œí¼ì— íŒŒë¼ë¯¸í„°ë¥¼ Pruning í•  ê±´ê°€?\nPruning Ratio â†’ ì „ì²´ íŒŒë¼ë¯¸í„°ì—ì„œ Pruningì„ ì–¼ë§ˆë§Œí¼ì˜ ë¹„ìœ¨ë¡œ?\nFine Turning â†’ Pruning ì´í›„ì— ì–´ë–»ê²Œ Fine-Tuning í•  ê±´ê°€?\nADMM â†’ Pruning ì´í›„, ì–´ë–»ê²Œ Convexê°€ ëœë‹¤ê³  í•  ìˆ˜ ìˆì§€?\nLottery Ticket Hypothesis â†’ Trainingë¶€í„° Pruningê¹Œì§€ ëª¨ë¸ì„ ë§Œë“¤ì–´ ë³´ì!\nSystem Support â†’ í•˜ë“œì›¨ì–´ë‚˜ ì†Œí”„íŠ¸ì›¨ì–´ì ìœ¼ë¡œ Pruningì„ ì§€ì›í•˜ëŠ” ê²½ìš°ëŠ”?"
  },
  {
    "objectID": "posts/lecs/lec03.html#determine-the-pruning-granularity",
    "href": "posts/lecs/lec03.html#determine-the-pruning-granularity",
    "title": "ğŸ§‘â€ğŸ« Lecture 3",
    "section": "2. Determine the Pruning Granularity",
    "text": "2. Determine the Pruning Granularity\n\n\n\nThe case of convolutional layers, red box is preserved and white one is pruned referred from MIT-TinyML-lecture03-Pruning-1\n\n\nì—¬ê¸°ì„œ ê³ ë ¤ìš”ì†ŒëŠ” â€œì–¼ë§ˆë§Œí¼ ë‰´ëŸ°ì„ ê·¸ë£¹í™”í•˜ì—¬ ê³ ë ¤í•  ê²ƒì¸ê°€?â€ ì…ë‹ˆë‹¤. Regularí•œ ì •ë¡œë„ ë¶„ë¥˜í•˜ë©´ì„œ Irregularí•œ ê²½ìš°ì™€ Regularí•œ ê²½ìš°ì˜ íŠ¹ì§•ì„ ì•„ë˜ì²˜ëŸ¼ ë§í•©ë‹ˆë‹¤.\n\nFine-grained/Unstructured\n\nMore flexible pruning index choice\nHard to accelerate (irregular data expression)\nCan deliver speed up on some custom hardware\n\nCoarse-grained/Structured\n\nLess flexible pruning index choice (a subset of the fine-grained case)\nEasy to accelerate\n\n\nPruningì„ í•œë‹¤ê³  ëª¨ë¸ ì¶œë ¥ì´ ë‚˜ì˜¤ëŠ” ì‹œê°„ì´ ì§§ì•„ì§€ëŠ” ê²ƒì´ ì•„ë‹˜ë„ ì–¸ê¸‰í•©ë‹ˆë‹¤. Hardware Accelerationì˜ ê°€ëŠ¥ë„ê°€ ìˆëŠ”ë°, ì´ íŠ¹ì§•ì„ ë³´ë©´ ì•Œ ìˆ˜ ìˆë“¯, Pruningì˜ ììœ ë„ì™€ Hardware Accelerationì´ trade-off, ì¦‰ ê²½ëŸ‰í™” ì •ë„ì™€ Latencyì‚¬ì´ì— trade-off ê°€ ìˆì„ ê²ƒì´ ì˜ˆì¸¡ë©ë‹ˆë‹¤. í•˜ë‚˜ì”©, ìë£Œë¥¼ ë³´ë©´ì„œ ì‚´í´ ë³´ê² ìŠµë‹ˆë‹¤.\n\n2.1 Pattern-based Pruning\nIrregularì—ì„œë„ Pattern-based Pruningì€ ì—°ì†ì ì¸ ë‰´ëŸ° Mê°œ ì¤‘ Nê°œë¥¼ Pruning í•˜ëŠ” ë°©ë²•ì´ë‹¤. ì¼ë°˜ì ìœ¼ë¡œëŠ” N:M = 2:4 ìœ¼ë¡œ í•œë‹¤ê³  ì†Œê°œí•œë‹¤.\n\n\n\nReference. Accelerating Inference with Sparsity Using the NVIDIA Ampere Architecture and NVIDIA TensorRT\n\n\nReference. Accelerating Inference with Sparsity Using the NVIDIA Ampere Architecture and NVIDIA TensorRT\nì˜ˆì‹œë¥¼ ë“¤ì–´ ë³´ë©´, ìœ„ì™€ ê°™ì€ Matrixì—ì„œ í–‰ì„ ë³´ì‹œë©´ 8ê°œì˜ Weightì¤‘ 4ê°œê°€ Non-zeroì¸ ê²ƒì„ ë³¼ ìˆ˜ ìˆìŠµë‹ˆë‹¤. ì—¬ê¸°ì„œ Zeroì¸ ë¶€ë¶„ì„ ì—†ì• ê³  2bit indexë¡œ í•˜ì—¬ Matrix ì—°ì‚°ì„ í•˜ë©´ Nvidiaâ€™s Ampere GPUì—ì„œ ì†ë„ë¥¼ 2ë°°ê¹Œì§€ ë†’ì¼ ìˆ˜ ìˆë‹¤ê³  í•œë‹¤. ì—¬ê¸°ì„œ SparsityëŠ” â€œì–¼ë§ˆë§Œí¼ ê²½ëŸ‰í™” ëëŠ”ì§€?â€ ì´ë¼ê³  ìƒê°í•˜ë©´ ëœë‹¤.\n\nN:M sparsity means that in each contiguous M elements, N of them is pruned\nA classic case is 2:4 sparsity (50% sparsity)\nIt is supported by Nvidiaâ€™s Ampere GPU Architecture, which delivers up to 2x speed up and usually maintains accuracy.\n\n\n\n\nReference. https://developer.nvidia.com/blog/accelerating-inference-with-sparsity-using-ampere-and-tensorrt/\n\n\n\n\n2.2 Channel-level Pruning\në°˜ëŒ€ë¡œ íŒ¨í„´ì´ ìƒëŒ€ì ìœ¼ë¡œ regular í•œ ìª½ì¸ Channel-level Pruningì€ ì¶”ë¡ ì‹œê°„ì„ ì¤„ì¼ ìˆ˜ ìˆëŠ” ë°˜ë©´ì— ê²½ëŸ‰í™” ë¹„ìœ¨ì´ ì ë‹¤ê³  ë§í•œë‹¤. ì•„ë˜ ê·¸ë¦¼ì„ ë³´ì‹œë©´ Layerë§ˆë‹¤ Sparsityê°€ ë‹¤ë¥¸ ê±¸ ë³´ì‹¤ ìˆ˜ ìˆë‹¤.\n\nPro: Direct speed up!\nCon: smaller compression ratio\n\n\n\n\nReference AMC: Automl for Model Compression and Acceleration on Mobile Devices [HeÂ et al., ECCV 2018]\n\n\nì•„ë˜ì— ìë£Œì—ì„œëŠ” Channel ë³„ë¡œ í•œ Pruningì˜ ê²½ìš° ì „ì²´ ë‰´ë ¨ì„ ê°€ì§€ê³  í•œ Pruningë³´ë‹¤ ì¶”ë¡  ì‹œê°„ì„ ë” ì¤„ì¼ ìˆ˜ ìˆë‹¤ê³  ë§í•œë‹¤.\n\n\n\nReference AMC: Automl for Model Compression and Acceleration on Mobile Devices [HeÂ et al., ECCV 2018]\n\n\nìë£Œë¥¼ ë³´ë©´ Sparsityì—ì„œëŠ” íŒ¨í„´í™” ë¼ ìˆìœ¼ë©´ ê°€ì†í™”ê°€ ìš©ì´í•´ Latency, ì¶”ë¡  ì‹œê°„ì„ ì¤„ì¼ ìˆ˜ ìˆì§€ë§Œ ê·¸ ë§Œí¼ Pruningí•˜ëŠ” ë‰´ëŸ°ì˜ ìˆ˜ê°€ ì ì–´ ê²½ëŸ‰í™” ë¹„ìœ¨ì´ ì¤„ ê²ƒìœ¼ë¡œ ë³´ì¸ë‹¤. í•˜ì§€ë§Œ ë¹„êµì  ë¶ˆê·œì¹™í•œ ìª½ì— ì†í•˜ëŠ” Pattern-based Pruningì˜ ê²½ìš°ê°€ í•˜ë“œì›¨ì–´ì—ì„œ ì§€ì›í•´ì£¼ëŠ” ê²½ìš°, ëª¨ë¸ í¬ê¸°ì™€ Latencyë¥¼ ë‘˜ ë‹¤ ìµœì ìœ¼ë¡œ ì¡ì„ ìˆ˜ ìˆì„ ê²ƒìœ¼ë¡œ ë³´ì¸ë‹¤."
  },
  {
    "objectID": "posts/lecs/lec03.html#determine-the-pruning-criterion",
    "href": "posts/lecs/lec03.html#determine-the-pruning-criterion",
    "title": "ğŸ§‘â€ğŸ« Lecture 3",
    "section": "3. Determine the Pruning Criterion",
    "text": "3. Determine the Pruning Criterion\nê·¸ë ‡ë‹¤ë©´ ì–´ë–¤ íŒŒë¼ë¯¸í„°ë¥¼ ê°€ì§€ëŠ” ë‰´ëŸ°ì„ ìš°ë¦¬ëŠ” ì˜ë¼ë‚´ì•¼ í• ê¹Œìš”? Synapseì™€ Neuronìœ¼ë¡œ ë‚˜ëˆ ì„œ ì‚´í´ë³´ì.\n\nWhich synapses? Which neurons? Which one is less important?\nHow to Select Synapses and Select Neurons to Prune\n\n\n3.1 Select of Synapses\ní¬ê²Œ ì„¸ ê°€ì§€ë¡œ ë¶„ë¥˜í•˜ëŠ”ë°, ê° ë‰´ëŸ°ì˜ í¬ê¸°, ê° ì±„ë„ì— ì „ì²´ ë‰´ëŸ°ì— ëŒ€í•œ í¬ê¸°, ê·¸ë¦¬ê³  í…Œì¼ëŸ¬ ê¸‰ìˆ˜ë¥¼ ì´ìš©í•˜ì—¬ gradientì™€ weightë¥¼ ëª¨ë‘ ê³ ë ¤í•œ í¬ê¸°ë¥¼ ì†Œê°œí•œë‹¤. Song han êµìˆ˜ë‹˜ì´ ë°©ë²•ë“¤ì„ ì†Œê°œí•˜ê¸°ì— ì•ì„œì„œ ìœ ìˆ˜ì˜ ê¸°ì—…ë“¤ë„ ì§€ë‚œ 5ë…„ ë™ì•ˆ ì£¼ë¡œ Magnitude-based Pruningë§Œì„ ì‚¬ìš©í•´ì™”ë‹¤ê³  í•˜ëŠ”ë°, 2023ë…„ì´ ë¼ì„œ On-device AIê°€ ê°ê´‘ë°›ê¸° ì‹œì‘í•´ì„œ ì ì°¨ì ìœ¼ë¡œ ê´€ì‹¬ì„ ë°›ê¸° ì‹œì‘í•œ ê±´ê°€ ì‹¶ê¸°ë„ í•˜ë‹¤.\n3.1.1 Magnitude-based Pruning\ní¬ê¸°ë¥¼ ê¸°ì¤€ìœ¼ë¡œ í•˜ëŠ” ê²½ìš°, â€œì–¼ë§ˆë§Œí¼ ë‰´ëŸ° ê·¸ë£¹ì—ì„œ ê³ ë ¤í•  ê²ƒì¸ê°€?â€ì™€ â€œê·¸ë£¹ë‚´ì—ì„œ ì–´ë–¤ ì •ê·œí™”ë¥¼ ì‚¬ìš©í•  ê²ƒì¸ê°€?ë¥¼ ê³ ë ¤í•œë‹¤.\n\nHeuristic pruning criterion, Element-wise Pruning\n\\[\nImportance = \\lvert W \\lvert\n\\]\n\n\n\nReference. MIT-TinyML-lecture03-Pruning-1\n\n\nHeuristic pruning criterion, Row-wise Pruning, L1-norm magnitude\n\\[\nImportance = \\sum_{i\\in S}\\lvert w_i \\lvert, \\\\where\\ W^{(S)}\\ is\\ the\\ structural\\ set\\ S\\ of\\ parameters\\ W\n\\]\n\n\n\nReference. MIT-TinyML-lecture03-Pruning-1\n\n\nHeuristic pruning criterion, Row-wise Pruning, L2-norm magnitude\n\\[\nImportance = \\sum_{i\\in S}\\lvert w_i \\lvert, \\\\where\\ W^{(S)}\\ is\\ the\\ structural\\ set\\ S\\ of\\ parameters\\ W\n\\]\n\n\n\nReference. MIT-TinyML-lecture03-Pruning-1\n\n\nHeuristic pruning criterion, \\(L_p\\)- norm\n\\[\n\\lvert\\lvert W^{(S)}\\lvert\\lvert=\\huge( \\large\n   \\sum_{i\\in S} \\lvert w_i \\lvert^p\n\\huge) \\large^{\\frac{1}{p}}\n\\]\n\n3.1.2 Scaling-based Pruning\në‘ ë²ˆì§¸ë¡œ Scalingì„ í•˜ëŠ” ê²½ìš° ì±„ë„ë§ˆë‹¤ Scaling Factorë¥¼ ë‘¬ì„œ Pruningì„ í•œë‹¤. ê·¸ëŸ¼ Scaling Factorë¥¼ ì–´ë–»ê²Œ ë‘¬ì•¼ í• ê¹Œ? ê°•ì˜ì—ì„œ ì†Œê°œí•˜ëŠ” ì´ ë…¼ë¬¸ì—ì„œëŠ” Scaling factor \\(\\gamma\\) íŒŒë¼ë¯¸í„°ë¥¼ trainable íŒŒë¼ë¯¸í„°ë¡œ ë‘ë©´ì„œ batch normalization layerì— ì‚¬ìš©í•œë‹¤.\n\nScale factor is associated with each filter(i.e.Â output channel) in convolution layers.\nThe filters or output channels with small scaling factor magnitude will be pruned\nThe scaling factors can be reused from batch normalization layer\n\\[\n  z_o = \\gamma\\dfrac{z_i-\\mu_{B}}{\\sqrt{\\sigma_B^2+\\epsilon}}+\\beta\n  \\]\n\n\n\n\nReference. MIT-TinyML-lecture03-Pruning-1\n\n\n3.1.3 Talyor Expansion Analysis on Pruning Error\nì„¸ ë²ˆì§¸ ë°©ë²•ì€ í…Œì¼ëŸ¬ ê¸‰ìˆ˜ë¥¼ ì´ìš©í•˜ì—¬ Objective functionì„ ìµœì†Œí™” í•˜ëŠ” ì§€ì ì„ ì°¾ëŠ” ë°©ë²•ì…ë‹ˆë‹¤. Talyor Seriesì— ëŒ€í•œ ìì„¸í•œ ë‚´ìš©ì€ ì—¬ê¸°ì„œ!\n\nEvaluate pruning error induced by pruning synapses.\nMinimize the objective function L(x; W)\nA Taylor series can approximate the induced error.\n\n\\[\n\\delta L = L(x;W)-L(x;W_p=W-\\delta W) \\\\ = \\sum_i g_i\\delta w_i + \\frac{1}{2} \\sum_i h_{ii}\\delta w_i^2 + \\frac{1}{2}\\sum_{i\\not=j}h_{ij}\\delta w_i \\delta w_j + O(\\lvert\\lvert \\delta W \\lvert\\lvert^3)\n\\] \\[\nwhere\\ g_i=\\dfrac{\\delta L}{\\delta w_i}, h_{i, j} = \\dfrac{\\delta^2 L}{\\delta w_i \\delta w_j}\n\\]\n\nSecond-Order-based Pruning\n\n\n\nReference. MIT-TinyML-lecture03-Pruning-1\n\n\n\n\n\nReference. MIT-TinyML-lecture03-Pruning-1\n\n\nOptimal Brain Damage[LeCunÂ et al.,Â NeurIPS 1989] ë…¼ë¬¸ì—ì„œëŠ” ì´ ë°©ë²•ì„ ì´ìš©í•˜ê¸° ìœ„í•´ ì„¸ ê°€ì§€ë¥¼ ê°€ì •í•œë‹¤.\n\nObjective function Lì´ quadratic ì´ê¸° ë•Œë¬¸ì— ë§ˆì§€ë§‰ í•­ì´ ë¬´ì‹œëœë‹¤(ì´ëŠ” Talyor Seriesì˜ Error í•­ì„ ì•Œë©´ ì´í•´ê°€ ë” ì‰½ë‹¤!)\në§Œì•½ ì‹ ê²½ë§ì´ ìˆ˜ë ´í•˜ê²Œë˜ë©´, ì²« ë²ˆì§¸í•­ë„ ë¬´ì‹œëœë‹¤.\nê° íŒŒë¼ë¯¸í„°ê°€ ë…ë¦½ì ì´ë¼ë©´ Cross-termë„ ë¬´ì‹œëœë‹¤.\n\nê·¸ëŸ¬ë©´ ì‹ì„ ì•„ë˜ì²˜ëŸ¼ ì •ë¦¬í•  ìˆ˜ ìˆëŠ”ë°, ì¤‘ìš”í•œ ë¶€ë¶„ì€ Hessian Matrix Hì— ì‚¬ìš©í•˜ëŠ” Computationì´ ì–´ë µë‹¤ëŠ” ì !\n\\[\n\\delta L_i = L(x;W)-L(x;W_p\\lvert w_i=0)\\approx \\dfrac{1}{2} h_{ii}w_i^2,\\ where\\ h_{ii}=\\dfrac{\\partial^2 L}{\\partial w_i \\partial w_j}\n\\]\n\\[\nimportance_{w_i} = \\lvert \\delta L_i\\lvert = \\frac{1}{2}h_{ii}w_i^2\n\\] \\[\n*\\ h_{ii} \\text{ is non-negative}\n\\]\nFirst-Order-based Pruning\n\nì°¸ê³ ë¡œ ì´ ë°©ë²•ì€ 2023ë…„ì—ëŠ” ì†Œê°œí•˜ì§€ ì•ŠëŠ”ë‹¤.\n\n\n\n\nReference. MIT-TinyML-lecture03-Pruning-1\n\n\n\nIf only first-order expansion is considered under an i.i.d(Independent and identically distributed) assumption,\n\n\\[\n\\delta L_i = L(x;W) - L(x; W_P\\lvert w_i=0) \\approx g_iw_i,\\ where\\ g_i=\\dfrac{\\partial L}{\\partial w_i}\n\\] \\[\nimportance_{w_i} = \\lvert \\delta L_i \\lvert = \\lvert g_i w_i \\lvert \\ or \\ importance_{w_i} = \\lvert \\delta L_i \\lvert^2 = (g_i w_i)^2\n\\]\n\nFor coarse-grained pruning, we have,\n\\[\n  importance_{\\ W^{(S)}} = \\sum_{i \\in S}\\lvert \\delta L_i \\lvert^2 = \\sum_{i \\in S} (g_i w_i)^2,\\ where \\ W^{(S)}is\\ the\\ structural\\ set\\ of\\ parameters\n  \\]\n\n\n\n\n3.2 Select of Neurons\nì–´ë–¤ Neuronì„ ì—†ì•¨ ì§€ë¥¼ ê³ ë ¤(Less useful â†’ Remove) í•œ ì´ ë°©ë²•ì€ Neuronì˜ ê²½ìš°ë„ ìˆì§€ë§Œ ì•„ë˜ ê·¸ë¦¼ì²˜ëŸ¼ Channelë¡œ ê³ ë ¤í•  ìˆ˜ë„ ìˆë‹¤. í™•ì‹¤íˆ ì „ì— ì†Œê°œí–ˆë˜ ë°©ë²•ë“¤ë³´ë‹¤ â€œCoarse-grained pruningâ€ì¸ ë°©ë²•ì´ë‹¤.\n\n\nPercentage-of-Zero-based Pruning\nì²«ë²ˆì§¸ëŠ” Channelë§ˆë‹¤ 0ì˜ ë¹„ìœ¨ì„ ë´ì„œ ë¹„ìœ¨ì´ ë†’ì€ Channel ì„ ì—†ë‚´ëŠ” ë°©ë²•ì´ë‹¤. ReLU activationì„ ì‚¬ìš©í•˜ë©´ Outputì´ 0ì´ ë‚˜ì˜¤ëŠ”ë°, ì—¬ê¸°ì„œ 0ì˜ ë¹„ìœ¨, Average Percentage of Zero activations(APoZ)ë¼ê³  ë¶€ë¥´ëŠ” ê²ƒì„ ë³´ê³  ê°€ì§€ì¹˜ê¸°í•  Channelì„ ì œê±°í•œë‹¤.\n\nReLU activation will generate zeros in the output activation\nSimilar to magnitude of weights, the Average Percentage of Zero activations(APoZ) can be exploited to measure the importance the neuron has\n\n\n\n\nReference. MIT-TinyML-lecture03-Pruning-1\n\n\nFirst-Order-based Pruning\n\nì°¸ê³ ë¡œ ì´ ë°©ë²•ì€ 2023ë…„ì—ëŠ” ì†Œê°œí•˜ì§€ ì•ŠëŠ” ë°©ë²•ì´ë‹¤.\nMinimize the error on loss function introduced by pruning neurons\nSimilar to previous Taylor expansion on weights, the induced error of the objective functionÂ L(x;Â W)Â can be approximated by a Taylor series expanded on activations.\n\\[\n  \\delta L_i = L(x; W) - L(x\\lvert x_i = 0; W) \\approx \\dfrac{\\partial L}{\\partial x_i}x_i\n  \\]\nFor a structural set of neuronsÂ \\(x^{(S)}\\)Â (e.g., a channel plane),\n\\[\n  \\lvert \\delta L_{x^{(S)}} \\lvert\\ = \\Large\\lvert \\small\\sum_{i\\in S}\\dfrac{\\partial L}{\\partial x_i}x_i\\Large\\lvert\n  \\]\n\nRegression-based Pruning\nì´ ë°©ë²•ì€ Quantizedí•œ ë ˆì´ì–´ì˜ output \\(\\hat Z\\)(construction error of the corresponding layerâ€™s outputs)ì™€ \\(Z\\)ë¥¼ Trainingì„ í†µí•´ ì°¨ì´ë¥¼ ì¤„ì´ëŠ” ë°©ë²•ì´ë‹¤. ì°¸ê³ ë¡œ ë¬¸ì œë¥¼ í‘¸ëŠ” ìì„¸í•œ ê³¼ì •ì€ 2022ë…„ ê°•ì˜ì—ë§Œ ë‚˜ì™€ ìˆë‹¤.\n\n\\[\nZ=XW^T=\\sum_{c=0}^{c_i-1}X_cW_c^T\n\\]\n\n\n\nReference. MIT-TinyML-lecture03-Pruning-1\n\n\në¬¸ì œë¥¼ ì‹ìœ¼ë¡œ ì •ì˜í•´ë³´ë©´ ì•„ë˜ì™€ ê°™ì€ë°,\n\n\\(\\beta\\) is the coefficient vector of length \\(c_i\\) for channel selection.\n\\(\\beta_c = 0\\) means channel \\(c\\) is pruned.\n\\(N_c\\) is the number of none zero channel\n\nìš°ì„  ë¬¸ì œë¥¼ í‘¸ëŠ” ë‹¨ê³„ëŠ” ë‘ ë‹¨ê³„ë¡œ ë‚˜ëˆˆë‹¤. Channelì˜ Scale \\(\\beta\\)ë¥¼ ìš°ì„  ê³„ì‚°í•œ í›„ì— \\(W\\)ë¥¼ Quantizedí•œ ë ˆì´ì–´ì˜ output \\(\\hat Z\\)(construction error of the corresponding layerâ€™s outputs)ì™€ \\(Z\\)ì˜ ì°¨ì´ê°€ ìµœì†Œí™”ë˜ëŠ” ì§€ì ê¹Œì§€ Trainingì‹œí‚¨ë‹¤.\nSolve the problem in two folds:\n\nFix W, solve \\(\\beta\\) for channel selection â†’ NP(Nondeterministic polynomial)-hard\nFix \\(\\beta\\), solve W to minimize reconstruction error(Weight Reconstruction)\n\nê° ë¬¸ì œë¥¼ í‘¸ëŠ” ê³¼ì •ì„ ì¡°ê¸ˆ ë” ìì„¸íˆ ì‚´í´ë´ë³´ì. ë³¸ ë‚´ìš©ì€ 2022ë…„ ê°•ì˜ì— ìˆìœ¼ë‹ˆ ì°¸ê³ !\nNP(Nondeterministic polynomial)-hardëŠ” ì•„ë˜ì™€ ê°™ì´ ì‹ìœ¼ë¡œ ì •ë¦¬í•  ìˆ˜ ìˆë‹¤.\n\\[\n\\underset{\\beta}{argmin} \\lvert\\lvert Z- \\sum_{c=0}^{c_i-1} \\beta_cX_cW_c^T \\lvert\\lvert_F^2 = \\lvert\\lvert \\sum_{c=0}^{c_i-1}X_cW_c^T - \\sum_{c=0}^{c_i-1} \\beta_cX_cW_c^T \\lvert\\lvert_F^2\n\\] \\[\n= \\lvert\\lvert\\sum_{c=0}^{c_i-1} (1-\\beta_c)X_cW_c^T \\lvert\\lvert_F^2, \\ s.t.\\ \\lvert\\lvert\\beta\\lvert\\lvert_0 \\ \\leq N_c\n\\]\nê°•ì˜ì—ì„œ ì†Œê°œí•˜ëŠ” ThiNetì´ë¼ëŠ” ë…¼ë¬¸ì—ì„œëŠ” greedy solutionì„ ì´ìš©í•´ì„œ ì±„ë„ í•˜ë‚˜í•˜ë‚˜ì”© Pruning í•´ë³´ë©° objective functionì˜ l2-norm ìµœì†Ÿê°’ì„ êµ¬í•œë‹¤.\n1: S = []\n2: while len(S) &lt; N:\n3:   min_norm, min_c = +inf, 0\n4:   for c in range(c_i):\n5:     tmpS=S+[c]\n6:     Z = X[:,tmpS] * W[:,tmpS].t()\n7:     norm = Z.norm(2)\n8:     if norm &lt; min_norm:\n9:       min_norm, min_c = norm, c\n10:   S.append(min_c)\n11:   c_i.pop(min_c)\nì—¬ê¸°ì„œ ë”í•´ì„œ \\(\\beta\\) ë¥¼ êµ¬í•˜ëŠ” ê³¼ì •ì—ì„œ ì¼ë°˜í™”ë¥¼ ìœ„í•´ LASSO ë°©ì‹ì„ ì‚¬ìš©í•œë‹¤(LASSOì— ëŒ€í•œ ìì„¸í•œ ë‚´ìš©ì€ ì—¬ê¸°ì„œ). Relax the \\(l_0\\) to \\(l_1\\) regularization (LASSO):\n\\[\n\\underset{\\beta}{argmin}\\ \\lvert\\lvert Z- \\sum_{c=0}^{c_i-1}\\beta_cX_cW_c^T\\lvert\\lvert^2_F+\\lambda\\lvert\\lvert \\beta \\lvert\\lvert_1\n\\]\n\n\\(\\lambda\\) is a penalty coefficient. By increasing \\(\\lambda\\), there will be more zeros in \\(\\beta\\).\nGradually increase \\(\\lambda\\) and solve the LASSO regression for \\(\\beta\\), until \\(\\lvert\\lvert \\beta \\lvert\\lvert_0==N_c\\) is met.\nWhy \\(\\lvert\\lvert \\beta \\lvert\\lvert_0==N_c\\)?\nì—¬ê¸°ì— ëŒ€í•´ì„œëŠ” ë”°ë¡œ ì–¸ê¸‰ë˜ì§€ ì•Šì•˜ì§€ë§Œ, ì˜ë¯¸ìƒ scale ì „ì²´ Nê°œ ì¤‘ì—ì„œ ìµœì ê°’ì„ ì°¾ì•„ì•¼í•œë‹¤ë©´ ì „ì²´ë¥¼ Nìœ¼ë¡œ ìœ ì§€í•˜ë©´ì„œ ìµœì ê°’ì„ ì°¾ê¸° ìœ„í•´ì„œê°€ ì•„ë‹ê¹Œ?\n\në‘ ë²ˆì§¸ëŠ” êµ¬í•œ \\(\\beta\\)ë¥¼ ê³ ì •í•œ ìƒíƒœë¡œ Weightë¥¼ Quantized ì „í›„ì˜ ì°¨ì´ë¥¼ ìµœì†Œí™” í•˜ê²Œ â€œWeight Reconstructionâ€ í•œë‹¤. êµ¬í•˜ëŠ” ê³¼ì •ì€ least square approachë¥¼ ì´ìš©í•œ unique closed-form solution ì´ë¯€ë¡œ ì•„ë˜ë¥¼ ì°¸ì¡°í•˜ì.\n\\[\n\\underset{\\beta}{argmin}\\ \\lvert\\lvert Z- \\sum_{c=0}^{c_i-1}\\beta_cX_cW_c^T\\lvert\\lvert^2_F\n\\]\n\n\\(\\beta\\) is a coefficient vector from the previous step\nThis is a classic linear regression problem, which has a unique closed-form solution using the least square approach.\n\\[\n  \\underset{W}{argmin} \\lvert\\lvert Z-\\hat{Z} \\lvert\\lvert^2_F = \\lvert\\lvert Z-UW^T \\lvert\\lvert_F^2\n  \\]\nwhere\n\\[\n  U= \\Large[ \\small\\beta_0X_0\\ \\beta_1X_1 \\ \\cdots \\beta_cX_c \\cdots \\beta_{c_i-1}X_{c_i-1} \\Large]\n  \\]\nand thus,\n\\[\n  W^T = (U^TU)^{-1}U^T Z\n  \\]\n\nQ. How \\((U^TU)^{-1}\\) exists?\nLeast Square method, ì„ì˜ì˜ ë²¡í„° \\(v = (v_0, v_1, \\dots, v_n)\\) ê°€ ìˆì„ ë•Œ \\(v^Tv\\) ì˜ ì—­í–‰ë ¬ì€ í•­ìƒ ìˆì„ê¹Œ? ê°€ì •ì—ì„œ â€œa unique closed-form solutionâ€ë¼ê³  í–ˆìœ¼ë¯€ë¡œ ì´ëŠ” ì¦‰ linearly independenë¡œ ê³ ë ¤í•  ìˆê³  ì—­í–‰ë ¬ì´ ìˆë‹¤(\\(v^Tv\\) is invertible)ëŠ” ì´ì•¼ê¸°ì´ë‹¤."
  },
  {
    "objectID": "posts/lecs/lec03.html#discussion",
    "href": "posts/lecs/lec03.html#discussion",
    "title": "ğŸ§‘â€ğŸ« Lecture 3",
    "section": "4. Discussion",
    "text": "4. Discussion\n\nPruningì„ Dropoutì´ë‘ ë¹„êµí•´ì„œ ì–´ë–¤ ì°¨ì´ì ì´ ìˆëŠ”ê°€?\në‘ ê°€ì§€ ë°©ë²•ì€ ë¶„ëª…íˆ Neuronê³¼ Synapseë¥¼ ì—†ëŒ„ë‹¤ëŠ” ì¸¡ë©´ì—ì„œëŠ” ë¹„ìŠ·í•˜ë‹¤. í•˜ì§€ë§Œ ë‘ ê°€ì§€ ì¸¡ë©´ì—ì„œ ì°¨ì´ì ì´ ìˆëŠ”ë°, í•œ ê°€ì§€ëŠ” ëª©ì í•˜ëŠ” ë°”ì´ê³ , ë‘ ë²ˆì§¸ëŠ” ì‹œì ì´ë‹¤. Dropoutì€ ëª©ì í•˜ëŠ” ë°”ê°€ í›ˆë ¨ì¤‘ì— overfittingì„ ë°©ì§€í•˜ê¸° ìœ„í•¨ì´ ìˆê³  Pruningì˜ ê²½ìš°ëŠ” í›ˆë ¨ì„ ë§ˆì¹œ ëª¨ë¸ì˜ í¬ê¸°ë¥¼ ì¤„ì´ëŠ” ê²ƒì— ìˆë‹¤. ê·¸ë¦¬ê³  ë‘ ë²ˆì§¸ ì‹œì ì˜ ê²½ìš° Dropoutì€ í›ˆë ¨ì¤‘ì— ì´ë¤„ì§€ëŠ” ë°˜ë©´ Pruningì€ í›ˆë ¨ì„ ë§ˆì¹˜ê³ , ê·¸ í¬ê¸°ë¥¼ ì¤„ì¸ í›„ì— ì„±ëŠ¥ì´ ë–¨ì–´ì§€ë©´ ê·¸ì— ë§ê²Œ Fine-tuningì„ í•œë‹¤.\nìŠ¤í„°ë””ì—ì„œëŠ” â€œì™œ dropoutì„ í†µí•´ ì‚¬ì´ì¦ˆë¥¼ ì¤„ì´ì§€ ì•Šì•˜ëŠ”ê°€? ê·¸ë¦¬ê³  êµ¬ì§€ í›ˆë ¨ì„ ë§ˆì¹œ ë‹¤ìŒì— í•  í•„ìš”ê°€ ìˆë‚˜?â€ ë¼ê³  ì§ˆë¬¸ì´ ë‚˜ì™”ì—ˆë‹¤. ë¬¼ë¡  í›ˆë ¨ ì¤‘ì— ëª¨ë¸ì˜ ì‚¬ì´ì¦ˆë¥¼ ì‘ê²Œ ë§Œë“¤ ìˆ˜ ìˆìœ¼ë©´, ê°€ëŠ¥í•œ ê·¸ë ‡ê²Œ í•˜ë©´ ë  ê²ƒì´ë‹¤. í•˜ì§€ë§Œ, ì´ ë˜í•œ ë‘ê°€ì§€ ì¸¡ë©´ì„ ê³ ë ¤í•  í•„ìš”ê°€ ìˆë‹¤. í•˜ë‚˜ëŠ” â€œê³¼ì—° ëª¨ë¸ì˜ ì‚¬ì´ì¦ˆë¥¼ í›ˆë ¨ ì¤‘ í˜¹ì€ ì „ì— ì¤„ì—¬ë‚˜ê°€ë©´ì„œ ì¶©ë¶„íˆ ì„±ëŠ¥ì„ ë‚¼ ìˆ˜ ìˆëŠ”ê°€?â€ì´ê³  ë‹¤ë¥¸ í•˜ë‚˜ëŠ” Pruningì´ë‚˜ ëª¨ë¸ ê²½ëŸ‰í™”ëŠ” ìµœì í™”ì— ì´ˆì ì„ ë§ì¶˜ë‹¤ê³  ìƒê°í•œë‹¤. ê·¸ë ‡ê¸° ë•Œë¬¸ì— í›ˆë ¨ ì¤‘ê°„ì— Channel pruningê³¼ ê°™ì€ ê¸°ë²•ì„ ì‚¬ìš©í•  ìˆ˜ ìˆì„ ì§€ëŠ” ë¯¸ì§€ìˆ˜ì´ê³ , ì„¤ë ¹ Fine-grained Pruningê³¼ ê°™ì€ ê¸°ë²•ì„ ì‚¬ìš©í•œë‹¤ í•˜ë”ë¼ë„ ì´ëŠ” ëª¨ë¸ì˜ ì‚¬ì´ì¦ˆë§Œ ì¤„ì–´ ë“¤ ë¿, ë‚˜ë¨¸ì§€ ë©”ëª¨ë¦¬(e.g.Â RAM)ì´ë‚˜ Latencyê°™ì€ ì„±ëŠ¥ì€ ì¢‹ê²Œ ê°€ì ¸ê°ˆ ìˆ˜ ìˆì„ì§€ë„ ë¯¸ì§€ìˆ˜ë¼ê³  ìƒê°í•œë‹¤.\ní•„ìëŠ” ìœ„ì™€ ê°™ì€ ìµœì í™”ë¥¼ í†µí•œ ì„±ëŠ¥ ê°œì„ ì„ ì´ ê¸€ì—ì„œì²˜ëŸ¼ 2022ë…„ TinyML ê°•ì˜ì—ì„œ ì œê³µí•˜ëŠ” ì‹¤ìŠµì„ í†µí•´ ê²½í—˜í–ˆì—ˆë‹¤. ì•ì„  ì˜ˆì‹œëŠ” OSë¥¼ ê°€ì§„ ë””ë°”ì´ìŠ¤ê°€ ì•„ë‹Œ Bare-metal firmwareë¡œ í™˜ê²½ì´ ì¡°ê¸ˆ íŠ¹ìˆ˜í•˜ê¸°ë„ í•˜ê³ , ì‹¤ì œë¡œ Torchë‚˜ Tensorflowliteì—ì„œ ì œê³µí•˜ëŠ” ëª¨ë¸ ê²½ëŸ‰í™”ë¥¼ ì§ì ‘ì ìœ¼ë¡œ ë¶„ì„í•´ë´ì•¼ ì‹¤ì§ˆì ì¸ ì˜ˆì‹œë¥¼ ì•Œ ìˆ˜ ìˆê² ì§€ë§Œ, í˜¹ì—¬ ì´í•´í•´ ì°¸ê³ ê°€ ë ê¹Œ ë§ë¶™ì—¬ ë†“ëŠ”ë‹¤."
  },
  {
    "objectID": "posts/lecs/lec03.html#reference",
    "href": "posts/lecs/lec03.html#reference",
    "title": "ğŸ§‘â€ğŸ« Lecture 3",
    "section": "5. Reference",
    "text": "5. Reference\n\nMIT-TinyML-lecture03-Pruning-1\nAMC: Automl for Model Compression and Acceleration on Mobile Devices, 2018\nLearning Efficient Convolutional Networks through Network Slimming, 2017\nThiNet: A Filter Level Pruning Method for Deep Neural Network Compression, 2017\nChannel Pruning for Accelerating Very Deep Neural Networks"
  }
]