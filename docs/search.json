[
  {
    "objectID": "about.html",
    "href": "about.html",
    "title": "TinyML Study Group",
    "section": "",
    "text": "@Gueltto9th"
  },
  {
    "objectID": "about.html#lec-4lab",
    "href": "about.html#lec-4lab",
    "title": "TinyML Study Group",
    "section": "21Lec + 4Lab",
    "text": "21Lec + 4Lab\n\nLec1/2ì™€ Lab0ì€ ì œì™¸\nê°•ì˜ë¥¼ ë“£ê³  1ëª…ì”© ëŒì•„ê°€ë©´ì„œ ê°•ì˜ ë³µìŠµ recap ë°œí‘œ\në‹¤ë¥¸ ì‚¬ëŒë“¤ì€ ì§ˆë¬¸/ë””ìŠ¤ì»¤ì…˜ í† í”½ ê°€ì ¸ì˜¤ê¸°\nì£¼ 1íšŒ (ì•½ 16ì£¼ - 4ê°œì›” ì´ë‚´ ì™„ë£Œ ëª©í‘œ)\n\n\n\n[Chapter I: Efficient Inference]\n\nLecture 3: Pruning and Sparsity (Part I) @ooshyun\nLecture 4: Pruning and Sparsity (Part II) @curieuxjy\nLab 1 @CastleFlag\nLecture 5: Quantization (Part I) @ooshyun\nLecture 6: Quantization (Part II) @ooshyun\nLab 2 @curieuxjy\nLecture 7: Neural Architecture Search (Part I) @CastleFlag\nLecture 8: Neural Architecture Search (Part II) @CastleFlag\nLab 3 @ooshyun\nLecture 9: Knowledge Distillation @curieuxjy\nLecture 10: MCUNet: TinyML on Microcontrollers @curieuxjy\nLecture 11: TinyEngine and Parallel Processing @CastleFlag\n\n\n\n[Chapter II: Domain-Specific Optimization]\n\nLecture 12: Transformer and LLM (Part I) @ooshyun\nLecture 13: Transformer and LLM (Part II) @curieuxjy\nLecture 14: Vision Transformer @CastleFlag\nLab 4 @ooshyun\nLecture 15: GAN, Video, and Point Cloud @curieuxjy\nLecture 16: Diffusion Model @CastleFlag\nLecture 17: Distributed Training (Part I) @ooshyun\nLecture 18: Distributed Training (Part II) @curieuxjy\nLab 5 @CastleFlag\nLecture 19: On-Device Training and Transfer Learning @ooshyun\nLecture 20: Efficient Fine-tuning and Prompt Engineering @curieuxjy\n\n\n\n[Chapter IV: Advanced Topics]\n\nLecture 21: Basics of Quantum Computing @CastleFlag\nLecture 22: Quantum Machine Learning @ooshyun\nLecture 23: Noise Robust Quantum ML @curieuxjy"
  },
  {
    "objectID": "posts/lecs/lec03.html",
    "href": "posts/lecs/lec03.html",
    "title": "ğŸ§‘â€ğŸ« Lecture 3",
    "section": "",
    "text": "ì•ìœ¼ë¡œ ì´ 5ì¥ì— ê±¸ì³ì„œ ë”¥ëŸ¬ë‹ ëª¨ë¸ ê²½ëŸ‰í™” ê¸°ë²•ë“¤ì— ëŒ€í•´ì„œ ì†Œê°œí•˜ë ¤ê³  í•œë‹¤. ê²½ëŸ‰í™” ê¸°ë²•ìœ¼ë¡œëŠ” Pruning, Quantization, Neural Network Architecture Search, Knowledge Distillation, ê·¸ë¦¬ê³  Tiny Engineì—ì„œ ëŒë¦¬ê¸° ìœ„í•œ ë°©ë²•ì„ ì§„í–‰í•  ì˜ˆì •ì¸ë° ë³¸ ë‚´ìš©ì€ MITì—ì„œ Song Han êµìˆ˜ë‹˜ì´ Fall 2022ì— í•œ ê°•ì˜ TinyML and Efficient Deep Learning Computing 6.S965ë¥¼ ë°”íƒ•ìœ¼ë¡œ ì¬ì •ë¦¬í•œ ë‚´ìš©ì´ë‹¤. ê°•ì˜ ìë£Œì™€ ì˜ìƒì€ ì´ ë§í¬ë¥¼ ì°¸ì¡°í•˜ì!\nì²« ë²ˆì§¸ ë‚´ìš©ìœ¼ë¡œ â€œê°€ì§€ì¹˜ê¸°â€ë¼ëŠ” ì˜ë¯¸ë¥¼ ê°€ì§„ Pruningì— ëŒ€í•´ì„œ ì´ì•¼ê¸°, ì‹œì‘!"
  },
  {
    "objectID": "posts/lecs/lec03.html#introduction-to-pruning",
    "href": "posts/lecs/lec03.html#introduction-to-pruning",
    "title": "ğŸ§‘â€ğŸ« Lecture 3",
    "section": "1. Introduction to Pruning",
    "text": "1. Introduction to Pruning\nPruningì´ë€ ì˜ë¯¸ì²˜ëŸ¼ Neural Networkì—ì„œ ë§¤ê°œë³€ìˆ˜(ë…¸ë“œ)ë¥¼ ì œê±°í•˜ëŠ” ë°©ë²•ì…ë‹ˆë‹¤. ì´ëŠ” Dropoutí•˜ê³  ë¹„ìŠ·í•œ ì˜ë¯¸ë¡œ ë³¼ ìˆ˜ ìˆëŠ”ë°, Dropoutì˜ ê²½ìš° ëª¨ë¸ í›ˆë ¨ ë„ì¤‘ ëœë¤ì ìœ¼ë¡œ íŠ¹ì • ë…¸ë“œë¥¼ ì œì™¸ì‹œí‚¤ê³  í›ˆë ¨ì‹œì¼œ ëª¨ë¸ì˜ Robustnessë¥¼ ë†’ì´ëŠ” ë°©ë²•ìœ¼ë¡œ í›ˆë ¨ì„ í•˜ê³ ë‚˜ì„œë„ ëª¨ë¸ì˜ ë…¸ë“œëŠ” ê·¸ëŒ€ë¡œ ìœ ì§€ê°€ ëœë‹¤. ë°˜ë©´ Pruningì˜ ê²½ìš° í›ˆë ¨ì„ ë§ˆì¹œ í›„ì—, íŠ¹ì • Threshold ì´í•˜ì˜ ë§¤ê°œë³€ìˆ˜(ë…¸ë“œ)ì˜ ê²½ìš° ì‹œ Neural Networkì—ì„œ ì œì™¸ì‹œì¼œ ëª¨ë¸ì˜ í¬ê¸°ë¥¼ ì¤„ì´ë©´ì„œ ë™ì‹œì— ì¶”ë¡  ì†ë„ ë˜í•œ ë†’ì¼ ìˆ˜ ìˆë‹¤.\n\\[\n\\underset{W_p}{argmin}\\ L(x;W_p), \\text{ subject to } \\lvert\\lvert W_p\\lvert\\lvert_0\\ &lt; N\n\\]\n\nL represents the objective function for neural network training\n\\(x\\) is input, \\(W\\) is original weights, \\(W_p\\) is pruned weights\n\\(\\lvert\\lvert W_p\\lvert\\lvert_0\\) calcuates the #nonzeros in \\(W_p\\) and \\(N\\) is the target #nonzeros\n\nì´ëŠ” ìœ„ì™€ ê°™ì€ ì‹ìœ¼ë¡œ í‘œí˜„í•  ìˆ˜ ìˆë‹¤. íŠ¹ì • W ì˜ ê²½ìš° 0 ìœ¼ë¡œ ë§Œë“¤ì–´ ë…¸ë“œë¥¼ ì—†ì• ëŠ” ê²½ìš°ë¼ê³  ë³¼ ìˆ˜ ìˆê² ìŠµë‹ˆë‹¤. ê·¸ë ‡ê²Œ Pruningí•œ Neural NetworkëŠ” ì•„ë˜ ê·¸ë¦¼ ì²˜ëŸ¼ ëœë‹¤.\n\n\n\nReference. MIT-TinyML-lecture03-Pruning-1\n\n\nê·¸ëŸ¼ ì™œ Pruningì„ í•˜ëŠ” ê±¸ê¹Œ? ê°•ì˜ì—ì„œ Pruningì„ ì‚¬ìš©í•˜ë©´ Latency, Memeoryì™€ ê°™ì€ ë¦¬ì†ŒìŠ¤ë¥¼ í™•ë³´í•  ìˆ˜ ìˆë‹¤ê³  ê´€ë ¨ëœ ì•„ë˜ê°™ì€ ì—°êµ¬ê²°ê³¼ë¥¼ ê°™ì´ ë³´ì—¬ì¤€ë‹¤.\n\n\n\nReference. MIT-TinyML-lecture03-Pruning-1\n\n\nSong Han êµìˆ˜ë‹˜ì€ Vision ë”¥ëŸ¬ë‹ ëª¨ë¸ ê²½ëŸ‰í™” ì—°êµ¬ë¥¼ ì£¼ë¡œí•˜ì…”ì„œ, CNNì„ ê¸°ë°˜ìœ¼ë¡œ í•œ ëª¨ë¸ì„ ì˜ˆì‹œë¡œ ë³´ì—¬ì£¼ì‹ ë‹¤. ëª¨ë‘ Pruningì´í›„ì— ëª¨ë¸ ì‚¬ì´ì¦ˆì˜ ê²½ìš° ìµœëŒ€ 12ë°° ì¤„ì–´ ë“¤ë©° ì—°ì‚°ì˜ ê²½ìš° 6.3ë°°ê¹Œì§€ ì¤„ì–´ ë“  ê²ƒì„ ë³¼ ìˆ˜ ë‹¤.\nê·¸ë ‡ë‹¤ë©´ ì €ë ‡ê²Œ â€œí¬ê¸°ê°€ ì¤„ì–´ë“  ëª¨ë¸ì´ ì„±ëŠ¥ì„ ìœ ì§€í•  ìˆ˜ ìˆì„ê¹Œ?â€œ\n\n\n\nReference. MIT-TinyML-lecture03-Pruning-1\n\n\nê·¸ë˜í”„ì—ì„œ ëª¨ë¸ì˜ Weight ë¶„í¬ë„ë¥¼ ìœ„ ê·¸ë¦¼ì—ì„œ ë³´ë©´, Pruningì„ í•˜ê³  ë‚œ ì´í›„ì— Weight ë¶„í¬ë„ì˜ ì¤‘ì‹¬ì— íŒŒë¼ë¯¸í„°ê°€ ì˜ë ¤ë‚˜ê°„ ê²Œ ë³´ì¸ë‹¤. ì´í›„ Fine Tuningì„ í•˜ê³  ë‚œ ë‹¤ìŒì˜ ë¶„í¬ê°€ ë‚˜ì™€ ìˆëŠ”ë°, ì–´ëŠ ì •ë„ ì •í™•ë„ëŠ” ë–¨ì–´ì§€ì§€ë§Œ ì„±ëŠ¥ì´ ìœ ì§€ë˜ëŠ” ê±¸ ê´€ì°°í•  ìˆ˜ ìˆë‹¤.\n\n\n\nReference. MIT-TinyML-lecture03-Pruning-1\n\n\nê·¸ëŸ° Fine tuningì„ ë°˜ë³µì ìœ¼ë¡œ í•˜ê²Œ ëœë‹¤ë©´(Iterative Pruning and Fine tuning) ê·¸ë˜í”„ì—ì„œëŠ” ìµœëŒ€ 90í”„ë¡œ ì´ìƒì˜ íŒŒë¼ë¯¸í„°ë¥¼ ëœì–´ë‚¼ ìˆ˜ ìˆë‹¤ê³  í•œë‹¤.\në¬¼ë¡  íŠ¹ì • ëª¨ë¸ì—ì„œ, íŠ¹ì • Taskë¥¼ ëŒ€ìƒìœ¼ë¡œ í•œ ê²ƒì´ë¼ ì¼ë°˜í™”í•  ìˆ˜ëŠ” ì—†ì§€ë§Œ ë¦¬ì†ŒìŠ¤ë¥¼ ê³ ë ¤í•˜ëŠ” ìƒí™©ì´ë¼ë©´ ì¶©ë¶„íˆ ì‹œë„í•´ë³¼ ë§Œí•œ ê°€ì¹˜ê°€ ìˆì–´ ë³´ì¸ë‹¤. ê·¸ëŸ¼ ì´ë ‡ê²Œ ì„±ëŠ¥ì„ ìœ ì§€í•˜ë©´ì„œ Pruningì„ í•˜ê¸° ìœ„í•´ì„œ ì–´ë–¤ ìš”ì†Œë¥¼ ê³ ë ¤í•´ì•¼ í• ì§€ ë” ìì„¸íˆ ì´ì•¼ê¸°í•´ë³´ì!\nì†Œê°œí•˜ëŠ” ê³ ë ¤ìš”ì†ŒëŠ” ì•„ë˜ì™€ ê°™ë‹¤. Pruning íŒ¨í„´ë¶€í„° ì°¨ë¡€ëŒ€ë¡œ ì‹œì‘!\n\nPruning Granularity â†’ Pruning íŒ¨í„´\nPruning Criterion â†’ ì–¼ë§ˆë§Œí¼ì— íŒŒë¼ë¯¸í„°ë¥¼ Pruning í•  ê±´ê°€?\nPruning Ratio â†’ ì „ì²´ íŒŒë¼ë¯¸í„°ì—ì„œ Pruningì„ ì–¼ë§ˆë§Œí¼ì˜ ë¹„ìœ¨ë¡œ?\nFine Turning â†’ Pruning ì´í›„ì— ì–´ë–»ê²Œ Fine-Tuning í•  ê±´ê°€?\nADMM â†’ Pruning ì´í›„, ì–´ë–»ê²Œ Convexê°€ ëœë‹¤ê³  í•  ìˆ˜ ìˆì§€?\nLottery Ticket Hypothesis â†’ Trainingë¶€í„° Pruningê¹Œì§€ ëª¨ë¸ì„ ë§Œë“¤ì–´ ë³´ì!\nSystem Support â†’ í•˜ë“œì›¨ì–´ë‚˜ ì†Œí”„íŠ¸ì›¨ì–´ì ìœ¼ë¡œ Pruningì„ ì§€ì›í•˜ëŠ” ê²½ìš°ëŠ”?"
  },
  {
    "objectID": "posts/lecs/lec03.html#determine-the-pruning-granularity",
    "href": "posts/lecs/lec03.html#determine-the-pruning-granularity",
    "title": "ğŸ§‘â€ğŸ« Lecture 3",
    "section": "2. Determine the Pruning Granularity",
    "text": "2. Determine the Pruning Granularity\n\n\n\nThe case of convolutional layers, red box is preserved and white one is pruned referred from MIT-TinyML-lecture03-Pruning-1\n\n\nì—¬ê¸°ì„œ ê³ ë ¤ìš”ì†ŒëŠ” â€œì–¼ë§ˆë§Œí¼ ë‰´ëŸ°ì„ ê·¸ë£¹í™”í•˜ì—¬ ê³ ë ¤í•  ê²ƒì¸ê°€?â€ ì…ë‹ˆë‹¤. Regularí•œ ì •ë¡œë„ ë¶„ë¥˜í•˜ë©´ì„œ Irregularí•œ ê²½ìš°ì™€ Regularí•œ ê²½ìš°ì˜ íŠ¹ì§•ì„ ì•„ë˜ì²˜ëŸ¼ ë§í•©ë‹ˆë‹¤.\n\nFine-grained/Unstructured\n\nMore flexible pruning index choice\nHard to accelerate (irregular data expression)\nCan deliver speed up on some custom hardware\n\nCoarse-grained/Structured\n\nLess flexible pruning index choice (a subset of the fine-grained case)\nEasy to accelerate\n\n\nPruningì„ í•œë‹¤ê³  ëª¨ë¸ ì¶œë ¥ì´ ë‚˜ì˜¤ëŠ” ì‹œê°„ì´ ì§§ì•„ì§€ëŠ” ê²ƒì´ ì•„ë‹˜ë„ ì–¸ê¸‰í•©ë‹ˆë‹¤. Hardware Accelerationì˜ ê°€ëŠ¥ë„ê°€ ìˆëŠ”ë°, ì´ íŠ¹ì§•ì„ ë³´ë©´ ì•Œ ìˆ˜ ìˆë“¯, Pruningì˜ ììœ ë„ì™€ Hardware Accelerationì´ trade-off, ì¦‰ ê²½ëŸ‰í™” ì •ë„ì™€ Latencyì‚¬ì´ì— trade-off ê°€ ìˆì„ ê²ƒì´ ì˜ˆì¸¡ë©ë‹ˆë‹¤. í•˜ë‚˜ì”©, ìë£Œë¥¼ ë³´ë©´ì„œ ì‚´í´ ë³´ê² ìŠµë‹ˆë‹¤.\n\n2.1 Pattern-based Pruning\nIrregularì—ì„œë„ Pattern-based Pruningì€ ì—°ì†ì ì¸ ë‰´ëŸ° Mê°œ ì¤‘ Nê°œë¥¼ Pruning í•˜ëŠ” ë°©ë²•ì´ë‹¤. ì¼ë°˜ì ìœ¼ë¡œëŠ” N:M = 2:4 ìœ¼ë¡œ í•œë‹¤ê³  ì†Œê°œí•œë‹¤.\n\n\n\nReference. Accelerating Inference with Sparsity Using the NVIDIA Ampere Architecture and NVIDIA TensorRT\n\n\nReference. Accelerating Inference with Sparsity Using the NVIDIA Ampere Architecture and NVIDIA TensorRT\nì˜ˆì‹œë¥¼ ë“¤ì–´ ë³´ë©´, ìœ„ì™€ ê°™ì€ Matrixì—ì„œ í–‰ì„ ë³´ì‹œë©´ 8ê°œì˜ Weightì¤‘ 4ê°œê°€ Non-zeroì¸ ê²ƒì„ ë³¼ ìˆ˜ ìˆìŠµë‹ˆë‹¤. ì—¬ê¸°ì„œ Zeroì¸ ë¶€ë¶„ì„ ì—†ì• ê³  2bit indexë¡œ í•˜ì—¬ Matrix ì—°ì‚°ì„ í•˜ë©´ Nvidiaâ€™s Ampere GPUì—ì„œ ì†ë„ë¥¼ 2ë°°ê¹Œì§€ ë†’ì¼ ìˆ˜ ìˆë‹¤ê³  í•œë‹¤. ì—¬ê¸°ì„œ SparsityëŠ” â€œì–¼ë§ˆë§Œí¼ ê²½ëŸ‰í™” ëëŠ”ì§€?â€ ì´ë¼ê³  ìƒê°í•˜ë©´ ëœë‹¤.\n\nN:M sparsity means that in each contiguous M elements, N of them is pruned\nA classic case is 2:4 sparsity (50% sparsity)\nIt is supported by Nvidiaâ€™s Ampere GPU Architecture, which delivers up to 2x speed up and usually maintains accuracy.\n\n\n\n\nReference. https://developer.nvidia.com/blog/accelerating-inference-with-sparsity-using-ampere-and-tensorrt/\n\n\n\n\n2.2 Channel-level Pruning\në°˜ëŒ€ë¡œ íŒ¨í„´ì´ ìƒëŒ€ì ìœ¼ë¡œ regular í•œ ìª½ì¸ Channel-level Pruningì€ ì¶”ë¡ ì‹œê°„ì„ ì¤„ì¼ ìˆ˜ ìˆëŠ” ë°˜ë©´ì— ê²½ëŸ‰í™” ë¹„ìœ¨ì´ ì ë‹¤ê³  ë§í•œë‹¤. ì•„ë˜ ê·¸ë¦¼ì„ ë³´ì‹œë©´ Layerë§ˆë‹¤ Sparsityê°€ ë‹¤ë¥¸ ê±¸ ë³´ì‹¤ ìˆ˜ ìˆë‹¤.\n\nPro: Direct speed up!\nCon: smaller compression ratio\n\n\n\n\nReference AMC: Automl for Model Compression and Acceleration on Mobile Devices [HeÂ et al., ECCV 2018]\n\n\nì•„ë˜ì— ìë£Œì—ì„œëŠ” Channel ë³„ë¡œ í•œ Pruningì˜ ê²½ìš° ì „ì²´ ë‰´ë ¨ì„ ê°€ì§€ê³  í•œ Pruningë³´ë‹¤ ì¶”ë¡  ì‹œê°„ì„ ë” ì¤„ì¼ ìˆ˜ ìˆë‹¤ê³  ë§í•œë‹¤.\n\n\n\nReference AMC: Automl for Model Compression and Acceleration on Mobile Devices [HeÂ et al., ECCV 2018]\n\n\nìë£Œë¥¼ ë³´ë©´ Sparsityì—ì„œëŠ” íŒ¨í„´í™” ë¼ ìˆìœ¼ë©´ ê°€ì†í™”ê°€ ìš©ì´í•´ Latency, ì¶”ë¡  ì‹œê°„ì„ ì¤„ì¼ ìˆ˜ ìˆì§€ë§Œ ê·¸ ë§Œí¼ Pruningí•˜ëŠ” ë‰´ëŸ°ì˜ ìˆ˜ê°€ ì ì–´ ê²½ëŸ‰í™” ë¹„ìœ¨ì´ ì¤„ ê²ƒìœ¼ë¡œ ë³´ì¸ë‹¤. í•˜ì§€ë§Œ ë¹„êµì  ë¶ˆê·œì¹™í•œ ìª½ì— ì†í•˜ëŠ” Pattern-based Pruningì˜ ê²½ìš°ê°€ í•˜ë“œì›¨ì–´ì—ì„œ ì§€ì›í•´ì£¼ëŠ” ê²½ìš°, ëª¨ë¸ í¬ê¸°ì™€ Latencyë¥¼ ë‘˜ ë‹¤ ìµœì ìœ¼ë¡œ ì¡ì„ ìˆ˜ ìˆì„ ê²ƒìœ¼ë¡œ ë³´ì¸ë‹¤."
  },
  {
    "objectID": "posts/lecs/lec03.html#determine-the-pruning-criterion",
    "href": "posts/lecs/lec03.html#determine-the-pruning-criterion",
    "title": "ğŸ§‘â€ğŸ« Lecture 3",
    "section": "3. Determine the Pruning Criterion",
    "text": "3. Determine the Pruning Criterion\nê·¸ë ‡ë‹¤ë©´ ì–´ë–¤ íŒŒë¼ë¯¸í„°ë¥¼ ê°€ì§€ëŠ” ë‰´ëŸ°ì„ ìš°ë¦¬ëŠ” ì˜ë¼ë‚´ì•¼ í• ê¹Œìš”? Synapseì™€ Neuronìœ¼ë¡œ ë‚˜ëˆ ì„œ ì‚´í´ë³´ì.\n\nWhich synapses? Which neurons? Which one is less important?\nHow to Select Synapses and Select Neurons to Prune\n\n\n3.1 Select of Synapses\ní¬ê²Œ ì„¸ ê°€ì§€ë¡œ ë¶„ë¥˜í•˜ëŠ”ë°, ê° ë‰´ëŸ°ì˜ í¬ê¸°, ê° ì±„ë„ì— ì „ì²´ ë‰´ëŸ°ì— ëŒ€í•œ í¬ê¸°, ê·¸ë¦¬ê³  í…Œì¼ëŸ¬ ê¸‰ìˆ˜ë¥¼ ì´ìš©í•˜ì—¬ gradientì™€ weightë¥¼ ëª¨ë‘ ê³ ë ¤í•œ í¬ê¸°ë¥¼ ì†Œê°œí•œë‹¤. Song han êµìˆ˜ë‹˜ì´ ë°©ë²•ë“¤ì„ ì†Œê°œí•˜ê¸°ì— ì•ì„œì„œ ìœ ìˆ˜ì˜ ê¸°ì—…ë“¤ë„ ì§€ë‚œ 5ë…„ ë™ì•ˆ ì£¼ë¡œ Magnitude-based Pruningë§Œì„ ì‚¬ìš©í•´ì™”ë‹¤ê³  í•˜ëŠ”ë°, 2023ë…„ì´ ë¼ì„œ On-device AIê°€ ê°ê´‘ë°›ê¸° ì‹œì‘í•´ì„œ ì ì°¨ì ìœ¼ë¡œ ê´€ì‹¬ì„ ë°›ê¸° ì‹œì‘í•œ ê±´ê°€ ì‹¶ê¸°ë„ í•˜ë‹¤.\n3.1.1 Magnitude-based Pruning\ní¬ê¸°ë¥¼ ê¸°ì¤€ìœ¼ë¡œ í•˜ëŠ” ê²½ìš°, â€œì–¼ë§ˆë§Œí¼ ë‰´ëŸ° ê·¸ë£¹ì—ì„œ ê³ ë ¤í•  ê²ƒì¸ê°€?â€ì™€ â€œê·¸ë£¹ë‚´ì—ì„œ ì–´ë–¤ ì •ê·œí™”ë¥¼ ì‚¬ìš©í•  ê²ƒì¸ê°€?ë¥¼ ê³ ë ¤í•œë‹¤.\n\nHeuristic pruning criterion, Element-wise Pruning\n\\[\nImportance = \\lvert W \\lvert\n\\]\n\n\n\nReference. MIT-TinyML-lecture03-Pruning-1\n\n\nHeuristic pruning criterion, Row-wise Pruning, L1-norm magnitude\n\\[\nImportance = \\sum_{i\\in S}\\lvert w_i \\lvert, \\\\where\\ W^{(S)}\\ is\\ the\\ structural\\ set\\ S\\ of\\ parameters\\ W\n\\]\n\n\n\nReference. MIT-TinyML-lecture03-Pruning-1\n\n\nHeuristic pruning criterion, Row-wise Pruning, L2-norm magnitude\n\\[\nImportance = \\sum_{i\\in S}\\lvert w_i \\lvert, \\\\where\\ W^{(S)}\\ is\\ the\\ structural\\ set\\ S\\ of\\ parameters\\ W\n\\]\n\n\n\nReference. MIT-TinyML-lecture03-Pruning-1\n\n\nHeuristic pruning criterion, \\(L_p\\)- norm\n\\[\n\\lvert\\lvert W^{(S)}\\lvert\\lvert=\\huge( \\large\n   \\sum_{i\\in S} \\lvert w_i \\lvert^p\n\\huge) \\large^{\\frac{1}{p}}\n\\]\n\n3.1.2 Scaling-based Pruning\në‘ ë²ˆì§¸ë¡œ Scalingì„ í•˜ëŠ” ê²½ìš° ì±„ë„ë§ˆë‹¤ Scaling Factorë¥¼ ë‘¬ì„œ Pruningì„ í•œë‹¤. ê·¸ëŸ¼ Scaling Factorë¥¼ ì–´ë–»ê²Œ ë‘¬ì•¼ í• ê¹Œ? ê°•ì˜ì—ì„œ ì†Œê°œí•˜ëŠ” ì´ ë…¼ë¬¸ì—ì„œëŠ” Scaling factor \\(\\gamma\\) íŒŒë¼ë¯¸í„°ë¥¼ trainable íŒŒë¼ë¯¸í„°ë¡œ ë‘ë©´ì„œ batch normalization layerì— ì‚¬ìš©í•œë‹¤.\n\nScale factor is associated with each filter(i.e.Â output channel) in convolution layers.\nThe filters or output channels with small scaling factor magnitude will be pruned\nThe scaling factors can be reused from batch normalization layer\n\\[\n  z_o = \\gamma\\dfrac{z_i-\\mu_{B}}{\\sqrt{\\sigma_B^2+\\epsilon}}+\\beta\n  \\]\n\n\n\n\nReference. MIT-TinyML-lecture03-Pruning-1\n\n\n3.1.3 Talyor Expansion Analysis on Pruning Error\nì„¸ ë²ˆì§¸ ë°©ë²•ì€ í…Œì¼ëŸ¬ ê¸‰ìˆ˜ë¥¼ ì´ìš©í•˜ì—¬ Objective functionì„ ìµœì†Œí™” í•˜ëŠ” ì§€ì ì„ ì°¾ëŠ” ë°©ë²•ì…ë‹ˆë‹¤. Talyor Seriesì— ëŒ€í•œ ìì„¸í•œ ë‚´ìš©ì€ ì—¬ê¸°ì„œ!\n\nEvaluate pruning error induced by pruning synapses.\nMinimize the objective function L(x; W)\nA Taylor series can approximate the induced error.\n\n\\[\n\\delta L = L(x;W)-L(x;W_p=W-\\delta W) \\\\ = \\sum_i g_i\\delta w_i + \\frac{1}{2} \\sum_i h_{ii}\\delta w_i^2 + \\frac{1}{2}\\sum_{i\\not=j}h_{ij}\\delta w_i \\delta w_j + O(\\lvert\\lvert \\delta W \\lvert\\lvert^3)\n\\] \\[\nwhere\\ g_i=\\dfrac{\\delta L}{\\delta w_i}, h_{i, j} = \\dfrac{\\delta^2 L}{\\delta w_i \\delta w_j}\n\\]\n\nSecond-Order-based Pruning\n\n\n\nReference. MIT-TinyML-lecture03-Pruning-1\n\n\n\n\n\nReference. MIT-TinyML-lecture03-Pruning-1\n\n\nOptimal Brain Damage[LeCunÂ et al.,Â NeurIPS 1989] ë…¼ë¬¸ì—ì„œëŠ” ì´ ë°©ë²•ì„ ì´ìš©í•˜ê¸° ìœ„í•´ ì„¸ ê°€ì§€ë¥¼ ê°€ì •í•œë‹¤.\n\nObjective function Lì´ quadratic ì´ê¸° ë•Œë¬¸ì— ë§ˆì§€ë§‰ í•­ì´ ë¬´ì‹œëœë‹¤(ì´ëŠ” Talyor Seriesì˜ Error í•­ì„ ì•Œë©´ ì´í•´ê°€ ë” ì‰½ë‹¤!)\në§Œì•½ ì‹ ê²½ë§ì´ ìˆ˜ë ´í•˜ê²Œë˜ë©´, ì²« ë²ˆì§¸í•­ë„ ë¬´ì‹œëœë‹¤.\nê° íŒŒë¼ë¯¸í„°ê°€ ë…ë¦½ì ì´ë¼ë©´ Cross-termë„ ë¬´ì‹œëœë‹¤.\n\nê·¸ëŸ¬ë©´ ì‹ì„ ì•„ë˜ì²˜ëŸ¼ ì •ë¦¬í•  ìˆ˜ ìˆëŠ”ë°, ì¤‘ìš”í•œ ë¶€ë¶„ì€ Hessian Matrix Hì— ì‚¬ìš©í•˜ëŠ” Computationì´ ì–´ë µë‹¤ëŠ” ì !\n\\[\n\\delta L_i = L(x;W)-L(x;W_p\\lvert w_i=0)\\approx \\dfrac{1}{2} h_{ii}w_i^2,\\ where\\ h_{ii}=\\dfrac{\\partial^2 L}{\\partial w_i \\partial w_j}\n\\]\n\\[\nimportance_{w_i} = \\lvert \\delta L_i\\lvert = \\frac{1}{2}h_{ii}w_i^2\n\\] \\[\n*\\ h_{ii} \\text{ is non-negative}\n\\]\nFirst-Order-based Pruning\n\nì°¸ê³ ë¡œ ì´ ë°©ë²•ì€ 2023ë…„ì—ëŠ” ì†Œê°œí•˜ì§€ ì•ŠëŠ”ë‹¤.\n\n\n\n\nReference. MIT-TinyML-lecture03-Pruning-1\n\n\n\nIf only first-order expansion is considered under an i.i.d(Independent and identically distributed) assumption,\n\n\\[\n\\delta L_i = L(x;W) - L(x; W_P\\lvert w_i=0) \\approx g_iw_i,\\ where\\ g_i=\\dfrac{\\partial L}{\\partial w_i}\n\\] \\[\nimportance_{w_i} = \\lvert \\delta L_i \\lvert = \\lvert g_i w_i \\lvert \\ or \\ importance_{w_i} = \\lvert \\delta L_i \\lvert^2 = (g_i w_i)^2\n\\]\n\nFor coarse-grained pruning, we have,\n\\[\n  importance_{\\ W^{(S)}} = \\sum_{i \\in S}\\lvert \\delta L_i \\lvert^2 = \\sum_{i \\in S} (g_i w_i)^2,\\ where \\ W^{(S)}is\\ the\\ structural\\ set\\ of\\ parameters\n  \\]\n\n\n\n\n3.2 Select of Neurons\nì–´ë–¤ Neuronì„ ì—†ì•¨ ì§€ë¥¼ ê³ ë ¤(Less useful â†’ Remove) í•œ ì´ ë°©ë²•ì€ Neuronì˜ ê²½ìš°ë„ ìˆì§€ë§Œ ì•„ë˜ ê·¸ë¦¼ì²˜ëŸ¼ Channelë¡œ ê³ ë ¤í•  ìˆ˜ë„ ìˆë‹¤. í™•ì‹¤íˆ ì „ì— ì†Œê°œí–ˆë˜ ë°©ë²•ë“¤ë³´ë‹¤ â€œCoarse-grained pruningâ€ì¸ ë°©ë²•ì´ë‹¤.\n\n\nPercentage-of-Zero-based Pruning\nì²«ë²ˆì§¸ëŠ” Channelë§ˆë‹¤ 0ì˜ ë¹„ìœ¨ì„ ë´ì„œ ë¹„ìœ¨ì´ ë†’ì€ Channel ì„ ì—†ë‚´ëŠ” ë°©ë²•ì´ë‹¤. ReLU activationì„ ì‚¬ìš©í•˜ë©´ Outputì´ 0ì´ ë‚˜ì˜¤ëŠ”ë°, ì—¬ê¸°ì„œ 0ì˜ ë¹„ìœ¨, Average Percentage of Zero activations(APoZ)ë¼ê³  ë¶€ë¥´ëŠ” ê²ƒì„ ë³´ê³  ê°€ì§€ì¹˜ê¸°í•  Channelì„ ì œê±°í•œë‹¤.\n\nReLU activation will generate zeros in the output activation\nSimilar to magnitude of weights, the Average Percentage of Zero activations(APoZ) can be exploited to measure the importance the neuron has\n\n\n\n\nReference. MIT-TinyML-lecture03-Pruning-1\n\n\nFirst-Order-based Pruning\n\nì°¸ê³ ë¡œ ì´ ë°©ë²•ì€ 2023ë…„ì—ëŠ” ì†Œê°œí•˜ì§€ ì•ŠëŠ” ë°©ë²•ì´ë‹¤.\nMinimize the error on loss function introduced by pruning neurons\nSimilar to previous Taylor expansion on weights, the induced error of the objective functionÂ L(x;Â W)Â can be approximated by a Taylor series expanded on activations.\n\\[\n  \\delta L_i = L(x; W) - L(x\\lvert x_i = 0; W) \\approx \\dfrac{\\partial L}{\\partial x_i}x_i\n  \\]\nFor a structural set of neuronsÂ \\(x^{(S)}\\)Â (e.g., a channel plane),\n\\[\n  \\lvert \\delta L_{x^{(S)}} \\lvert\\ = \\Large\\lvert \\small\\sum_{i\\in S}\\dfrac{\\partial L}{\\partial x_i}x_i\\Large\\lvert\n  \\]\n\nRegression-based Pruning\nì´ ë°©ë²•ì€ Quantizedí•œ ë ˆì´ì–´ì˜ output \\(\\hat Z\\)(construction error of the corresponding layerâ€™s outputs)ì™€ \\(Z\\)ë¥¼ Trainingì„ í†µí•´ ì°¨ì´ë¥¼ ì¤„ì´ëŠ” ë°©ë²•ì´ë‹¤. ì°¸ê³ ë¡œ ë¬¸ì œë¥¼ í‘¸ëŠ” ìì„¸í•œ ê³¼ì •ì€ 2022ë…„ ê°•ì˜ì—ë§Œ ë‚˜ì™€ ìˆë‹¤.\n\n\\[\nZ=XW^T=\\sum_{c=0}^{c_i-1}X_cW_c^T\n\\]\n\n\n\nReference. MIT-TinyML-lecture03-Pruning-1\n\n\në¬¸ì œë¥¼ ì‹ìœ¼ë¡œ ì •ì˜í•´ë³´ë©´ ì•„ë˜ì™€ ê°™ì€ë°,\n\n\\(\\beta\\) is the coefficient vector of length \\(c_i\\) for channel selection.\n\\(\\beta_c = 0\\) means channel \\(c\\) is pruned.\n\\(N_c\\) is the number of none zero channel\n\nìš°ì„  ë¬¸ì œë¥¼ í‘¸ëŠ” ë‹¨ê³„ëŠ” ë‘ ë‹¨ê³„ë¡œ ë‚˜ëˆˆë‹¤. Channelì˜ Scale \\(\\beta\\)ë¥¼ ìš°ì„  ê³„ì‚°í•œ í›„ì— \\(W\\)ë¥¼ Quantizedí•œ ë ˆì´ì–´ì˜ output \\(\\hat Z\\)(construction error of the corresponding layerâ€™s outputs)ì™€ \\(Z\\)ì˜ ì°¨ì´ê°€ ìµœì†Œí™”ë˜ëŠ” ì§€ì ê¹Œì§€ Trainingì‹œí‚¨ë‹¤.\nSolve the problem in two folds:\n\nFix W, solve \\(\\beta\\) for channel selection â†’ NP(Nondeterministic polynomial)-hard\nFix \\(\\beta\\), solve W to minimize reconstruction error(Weight Reconstruction)\n\nê° ë¬¸ì œë¥¼ í‘¸ëŠ” ê³¼ì •ì„ ì¡°ê¸ˆ ë” ìì„¸íˆ ì‚´í´ë´ë³´ì. ë³¸ ë‚´ìš©ì€ 2022ë…„ ê°•ì˜ì— ìˆìœ¼ë‹ˆ ì°¸ê³ !\nNP(Nondeterministic polynomial)-hardëŠ” ì•„ë˜ì™€ ê°™ì´ ì‹ìœ¼ë¡œ ì •ë¦¬í•  ìˆ˜ ìˆë‹¤.\n\\[\n\\underset{\\beta}{argmin} \\lvert\\lvert Z- \\sum_{c=0}^{c_i-1} \\beta_cX_cW_c^T \\lvert\\lvert_F^2 = \\lvert\\lvert \\sum_{c=0}^{c_i-1}X_cW_c^T - \\sum_{c=0}^{c_i-1} \\beta_cX_cW_c^T \\lvert\\lvert_F^2\n\\] \\[\n= \\lvert\\lvert\\sum_{c=0}^{c_i-1} (1-\\beta_c)X_cW_c^T \\lvert\\lvert_F^2, \\ s.t.\\ \\lvert\\lvert\\beta\\lvert\\lvert_0 \\ \\leq N_c\n\\]\nê°•ì˜ì—ì„œ ì†Œê°œí•˜ëŠ” ThiNetì´ë¼ëŠ” ë…¼ë¬¸ì—ì„œëŠ” greedy solutionì„ ì´ìš©í•´ì„œ ì±„ë„ í•˜ë‚˜í•˜ë‚˜ì”© Pruning í•´ë³´ë©° objective functionì˜ l2-norm ìµœì†Ÿê°’ì„ êµ¬í•œë‹¤.\n1: S = []\n2: while len(S) &lt; N:\n3:   min_norm, min_c = +inf, 0\n4:   for c in range(c_i):\n5:     tmpS=S+[c]\n6:     Z = X[:,tmpS] * W[:,tmpS].t()\n7:     norm = Z.norm(2)\n8:     if norm &lt; min_norm:\n9:       min_norm, min_c = norm, c\n10:   S.append(min_c)\n11:   c_i.pop(min_c)\nì—¬ê¸°ì„œ ë”í•´ì„œ \\(\\beta\\) ë¥¼ êµ¬í•˜ëŠ” ê³¼ì •ì—ì„œ ì¼ë°˜í™”ë¥¼ ìœ„í•´ LASSO ë°©ì‹ì„ ì‚¬ìš©í•œë‹¤(LASSOì— ëŒ€í•œ ìì„¸í•œ ë‚´ìš©ì€ ì—¬ê¸°ì„œ). Relax the \\(l_0\\) to \\(l_1\\) regularization (LASSO):\n\\[\n\\underset{\\beta}{argmin}\\ \\lvert\\lvert Z- \\sum_{c=0}^{c_i-1}\\beta_cX_cW_c^T\\lvert\\lvert^2_F+\\lambda\\lvert\\lvert \\beta \\lvert\\lvert_1\n\\]\n\n\\(\\lambda\\) is a penalty coefficient. By increasing \\(\\lambda\\), there will be more zeros in \\(\\beta\\).\nGradually increase \\(\\lambda\\) and solve the LASSO regression for \\(\\beta\\), until \\(\\lvert\\lvert \\beta \\lvert\\lvert_0==N_c\\) is met.\nWhy \\(\\lvert\\lvert \\beta \\lvert\\lvert_0==N_c\\)?\nì—¬ê¸°ì— ëŒ€í•´ì„œëŠ” ë”°ë¡œ ì–¸ê¸‰ë˜ì§€ ì•Šì•˜ì§€ë§Œ, ì˜ë¯¸ìƒ scale ì „ì²´ Nê°œ ì¤‘ì—ì„œ ìµœì ê°’ì„ ì°¾ì•„ì•¼í•œë‹¤ë©´ ì „ì²´ë¥¼ Nìœ¼ë¡œ ìœ ì§€í•˜ë©´ì„œ ìµœì ê°’ì„ ì°¾ê¸° ìœ„í•´ì„œê°€ ì•„ë‹ê¹Œ?\n\në‘ ë²ˆì§¸ëŠ” êµ¬í•œ \\(\\beta\\)ë¥¼ ê³ ì •í•œ ìƒíƒœë¡œ Weightë¥¼ Quantized ì „í›„ì˜ ì°¨ì´ë¥¼ ìµœì†Œí™” í•˜ê²Œ â€œWeight Reconstructionâ€ í•œë‹¤. êµ¬í•˜ëŠ” ê³¼ì •ì€ least square approachë¥¼ ì´ìš©í•œ unique closed-form solution ì´ë¯€ë¡œ ì•„ë˜ë¥¼ ì°¸ì¡°í•˜ì.\n\\[\n\\underset{\\beta}{argmin}\\ \\lvert\\lvert Z- \\sum_{c=0}^{c_i-1}\\beta_cX_cW_c^T\\lvert\\lvert^2_F\n\\]\n\n\\(\\beta\\) is a coefficient vector from the previous step\nThis is a classic linear regression problem, which has a unique closed-form solution using the least square approach.\n\\[\n  \\underset{W}{argmin} \\lvert\\lvert Z-\\hat{Z} \\lvert\\lvert^2_F = \\lvert\\lvert Z-UW^T \\lvert\\lvert_F^2\n  \\]\nwhere\n\\[\n  U= \\Large[ \\small\\beta_0X_0\\ \\beta_1X_1 \\ \\cdots \\beta_cX_c \\cdots \\beta_{c_i-1}X_{c_i-1} \\Large]\n  \\]\nand thus,\n\\[\n  W^T = (U^TU)^{-1}U^T Z\n  \\]\n\nQ. How \\((U^TU)^{-1}\\) exists?\nLeast Square method, ì„ì˜ì˜ ë²¡í„° \\(v = (v_0, v_1, \\dots, v_n)\\) ê°€ ìˆì„ ë•Œ \\(v^Tv\\) ì˜ ì—­í–‰ë ¬ì€ í•­ìƒ ìˆì„ê¹Œ? ê°€ì •ì—ì„œ â€œa unique closed-form solutionâ€ë¼ê³  í–ˆìœ¼ë¯€ë¡œ ì´ëŠ” ì¦‰ linearly independenë¡œ ê³ ë ¤í•  ìˆê³  ì—­í–‰ë ¬ì´ ìˆë‹¤(\\(v^Tv\\) is invertible)ëŠ” ì´ì•¼ê¸°ì´ë‹¤."
  },
  {
    "objectID": "posts/lecs/lec03.html#discussion",
    "href": "posts/lecs/lec03.html#discussion",
    "title": "ğŸ§‘â€ğŸ« Lecture 3",
    "section": "4. Discussion",
    "text": "4. Discussion\n\nPruningì„ Dropoutì´ë‘ ë¹„êµí•´ì„œ ì–´ë–¤ ì°¨ì´ì ì´ ìˆëŠ”ê°€?\në‘ ê°€ì§€ ë°©ë²•ì€ ë¶„ëª…íˆ Neuronê³¼ Synapseë¥¼ ì—†ëŒ„ë‹¤ëŠ” ì¸¡ë©´ì—ì„œëŠ” ë¹„ìŠ·í•˜ë‹¤. í•˜ì§€ë§Œ ë‘ ê°€ì§€ ì¸¡ë©´ì—ì„œ ì°¨ì´ì ì´ ìˆëŠ”ë°, í•œ ê°€ì§€ëŠ” ëª©ì í•˜ëŠ” ë°”ì´ê³ , ë‘ ë²ˆì§¸ëŠ” ì‹œì ì´ë‹¤. Dropoutì€ ëª©ì í•˜ëŠ” ë°”ê°€ í›ˆë ¨ì¤‘ì— overfittingì„ ë°©ì§€í•˜ê¸° ìœ„í•¨ì´ ìˆê³  Pruningì˜ ê²½ìš°ëŠ” í›ˆë ¨ì„ ë§ˆì¹œ ëª¨ë¸ì˜ í¬ê¸°ë¥¼ ì¤„ì´ëŠ” ê²ƒì— ìˆë‹¤. ê·¸ë¦¬ê³  ë‘ ë²ˆì§¸ ì‹œì ì˜ ê²½ìš° Dropoutì€ í›ˆë ¨ì¤‘ì— ì´ë¤„ì§€ëŠ” ë°˜ë©´ Pruningì€ í›ˆë ¨ì„ ë§ˆì¹˜ê³ , ê·¸ í¬ê¸°ë¥¼ ì¤„ì¸ í›„ì— ì„±ëŠ¥ì´ ë–¨ì–´ì§€ë©´ ê·¸ì— ë§ê²Œ Fine-tuningì„ í•œë‹¤.\nìŠ¤í„°ë””ì—ì„œëŠ” â€œì™œ dropoutì„ í†µí•´ ì‚¬ì´ì¦ˆë¥¼ ì¤„ì´ì§€ ì•Šì•˜ëŠ”ê°€? ê·¸ë¦¬ê³  êµ¬ì§€ í›ˆë ¨ì„ ë§ˆì¹œ ë‹¤ìŒì— í•  í•„ìš”ê°€ ìˆë‚˜?â€ ë¼ê³  ì§ˆë¬¸ì´ ë‚˜ì™”ì—ˆë‹¤. ë¬¼ë¡  í›ˆë ¨ ì¤‘ì— ëª¨ë¸ì˜ ì‚¬ì´ì¦ˆë¥¼ ì‘ê²Œ ë§Œë“¤ ìˆ˜ ìˆìœ¼ë©´, ê°€ëŠ¥í•œ ê·¸ë ‡ê²Œ í•˜ë©´ ë  ê²ƒì´ë‹¤. í•˜ì§€ë§Œ, ì´ ë˜í•œ ë‘ê°€ì§€ ì¸¡ë©´ì„ ê³ ë ¤í•  í•„ìš”ê°€ ìˆë‹¤. í•˜ë‚˜ëŠ” â€œê³¼ì—° ëª¨ë¸ì˜ ì‚¬ì´ì¦ˆë¥¼ í›ˆë ¨ ì¤‘ í˜¹ì€ ì „ì— ì¤„ì—¬ë‚˜ê°€ë©´ì„œ ì¶©ë¶„íˆ ì„±ëŠ¥ì„ ë‚¼ ìˆ˜ ìˆëŠ”ê°€?â€ì´ê³  ë‹¤ë¥¸ í•˜ë‚˜ëŠ” Pruningì´ë‚˜ ëª¨ë¸ ê²½ëŸ‰í™”ëŠ” ìµœì í™”ì— ì´ˆì ì„ ë§ì¶˜ë‹¤ê³  ìƒê°í•œë‹¤. ê·¸ë ‡ê¸° ë•Œë¬¸ì— í›ˆë ¨ ì¤‘ê°„ì— Channel pruningê³¼ ê°™ì€ ê¸°ë²•ì„ ì‚¬ìš©í•  ìˆ˜ ìˆì„ ì§€ëŠ” ë¯¸ì§€ìˆ˜ì´ê³ , ì„¤ë ¹ Fine-grained Pruningê³¼ ê°™ì€ ê¸°ë²•ì„ ì‚¬ìš©í•œë‹¤ í•˜ë”ë¼ë„ ì´ëŠ” ëª¨ë¸ì˜ ì‚¬ì´ì¦ˆë§Œ ì¤„ì–´ ë“¤ ë¿, ë‚˜ë¨¸ì§€ ë©”ëª¨ë¦¬(e.g.Â RAM)ì´ë‚˜ Latencyê°™ì€ ì„±ëŠ¥ì€ ì¢‹ê²Œ ê°€ì ¸ê°ˆ ìˆ˜ ìˆì„ì§€ë„ ë¯¸ì§€ìˆ˜ë¼ê³  ìƒê°í•œë‹¤.\ní•„ìëŠ” ìœ„ì™€ ê°™ì€ ìµœì í™”ë¥¼ í†µí•œ ì„±ëŠ¥ ê°œì„ ì„ ì´ ê¸€ì—ì„œì²˜ëŸ¼ 2022ë…„ TinyML ê°•ì˜ì—ì„œ ì œê³µí•˜ëŠ” ì‹¤ìŠµì„ í†µí•´ ê²½í—˜í–ˆì—ˆë‹¤. ì•ì„  ì˜ˆì‹œëŠ” OSë¥¼ ê°€ì§„ ë””ë°”ì´ìŠ¤ê°€ ì•„ë‹Œ Bare-metal firmwareë¡œ í™˜ê²½ì´ ì¡°ê¸ˆ íŠ¹ìˆ˜í•˜ê¸°ë„ í•˜ê³ , ì‹¤ì œë¡œ Torchë‚˜ Tensorflowliteì—ì„œ ì œê³µí•˜ëŠ” ëª¨ë¸ ê²½ëŸ‰í™”ë¥¼ ì§ì ‘ì ìœ¼ë¡œ ë¶„ì„í•´ë´ì•¼ ì‹¤ì§ˆì ì¸ ì˜ˆì‹œë¥¼ ì•Œ ìˆ˜ ìˆê² ì§€ë§Œ, í˜¹ì—¬ ì´í•´í•´ ì°¸ê³ ê°€ ë ê¹Œ ë§ë¶™ì—¬ ë†“ëŠ”ë‹¤."
  },
  {
    "objectID": "posts/lecs/lec03.html#reference",
    "href": "posts/lecs/lec03.html#reference",
    "title": "ğŸ§‘â€ğŸ« Lecture 3",
    "section": "5. Reference",
    "text": "5. Reference\n\nMIT-TinyML-lecture03-Pruning-1\nAMC: Automl for Model Compression and Acceleration on Mobile Devices, 2018\nLearning Efficient Convolutional Networks through Network Slimming, 2017\nThiNet: A Filter Level Pruning Method for Deep Neural Network Compression, 2017\nChannel Pruning for Accelerating Very Deep Neural Networks"
  },
  {
    "objectID": "posts/lecs/lec05.html",
    "href": "posts/lecs/lec05.html",
    "title": "ğŸ§‘â€ğŸ« Lecture 5-6",
    "section": "",
    "text": "ì´ë²ˆ ê¸€ì—ì„œëŠ” MIT HAN LABì—ì„œ ê°•ì˜í•˜ëŠ” TinyML and Efficient Deep Learning Computingì— ë‚˜ì˜¤ëŠ” Quantization ë°©ë²•ì„ ì†Œê°œí•˜ë ¤ í•œë‹¤. Quantization(ì–‘ìí™”) ì‹ í˜¸ì™€ ì´ë¯¸ì§€ì—ì„œ ì•„ë‚ ë¡œê·¸ë¥¼ ë””ì§€í„¸ë¡œ ë³€í™˜í•˜ëŠ” ê³¼ì •ì—ì„œ ì‚¬ìš©í•˜ëŠ” ê°œë…ì´ë‹¤. ì•„ë˜ ê·¸ë¦¼ê³¼ ê°™ì´ ì—°ì†ì ì¸ ì„¼ì„œë¡œ ë¶€í„° ë“¤ì–´ì˜¤ëŠ” ì•„ë‚ ë¡œê·¸ ë°ì´í„° ë‚˜ ì´ë¯¸ì§€ë¥¼ í‘œí˜„í•˜ê¸° ìœ„í•´ ë‹¨ìœ„ ì‹œê°„ì— ëŒ€í•´ì„œ ë°ì´í„°ë¥¼ ìƒ˜í”Œë§í•˜ì—¬ ë°ì´í„°ë¥¼ ìˆ˜ì§‘í•œë‹¤.\në””ì§€í„¸ë¡œ ë°ì´í„°ë¥¼ ë³€í™˜í•˜ê¸° ìœ„í•´ ë°ì´í„° íƒ€ì…ì„ ì •í•˜ë©´ì„œ ì´ë¥¼ í•˜ë‚˜ì”© ì–‘ìí™”í•œë‹¤. ì–‘ìˆ˜ì™€ ìŒìˆ˜ë¥¼ í‘œí˜„í•˜ê¸° ìœ„í•´ Unsigned Integer ì—ì„œ Signed Integer, Signedì—ì„œë„ Sign-Magnitude ë°©ì‹ê³¼ Twoâ€™s Complementë°©ì‹ìœ¼ë¡œ, ê·¸ë¦¬ê³  ë” ë§ì€ ì†Œìˆ«ì  ìë¦¬ë¥¼ í‘œí˜„í•˜ê¸° ìœ„í•´ Fixed-pointì—ì„œ Floating pointë¡œ ë°ì´í„° íƒ€ì…ì—ì„œ ìˆ˜ì˜ ë²”ì£¼ë¥¼ í™•ì¥ì‹œí‚¨ë‹¤. ì°¸ê³ ë¡œ Deviceì˜ Computationalityì™€ ML ëª¨ë¸ì˜ ì„±ëŠ¥ì§€í‘œì¤‘ í•˜ë‚˜ì¸ FLOPì´ ë°”ë¡œ floating point operations per secondì´ë‹¤.\nì´ ê¸€ì—ì„œ floating pointë¥¼ ì´í•´í•˜ë©´, fixed pointë¥¼ ì‚¬ìš©í•˜ëŠ” ê²ƒì´ ë§¤ëª¨ë¦¬ì—ì„œ, ê·¸ë¦¬ê³  ì—°ì‚°ì—ì„œ ë” íš¨ìœ¨ì ì¼ ê²ƒì´ë¼ê³  ì˜ˆìƒí•´ë³¼ ìˆ ìˆ˜ ìˆë‹¤. MLëª¨ë¸ì„ í´ë¼ìš°ë“œ ì„œë²„ì—ì„œ ëŒë¦´ ë•ŒëŠ” í¬ê²Œ ë¬¸ì œë˜ì§€ ì•Šì•˜ì§€ë§Œ ì•„ë˜ ë‘ ê°€ì§€ í‘œë¥¼ ë³´ë©´ ì—ë„ˆì§€ì†Œëª¨, ì¦‰ ë°°í„°ë¦¬ íš¨ìœ¨ì—ì„œ í¬ê²Œ ì°¨ì´ê°€ ë³´ì¸ë‹¤. ê·¸ë ‡ê¸° ë•Œë¬¸ì— ëª¨ë¸ì—ì„œ Floating pointë¥¼ fixed pointë¡œ ë” ë§ì´ ë°”ê¾¸ë ¤ê³  í•˜ëŠ”ë° ì´ ë°©ë²•ìœ¼ë¡œ ë‚˜ì˜¨ ê²ƒì´ ë°”ë¡œ Quatizationì´ë‹¤.\nì´ë²ˆ ê¸€ì—ì„œëŠ” Quntization ì¤‘ì—ì„œ Quantization ë°©ë²•ê³¼ ê·¸ ì¤‘ Linearí•œ ë°©ë²•ì— ëŒ€í•´ ë” ìì„¸í•˜ê²Œ, ê·¸ë¦¬ê³  Post-training Quantizationê¹Œì§€ ë‹¤ë£¨ê³ , ë‹¤ìŒ ê¸€ì—ì„œëŠ” Quantization-Aware Training, Binary/Tenary Quantization, Mixed Precision Quantizationê¹Œì§€ ë‹¤ë£¨ë ¤ê³  í•œë‹¤."
  },
  {
    "objectID": "posts/lecs/lec05.html#common-network-quantization",
    "href": "posts/lecs/lec05.html#common-network-quantization",
    "title": "ğŸ§‘â€ğŸ« Lecture 5-6",
    "section": "1. Common Network Quantization",
    "text": "1. Common Network Quantization\nì•ì„œì„œ ì†Œê°œí•œ ê²ƒì²˜ëŸ¼ Neural Netoworkë¥¼ ìœ„í•œ Quantizationì€ ë‹¤ìŒê³¼ ê°™ì´ ë‚˜ëˆŒ ìˆ˜ ìˆë‹¤. Quantization ë°©ë²•ì„ í•˜ë‚˜ì”© ì•Œì•„ë³´ì.\n\n\n\nReference. MIT-TinyML-lecture5-Quantization-1\n\n\n\n\n\n Reference. MIT-TinyML-lecture5-Quantization-1 in https://efficientml.ai \n\n\n\n1.1 K-Means-based Quantization\nê·¸ ì¤‘ ì²« ë²ˆì§¸ë¡œ K-means-based Quantizationì´ ìˆë‹¤. Deep Compression [HanÂ et al., ICLR 2016] ë…¼ë¬¸ì— ì†Œê°œí–ˆë‹¤ëŠ” ì´ ë°©ë²•ì€ ì¤‘ì‹¬ê°’ì„ ê¸°ì¤€ìœ¼ë¡œ clusteringì„ í•˜ëŠ” ë°©ë²•ì´ë‹¤. ì˜ˆì œë¥¼ ë´ë³´ì.\n\n\n\nReference. MIT-TinyML-lecture5-Quantization-1\n\n\nìœ„ ì˜ˆì œëŠ” weightë¥¼ codebookì—ì„œ -1, 0, 1.5, 2ë¡œ ë‚˜ëˆ  ê°ê°ì— ë§ëŠ” ì¸ë±ìŠ¤ë¡œ í‘œê¸°í•œë‹¤. ì´ë ‡ê²Œ ì—°ì‚°ì„ í•˜ë©´ ê¸°ì¡´ì— 64bytesë¥¼ ì‚¬ìš©í–ˆë˜ weightê°€ 20bytesë¡œ ì¤„ì–´ë“ ë‹¤. codebookìœ¼ë¡œ ì˜ˆì œëŠ” 2bitë¡œ ë‚˜ëˆ´ì§€ë§Œ, ì´ë¥¼ N-bitë§Œí¼ ì¤„ì¸ë‹¤ë©´ ìš°ë¦¬ëŠ” ì´ 32/Në°°ì˜ ë©”ëª¨ë¦¬ë¥¼ ì¤„ì¼ ìˆ˜ ìˆë‹¤. í•˜ì§€ë§Œ ì´ ê³¼ì •ì—ì„œ quantizatio error, ì¦‰ quantizationì„ í•˜ê¸° ì „ê³¼ í•œ í›„ì— ì˜¤ì°¨ê°€ ìƒê¸°ëŠ” ê²ƒì„ ìœ„ ì˜ˆì œì—ì„œ ë³¼ ìˆ˜ ìˆë‹¤. ë©”ëª¨ë¦¬ ì‚¬ìš©ëŸ‰ì„ ì¤„ì´ëŠ” ê²ƒë„ ì¢‹ì§€ë§Œ, ì´ ë•Œë¬¸ì— ì„±ëŠ¥ì— ì˜¤ì°¨ê°€ ìƒê¸°ì§€ ì•Šê²Œ í•˜ê¸°ìœ„í•´ ì´ ì˜¤ì°¨ë¥¼ ì¤„ì´ëŠ” ê²ƒ ë˜í•œ ì¤‘ìš”í•˜ë‹¤.\n\n\n\nReference. MIT-TinyML-lecture5-Quantization-1\n\n\nì´ë¥¼ ë³´ì™„í•˜ê¸° ìœ„í•´ Quantizedí•œ Weightë¥¼ ìœ„ì— ê·¸ë¦¼ì²˜ëŸ¼ Fine-tuningí•˜ê¸°ë„ í•œë‹¤. centroidë¥¼ fine-tuningí•œë‹¤ê³  ìƒê°í•˜ë©´ ë˜ëŠ”ë°, ê° centroidì—ì„œ ìƒê¸°ëŠ” ì˜¤ì°¨ë¥¼ í‰ê· ë‚´ tuningí•˜ëŠ” ë°©ë²•ì´ë‹¤. ì´ ë°©ë²•ì„ ì œì•ˆí•œ ë…¼ë¬¸ ì—ì„œëŠ” Convolution ë ˆì´ì–´ì—ì„œëŠ” 4bitê¹Œì§€ centroidë¥¼ ê°€ì¡Œì„ ë•Œ, Full-Connected layerì—ì„œëŠ” 2 bitê¹Œì§€ centroidë¥¼ ê°€ì¡Œì„ ë•Œ ì„±ëŠ¥ì— í•˜ë½ì´ ì—†ë‹¤ê³  ë§í•˜ê³  ìˆì—ˆë‹¤.\n\n\n\nReference. Deep Compression [Han et al., ICLR 2016]\n\n\nì´ë ‡ê²Œ Quantization ëœ WeightëŠ” ìœ„ì²˜ëŸ¼ ì—°ì†ì ì¸ ê°’ì—ì„œ ì•„ë˜ì²˜ëŸ¼ Discreteí•œ ê°’ìœ¼ë¡œ ë°”ë€ë‹¤.\n\n\n\nReference. Deep Compression [Han et al., ICLR 2016]\n\n\në…¼ë¬¸ì€ ì´ë ‡ê²Œ Quantizationí•œ weightë¥¼ í•œ ë²ˆ ë” Huffman codingë¥¼ ì´ìš©í•´ ìµœì í™”ì‹œí‚¨ë‹¤. ì§§ê²Œ ì„¤ëª…í•˜ìë©´, ë¹ˆë„ìˆ˜ê°€ ë†’ì€ ë¬¸ìëŠ” ì§§ì€ ì´ì§„ì½”ë“œë¥¼, ë¹ˆë„ ìˆ˜ê°€ ë‚®ì€ ë¬¸ìì—ëŠ” ê¸´ ì´ì§„ì½”ë“œë¥¼ ì“°ëŠ” ë°©ë²•ì´ë‹¤. ì••ì¶• ê²°ê³¼ë¡œ Generalí•œ ëª¨ë¸ê³¼ ì••ì¶• ë¹„ìœ¨ì´ ê½¤ í° SqueezeNetì„ ì˜ˆë¡œ ë“ ë‹¤. ìì„¸í•œ ë‚´ìš©ì€ ë…¼ë¬¸ì„ ì°¸ê³ í•˜ëŠ” ê±¸ë¡œ.\n\n\n\nReference. Deep Compression [Han et al., ICLR 2016]\n\n\n\n\n\nReference. Deep Compression [Han et al., ICLR 2016]\n\n\ninferenceë¥¼ ìœ„í•´ weightë¥¼ Decodingí•˜ëŠ” ê³¼ì •ì€ inferenceê³¼ì •ì—ì„œ ì €ì¥í•œ clusterì˜ ì¸ë±ìŠ¤ë¥¼ ì´ìš©í•´ codebookì—ì„œ í•´ë‹¹í•˜ëŠ” ê°’ì„ ì°¾ì•„ë‚´ëŠ” ê²ƒì´ë‹¤. ì´ ë°©ë²•ì€ ì €ì¥ ê³µê°„ì„ ì¤„ì¼ ìˆ˜ëŠ” ìˆì§€ë§Œ, floating point Computationì´ë‚˜ ë©”ëª¨ë¦¬ ì ‘ê·¼í•˜ëŠ” ë°©ì‹ìœ¼ë¡œ centroidë¥¼ ì“°ëŠ” í•œê³„ê°€ ìˆì„ ìˆ˜ ë°–ì— ì—†ë‹¤.\n\n\n\nReference. MIT-TinyML-lecture5-Quantization-1\n\n\n\n\n\n Reference. Deep Compression [Han et al., ICLR 2016] \n\n\n\n\n1.2 Linear Quantization\në‘ ë²ˆì§¸ ë°©ë²•ì€ Linear Quatizationì´ë‹¤. floating-pointì¸ weightë¥¼ N-bitì˜ ì •ìˆ˜ë¡œ affine mappingì„ ì‹œí‚¤ëŠ” ë°©ë²•ì´ë‹¤. ê°„ë‹¨í•˜ê²Œ ì‹ìœ¼ë¡œ ë³´ëŠ” ê²Œ ë” ì´í•´ê°€ ì‰½ë‹¤.\n\n\n\nReference. MIT-TinyML-lecture5-Quantization-1\n\n\nì—¬ê¸°ì„œ S(Scale of Linear Quantization)ì™€ Z(Zero point of Linear Quantization)ê°€ ìˆëŠ”ë° ì´ ë‘˜ì´ quantization parameter ë¡œì¨ tuningì„ í•  ìˆ˜ ìˆëŠ” ê°’ì¸ ê²ƒì´ë‹¤.\n\n\n\nReference. MIT-TinyML-lecture5-Quantization-1\n\n\n\n\n1.3 Scale and Zero point\n\n\n\nReference. MIT-TinyML-lecture5-Quantization-1\n\n\nì´ Scaleê³¼ Zero point ë‘ íŒŒë¼ë¯¸í„°ë¥¼ ì´ìš©í•´ì„œ affine mappingì€ ìœ„ ê·¸ë¦¼ê³¼ ê°™ë‹¤. Bit ìˆ˜(Bit Width)ê°€ ë‚®ì•„ì§€ë©´ ë‚®ì•„ì§ˆ ìˆ˜ë¡, floating pointì—ì„œ í‘œí˜„í•  ìˆëŠ” ìˆ˜ ë˜í•œ ì¤„ì–´ë“¤ ê²ƒì´ë‹¤. ê·¸ë ‡ë‹¤ë©´ Scaleì™€ Zero pointëŠ” ê°ê° ì–´ë–»ê²Œ ê³„ì‚°í• ê¹Œ?\nìš°ì„  floating-point ì¸ ìˆ«ìì˜ ë²”ìœ„ ì¤‘ ìµœëŒ€ê°’ê³¼ ìµœì†Ÿê°’ì— ë§ê²Œ ë‘ ì‹ì„ ì„¸ìš°ê³  ì´ë¥¼ ì—°ë¦½ë°©ì •ì‹ìœ¼ë¡œ Scaleê³¼ Zero pointì„ êµ¬í•  ìˆ˜ ìˆë‹¤.\n\nScale point \\[\n  r_{max} = S(q_{max}-Z)\n  \\] \\[\n  r_{min} = S(q_{min}-Z)\n  \\]\n\\[\n  r_{max} - r_{min} = S(q_{max} - q_{min})\n  \\]\n\\[\n  S = \\dfrac{r_{max}-r_{min}}{q_{max}-q_{min}}\n  \\]\nZero point \\[\n  r_{min} = S(q_{min}-Z)\n  \\]\n\\[\n  Z=q_{min}-\\dfrac{r_{min}}{S}\n  \\]\n\\[\n  Z = round\\Big(q_{min}-\\dfrac{r_{min}}{S}\\Big)\n  \\]\n\nì˜ˆë¥¼ ë“¤ì–´, ì•„ë˜ì™€ ê°™ì€ ì˜ˆì œì—ì„œ \\(r_{max}\\) ëŠ”\\(2.12\\) ì´ê³  \\(r_{min}\\) ì€ \\(-1.08\\) ë¡œ Scaleì„ ê³„ì‚°í•˜ë©´ ì•„ë˜ ê·¸ë¦¼ì²˜ëŸ¼ ëœë‹¤. Zero pointëŠ” \\(-1\\) ë¡œ ê³„ì‚°í•  ìˆ˜ ìˆë‹¤.\n\n\n\n\nReference. MIT-TinyML-lecture5-Quantization-1\n\n\nê·¸ëŸ¼ Symmetricí•˜ê²Œ rì˜ ë²”ìœ„ë¥¼ ì œí•œí•˜ëŠ” ê²ƒê³¼ ê°™ì€ ë‹¤ë¥¸ Linear Quantizationì€ ì—†ì„ê¹Œ? ì´ë¥¼ ì•ì„œ, Quatizedëœ ê°’ë“¤ì´ Matrix Multiplicationì„ í•˜ë©´ì„œ ë¯¸ë¦¬ ê³„ì‚°ë  ìˆ˜ ìˆëŠ” ìˆ˜ (Quantized Weight, Scale, Zero point)ê°€ ìˆìœ¼ë‹ˆ inferenceì‹œ ì—°ì‚°ëŸ‰ì„ ì¤„ì´ê¸° ìœ„í•´ ë¯¸ë¦¬ ê³„ì‚°í•  ìˆ˜ ìˆëŠ” íŒŒë¼ë¯¸í„°ëŠ” ì—†ì„ê¹Œ?\n\n\n1.4 Quantized Matrix Multiplication\nì…ë ¥ X, Weight W, ê²°ê³¼ Yê°€ Matrix Multiplicationì„ í–ˆë‹¤ê³  í•  ë•Œ ì‹ì„ ê³„ì‚°í•´ë³´ì.\n\\[\nY=WX\n\\]\n\\[\nS_Y(q_Y-Z_Y) = S_W(q_W-Z_W) \\cdot S_X(q_X-Z_X\n\\]\n\\[\n\\vdots\n\\]\n\n\n\nReference. MIT-TinyML-lecture5-Quantization-1\n\n\nì—¬ê¸°ì„œ ë§ˆì§€ë§‰ ì •ë¦¬í•œ ì‹ì„ ì‚´í´ë³´ë©´,\n\\(Z_x\\) ì™€ \\(q_w, Z_w, Z_X\\) ì˜ ê²½ìš°ëŠ” ë¯¸ë¦¬ ì—°ì‚°ì´ ê°€ëŠ¥í•˜ë‹¤. ë˜ \\(S_wS_X/S_Y\\) ì˜ ê²½ìš° í•­ìƒ ìˆ˜ì˜ ë²”ìœ„ê°€ \\((0, 1)\\) ë¡œ \\(2^{-n}M_0\\) , \\(M_0 \\in [0.5, 1)\\) ë¡œ ë³€í˜•í•˜ë©´ N-bit Integerë¡œ Fixed-point í˜•íƒœë¡œ í‘œí˜„ ê°€ëŠ¥í•˜ë‹¤. ì—¬ê¸°ì— \\(Z_w\\)ê°€ 0ì´ë©´ ì–´ë–¨ê¹Œ? ë˜ ë¯¸ë¦¬ ê³„ì‚°í•  ìˆ˜ ìˆëŠ” í•­ì´ ë³´ì¸ë‹¤.\n\n\n1.5 Symmetric Linear Quantization\n\n\n\nReference. MIT-TinyML-lecture5-Quantization-1\n\n\n\\(Z_w = 0\\) ì´ë¼ê³  í•¨ì€ ë°”ë¡œ ìœ„ì™€ ê°™ì€ Weight ë¶„í¬ì¸ë°, ë°”ë¡œ Symmetricí•œ Linear Quantizationìœ¼ë¡œ \\(Z_w\\)ë¥¼ 0ìœ¼ë¡œ ë§Œë“¤ì–´ \\(Z_w q_x\\)í•­ì„ 0ìœ¼ë¡œ ë‘˜ ìˆ˜ ìˆì–´ ì—°ì‚°ì„ ë˜ ì¤„ì¼ ìˆ˜ ìˆì„ ê²ƒì´ë‹¤.\nSymmetric Linear Quantizationì€ ì£¼ì–´ì§„ ë°ì´í„°ì—ì„œ Full range modeì™€ Restrict range modeë¡œ ë‚˜ë‰œë‹¤.\nì²« ë²ˆì§¸ Full range mode ëŠ” Scaleì„ real number(ë°ì´í„°, weight)ì—ì„œ ë²”ìœ„ê°€ ë„“ì€ ìª½ì— ë§ì¶”ëŠ” ê²ƒì´ë‹¤. ì˜ˆë¥¼ ë“¤ì–´ ì•„ë˜ì˜ ê²½ìš°, r_minì´ r_maxë³´ë‹¤ ì ˆëŒ“ê°’ì´ ë” í¬ê¸° ë•Œë¬¸ì— r_minì— ë§ì¶° q_minì„ ê°€ì§€ê³  Scaleì„ êµ¬í•œë‹¤. ì´ ë°©ë²•ì€ Pytorch native quantizationê³¼ ONNXì—ì„œ ì‚¬ìš©ëœë‹¤ê³  ê°•ì˜ì—ì„œ ì†Œê°œí•œë‹¤.\n\n\n\nReference. MIT-TinyML-lecture5-Quantization-1\n\n\në‘ ë²ˆì§¸ Restrict range modeëŠ” Scaleì„ real number(ë°ì´í„°, weight)ì—ì„œ ë²”ìœ„ê°€ ì¢ì€ ìª½ì— ë§ì¶”ëŠ” ê²ƒì´ë‹¤. ì˜ˆë¥¼ ë“¤ì–´ ì•„ë˜ì˜ ê²½ìš°, r_minê°€ r_maxë³´ë‹¤ ì ˆëŒ“ê°’ì´ ë” í¬ê¸° ë•Œë¬¸ì— r_minì— ë§ì¶”ë©´ì„œ q_maxì— ë§ë„ë¡ Scaleì„ êµ¬í•œë‹¤. ì´ ë°©ë²•ì€ TensorFlow, NVIDIA TensorRT, Intel DNNLì—ì„œ ì‚¬ìš©ëœë‹¤ê³  ê°•ì˜ì—ì„œ ì†Œê°œí•œë‹¤.\n\n\n\nReference. MIT-TinyML-lecture5-Quantization-1\n\n\nê·¸ë ‡ë‹¤ë©´ ì™œ Symmetric ì¨ì•¼í• ê¹Œ? Asymmetric ë°©ë²•ê³¼ Symmetric ë°©ë²•ì˜ ì°¨ì´ëŠ” ë­˜ê¹Œ? (feat. Neural Network Distiller) ì•„ë˜ ê·¸ë¦¼ì„ ì°¸ê³ í•˜ë©´ ë˜ì§€ë§Œ, ê°€ì¥ í° ì°¨ì´ë¡œ ë³´ì´ëŠ” ê²ƒì€ Computation vs Compactful quantized rangeë¡œ ì´í•´ê°„ë‹¤.\n\n\n\nReference. MIT-TinyML-lecture5-Quantization-1\n\n\n\n\n1.6 Linear Quantization examples\nê·¸ëŸ¼ Quatization ë°©ë²•ì— ëŒ€í•´ ì•Œì•„ë´¤ìœ¼ë‹ˆ ì´ë¥¼ Full-Connected Layer, Convolution Layerì— ì ìš©í•´ë³´ê³  ì–´ë–¤ íš¨ê³¼ê°€ ìˆëŠ”ì§€ ì•Œì•„ë³´ì.\n\n1.6.1 Full-Connected Layer\nì•„ë˜ì²˜ëŸ¼ ì‹ì„ ì „ê°œí•´ë³´ë©´ ë¯¸ë¦¬ ì—°ì‚°í•  ê³„ì‚°í•  ìˆ˜ ìˆëŠ” í•­ê³¼ N-bit integerë¡œ í‘œí˜„í•  ìˆëŠ” í•­ìœ¼ë¡œ ë‚˜ëˆŒ ìˆ˜ ìˆë‹¤(ì „ê°œí•˜ëŠ” ì´ìœ ëŠ” ì•„ë§ˆ ë¯¸ë¦¬ ê³„ì‚°í•  ìˆ˜ ìˆëŠ” í•­ì„ ì•Œì•„ë³´ê¸° ìœ„í•¨ì´ ì•„ë‹ê¹Œ ì‹¶ë‹¤).\n\\[\nY=WX+b\n\\]\n\\[\n\\downarrow\n\\]\n\\[\nS_Y(q_Y - Z_Y) = S_W(q_W - Z_W) \\cdot S_X(q_X - Z_X) + S_b(q_b - Z_b)\n\\]\n\\[\n\\downarrow \\ Z_w=0\n\\]\n\\[\nS_Y(q_Y - Z_Y) = S_WS_X(q_Wq_X - Z_xq_W) + S_b(q_b - Z_b)\n\\]\n\\[\n\\downarrow \\ Z_b=0, S_b=S_WS_X\n\\]\n\\[\nS_Y(q_Y - Z_Y) = S_WS_X(q_Wq_X - Z_xq_W+q_b)\n\\]\n\\[\n\\downarrow\n\\]\n\\[\nq_Y = \\dfrac{S_WS_X}{S_Y}(q_Wq_X+q_b - Z_Xq_W) + Z_Y\n\\]\n\\[\n\\downarrow \\ q_{bias}=q_b-Z_xq_W\\\\\n\\]\n\\[\nq_Y = \\dfrac{S_WS_X}{S_Y}(q_Wq_X+ q_{bias}) + Z_Y\\\\\n\\]\nê°„ë‹¨íˆ í‘œê¸°í•˜ê¸° ìœ„í•´ \\(Z_W=0, Z_b=0, S_b = S_W S_X\\) ì´ë¼ê³  ê°€ì •í•œë‹¤.\n\n\n\nReference. MIT-TinyML-lecture5-Quantization-1\n\n\n\n\n1.6.2 Convolutional Layer\nConvolution Layerì˜ ê²½ìš°ëŠ” Weightì™€ Xì˜ ê³±ì˜ ê²½ìš°ë¥¼ Convolutionìœ¼ë¡œ ë°”ê¿”ì„œ ìƒê°í•´ë³´ë©´ ëœë‹¤. ê·¸ë„ ê·¸ëŸ´ ê²ƒì´ Convolutionì€ Kernelê³¼ Inputì˜ ê³±ì˜ í•©ìœ¼ë¡œ ì´ë£¨ì–´ì ¸ ìˆê¸° ë•Œë¬¸ì— Full-Connectedì™€ ê±°ì˜ ìœ ì‚¬í•˜ê²Œ ì „ê°œë  ìˆ˜ ìˆì„ ê²ƒì´ë‹¤.\n\n\n\nReference. MIT-TinyML-lecture5-Quantization-1"
  },
  {
    "objectID": "posts/lecs/lec05.html#post-training-quantization-ptq",
    "href": "posts/lecs/lec05.html#post-training-quantization-ptq",
    "title": "ğŸ§‘â€ğŸ« Lecture 5-6",
    "section": "2. Post-training Quantization (PTQ)",
    "text": "2. Post-training Quantization (PTQ)\nê·¸ëŸ¼ ì•ì„œì„œ Quantizaedí•œ Layerë¥¼ Fine tuningí•  ì—†ì„ê¹Œ? â€œHow should we get the optimal linear quantization parameters (S, Z)?â€ ì´ ì§ˆë¬¸ì— ëŒ€í•´ì„œ Weight, Activation, Bias ì„¸ ê°€ì§€ì™€ ê·¸ì— ëŒ€í•˜ì—¬ ë…¼ë¬¸ì—ì„œ ë³´ì—¬ì£¼ëŠ” ê²°ê³¼ê¹Œì§€ ì•Œì•„ë³´ì.\n\n2.1 Weight quantization\nTL;DR. ì´ ê°•ì˜ì—ì„œ ì†Œê°œí•˜ëŠ” Weight quantizationì€ Grandularityì— ë”°ë¼ Whole(Per-Tensor), Channel, ê·¸ë¦¬ê³  Layerë¡œ ë“¤ì–´ê°„ë‹¤.\n\n2.1.1 Granularity\nWeight quantizationì—ì„œ Granularityì— ë”°ë¼ì„œ Per-Tensor, Per-Channel, Group, ê·¸ë¦¬ê³  Generalized í•˜ëŠ” ë°©ë²•ìœ¼ë¡œ í™•ì¥ì‹œì¼œ Shared Micro-exponent(MX) data typeì„ ì°¨ë¡€ë¡œ ë³´ì—¬ì¤€ë‹¤. Scaleì„ ëª‡ ê°œë‚˜ ë‘˜ ê²ƒì´ëƒ, ê·¸ Scaleì„ ì ìš©í•˜ëŠ” ë²”ìœ„ë¥¼ ì–´ë–»ê²Œ ë‘˜ ê²ƒì´ëƒ, ê·¸ë¦¬ê³  Scaleì„ ì–¼ë§ˆë‚˜ ë””í…Œì¼í•˜ê²Œ(e.g.Â floating-point)í•  ê²ƒì´ëƒì— ì´ˆì ì„ ë‘”ë‹¤.\n\n\n\nReference. MIT-TinyML-lecture5-Quantization-2\n\n\nì²« ë²ˆì§¸ëŠ” Per-Tensor Quantization íŠ¹ë³„í•˜ê²Œ ì„¤ëª…í•  ê²ƒ ì—†ì´ ì´ì „ê¹Œì§€ ì„¤ëª…í–ˆë˜ í•˜ë‚˜ì˜ Scaleì„ ì‚¬ìš©í•˜ëŠ” Linear Quantizationì´ë¼ê³  ìƒê°í•˜ë©´ ë˜ê² ë‹¤. íŠ¹ì§•ìœ¼ë¡œëŠ” Large modelì— ëŒ€í•´ì„œëŠ” ì„±ëŠ¥ì´ ê´œì°®ì§€ë§Œ ì‘ì€ ëª¨ë¸ë¡œ ë–¨ì–´ì§€ë©´ ì„±ëŠ¥ì´ ê¸‰ê²©í•˜ê²Œ ë–¨ì–´ì§„ë‹¤ê³  ì„¤ëª…í•œë‹¤. Channelë³„ë¡œ weight ë²”ì£¼ê°€ ë„“ì€ ê²½ìš°ë‚˜ outlier weightê°€ ìˆëŠ” ê²½ìš° quantization ì´í›„ì— ì„±ëŠ¥ì´ í•˜ë½í–ˆë‹¤ê³  ë§í•œë‹¤.\n\n\n\nReference. MIT-TinyML-lecture5-Quantization-2\n\n\nê·¸ë˜ì„œ ê·¸ í•´ê²°ë°©ì•ˆìœ¼ë¡œ ë‚˜ì˜¤ëŠ” ê²ƒì´ ë‘ ë²ˆì§¸ ë°©ë²•ì¸ Per-Channel Quantizationì´ë‹¤. ìœ„ ì˜ˆì œì—ì„œ ë³´ë©´ Channel ë§ˆë‹¤ ìµœëŒ€ê°’ê³¼ ê°ê°ì— ë§ëŠ” Scaleì„ ë”°ë¡œ ê°€ì§€ëŠ” ê²ƒì„ ë³¼ ìˆ˜ ìˆë‹¤. ê·¸ë¦¬ê³  ì ìš©í•œ ê²°ê³¼ì¸ ì•„ë˜ ê·¸ë¦¼ì„ ë³´ë©´ Per-Channelê³¼ Per-Tensorë¥¼ ë¹„êµí•´ë³´ë©´ Per-Channelì´ ê¸°ì¡´ì— floating point weightì™€ì˜ ì°¨ì´ê°€ ë” ì ë‹¤. í•˜ì§€ë§Œ, ë§Œì•½ í•˜ë“œì›¨ì–´ì—ì„œ Per-Channel Quantizationì„ ì§€ì›í•˜ì§€ ì•ŠëŠ”ë‹¤ë©´ ë¶ˆí•„ìš”í•œ ì—°ì‚°ì„ ì¶”ê°€ë¡œ í•´ì•¼í•˜ê¸° ë•Œë¬¸ì— ì´ëŠ” ì í•©í•œ ë°©ë²•ì´ ë  ìˆ˜ ì—†ë‹¤ëŠ” ì ë„ ê³ ë ¤í•´ì•¼í•  ê²ƒì´ë‹¤(ì´ëŠ” ì´ì „ Tiny Engineì— ëŒ€í•œ ê¸€ì—ì„œ Channelë‚´ì— ìºì‹±ì„ ì´ìš©í•œ ìµœì í™”ì™€ ì—°ê´€ì´ ìˆë‹¤). ê·¸ëŸ¼ ë˜ ë‹¤ë¥¸ ë°©ë²•ì€ ì—†ì„ê¹Œ?\n\n\n\nReference. MIT-TinyML-lecture5-Quantization-2\n\n\nì„¸ ë²ˆì§¸ ë°©ë²•ì€ Group Quantizationìœ¼ë¡œ ì†Œê°œí•˜ëŠ” Per-vector Scaled Quantizationì™€ Shared Micro-exponent(MX) data type ì´ë‹¤. Per-vector Scaled Quantizationì€ 2023ë…„ë„ ê°•ì˜ë¶€í„° ì†Œê°œí•˜ëŠ”ë°, ì´ ë°©ë²•ì€ Scale factorë¥¼ ê·¸ë£¹ë³„ë¡œ í•˜ë‚˜, Per-Tensorë¡œ í•˜ë‚˜ë¡œ ë‘ê°œë¥¼ ë‘ëŠ” ë°©ë²•ì´ë‹¤. ì•„ë˜ì˜ ê·¸ë¦¼ì„ ë³´ë©´,\n\n\n\nReference. MIT-TinyML-lecture5-Quantization-2\n\n\n\\[\nr=S(q-Z) \\rightarrow r=\\gamma \\cdot S_{q}(q-Z)\n\\]\n\\(S_q\\) ë¡œ vectorë³„ ìŠ¤ì¼€ì¼ë§ì„ í•˜ë‚˜, \\(\\gamma\\) ë¡œ Tensorì— ìŠ¤ì¼€ì¼ë§ì„ í•˜ë©° ê°ë§ˆëŠ” floating pointë¡œ í•˜ëŠ” ê²ƒì„ ë³¼ ìˆ˜ìˆë‹¤. ì•„ë¬´ë˜ë„ vectorë‹¨ìœ„ë¡œ ìŠ¤ì¼€ì¼ë§ì„ í•˜ê²Œë˜ë©´ channelê³¼ ë¹„êµí•´ì„œ í•˜ë“œì›¨ì–´ í”Œë«í¼ì— ë§ê²Œ accuracyì˜ trade-offë¥¼ ì¡°ì ˆí•˜ê¸° ë” ìˆ˜ì›”í•  ê²ƒìœ¼ë¡œ ë³´ì¸ë‹¤.\nì—¬ê¸°ì„œ ê°•ì˜ëŠ” ì§€í‘œì¸ Memory Overheadë¡œ â€œEffective Bit Widthâ€ë¥¼ ì†Œê°œí•œë‹¤. ì´ëŠ” Microsoftì—ì„œ ì œê³µí•˜ëŠ” Quantization Approach MX4, MX6, MX9ê³¼ ì—°ê²°ë¼ ìˆëŠ”ë°, ì´ ë°ì´í„°íƒ€ì…ì€ ì¡°ê¸ˆ ì´í›„ì— ë” ìì„¸íˆ ì„¤ëª…í•  ê²ƒì´ë‹¤. Effective Bit Width? ì˜ˆì‹œ í•˜ë‚˜ë¥¼ ë“¤ì–´ ì´í•´í•´ë³´ì. ë§Œì•½ 4-bit Quatizationì„ 4-bit per-vector scaleì„ 16 elements(4ê°œì˜ weightê°€ ê°ê° 4bitë¥¼ ê°€ì§„ë‹¤ê³  ìƒê°í•˜ë©´ 16 elementë¡œ ê³„ì‚°ëœë‹¤ ìœ ì¶”í•  ìˆë‹¤) ë¼ë©´, Effective Bit WidthëŠ” 4(Scale bit) + 4(Vector Scale bit) / 16(Vector Size) = 4.25ê°€ ëœë‹¤. Elementë‹¹ Scale bitë¼ê³  ê°„ë‹¨í•˜ê²Œ ìƒê°í•  ìˆ˜ë„ ìˆì„ ë“¯ ì‹¶ë‹¤.\në§ˆì§€ë§‰ Per-vector Scaled Quantizationì„ ì´í•´í•˜ë‹¤ë³´ë©´ ì´ì „ì— Per-Tensor, Per-Channelë„ ê·¸ë£¹ìœ¼ë¡œ ì–¼ë§ˆë§Œí¼ ë¬¶ëŠ” ì°¨ì´ê°€ ìˆê³ , ì´ëŠ” ì´ë“¤ì„ ì¼ë°˜í™”í•  ìˆ˜ ìˆì–´ ë³´ì¸ë‹¤. ê°•ì˜ì—ì„œ ë°”ë¡œ ë‹¤ìŒì— ì†Œê°œí•˜ëŠ” ë°©ë²•ì´ ë°”ë¡œ Multi-level scaling schemeì´ë‹¤. Per-Channel Quantizationì™€ Per-Vector Quantization(VSQ, Vector-Scale Quantization)ë¶€í„° ë´ë³´ì.\n\n\n\nReference. With Shared Microexponents, A Little Shifting Goes a Long Way [Bita Rouhani et al.]\n\n\nPer-Channel QuantizationëŠ” Scale factorê°€ í•˜ë‚˜ë¡œ Effective Bit WidthëŠ” 4ê°€ ëœë‹¤. ê·¸ë¦¬ê³  VSQëŠ” ì´ì „ì— ê³„ì‚°í–ˆ ë“¯ 4.25ê°€ ë  ê²ƒì´ë‹¤(ì°¸ê³ ë¡œ Per Channelë¡œ ì ìš©ë˜ëŠ” Scaleì˜ ê²½ìš° elementì˜ ìˆ˜ê°€ ë§ì•„ì„œ ê·¸ëŸ°ì§€ ë”°ë¡œ Effective Bit Widthë¡œ ê³„ì‚°í•˜ì§€ëŠ” ì•ŠëŠ”ë‹¤). VSQê¹Œì§€ ë³´ë©´ì„œ Effective Bit WidthëŠ”,\nEffective Bit Width = Scale bit + Group 0 Scale bit / Group 0 Size +...\ne.g. VSQ Data type int4 = Scale bit (4) + Group 0 Scale bit(4) / Group 0 Size(16) = 4.25\nì´ë ‡ê²Œ ê³„ì‚°í•  ìˆ˜ ìˆë‹¤. ê·¸ë¦¬ê³ , MX4, MX6, MX9ê°€ ë‚˜ì˜¨ë‹¤. ì°¸ê³ ë¡œ SëŠ” Sign bit, Mì€ Mantissa bit, EëŠ” Exponent bitë¥¼ ì˜ë¯¸í•œë‹¤(Mantissaë‚˜ Exponentì— ëŒ€í•œ ìì„¸í•œ ë‚´ìš©ì€ floating point vs fixed point ê¸€ì„ ì°¸ê³ í•˜ì). ì•„ë˜ëŠ” Microsoftì—ì„œ ì œê³µí•˜ëŠ” Quantization Approach MX4, MX6, MX9ì— ëŒ€í•œ í‘œì´ë‹¤.\n\n\n\nReference. MIT-TinyML-lecture5-Quantization-1\n\n\n\n\n2.1.2 Weight Equalization\nì—¬ê¸°ê¹Œì§€ Weight Quatizationì—ì„œ ê·¸ë£¹ìœ¼ë¡œ ì–¼ë§ˆë§Œí¼ ë¬¶ëŠ”ì§€ì— ë”°ë¼(ê°•ì˜ì—ì„œëŠ” Granularity) Quatizationì„ í•˜ëŠ” ì—¬ëŸ¬ ë°©ë²•ì„ ì†Œê°œí–ˆë‹¤. ë‹¤ìŒìœ¼ë¡œ ì†Œê°œ í•  ë°©ë²•ì€ Weight Equalizationì´ë‹¤. 2022ë…„ì— ì†Œê°œí•´ì¤€ ë‚´ìš©ì¸ë°, ì´ëŠ” ië²ˆì§¸ layerì˜ output channelë¥¼ scaling down í•˜ë©´ì„œ i+1ë²ˆì§¸ layerì˜ input channelì„ scaling up í•´ì„œ Scaleë¡œ ì¸í•´ Quantization ì „í›„ë¡œ ìƒê¸°ëŠ” Layerê°„ ì°¨ì´ë¥¼ ì¤„ì´ëŠ” ë°©ë²•ì´ë‹¤.\n\n\n\nReference. Data-Free Quantization Through Weight Equalization and Bias Correction [Markus et al., ICCV 2019]\n\n\nì˜ˆë¥¼ ë“¤ì–´ ìœ„ì— ê·¸ë¦¼ì²˜ëŸ¼ Layer iì˜ output channelê³¼ Layer i+1ì˜ input channelì´ ìˆë‹¤. ì—¬ê¸°ì„œ ì‹ì„ ì „ê°œí•˜ë©´ ì•„ë˜ì™€ ê°™ì€ë°,\n\\[\n\\begin{aligned}\ny^{(i+1)}&=f(W^{(i+1)}x^{(i+1)}+b^{(i+1)}) \\\\\n         &=f(W^{(i+1)} \\cdot f(W^{(i)}x^{(i)}+b^{(i)}) +b^{(i+1)}) \\\\\n         &=f(W^{(i+1)}S \\cdot f(S^{-1}(W^{(i)}x^{(i)}+S^{-1}b^{(i)})) +b^{(i+1)})\n\\end{aligned}\n\\]\nwhere \\(S = diag(s)\\) , \\(s_j\\) is the weight equalization scale factor of output channel \\(j\\)\nì—¬ê¸°ì„œ Scale(S)ê°€ i+1ë²ˆì§¸ layerì˜ weightì—, ië²ˆì§¸ weightì— 1/S ë¡œ Scaleë  ë–„ ê¸°ì¡´ì— Scale í•˜ì§€ ì•Šì€ ì‹ê³¼ ìœ ì‚¬í•˜ê²Œ ìœ ì§€í•  ìˆëŠ” ê²ƒì„ ë³¼ ìˆ˜ ìˆë‹¤. ì¦‰,\n\\[\nr^{(i)}_{oc_j} / s = r^{(i+1)}_{ic_j} \\cdot s\n\\]\n\\[\ns_j = \\dfrac{1}{r^{(i+1)}_{ic=j}}\\sqrt{r_{oc=j}^{(i)}\\cdot r_{ic=j}^{(i+1)}}\n\\]\n\\[\nr^{(i)}_{oc_j} = r^{(i)}_{oc_j} / s = \\sqrt{r_{oc=j}^{(i)}\\cdot r_{ic=j}^{(i+1)}}\n\\]\n\\[\nr^{(i)}_{ic_j} =r^{(i)}_{ic_j} \\cdot s = \\sqrt{r_{oc=j}^{(i)}\\cdot r_{ic=j}^{(i+1)}}\n\\]\nì´ë ‡ê²Œ í•˜ë©´ ië²ˆì§¸ layerì˜ output channelê³¼ i+1ë²ˆì§¸ layerì˜ input channelì˜ Scaleì„ ê°ê° \\(S\\) ì™€ \\(1/S\\) ë¡œí•˜ë©° weightê°„ì˜ ê²©ì°¨ë¥¼ ì¤„ì¼ ìˆ˜ ìˆë‹¤.\n\n\n2.1.3 Adaptive rounding\n\n\n\nReference. MIT-TinyML-lecture5-Quantization-1\n\n\në§ˆì§€ë§‰ ì†Œê°œí•˜ëŠ” ë°©ë²•ì€ Adaptive rounding ì´ë‹¤. ë°˜ì˜¬ë¦¼ì€ Round-to-nearestìœ¼ë¡œ ë¶ˆë¦¬ëŠ” ì¼ë°˜ì ì¸ ë°˜ì˜¬ë¦¼ì„ ìƒê°í•  ìˆ˜ ìˆê³ , í•˜ë‚˜ì˜ ê¸°ì¤€ì„ ê°€ì§€ê³  ë°˜ì˜¬ë¦¼ì„ í•˜ëŠ” Adaptive Roundë¥¼ ìƒê°í•  í•  ìˆ˜ ìˆë‹¤. ê°•ì˜ì—ì„œëŠ” Round-to-nearestê°€ ìµœì ì˜ ë°©ë²•ì´ ë˜ì§€ ì•ŠëŠ”ë‹¤ê³  ë§í•˜ë©°, Adaptive roundë¡œ weightì— 0ë¶€í„° 1 ì‚¬ì´ì˜ ê°’ì„ ë”í•´ ìˆ˜ì‹ì²˜ëŸ¼ \\(\\tilde{w} = \\lfloor\\lfloor  w\\rfloor + \\delta\\rceil, \\delta \\in [0, 1]\\) ìµœì ì˜ Optimalí•œ ë°˜ì˜¬ë¦¼ ê°’ì„ êµ¬í•œë‹¤. $$\n\\[\\begin{aligned}\n&argmin_V\\lvert\\lvert Wx-\\tilde Wx\\lvert\\lvert ^2_F + \\lambda f_{reg}(V) \\\\\n\n\\rightarrow & argmin_V\\lvert\\lvert Wx-\\lfloor\\lfloor W \\rfloor + h(V) \\rceil x\\lvert\\lvert ^2_F + \\lambda f_{reg}(V)\n\\end{aligned}\\]\n$$ ### 2.2 Activation quantization ë‘ ë²ˆì§¸ë¡œ Activation quantizationì´ ìˆë‹¤. ëª¨ë¸ê²°ê³¼ë¡œ ë‚˜ì˜¤ëŠ” ê²°ê³¼ë¥¼ ì§ì ‘ì ìœ¼ë¡œ ê²°ì •í•˜ëŠ” Activation Quatizationì—ì„œëŠ” ë‘ ê°€ì§€ë¥¼ ê³ ë ¤í•œ ë°©ë²•ì„ ì†Œê°œí•œë‹¤. í•˜ë‚˜ëŠ” Activation ë ˆì´ì–´ì—ì„œ ê²°ê³¼ê°’ì„ Smoothingí•œ ë¶„í¬ë¥¼ ê°€ì§€ê²Œ í•˜ê¸° ìœ„í•´ Exponential Moving Average(EMA)ë¥¼ ì‚¬ìš©í•˜ëŠ” ë°©ë²•ì´ê³ , ë‹¤ë¥¸ í•˜ë‚˜ëŠ” ë‹¤ì–‘í•œ ì…ë ¥ê°’ì„ ê³ ë ¤í•´ batch samplesì„ FP32 ëª¨ë¸ê³¼ calibrationí•˜ëŠ” ë°©ë²•ì´ë‹¤.\nExponential Moving Average (EMA)ì€ ì•„ë˜ ì‹ì—ì„œ \\(\\alpha\\) ë¥¼ êµ¬í•˜ëŠ” ë°©ë²•ì´ë‹¤. \\[\n\\hat r^{(t)}_{max, min} = \\alpha r^{(t)}_{max, min} + (1-\\alpha) \\hat r^{(t)}_{max, min}  \n\\] Calibrationì˜ ì»¨ì…‰ì€ ë§ì€ inputì˜ min/max í‰ê· ì„ ì´ìš©í•˜ìëŠ” ê²ƒì´ë‹¤. ê·¸ë˜ì„œ trained FP32 modelê³¼ sample batchë¥¼ ê°€ì§€ê³  quantizedí•œ ëª¨ë¸ì˜ ê²°ê³¼ì™€ calibrationì„ ëŒë¦¬ë©´ì„œ ê·¸ ì°¨ì´ë¥¼ ìµœì†Œí™” ì‹œí‚¤ëŠ”ë°, ì—¬ê¸°ì— ì´ìš©í•˜ëŠ” ì§€í‘œëŠ” loss of informationì™€ Newton-Raphson methodë¥¼ ì‚¬ìš©í•œ Mean Square Error(MSE)ê°€ ìˆë‹¤. \\[\nMSE = \\underset{\\lvert r \\lvert_{max}}{min}\\ \\mathbb{E}[(X-Q(X))^2]\n\\] \\[\nKL\\ divergence=D_{KL}(P\\lvert\\lvert Q) = \\sum_i^N P(x_i)log\\dfrac{P(x_i)}{Q(x_i)}\n\\] ### 2.3 Quanization Bias Correction\në§ˆì§€ë§‰ìœ¼ë¡œ Quatizationìœ¼ë¡œ biased errorë¥¼ ì¡ëŠ”ë‹¤ëŠ” ê²ƒì„ ì†Œê°œí•œë‹¤. \\(\\epsilon = Q(W)-W\\) ì´ë¼ê³  ë‘ê³  ì•„ë˜ì²˜ëŸ¼ ì‹ì´ ì „ê°œì‹œí‚¤ë©´ ë§ˆì§€ë§‰ í•­ì—ì„œ ë³´ì´ëŠ” \\(-\\epsilon\\mathbb{E}[x]\\) ë¶€ë¶„ì´ biasë¥¼ quatizationì„ í•  ë•Œ ì œê±° ëœë‹¤ê³  í•œë‹¤(ì´ ë¶€ë¶„ì€ 2023ë…„ì—ëŠ” ì†Œê°œí•˜ì§„ ì•ŠëŠ”ë°, ë‹¹ì—°í•œ ê²ƒì´ì–´ì„œ ì•ˆí•˜ëŠ”ì§€, í˜¹ì€ ì˜í–¥ì´ í¬ì§€ ì•Šì•„ì„œ ê·¸ëŸ°ì§€ëŠ” ëª¨ë¥´ê² ë‹¤. Bias Quatizationì´í›„ì— MobileNetV2ì—ì„œ í•œ ë ˆì´ì–´ì˜ outputì„ ë³´ë©´ ì–´ëŠì •ë„ ì œê±°ë˜ëŠ” ê²ƒì²˜ëŸ¼ ë³´ì¸ë‹¤). \\[\n\\begin{aligned}\n\\mathbb{E}[y] &= \\mathbb{E}[Wx] + \\mathbb{E}[\\epsilon x] - \\mathbb{E}[\\epsilon x],\\ \\mathbb{E}[Q(W)x] = \\mathbb{E}[Wx] + \\mathbb{E}[\\epsilon x] \\\\\n\\mathbb{E}[y] &= \\mathbb{E}[Q(W)x] - \\epsilon\\mathbb{E}[x]\n\\end{aligned}\n\\]\n\n\n\nReference. MIT-TinyML-lecture5-Quantization-2\n\n\n\n\n\n2.4 Post-Training INT8 Linear Quantization Result\nì•ì„  Post-Training Quantizationì„ ì ìš©í•œ ê²°ê³¼ë¥¼ ë³´ì—¬ì¤€ë‹¤. ì´ë¯¸ì§€ê³„ì—´ ëª¨ë¸ì„ ëª¨ë‘ ì‚¬ìš©í–ˆìœ¼ë©°, ì„±ëŠ¥í•˜ë½í­ì€ ì§€í‘œë¡œ ë³´ì—¬ì¤€ë‹¤. ë¹„êµì  í° ëª¨ë¸ë“¤ì˜ ê²½ìš° ì¤€ìˆ˜í•œ ì„±ëŠ¥ì„ ë³´ì—¬ì£¼ì§€ë§Œ MobileNetV1, V2ì™€ ê°™ì€ ì‘ì€ ëª¨ë¸ì€ ìƒê°ë³´ë‹¤ Quantizationìœ¼ë¡œ ë–¨ì–´ì§€ëŠ” ì„±ëŠ¥í­(-11.8%, -2.1%) ì´ í° ê²ƒì„ ë³¼ ìˆ˜ ìˆë‹¤. ê·¸ëŸ¼ ì‘ì€ í¬ê¸°ì˜ ëª¨ë¸ë“¤ì€ ì–´ë–»ê²Œ Training í•´ì•¼í• ê¹Œ?\n\n\n\nReference. MIT-TinyML-lecture5-Quantization-2"
  },
  {
    "objectID": "posts/lecs/lec05.html#quantization-aware-trainingqat",
    "href": "posts/lecs/lec05.html#quantization-aware-trainingqat",
    "title": "ğŸ§‘â€ğŸ« Lecture 5-6",
    "section": "3. Quantization-Aware Training(QAT)",
    "text": "3. Quantization-Aware Training(QAT)\n\n3.1 Quantization-Aware Training\n\n\n\nReference. MIT-TinyML-lecture06-Quantization-2\n\n\n\nUsually, fine-tuning a pre-trained floating point model provides better accuracy than training from scratch.\n\nì´ì „ì— K-mean Quantizationì—ì„œ Fine-tuningë•Œ Centroidì— gradientë¥¼ ë°˜ì˜í–ˆì—ˆë‹¤. Quantization-Aware Trainingì€ ì´ì™€ ìœ ì‚¬í•˜ê²Œ Quantization - Reconstructionì„ í†µí•´ ë§Œë“¤ì–´ì§„ Weightë¡œ Trainingì„ í•˜ëŠ” ë°©ë²•ì„ ë§í•œë‹¤. ì˜ˆì‹œë¥¼ ë“¤ì–´ì„œ ìì„¸íˆ ì‚´í´ë³´ì.\n\n\n\nReference. MIT-TinyML-lecture06-Quantization-2\n\n\n\nA full precision copy of the weights W is maintained throughout the training.\nThe small gradients are accumulated without loss of precision\nOnce the model is trained, only the quantized weights are used for inference\n\nìœ„ ê·¸ë¦¼ì—ì„œ Layer Nì´ ë³´ì¸ë‹¤. ì´ Layer Nì€ weightsë¥¼ íŒŒë¼ë¯¸í„°ë¡œ ê°€ì§€ì§€ë§Œ, ì‹¤ì œë¡œ Training ê³¼ì •ì—ì„œ ì“°ì´ëŠ” weightëŠ” â€œweight quantizationâ€ì„ í†µí•´ Quantization - Reconstructionì„ í†µí•´ ë§Œë“¤ì–´ì§„ Weightë¥¼ ê°€ì§€ê³  í›ˆë ¨ì„ í•  ê²ƒì´ë‹¤.\n\n\n3.2 Straight-Through Estimator(STE)\n\n\n\nReference. MIT-TinyML-lecture06-Quantization-2\n\n\nê·¸ëŸ¼ í›ˆë ¨ì—ì„œ gradientëŠ” ì–´ë–»ê²Œ ì „ë‹¬í•  ìˆ˜ ìˆì„ê¹Œ? Quantizationì˜ ê°œë…ìƒ, weight quantizationì—ì„œ weightë¡œ ë„˜ì–´ê°€ëŠ” gradientëŠ” ì—†ì„ ìˆ˜ ë°–ì— ì—†ë‹¤. ê·¸ë ‡ê²Œ ë˜ë©´ ì‚¬ì‹¤ìƒ weightë¡œ back propagationì´ ë  ìˆ˜ ì—†ê²Œ ë˜ê³ , ê·¸ë˜ì„œ ì†Œê°œí•˜ëŠ” ê°œë…ì´ Straight-Through Estimator(STE) ì…ë‹ˆë‹¤. ë§ì´ ê±°ì°½í•´ì„œ ê·¸ë ‡ì§€, Q(W)ì—ì„œ ë°›ì€ gradientë¥¼ ê·¸ëŒ€ë¡œ weights ë¡œ ë„˜ê²¨ì£¼ëŠ” ë°©ì‹ì´ë‹¤.\n\nQuantization is discrete-valued, and thus the derivative is 0 almost everywhere â†’ NN will learn nothing!\nStraight-Through Estimator(STE) simply passes the gradients through the quantization as if it had been the identity function.\n\\[\n  g_W = \\dfrac{\\partial L}{\\partial  W} = \\dfrac{\\partial L}{\\partial  Q(W)}\n  \\]\n\n\n\n\nReference. MIT-TinyML-lecture06-Quantization-2\n\n\n\nReference\n\nNeural Networks for Machine Learning [HintonÂ et al., Coursera Video Lecture, 2012]\nEstimating or Propagating Gradients Through Stochastic Neurons for Conditional Computation [Bengio, arXiv 2013]\n\n\nì´ í›ˆë ¨ì˜ ê²°ê³¼ê°€ ê¶ê¸ˆí•˜ì‹œë‹¤ë©´ ì´ ë…¼ë¬¸ì„ ì°¸ê³ í•˜ì. ì°¸ê³ ë¡œ ë…¼ë¬¸ì—ì„œëŠ” MobileNetV1, V2 ê·¸ë¦¬ê³  NASNet-Mobileì„ ì´ìš©í•´ Post-Training Quantizationê³¼ Quantization-Aware Trainingì„ ë¹„êµí•˜ê³  ìˆë‹¤."
  },
  {
    "objectID": "posts/lecs/lec05.html#binary-and-ternary-quantization",
    "href": "posts/lecs/lec05.html#binary-and-ternary-quantization",
    "title": "ğŸ§‘â€ğŸ« Lecture 5-6",
    "section": "4. Binary and Ternary Quantization",
    "text": "4. Binary and Ternary Quantization\nì, ê·¸ëŸ¼ Quantizationì„ ê¶ê·¹ì ìœ¼ë¡œ 2bitë¡œ í•  ìˆ˜ëŠ” ì—†ì„ê¹Œ? ë°”ë¡œ Binary(1, -1)ê³¼ Tenary(1, 0, -1) ì´ë‹¤.\n\nCan we push the quantization precision to 1 bit?\n\n\n\nReference. MIT-TinyML-lecture06-Quantization-2\n\n\nReference\n\nBinaryConnect: Training Deep Neural Networks with Binary Weights during Propagations [CourbariauxÂ et al., NeurIPS 2015]\nXNOR-Net: ImageNet Classification using Binary Convolutional Neural Networks [RastegariÂ et al., ECCV 2016]\n\n\në¨¼ì € Weightë¥¼ 2bitë¡œ Quantizationì„ í•˜ê²Œ ë˜ë©´, ë©”ëª¨ë¦¬ì—ì„œëŠ” 32bitë¥¼ 1bitë¡œ ì¤„ì´ë‹ˆ 32ë°°ë‚˜ ì¤„ì¼ ìˆ˜ ìˆê³ , Computationë„ (8x5)+(-3x2)+(5x0)+(-1x1)ì—ì„œ 5-2+0-1 ë¡œ ì ˆë°˜ì„ ì¤„ì¼ ìˆ˜ ìˆë‹¤.\n\n4.1 Binarization: Deterministic Binarization\nê·¸ëŸ¼ Binarizationì—ì„œ +1ê³¼ -1ì„ ì–´ë–¤ ê¸°ì¤€ìœ¼ë¡œ í•´ì•¼í• ê¹Œ? ê°€ì¥ ì‰¬ìš´ ë°©ë²•ì€ threholdë¥¼ ê¸°ì¤€ìœ¼ë¡œ +-1ë¡œ ë‚˜ëˆ„ëŠ” ê²ƒì´ë‹¤.\nDirectly computes the bit value base on a threshold, usually 0 resulting in a sign function.\n\\[\nq = sign(r) = \\begin{dcases}\n+1, &r \\geq 0 \\\\\n-1, &r &lt; 0\n\\end{dcases}\n\\]\n\n\n4.2 Binarization: Stochastic Binarization\në‹¤ë¥¸ ë°©ë²•ìœ¼ë¡œëŠ” outputì—ì„œ hard-sigmoid functionì„ ê±°ì³ì„œ ë‚˜ì˜¨ ê°’ë§Œí¼ í™•ë¥ ì ìœ¼ë¡œ +-1ì´ ë‚˜ì˜¤ë„ë¡ í•˜ëŠ” ê²ƒì´ë‹¤. í•˜ì§€ë§Œ ì´ ë°©ë²•ì€ ë¬´ì‘ìœ„ë¡œ ë¹„íŠ¸ë¥¼ ìƒì„±í•˜ëŠ” í•˜ë“œì›¨ì–´ë¥¼ í•˜ëŠ” ê²ƒì´ ì–´ë µê¸° ë•Œë¬¸ì— ì‚¬ìš©í•˜ì§„ ì•ŠëŠ”ë‹¤ê³  ì–¸ê¸‰í•œë‹¤.\n\nUse global statistics or the value of input data to determine the probability of being -1 or +1\nIn Binary Connect(BC), probability is determined by hard sigmoid function \\(\\sigma(r)\\)\n\\[\n  q=\\begin{dcases}\n  +1, &\\text{with probability } p=\\sigma(r)\\\\\n  -1, & 1-p\n  \\end{dcases}\n  \\\\\n  where\\ \\sigma(r)=min(max(\\dfrac{r+1}{2}, 0), 1)\n  \\]\n\n\n\nReference. MIT-TinyML-lecture06-Quantization-2\n\n\nHarder to implement as it requires the hardware to generate random bits when quantizing.\n\n\n\n4.3 Binarization: Use Scale\nì•ì„  ë°©ë²•ì„ ì´ìš©í•´ì„œ ImageNet Top-1 ì„ í‰ê°€í•´ë³´ë©´ Quantizationì´í›„ -21.2%ë‚˜ ì„±ëŠ¥ì´ í•˜ë½í•˜ëŠ” ê±¸ ë³¼ ìˆ˜ ìˆë‹¤. â€œì–´ë–»ê²Œ ë³´ì™„í•  ìˆ˜ ìˆì„ê¹Œ?â€ í•œ ê²ƒì´ linear qunatizationì—ì„œ ì‚¬ìš©í–ˆë˜ Scale ê°œë…ì´ë‹¤.\n\nUsing Scale, Minimizing Quantization Error in Binarization\n\n\n\nReference. MIT-TinyML-lecture06-Quantization-2\n\n\n\nì—¬ê¸°ì„œ Scaleì€ \\(\\dfrac{1}{n}\\lvert\\lvert W \\lvert\\lvert_1\\) ë¡œ ê³„ì‚°í•  ìˆ˜ ìˆê³ , ì„±ëŠ¥ì€ í•˜ë½ì´ ê±°ì˜ ì—†ëŠ” ê²ƒë„ í™•ì¸í•  ìˆ˜ ìˆë‹¤. ì™œ \\(\\dfrac{1}{n}\\lvert\\lvert W \\lvert\\lvert_1\\)ì¸ì§€ëŠ” ì•„ë˜ ì¦ëª…ê³¼ì •ì„ ì°¸ê³ í•˜ì!\n\nWhy \\(\\alpha\\) is \\(\\dfrac{1}{n}\\lvert\\lvert W \\lvert\\lvert_1\\)?\n\\[\n  \\begin{aligned}\n  &J(B, \\alpha)=\\lvert\\lvert W-\\alpha B\\lvert\\lvert^2 \\\\\n  &\\alpha^*, B^*= \\underset{\\alpha, B}{argmin}\\ J(B, \\alpha) \\\\\n  &J(B,\\alpha) = \\alpha^2B^TB-2\\alpha W^T B + W^TW\\ since\\ B \\in \\{+1, -1\\}^n \\\\\n  &B^TB=n(constant), W^TW= constant(a \\ known\\ variable) \\\\\n  &J(B,\\alpha) = \\alpha^2n-2\\alpha W^T B + C \\\\\n  &B^* = \\underset{B}{argmax} \\{W^T B\\}\\ s.t.\\ B\\in \\{+1,-1 \\}^n \\\\\n  &\\alpha^*=\\dfrac{W^TB^*}{n} \\\\\n  &\\alpha^*=\\dfrac{W^Tsign(W)}{n} = \\dfrac{\\lvert W_i \\lvert}{n} = \\dfrac{1}{n}\\lvert\\lvert W\\lvert\\lvert_{l1}\n  \\end{aligned}\n  \\]\n\nReference. XNOR-Net: ImageNet Classification using Binary Convolutional Neural Networks [RastegariÂ et al., ECCV 2016]\nB*ëŠ” J(B,\\(\\alpha\\))ì—ì„œ ìµœì†Ÿê°’ì„ êµ¬í•´ì•¼í•˜ë¯€ë¡œ \\(W^T\\)B ê°€ ìµœëŒ€ì—¬ì•¼í•˜ê³  ê·¸ëŸ¬ê¸° ìœ„í•´ì„œëŠ” Wê°€ ì–‘ìˆ˜ì¼ë•ŒëŠ” Bë„ ì–‘ìˆ˜, Wê°€ ìŒìˆ˜ì¼ ë•ŒëŠ” Bë„ ìŒìˆ˜ì—¬ì•¼ \\(W^TB=\\sum\\lvert W \\lvert\\) ì´ ë˜ë©´ì„œ ìµœëŒ“ê°’ì´ ë  ìˆ˜ ìˆë‹¤.\n\n\n\n\n4.4 Binarization: Activation\nê·¸ëŸ¼ Activationê¹Œì§€ Quantizationì„ í•´ë´…ì‹œë‹¤.\n4.4.1 Activation\n\n\n\nUntitled\n\n\nì—¬ê¸°ì„œ ì¡°ê¸ˆ ë” ì—°ì‚°ì„ ìµœì í™” í•  ìˆ˜ ìˆì–´ë³´ì´ëŠ” ê²ƒì´ Matrix Muliplicationì´ XOR ì—°ì‚°ê³¼ ë¹„ìŠ·í•˜ê²Œ ë³´ì¸ë‹¤.\n4.4.2 XNOR bit count\n\n\n\nReference. MIT-TinyML-lecture06-Quantization-2\n\n\n\n\\(y_i=-n+ popcount(W_i\\ xnor\\ x) &lt;&lt; 1\\) â†’ popcount returns the number of 1\n\nê·¸ë˜ì„œ popcountê³¼ XNORì„ ì´ìš©í•´ì„œ Computationì—ì„œ ì¢€ ë” ìµœì í™”ë¥¼ ì§„í–‰í•©ë‹ˆë‹¤. ì´ë ‡ê²Œ ìµœì í™”ë¥¼ ì§„í–‰í•˜ê²Œ ë˜ë©´, ë©”ëª¨ë¦¬ëŠ” 32ë°°, Computationì€ 58ë°°ê°€ëŸ‰ ì¤„ì–´ë“¤ ìˆ˜ ìˆë‹¤ê³  ë§í•œë‹¤.\n\n\n\nReference. MIT-TinyML-lecture06-Quantization-2\n\n\nì´ë ‡ê²Œ Weight, Scale factor, Activation, ê·¸ë¦¬ê³  XNOR-Bitcout ê¹Œì§€. ì´ ë„¤ ê°€ì§€ ë‹¨ê³„ë¡œ Binary Quantizationì„ ë‚˜ëˆˆë‹¤. ë‹¤ìŒìœ¼ë¡œëŠ” Ternary Quantizationì€ ì•Œì•„ë³´ì.\n\n\n\nReference. XNOR-Net: ImageNet Classification using Binary Convolutional Neural Networks [RastegariÂ et al., ECCV 2016]\n\n\n\n\nBinarizing Input ì˜ ê²½ìš°ëŠ” averageë¥¼ ëª¨ë“  channelì— ê°™ì´ ì ìš©í•  ê²ƒì´ê¸° ë•Œë¬¸ì— ê·¸ cë§Œí¼ì„ average filterë¡œ í•œ ë²ˆì— ì ìš©í•œë‹¤ëŠ” ë§ì´ë‹¤.\n\n\n\n\n4.5 Ternary Weight Networks(TWN)\nTernaryëŠ” Binary Quantizationê³¼ ë‹¨ê³„ëŠ” ëª¨ë‘ ê°™ì§€ë§Œ, ê°€ì§ˆ ìˆ˜ ìˆëŠ” ê°’ìœ¼ë¡œ 0 ì„ ì¶”ê°€í•œë‹¤. ì•„ë˜ ê·¸ë¦¼ì€ Scaleì„ ì´ìš©í•´ì„œ Quantization Errorë¥¼ ì¤„ì´ëŠ” ë°©ë²•ì„ ë§í•˜ê³  ìˆë‹¤. \\[\nq = \\begin{dcases}\nr_t, &r &gt; \\Delta \\\\\n0, &\\lvert r\\lvert \\leq \\Delta \\\\\n-r_t, &r &lt; -\\Delta\n\\end{dcases} \\\\\nwhere\\ \\Delta = 0.7\\times \\mathbb{E}(\\lvert r \\lvert), r_t = \\mathbb{E}_{\\lvert r \\lvert &gt; \\Delta}(\\lvert r \\lvert )\n\\]  ### 4.6 Trained Ternary Quantization(TTQ)\nTenary Quantizationì—ì„œ ë˜ í•œê°€ì§€ ë‹¤ë¥´ê²Œ ì„¤ëª…í•˜ëŠ” ê²ƒì€ 1ê³¼ -1ë¡œë§Œ ì •í•´ì ¸ ìˆë˜ Binary Quantizationê³¼ ë‹¤ë¥´ê²Œ TenaryëŠ” 1, 0, -1ë¡œ Quantizationì„ í•œ í›„, ì¶”ê°€ì ì¸ í›ˆë ¨ì„ í†µí•´ \\(w_t\\)ì™€ \\(-w_t\\)ë¡œ fine-tuningì„ í•˜ëŠ” ë°©ë²•ë„ ì œì•ˆí•œë‹¤(í•´ë‹¹ ë…¼ë¬¸ì—ì„œëŠ” ì´ëŸ¬í•œ ê¸°ë²•ì„ ì´ìš©í•´ì„œ í•œ ê²°ê³¼ë¥¼ CIFAR-10 ì´ë¯¸ì§€ ë°ì´í„°ë¥¼ ê°€ì§€ê³  ResNets, AlexNet, ImageNetì—ì„œ ë³´ì—¬ì¤€ë‹¤). \\[\nq = \\begin{dcases}\nw_t, &r &gt; \\Delta \\\\\n0, &\\lvert r\\lvert \\leq \\Delta \\\\\n-w_t, &r &lt; -\\Delta\n\\end{dcases}\n\\] \n\n\n4.7 Accuracy Degradation\nBinary, Ternary Quantizationì„ ì‚¬ìš©í•œ ê²°ê³¼ë¥¼ ë³´ì—¬ì¤€ë‹¤(Resnet-18 ê²½ìš°ì—ëŠ” Ternary ê°€ ì˜¤íˆë ¤ Binaryë³´ë‹¤ ì„±ëŠ¥ì´ ë” ë–¨ì–´ì§„ë‹¤!)\n\nBinarization\n\n\n\nReference. Binarized Neural Networks: Training Deep Neural Networks with Weights and Activations Constrained to +1 orÂ âˆ’1. [CourbariauxÂ et al., Arxiv 2016], XNOR-Net: ImageNet Classification using Binary Convolutional Neural Networks [RastegariÂ et al., ECCV 2016]\n\n\nTernary Weight Networks (TWN)\n\n\n\nReference. Ternary Weight Networks [LiÂ et al., Arxiv 2016]\n\n\nTrained Ternary Quantization (TTQ)\n\n\n\nReference. Trained Ternary Quantization [ZhuÂ et al., ICLR 2017]"
  },
  {
    "objectID": "posts/lecs/lec05.html#low-bit-width-quantization",
    "href": "posts/lecs/lec05.html#low-bit-width-quantization",
    "title": "ğŸ§‘â€ğŸ« Lecture 5-6",
    "section": "5. Low Bit-Width Quantization",
    "text": "5. Low Bit-Width Quantization\në‚¨ì€ ë¶€ë¶„ë“¤ì€ ì—¬ëŸ¬ê°€ì§€ ì‹¤í—˜ / ì—°êµ¬ë“¤ì„ ì†Œê°œí•˜ê³  ìˆë‹¤.\n\nBinary Quantizationì€ Quantization Aware Trainingì„ í•  ìˆ˜ ìˆì„ê¹Œ?\n2,3 bitê³¼ 8bit ê·¸ ì¤‘ê°„ìœ¼ë¡œëŠ” Quantizationì„ í•  ìˆ˜ ì—†ì„ê¹Œ?\në ˆì´ì–´ì—ì„œ Quantizationì„ í•˜ì§€ ì•ŠëŠ” ë ˆì´ì–´, ì˜ˆë¥¼ ë“¤ì–´ ê²°ê³¼ì— ì˜í–¥ì„ ì˜ˆë¯¼í•˜ê²Œ ë¯¸ì¹˜ëŠ” ì²« ë²ˆì§¸ ë ˆì´ì–´ê°€ ê°™ì€ ê²½ìš° Quantizationì„ í•˜ì§€ ì•Šìœ¼ë©´ ì–´ë–»ê²Œ ë ê¹Œ?\nActivation í•¨ìˆ˜ë¥¼ ë°”ê¾¸ë©´ ì–´ë–¨ê¹Œ?\nì˜ˆë¥¼ ë“¤ì–´ ì²«ë²ˆì§¸ ë ˆì´ì–´ì˜ Në°° ë„“ê²Œ í•˜ëŠ” ê²ƒê³¼ ê°™ì´ ëª¨ë¸ êµ¬ì¡°ë¥¼ ë°”ê¾¸ë©´ ì–´ë–»ê²Œ ë ê¹Œ?\nì¡°ê¸ˆì”© Quantizationì„ í•  ìˆ˜ ì—†ì„ê¹Œ? (20% â†’ 40% â†’ â€¦ â†’ 100%)\n\nê°•ì˜ì—ì„œëŠ” í¬ê²Œ ì–¸ê¸‰í•˜ì§€ ì•Šê³  ê°„ ë‚´ìš©ë“¤ì´ë¼ ì„¤ëª…ì„ í•˜ì§€ëŠ” ì•Šê² ë‹¤. í•´ë‹¹ ë‚´ìš©ë“¤ì€ ìì„¸í•œ ë‚´ìš©ì„ ì•Œê³ ì‹¶ìœ¼ë©´ ê° íŒŒíŠ¸ì— ì–¸ê¸‰ëœ ë…¼ë¬¸ì„ ì°¸ì¡°í•˜ê¸¸!\n\n5.1 Train Binarized Neural Networks From Scratch\n\nStraight-Through Estimator(STE)\n\n\n\n\nReference. MIT-TinyML-lecture06-Quantization-2\n\n\n\nGradient pass straight to floating-point weights\nFloating-point weight with in [-1, 1]\nReference. Binarized Neural Networks: Training Deep Neural Networks with Weights and Activations Constrained to +1 orÂ âˆ’1. [Courbariaux et al., Arxiv 2016]\n\n\n\n5.2 Quantization-Aware Training: DoReFa-Net With Low Bit-Width Gradients\n\n\n\nReference. MIT-TinyML-lecture06-Quantization-2\n\n\n\nGradient Quantization\n\\[\n  Q(g) = 2 \\cdot max(\\lvert G \\lvert) \\cdot \\Large[ \\small quantize_k \\Large( \\small \\dfrac{g}{2\\cdot max(\\lvert G \\lvert)} + \\dfrac{1}{2} + N(k) \\Large ) \\small -\\dfrac{1}{2} \\Large]\\small\n  \\] \\[\n  where\\ N(k)=\\dfrac{\\sigma}{2^k-1} and\\ \\sigma \\thicksim Uniform(-0.5, 0.5)\n  \\]\n\nNoise function \\(N(k)\\) is added to compensate the potential bias introduced by gradient quantization.\n\nResult\n\n\n\nReference. DoReFa-Net: Training Low Bitwidth Convolutional Neural Networks with Low Bitwidth Gradients [ZhouÂ et al., arXiv 2016]\n\n\n\n\n\n5.3 Replace the Activation Function: Parameterized Clipping Activation Function\n\nThe most common activation function ReLU is unbounded. The dynamic range of inputs becomes problematic for low bit-width quantization due to very limited range and resolution.\nReLU is replaced with hard-coded bounded activation functions: ReLU6, ReLU1, etc\nThe clipping value per layer can be learned as well: PACT(Parametrized Clipping Activation Function)\n \\[\n  y=PACT(x;\\alpha) = 0.5(\\lvert x \\lvert - \\lvert x -\\alpha \\lvert + \\alpha ) = \\begin{dcases}\n  0, & x \\in [-\\infty, 0) \\\\\n  x, & x \\in [0, \\alpha) \\\\\n  \\alpha, & x \\in [\\alpha, +\\infty)\n  \\end{dcases}\n  \\]\nThe upper clipping value of the activation function is a trainable. With STE, the gradient is computed as\n\\[\n  \\dfrac{\\partial Q(y)}{\\partial \\alpha} = \\dfrac{\\partial Q(y)}{\\partial y} \\cdot \\dfrac{\\partial y}{\\partial \\alpha} = \\begin{dcases}\n  0 & x \\in (-\\infty, \\alpha)\\\\\n  1 & x \\in [\\alpha, +\\infty)\\\\\n  \\end{dcases}\n  \\]\n\\[\n  \\rightarrow\n  \\dfrac{\\partial L}{\\partial \\alpha} = \\dfrac{\\partial L}{\\partial Q(y)} \\cdot \\dfrac{\\partial Q(y)}{\\partial \\alpha} = \\begin{dcases}\n  0 & x \\in (-\\infty, \\alpha)\\\\\n  \\frac{\\partial L}{\\partial Q(y)} & x \\in [\\alpha, +\\infty)\\\\\n  \\end{dcases}\n  \\]\nThe larger \\(\\alpha\\), the more the parameterized clipping function resembles a ReLU function\n\nTo avoid large quantization errors due to a wide dynamic range \\([0, \\alpha]\\), L2-regularizer for \\(\\alpha\\) is included in the training loss function.\n\nResult\n\n\n\nReference. PACT: Parameterized Clipping Activation for Quantized Neural Networks [ChoiÂ et al., arXiv 2018]\n\n\n\n\n\n5.4 Modify the Neural Network Architecture\n\nWiden the neural network to compensate for the loss of information due to quantization\nex. Double the channels, reduce the quantization precision\n\n\n\nReference. WRPN: Wide Reduced-Precision Networks [MishraÂ et al., ICLR 2018]\n\n\nReplace a single floating-point convolution with multiple binary convolutions.\n\nTowards Accurate Binary Convolutional Neural Network [LinÂ et al., NeurIPS 2017]\nQuantization [Neural Network Distiller]\n\n\n\n\n5.5 No Quantization on First and Last Layer\n\nBecause it is more sensitive to quantization and small portion of the overall computation\nQuantizing these layers to 8-bit integer does not reduce accuracy\n\n\n\n5.6 Iterative Quantization: Incremental Network Quantization\n\nReference. Incremental Network Quantization: Towards Lossless CNNs with Low-precision Weights [ZhouÂ et al., ICLR 2017]\n\n\n\n\nReference. Incremental Network Quantization: Towards Lossless CNNs with Low-precision Weights [ZhouÂ et al., ICLR 2017]\n\n\n\nSetting\n\nWeight quantization only\nQuantize weights to \\(2^n\\) for faster computation (bit shift instead of multiply)\n\nAlgorithm\n\nStart from a pre-trained fp32 model\nFor the remaining fp32 weights\n\nPartition into two disjoint groups(e.g., according to magnitude)\nQuantize the first group (higher magnitude), and re-train the other group to recover accuracy\n\nRepeat until all the weights are quantized (a popular stride is {50%, 75%, 87.5%, 100%})\n\n\n\n\nReference. MIT-TinyML-lecture06-Quantization-2"
  },
  {
    "objectID": "posts/lecs/lec05.html#mixed-precision-quantization",
    "href": "posts/lecs/lec05.html#mixed-precision-quantization",
    "title": "ğŸ§‘â€ğŸ« Lecture 5-6",
    "section": "6. Mixed-precision quantization",
    "text": "6. Mixed-precision quantization\në§ˆì§€ë§‰ìœ¼ë¡œ ë ˆì´ì–´ë§ˆë‹¤ Quantization bitë¥¼ ë‹¤ë¥´ê²Œ ê°€ì ¸ê°€ë©´ ì–´ë–¨ì§€ì— ëŒ€í•´ì„œ ì´ì•¼ê¸°í•œë‹¤. í•˜ì§€ë§Œ ê²½ìš°ì˜ ìˆ˜ê°€ 8bit ë³´ë‹¤ ì‘ê±°ë‚˜ ê°™ê²Œ Quantizationì„ í•  ì‹œ, weightì™€ activationë¡œ ê²½ìš°ì˜ ìˆ˜ë¥¼ ê³ ë ¤ë¥¼ í•œë‹¤ë©´ Nê°œ ë ˆì´ì–´ì— ëŒ€í•´ì„œ \\((8 \\times 8)^N\\)ë¼ëŠ” ì–´ë§ˆì–´ë§ˆí•œ ê²½ìš°ì˜ ìˆ˜ê°€ ë‚˜ì˜¨ë‹¤. ê·¸ë¦¬ê³  ì´ì— ëŒ€í•´ì„œëŠ” ë‹¤ìŒ íŒŒíŠ¸ì— ë‚˜ê°ˆ Neural Architecture Search(NAS) ì—ì„œ ë‹¤ë£° ë“¯ ì‹¶ë‹¤.\n\n6.1 Uniform Quantization\n\n\n\n6.2 Mixed-precision Quantization\n\n\n\n6.3 Huge Design Space and Solution: Design Automation\n\n\nDesign Space: Each of Choices(8x8=64) â†’ \\(64^n\\)\n\n\n\nReference. HAQ: Hardware-Aware Automated Quantization with Mixed Precision [WangÂ et al., CVPR 2019]\n\n\nResult in Mixed-Precision Quantized MobileNetV1\n\n\n\nReference. HAQ: Hardware-Aware Automated Quantization with Mixed Precision [WangÂ et al., CVPR 2019]\n\n\n\nThis paper compares with Model size, Latency and Energy\n\n\nê°€ì¥ ë§ˆì§€ë§‰ì— ì–¸ê¸‰í•˜ëŠ” Edgeì™€ í´ë¼ìš°ë“œì—ì„œëŠ” Convolution ë ˆì´ì–´ì˜ ì¢…ë¥˜ ì¤‘ ë”í•˜ê³  ëœ Quantizationí•˜ëŠ” ë ˆì´ì–´ê°€ ê°ê° depthwiseì™€ pointwiseë¡œ ë‹¤ë¥´ë‹¤ê³  ì´ì•¼ê¸°í•œë‹¤. ì´ ë‚´ìš©ì— ëŒ€í•´ì„œ ë” ìì„¸íˆ ì´í•´í•˜ê¸° ìœ„í•´ì„œëŠ” ì•„ë§ˆë„ NASë¡œ ë„˜ì–´ê°€ë´ì•¼ ì•Œ ìˆ˜ ìˆì§€ ì•Šì„ê¹Œ ì‹¶ë‹¤.\n\nQuantization Policy for Edge and Cloud\n\n\n\nReference. HAQ: Hardware-Aware Automated Quantization with Mixed Precision [WangÂ et al., CVPR 2019]"
  },
  {
    "objectID": "posts/labs/lab00.html",
    "href": "posts/labs/lab00.html",
    "title": "ğŸ‘©â€ğŸ’» Lab 0",
    "section": "",
    "text": "Lab 0 PyTorch Tutorial\nIn this tutorial, we will explore how to train a neural network with PyTorch.\n\nSetup\nWe will first install a few packages that will be used in this tutorial:\n\n!pip install torchprofile 1&gt;/dev/null\n\nWe will then import a few libraries:\n\nimport random\nfrom collections import OrderedDict, defaultdict\n\nimport numpy as np\nimport torch\nfrom matplotlib import pyplot as plt\nfrom torch import nn\nfrom torch.optim import *\nfrom torch.optim.lr_scheduler import *\nfrom torch.utils.data import DataLoader\nfrom torchprofile import profile_macs\nfrom torchvision.datasets import *\nfrom torchvision.transforms import *\nfrom tqdm.auto import tqdm\n\nTo ensure the reproducibility, we will control the seed of random generators:\n\nrandom.seed(0)\nnp.random.seed(0)\ntorch.manual_seed(0)\n\n\n\nData\nIn this tutorial, we will use CIFAR-10 as our target dataset. This dataset contains images from 10 classes, where each image is of size 3x32x32, i.e.Â 3-channel color images of 32x32 pixels in size.\n\ntransforms = {\n  \"train\": Compose([\n    RandomCrop(32, padding=4),\n    RandomHorizontalFlip(),\n    ToTensor(),\n  ]),\n  \"test\": ToTensor(),\n}\n\ndataset = {}\nfor split in [\"train\", \"test\"]:\n  dataset[split] = CIFAR10(\n    root=\"data/cifar10\",\n    train=(split == \"train\"),\n    download=True,\n    transform=transforms[split],\n  )\n\nWe can visualize a few images in the dataset and their corresponding class labels:\n\nsamples = [[] for _ in range(10)]\nfor image, label in dataset[\"test\"]:\n  if len(samples[label]) &lt; 4:\n    samples[label].append(image)\n\nplt.figure(figsize=(20, 9))\nfor index in range(40):\n  label = index % 10\n  image = samples[label][index // 10]\n\n  # Convert from CHW to HWC for visualization\n  image = image.permute(1, 2, 0)\n\n  # Convert from class index to class name\n  label = dataset[\"test\"].classes[label]\n\n  # Visualize the image\n  plt.subplot(4, 10, index + 1)\n  plt.imshow(image)\n  plt.title(label)\n  plt.axis(\"off\")\nplt.show()\n\nTo train a neural network, we will need to feed data in batches. We create data loaders with batch size of 512:\n\ndataflow = {}\nfor split in ['train', 'test']:\n  dataflow[split] = DataLoader(\n    dataset[split],\n    batch_size=512,\n    shuffle=(split == 'train'),\n    num_workers=0,\n    pin_memory=True,\n  )\n\nWe can print the data type and shape from the training data loader:\n\nfor inputs, targets in dataflow[\"train\"]:\n  print(\"[inputs] dtype: {}, shape: {}\".format(inputs.dtype, inputs.shape))\n  print(\"[targets] dtype: {}, shape: {}\".format(targets.dtype, targets.shape))\n  break\n\n\n\nModel\nIn this tutorial, we will use a variant of VGG-11 (with fewer downsamples and a smaller classifier) as our model.\n\nclass VGG(nn.Module):\n  ARCH = [64, 128, 'M', 256, 256, 'M', 512, 512, 'M', 512, 512, 'M']\n\n  def __init__(self) -&gt; None:\n    super().__init__()\n\n    layers = []\n    counts = defaultdict(int)\n\n    def add(name: str, layer: nn.Module) -&gt; None:\n      layers.append((f\"{name}{counts[name]}\", layer))\n      counts[name] += 1\n\n    in_channels = 3\n    for x in self.ARCH:\n      if x != 'M':\n        # conv-bn-relu\n        add(\"conv\", nn.Conv2d(in_channels, x, 3, padding=1, bias=False))\n        add(\"bn\", nn.BatchNorm2d(x))\n        add(\"relu\", nn.ReLU(True))\n        in_channels = x\n      else:\n        # maxpool\n        add(\"pool\", nn.MaxPool2d(2))\n\n    self.backbone = nn.Sequential(OrderedDict(layers))\n    self.classifier = nn.Linear(512, 10)\n\n  def forward(self, x: torch.Tensor) -&gt; torch.Tensor:\n    # backbone: [N, 3, 32, 32] =&gt; [N, 512, 2, 2]\n    x = self.backbone(x)\n\n    # avgpool: [N, 512, 2, 2] =&gt; [N, 512]\n    x = x.mean([2, 3])\n\n    # classifier: [N, 512] =&gt; [N, 10]\n    x = self.classifier(x)\n    return x\n\nmodel = VGG().cuda()\n\nIts backbone is composed of eight conv-bn-relu blocks interleaved with four maxpoolâ€™s to downsample the feature map by 2^4 = 16 times:\n\nprint(model.backbone)\n\nAfter the feature map is pooled, its classifier predicts the final output with a linear layer:\n\nprint(model.classifier)\n\nAs this course focuses on efficiency, we will then inspect its model size and (theoretical) computation cost.\n\nThe model size can be estimated by the number of trainable parameters:\n\n\nnum_params = 0\nfor param in model.parameters():\n  if param.requires_grad:\n    num_params += param.numel()\nprint(\"#Params:\", num_params)\n\n\nThe computation cost can be estimated by the number of multiplyâ€“accumulate operations (MACs) using TorchProfile:\n\n\nnum_macs = profile_macs(model, torch.zeros(1, 3, 32, 32).cuda())\nprint(\"#MACs:\", num_macs)\n\nThis model has 9.2M parameters and requires 606M MACs for inference. We will work together in the next few labs to improve its efficiency.\n\n\nOptimization\nAs we are working on a classification problem, we will apply cross entropy as our loss function to optimize the model:\n\ncriterion = nn.CrossEntropyLoss()\n\nOptimization will be carried out using stochastic gradient descent (SGD) with momentum:\n\noptimizer = SGD(\n  model.parameters(),\n  lr=0.4,\n  momentum=0.9,\n  weight_decay=5e-4,\n)\n\nThe learning rate will be modulated using the following scheduler (which is adapted from this blog series):\n\nnum_epochs = 20\nsteps_per_epoch = len(dataflow[\"train\"])\n\n# Define the piecewise linear scheduler\nlr_lambda = lambda step: np.interp(\n  [step / steps_per_epoch],\n  [0, num_epochs * 0.3, num_epochs],\n  [0, 1, 0]\n)[0]\n\n# Visualize the learning rate schedule\nsteps = np.arange(steps_per_epoch * num_epochs)\nplt.plot(steps, [lr_lambda(step) * 0.4 for step in steps])\nplt.xlabel(\"Number of Steps\")\nplt.ylabel(\"Learning Rate\")\nplt.grid(\"on\")\nplt.show()\n\nscheduler = LambdaLR(optimizer, lr_lambda)\n\n\n\nTraining\nWe first define the training function that optimizes the model for one epoch (i.e., a pass over the training set):\n\ndef train(\n  model: nn.Module,\n  dataflow: DataLoader,\n  criterion: nn.Module,\n  optimizer: Optimizer,\n  scheduler: LambdaLR,\n) -&gt; None:\n  model.train()\n\n  for inputs, targets in tqdm(dataflow, desc='train', leave=False):\n    # Move the data from CPU to GPU\n    inputs = inputs.cuda()\n    targets = targets.cuda()\n\n    # Reset the gradients (from the last iteration)\n    optimizer.zero_grad()\n\n    # Forward inference\n    outputs = model(inputs)\n    loss = criterion(outputs, targets)\n\n    # Backward propagation\n    loss.backward()\n\n    # Update optimizer and LR scheduler\n    optimizer.step()\n    scheduler.step()\n\nWe then define the evaluation function that calculates the metric (i.e., accuracy in our case) on the test set:\n\n@torch.inference_mode()\ndef evaluate(\n  model: nn.Module,\n  dataflow: DataLoader\n) -&gt; float:\n  model.eval()\n\n  num_samples = 0\n  num_correct = 0\n\n  for inputs, targets in tqdm(dataflow, desc=\"eval\", leave=False):\n    # Move the data from CPU to GPU\n    inputs = inputs.cuda()\n    targets = targets.cuda()\n\n    # Inference\n    outputs = model(inputs)\n\n    # Convert logits to class indices\n    outputs = outputs.argmax(dim=1)\n\n    # Update metrics\n    num_samples += targets.size(0)\n    num_correct += (outputs == targets).sum()\n\n  return (num_correct / num_samples * 100).item()\n\nWith training and evaluation functions, we can finally start training the model! This will take around 10 minutes.\n\nfor epoch_num in tqdm(range(1, num_epochs + 1)):\n  train(model, dataflow[\"train\"], criterion, optimizer, scheduler)\n  metric = evaluate(model, dataflow[\"test\"])\n  print(f\"epoch {epoch_num}:\", metric)\n\nIf everything goes well, your trained model should be able to achieve &gt;92.5% of accuracy!\n\n\nVisualization\nWe can visualize the modelâ€™s prediction to see how the model truly performs:\n\nplt.figure(figsize=(20, 10))\nfor index in range(40):\n  image, label = dataset[\"test\"][index]\n\n  # Model inference\n  model.eval()\n  with torch.inference_mode():\n    pred = model(image.unsqueeze(dim=0).cuda())\n    pred = pred.argmax(dim=1)\n\n  # Convert from CHW to HWC for visualization\n  image = image.permute(1, 2, 0)\n\n  # Convert from class indices to class names\n  pred = dataset[\"test\"].classes[pred]\n  label = dataset[\"test\"].classes[label]\n\n  # Visualize the image\n  plt.subplot(4, 10, index + 1)\n  plt.imshow(image)\n  plt.title(f\"pred: {pred}\" + \"\\n\" + f\"label: {label}\")\n  plt.axis(\"off\")\nplt.show()"
  },
  {
    "objectID": "posts/lecs/lec04.html",
    "href": "posts/lecs/lec04.html",
    "title": "ğŸ§‘â€ğŸ« Lecture 4",
    "section": "",
    "text": "ì´ì „ í¬ìŠ¤íŒ…ì—ì„œ Pruningì— ëŒ€í•´ì„œ ë°°ì› ì—ˆë‹¤. ì´ë²ˆì—ëŠ” Pruningì— ëŒ€í•œ ë‚¨ì€ ì´ì•¼ê¸°ì¸ Pruning Ratioë¥¼ ì •í•˜ëŠ” ë°©ë²•, Fine-tuning ê³¼ì •ì— ëŒ€í•´ ì•Œì•„ë³´ê³ , ë§ˆì§€ë§‰ìœ¼ë¡œ Sparsityë¥¼ ìœ„í•œ System Supportì— ëŒ€í•´ ì•Œì•„ë³´ê³ ì í•œë‹¤."
  },
  {
    "objectID": "posts/lecs/lec04.html#pruning-ratio",
    "href": "posts/lecs/lec04.html#pruning-ratio",
    "title": "ğŸ§‘â€ğŸ« Lecture 4",
    "section": "1. Pruning Ratio",
    "text": "1. Pruning Ratio\nPruningì„ í•˜ê¸° ìœ„í•´ì„œ ì–´ëŠ ì •ë„ Pruningì„ í•´ì•¼ í• ì§€ ì–´ë–»ê²Œ ì •í•´ì•¼ í• ê¹Œ?\nì¦‰, ë‹¤ì‹œ ë§í•´ì„œ ëª‡ % ì •ë„ ê·¸ë¦¬ê³  ì–´ë–»ê²Œ Pruningì„ í•´ì•¼ ì¢‹ì„ê¹Œ?\n\n\n\nPruning ë°©ì‹ ë¹„êµ\n\n\nìš°ì„  Channel ë³„ Pruningì„ í•  ë•Œ, Channel êµ¬ë¶„ ì—†ì´ ë™ì¼í•œ Pruning ë¹„ìœ¨(Uniform)ì„ ì ìš©í•˜ë©´ ì„±ëŠ¥ì´ ì¢‹ì§€ ì•Šë‹¤. ì˜¤ë¥¸ìª½ ê·¸ë˜í”„ì—ì„œ ì§€í–¥í•´ì•¼ í•˜ëŠ” ë°©í–¥ì€ LatencyëŠ” ì ê²Œ, AccuracyëŠ” ë†’ê²Œì´ë¯€ë¡œ ì™¼ìª½ ìƒë‹¨ì˜ ì˜ì—­ì´ ë˜ë„ë¡ Pruningì„ ì§„í–‰í•´ì•¼ í•œë‹¤. ê·¸ë ‡ë‹¤ë©´ ê²°ë¡ ì€ Channel ë³„ êµ¬ë¶„ì„ í•´ì„œ ì–´ë–¤ Channelì€ Pruning ë¹„ìœ¨ì„ ë†’ê²Œ, ì–´ë–¤ Channelì€ Pruning ë¹„ìœ¨ì„ ë‚®ê²Œ í•´ì•¼ í•œë‹¤ëŠ” ì´ì•¼ê¸°ê°€ ëœë‹¤.\n\n1.1 Sensitiviy Analysis\nChannel ë³„ êµ¬ë¶„ì„ í•´ì„œ Pruningì„ í•œë‹¤ëŠ” ê¸°ë³¸ ì•„ì´ë””ì–´ëŠ” ì•„ë˜ì™€ ê°™ë‹¤.\n\nAccuracyì— ì˜í–¥ì„ ë§ì´ ì£¼ëŠ” LayerëŠ” Pruningì„ ì ê²Œ í•´ì•¼ í•œë‹¤.\nAccuracyì— ì˜í–¥ì„ ì ê²Œ ì£¼ëŠ” LayerëŠ” Pruningì„ ë§ì´ í•´ì•¼ í•œë‹¤.\n\nAccuracyë¥¼ ë˜ë„ë¡ì´ë©´ ì›ë˜ì˜ ëª¨ë¸ë³´ë‹¤ ëœ ë–¨ì–´ì§€ê²Œ ë§Œë“¤ë©´ì„œ Pruningì„ í•´ì„œ ëª¨ë¸ì„ ê°€ë³ê²Œ ë§Œë“œëŠ” ê²ƒì´ ëª©í‘œì´ê¸° ë•Œë¬¸ì— ë‹¹ì—°í•œ ì•„ì´ë””ì–´ì¼ ê²ƒì´ë‹¤. Accuracyì— ì˜í–¥ì„ ë§ì´ ì¤€ë‹¤ëŠ” ë§ì€ Sensitiveí•œ Layerì´ë‹¤ë¼ëŠ” í‘œí˜„ìœ¼ë¡œ ë‹¤ë¥´ê²Œ ë§í•  ìˆ˜ ìˆë‹¤. ë”°ë¼ì„œ ê° Layerì˜ Senstivityë¥¼ ì¸¡ì •í•´ì„œ Sensitiveí•œ LayerëŠ” Pruning Ratioë¥¼ ë‚®ê²Œ ì„¤ê³„í•˜ë©´ ëœë‹¤.\nLayerì˜ Sensitivityë¥¼ ì¸¡ì •í•˜ê¸° ìœ„í•´ Sensitivity Analysisë¥¼ ì§„í–‰í•´ë³´ì. ë‹¹ì—°íˆ íŠ¹ì • Layerì˜ Pruning Ratioê°€ ë†’ì„ ìˆ˜ë¡ weightê°€ ë§ì´ ê°€ì§€ì¹˜ê¸° ëœ ê²ƒì´ë¯€ë¡œ AccuracyëŠ” ë–¨ì–´ì§€ê²Œ ëœë‹¤.\n\n\n\nL0 Pruning Rate ê·¸ë˜í”„\n\n\n\n\n\n\n\n\nPruningì„ í•˜ëŠ” WeightëŠ” ì–´ë–»ê²Œ ê²°ì •í•˜ë‚˜ìš”?\n\n\n\n\n\nPruning Ratioì— ì˜í•´ Pruned ë˜ëŠ” weightëŠ” ì´ì „ ê°•ì˜ì—ì„œ ë°°ìš´ â€œImportance(weightì˜ ì ˆëŒ“ê°’ í¬ê¸°)â€ì— ë”°ë¼ ì„ íƒëœë‹¤.\n\n\n\nìœ„ì˜ ê·¸ë¦¼ì—ì„œ ì²˜ëŸ¼ Layer 0(L0)ë§Œì„ ê°€ì§€ê³  Pruning Ratioë¥¼ ë†’ì—¬ê°€ë©´ì„œ ê´€ì°°í•´ë³´ë©´, ì•½ 70% ì´í›„ë¶€í„°ëŠ” Accuracyê°€ ê¸‰ê²©í•˜ê²Œ ë–¨ì–´ì§€ëŠ” ê²ƒì„ ë³¼ ìˆ˜ ìˆë‹¤. L0ì—ì„œ Ratioë¥¼ ë†’ì—¬ê°€ë©° Accuracyì˜ ë³€í™”ë¥¼ ê´€ì°°í•œ ê²ƒì²˜ëŸ¼ ë‹¤ë¥¸ Layerë“¤ë„ ê´€ì°°í•´ë³´ì.\n\n\n\nLayerë³„ Sensitivity ë¹„êµ\n\n\nL1ì€ ë‹¤ë¥¸ Layerë“¤ì— ë¹„í•´ ìƒëŒ€ì ìœ¼ë¡œ Pruning Ratioë¥¼ ë†’ì—¬ê°€ë„ Accuracyì˜ ë–¨ì–´ì§€ëŠ” ì •ë„ê°€ ì•½í•œ ë°˜ë©´, L0ëŠ” ë‹¤ë¥¸ Layerë“¤ì— ë¹„í•´ ìƒëŒ€ì ìœ¼ë¡œ Pruning Ratioë¥¼ ë†’ì—¬ê°€ë©´ Accuracyì˜ ë–¨ì–´ì§€ëŠ” ì •ë„ê°€ ì‹¬í•œ ê²ƒì„ í™•ì¸í•  ìˆ˜ ìˆë‹¤. ë”°ë¼ì„œ L1ì€ Sensitivityê°€ ë†’ë‹¤ê³  ë³¼ ìˆ˜ ìˆìœ¼ë©° Pruningì„ ì ê²Œí•´ì•¼ í•˜ê³ , L0ì€ Sensitivityê°€ ë‚®ë‹¤ê³  ë³¼ ìˆ˜ ìˆìœ¼ë©° Pruningì„ ë§ê²Œí•´ì•¼ í•¨ì„ ì•Œ ìˆ˜ ìˆë‹¤.\nì—¬ê¸°ì„œ Sensitivity Analysisì—ì„œ ê³ ë ¤í•´ì•¼í•  ëª‡ê°€ì§€ ì‚¬í•­ë“¤ì— ëŒ€í•´ì„œ ì§šê³  ë„˜ì–´ê°€ì.\n\nSensitivity Analysisì—ì„œ ëª¨ë“  Layerë“¤ì´ ë…ë¦½ì ìœ¼ë¡œ ì‘ë™í•œë‹¤ëŠ” ê²ƒì„ ì „ì œë¡œ í•œë‹¤. ì¦‰, L0ì˜ Pruningì´ L1ì˜ íš¨ê³¼ì— ì˜í–¥ì„ ì£¼ì§€ ì•ŠëŠ” ë…ë¦½ì„±ì„ ê°€ì§„ë‹¤ëŠ” ê²ƒì„ ì „ì œë¡œ í•œë‹¤.\nLayerì˜ Pruning Ratioê°€ ë™ì¼í•˜ë‹¤ê³  í•´ì„œ Pruned Weightìˆ˜ê°€ ê°™ìŒì„ ì˜ë¯¸í•˜ì§€ ì•ŠëŠ”ë‹¤.\n\n100ê°œì˜ weightê°€ ìˆëŠ” layerì˜ 10% Pruning Ratio ì ìš©ì€ 10ê°œì˜ weightê°€ pruned ë˜ì—ˆìŒì„ ì˜ë¯¸í•˜ê³ , 500ê°œì˜ weightê°€ ìˆëŠ” layerì˜ 10% Pruning Ratio ì ìš©ì€ 50ê°œì˜ weightê°€ pruned ë˜ì—ˆìŒì„ ì˜ë¯¸í•œë‹¤.\nLayerì˜ ì „ì²´ í¬ê¸°ì— ë”°ë¼ Pruning Ratioì˜ ì ìš© íš¨ê³¼ëŠ” ë‹¤ë¥¼ ìˆ˜ ìˆë‹¤.\n\n\nSensitivity Analysisê¹Œì§€ ì§„í–‰í•œ í›„ì—ëŠ” ë³´í†µ ì‚¬ëŒì´ Accuracyê°€ ë–¨ì–´ì§€ëŠ” ì •ë„, thresholdë¥¼ ì •í•´ì„œ Pruning Ratioë¥¼ ì •í•œë‹¤.\n\n\n\nThreshold ì •í•˜ê¸°\n\n\nìœ„ ê·¸ë˜í”„ì—ì„œëŠ” Accuracyê°€ ì•½ 75%ìˆ˜ì¤€ìœ¼ë¡œ ìœ ì§€ë˜ëŠ” threhsold \\(T\\) ìˆ˜í‰ì„ ì„ ê¸°ì¤€ìœ¼ë¡œ L0ëŠ” ì•½ 74%, L4ëŠ” ì•½ 80%, L3ëŠ” ì•½ 82%, L2ëŠ” 90%ê¹Œì§€ Pruningì„ ì§„í–‰í•´ì•¼ ê² ë‹¤ê³  ì •í•œ ì˜ˆì‹œë¥¼ ë³´ì—¬ì¤€ë‹¤. ë¯¼ê°í•œ layerì¸ L0ëŠ” ìƒëŒ€ì ìœ¼ë¡œ Pruningì„ ì ê²Œ, ëœ ë¯¼ê°í•œ layerì¸ L2ëŠ” Pruningì„ ë§ê²Œ í•˜ëŠ” ê²ƒì„ í™•ì¸í•  ìˆ˜ ìˆë‹¤.\në¬¼ë¡  ì‚¬ëŒì´ ì •í•˜ëŠ” thresholdëŠ” ê°œì„ ì˜ ì—¬ì§€ê°€ ë¬¼ë¡  ìˆë‹¤. Pruning Ratioë¥¼ ì¢€ ë” Automaticí•˜ê²Œ ì°¾ëŠ” ë°©ë²•ì— ëŒ€í•´ ì•Œì•„ë³´ì.\n\n\n1.2 AMC\nAMCëŠ” AutoML for Model Compressionì˜ ì•½ìë¡œ, ê°•í™”í•™ìŠµ(Reinforcement Learning) ë°©ë²•ìœ¼ë¡œ ìµœì ì˜ Pruning Ratioë¥¼ ì°¾ë„ë¡ í•˜ëŠ” ë°©ë²•ì´ë‹¤.\n\n\n\nAMC ì „ì²´ êµ¬ì¡°\n\n\nAMCì˜ êµ¬ì¡°ëŠ” ìœ„ ê·¸ë¦¼ê³¼ ê°™ë‹¤. ê°•í™”í•™ìŠµ ì•Œê³ ë¦¬ì¦˜ ê³„ì—´ ì¤‘, Actor-Critic ê³„ì—´ì˜ ì•Œê³ ë¦¬ì¦˜ì¸ Deep Deterministic Policy Gradient(DDPG)ì„ í™œìš©í•˜ì—¬ Pruning Ratioë¥¼ ì •í•˜ëŠ” Actionì„ ì„ íƒí•˜ë„ë¡ í•™ìŠµí•œë‹¤. ìì„¸í•œ MDP(Markov Decision Process) ì„¤ê³„ëŠ” ì•„ë˜ì™€ ê°™ë‹¤.\n\n\n\nAMCì˜ MDP\n\n\nê°•í™”í•™ìŠµ Agentì˜ í•™ìŠµ ë°©í–¥ì„ ê²°ì •í•˜ëŠ” ì¤‘ìš”í•œ Reward Functionì€ ëª¨ë¸ì˜ Accuracyë¥¼ ê³ ë ¤í•´ì„œ Errorë¥¼ ì¤„ì´ë„ë¡ ìœ ë„í•  ë¿ë§Œ ì•„ë‹ˆë¼ Latencyë¥¼ ê°„ì ‘ì ìœ¼ë¡œ ê³ ë ¤í•  ìˆ˜ ìˆë„ë¡ ëª¨ë¸ì˜ ì—°ì‚°ëŸ‰ì„ ë‚˜íƒ€ë‚´ëŠ” FLOPë¥¼ ì ê²Œ í•˜ë„ë¡ ìœ ë„í•˜ë„ë¡ ì„¤ê³„í•œë‹¤. ì˜¤ë¥¸ìª½ì— ëª¨ë¸ë“¤ì˜ ì—°ì‚°ëŸ‰ ë³„(Operations) Top-1 Accuracy ê·¸ë˜í”„ë¥¼ ë³´ë©´ ì—°ì‚°ëŸ‰ì´ ë§ì„ìˆ˜ë¡ ë¡œê·¸í•¨ìˆ˜ì²˜ëŸ¼ Accuracyê°€ ì¦ê°€í•˜ëŠ” ê²ƒì„ ë³´ê³  ì´ë¥¼ ë³´ê³  ë°˜ì˜í•œ ë¶€ë¶„ì´ë¼ê³  ë³¼ ìˆ˜ ìˆë‹¤.\n\n\n\nAMCì˜ Reward Function\n\n\nì´ë ‡ê²Œ AMCë¡œ Pruningì„ ì§„í–‰í–ˆì„ ë•Œ, Human Expertê°€ Pruning í•œ ê²ƒê³¼ ë¹„êµí•´ë³´ì. ì•„ë˜ ëª¨ë¸ ì„¹ì…˜ë³„ Density íˆìŠ¤í† ê·¸ë¨ ê·¸ë˜í”„ì—ì„œ Totalì„ ë³´ë©´, ë™ì¼ Accuracyê°€ ë‚˜ì˜¤ë„ë¡ Pruningì„ ì§„í–‰í–ˆì„ ë•Œ AMCë¡œ Pruningì„ ì§„í–‰í•œ ê²ƒ(ì£¼í™©ìƒ‰)ì´ Human Expert Pruning ëª¨ë¸(íŒŒë€ìƒ‰)ë³´ë‹¤ Densityê°€ ë‚®ì€ ê²ƒì„ í™•ì¸í•  ìˆ˜ ìˆë‹¤. ì¦‰, AMCë¡œ Pruning ì§„í–‰í–ˆì„ ë•Œ ë” ë§ì€ weightë¥¼ Pruning ë” ê°€ë²¼ìš´ ëª¨ë¸ì„ ê°€ì§€ê³ ë„ Accuracyë¥¼ ìœ ì§€í–ˆë‹¤ê³  ë³¼ ìˆ˜ ìˆë‹¤.\n\n\n\nAMCì˜ Density Graph\n\n\në‘ë²ˆì§¸ êº¾ì€ ì„  ê·¸ë˜í”„ì—ì„œ AMCë¥¼ ê°€ì§€ê³  Pruningê³¼ Fine-tuningì„ ë²ˆê°ˆì•„ ê°€ë©° ì—¬ëŸ¬ ìŠ¤í…ìœ¼ë¡œ ì§„í–‰í•´ê°€ë©´ì„œ ê´€ì°°í•œ ê²ƒì„ ì¡°ê¸ˆ ë” ìì„¸íˆ ì‚´í´ë³´ì. ê° Iteration(Pruning+Fine-tuning)ì„ stage1, 2, 3, 4ë¡œ ë‚˜íƒ€ë‚´ì–´ plotí•œ ê²ƒì„ ë³´ë©´, 1x1 convë³´ë‹¤ 3x3 convì—ì„œ Densityê°€ ë” ë‚®ì€ ê²ƒì„ í™•ì¸í•  ìˆ˜ ìˆë‹¤. ì¦‰, 3x3 convì—ì„œ 1x1 convë³´ë‹¤ Pruningì„ ë§ì´ í•œ ê²ƒì„ ë³¼ ìˆ˜ ìˆë‹¤. ì´ë¥¼ í•´ì„í•´ë³´ìë©´, AMCê°€ 3x3 convì„ Pruningí•˜ë©´ 9ê°œì˜ weightë¥¼ pruningí•˜ê³  ì´ëŠ” 1x1 conv pruningí•´ì„œ 1ê°œì˜ weightë¥¼ ì—†ì• ëŠ” ê²ƒë³´ë‹¤ í•œë²ˆì— ë” ë§ì€ weight ìˆ˜ë¥¼ ì¤„ì¼ ìˆ˜ ìˆê¸° ë•Œë¬¸ì— 3x3 conv pruningì„ ì ê·¹ í™œìš©í–ˆì„ ê²ƒìœ¼ë¡œ ë³¼ ìˆ˜ ìˆë‹¤.\n\n\n\nAMC Result\n\n\nì´ AMC ì‹¤í—˜ ê²°ê³¼í‘œì—ì„œ ë³´ë©´, FLOPì™€ Time ê°ê° 50%ë¡œ ì¤„ì¸ AMC ëª¨ë¸ ë‘˜ë‹¤ Top-1 Accuracyê°€ ê¸°ì¡´ì˜ 1.0 MobileNetì˜ Accuracyë³´ë‹¤ ì•½ 0.1~0.4% ì •ë„ë§Œ ì¤„ê³  Latencyë‚˜ SpeedUpì´ íš¨ìœ¨ì ìœ¼ë¡œ ì¡°ì •ëœ ê²ƒì„ í™•ì¸í•  ìˆ˜ ìˆë‹¤.\n\n\n\n\n\n\nAMC ì‹¤í—˜ ê²°ê³¼í‘œì—ì„œ 0.75 MobileNetì˜ SpeedUpì´ ì™œ 1.7x ì¸ê°€ìš”?\n\n\n\n\n\nê²°ê³¼í‘œì—ì„œ 0.75 MobileNetì€ 25%ì˜ weightë¥¼ ê°ì†Œì‹œí‚¨ ê²ƒì´ê¸° ë•Œë¬¸ì— SpeedUpì´ \\(\\frac{4}{3} \\simeq 1.3\\)xì´ì–´ì•¼ í•œë‹¤ê³  ìƒê°í•  ìˆ˜ ìˆë‹¤. í•˜ì§€ë§Œ ì—°ì‚°ëŸ‰ì€ quadraticí•˜ê²Œ ê°ì†Œí•˜ê²Œ ë˜ê¸° ë•Œë¬¸ì— \\(\\frac{4}{3} \\cdot \\frac{4}{3} \\simeq 1.7\\)xë¡œ SpeedUpì´ ëœë‹¤.\n\n\n\n\n\n1.3 NetAdapt\në˜ ë‹¤ë¥¸ Pruning Ratioë¥¼ ì •í•˜ëŠ” ê¸°ë²•ìœ¼ë¡œ NetAdaptì´ ìˆë‹¤. Latency Constraintë¥¼ ê°€ì§€ê³  layerë§ˆë‹¤ pruningì„ ì ìš©í•´ë³¸ë‹¤. ì˜ˆë¥¼ ë“¤ì–´, ì¤„ì¼ ëª©í‘œ latency ëŸ‰ì„ lmsë¡œ ì •í•˜ë©´, 10ms â†’ 9msë¡œ ì¤„ ë•Œê¹Œì§€ layerì˜ pruning ratioë¥¼ ë†’ì—¬ê°€ëŠ” ë°©ë²•ì´ë‹¤.\n\n\n\nNetAdapt\n\n\nNetAdaptì˜ ì „ì²´ì ì¸ ê³¼ì •ì€ ì•„ë˜ì™€ ê°™ì´ ì§„í–‰ëœë‹¤. ê¸°ì¡´ ëª¨ë¸ì—ì„œ ê° layerë¥¼ Latency Constraintì— ë„ë‹¬í•˜ë„ë¡ Pruningí•˜ë©´ì„œ Accuracy(\\(Acc_A\\)ë“±)ì„ ë°˜ë³µì ìœ¼ë¡œ ì¸¡ì •í•œë‹¤.\n\nê° layerì˜ pruning ratioë¥¼ ì¡°ì ˆí•œë‹¤.\nShort-term fine tuningì„ ì§„í–‰í•œë‹¤.\nLatency Constraintì— ë„ë‹¬í–ˆëŠ”ì§€ í™•ì¸í•œë‹¤.\nLatency Constraint ë„ë‹¬í•˜ë©´ í•´ë‹¹ layerì˜ ìµœì ì˜ Pruning ratioë¡œ íŒë‹¨í•œë‹¤.\nê° layerì˜ ìµœì  Pruning ratioê°€ ì •í•´ì¡Œë‹¤ë©´ ë§ˆì§€ë§‰ìœ¼ë¡œ Long-term fine tuningì„ ì§„í–‰í•œë‹¤.\n\n\n\n\nNetAdapt ê³¼ì •\n\n\nì´ì™€ ê°™ì´ NetAdaptì˜ ê³¼ì •ì„ ì§„í–‰í•˜ë©´ ì•„ë˜ì™€ ê°™ì€ ì‹¤í—˜ ê²°ê³¼ë¥¼ ë³¼ ìˆ˜ ìˆë‹¤. Uniformí•˜ê²Œ Pruningì„ ì§„í–‰í•œ Multipilersë³´ë‹¤ NetAdaptê°€ 1.7x ë” ë¹ ë¥´ê³  ì˜¤íˆë ¤ AccuracyëŠ” ì•½ 0.3% ì •ë„ ë†’ì€ ê²ƒì„ ì•Œ ìˆ˜ ìˆë‹¤.\n\n\n\nNetAdaptì˜ Latency / Top-1 Accuracy ê·¸ë˜í”„"
  },
  {
    "objectID": "posts/lecs/lec04.html#fine-tuningtrain",
    "href": "posts/lecs/lec04.html#fine-tuningtrain",
    "title": "ğŸ§‘â€ğŸ« Lecture 4",
    "section": "2. Fine-tuning/Train",
    "text": "2. Fine-tuning/Train\nPrunned ëª¨ë¸ì˜ í¼í¬ë¨¼ìŠ¤ë¥¼ í–¥ìƒí•˜ê¸° ìœ„í•´ì„œëŠ” Pruningë¥¼ ì§„í–‰í•˜ê³  ë‚˜ì„œ Fine-tuning ê³¼ì •ì´ í•„ìš”í•˜ë‹¤.\n\n2.1 Iterative Pruning\në³´í†µ Pruned ëª¨ë¸ì˜ Fine-tuning ê³¼ì •ì—ì„œëŠ” ê¸°ì¡´ì— í•™ìŠµí–ˆë˜ learning rateë³´ë‹¤ ì‘ì€ rateë¥¼ ì‚¬ìš©í•œë‹¤. ì˜ˆë¥¼ë“¤ì–´ ê¸°ì¡´ì˜ ëª¨ë¸ì„ í•™ìŠµí•  ë•Œ ì‚¬ìš©í•œ learning rateì˜ \\(1/100\\) ë˜ëŠ” \\(1/10\\)ì„ ì‚¬ìš©í•œë‹¤. ë˜í•œ Pruning ê³¼ì •ê³¼ Fine-tuning ê³¼ì •ì€ 1ë²ˆë§Œ ì§„í–‰í•˜ê¸°ë³´ë‹¤ ì ì°¨ì ìœ¼ë¡œ pruning ratioë¥¼ ëŠ˜ë ¤ê°€ë©° Pruning, Fine-tuningì„ ë²ˆê°ˆì•„ê°€ë©° ì—¬ëŸ¬ë²ˆ ì§„í–‰í•˜ëŠ”ê²Œ ë” ì¢‹ë‹¤.\n\n\n\nIterative Pruning + Fine-tuning ë¹„êµ ê·¸ë˜í”„\n\n\n\n\n2.2 Regularization\nTinyMLì˜ ëª©í‘œëŠ” ê°€ëŠ¥í•œ ë§ì€ weightë“¤ì„ 0ìœ¼ë¡œ ë§Œë“œëŠ” ê²ƒìœ¼ë¡œ ìƒê°í•  ìˆ˜ ìˆë‹¤. ê·¸ë˜ì•¼ ëª¨ë¸ì„ ê°€ë³ê²Œ ë§Œë“¤ ìˆ˜ ìˆê¸° ë•Œë¬¸ì´ë‹¤. ê·¸ë˜ì„œ Regularization ê¸°ë²•ì„ ì´ìš©í•´ì„œ ëª¨ë¸ì˜ weightë“¤ì„ 0ìœ¼ë¡œ, í˜¹ì€ 0ê³¼ ê°€ê¹ê²Œ ì‘ì€ ê°’ì„ ê°€ì§€ë„ë¡ ë§Œë“ ë‹¤. ì‘ì€ ê°’ì˜ weightê°€ ë˜ë„ë¡ í•˜ëŠ” ì´ìœ ëŠ” 0ê³¼ ê°€ê¹Œìš´ ì‘ì€ ê°’ë“¤ì€ ë‹¤ìŒ layerë“¤ë¡œ ë„˜ì–´ê°€ë©´ì„œ 0ì´ ë  ê°€ëŠ¥ì„±ì´ ë†’ì•„ì§€ê¸° ë•Œë¬¸ì´ë‹¤. ê¸°ì¡´ì˜ ë”¥ëŸ¬ë‹ ëª¨ë¸ë“¤ì˜ ê³¼ì í•©(Overfitting)ì„ ë§‰ê¸° ìœ„í•œ Regularization ê¸°ë²•ë“¤ê³¼ ë‹¤ë¥´ì§€ ì•Šìœ¼ë‚˜ ì˜ë„ì™€ ëª©ì ì€ ë‹¤ë¥¸ ê²ƒì„ ì§šì–´ë³¼ ìˆ˜ ìˆë‹¤.\n\n\n\nPruningì„ ìœ„í•œ Regularization\n\n\n\n\n2.3 The Lottery Ticket Hypothesis\n2019ë…„ ICLRì—ì„œ ë°œí‘œëœ ë…¼ë¬¸ì—ì„œ Jonathan Frankleê³¼ Michael Carbinì´ ì†Œê°œí•œ The Lottery Ticket Hypothesis(LTH)ì€ ì‹¬ì¸µ ì‹ ê²½ë§(DNN) í›ˆë ¨ì— ëŒ€í•œ í¥ë¯¸ë¡œìš´ ì•„ì´ë””ì–´ë¥¼ ì œì•ˆí•œë‹¤. ë¬´ì‘ìœ„ë¡œ ì´ˆê¸°í™”ëœ ëŒ€ê·œëª¨ ì‹ ê²½ë§ ë‚´ì— ë” ì‘ì€ í•˜ìœ„ ë„¤íŠ¸ì›Œí¬(Winning Ticket)ê°€ ì¡´ì¬í•œë‹¤ëŠ” ê²ƒì„ ë§í•œë‹¤. ì´ í•˜ìœ„ ë„¤íŠ¸ì›Œí¬ëŠ” ì²˜ìŒë¶€í„° ë³„ë„ë¡œ í›ˆë ¨í•  ë•Œ ì›ë˜ ë„¤íŠ¸ì›Œí¬ì˜ ì„±ëŠ¥ì— ë„ë‹¬í•˜ê±°ë‚˜ ëŠ¥ê°€í•  ìˆ˜ ìˆë‹¤. ì´ ê°€ì„¤ì€ ì´ëŸ¬í•œ Winning Ticketì´ í•™ìŠµí•˜ëŠ” ë° ì í•©í•œ ì´ˆê¸° ê°€ì¤‘ì¹˜ë¥¼ ê°–ëŠ”ë‹¤ê³  ê°€ì •í•œë‹¤.\n\n\n\nLTH ì„¤ëª… ê·¸ë¦¼"
  },
  {
    "objectID": "posts/lecs/lec04.html#system-support-for-sparsity",
    "href": "posts/lecs/lec04.html#system-support-for-sparsity",
    "title": "ğŸ§‘â€ğŸ« Lecture 4",
    "section": "3. System Support for Sparsity",
    "text": "3. System Support for Sparsity\nDNNì„ ê°€ì†í™” ì‹œí‚¤ëŠ” ë°©ë²•ì€ í¬ê²Œ 3ê°€ì§€, Sparse Weight, Sparse Activation, Weight Sharingì´ ìˆë‹¤. Sparse Weight, Sparse Activationì€ Pruningì´ê³  Weight Sharingì€ Quantizationì˜ ë°©ë²•ì´ë‹¤.\n\n\n\nDNNì„ ê°€ì†í™” ì‹œí‚¤ëŠ” ë°©ë²•\n\n\n\nSparse Weight: Weightë¥¼ Pruningí•˜ì—¬ Computationì€ Pruning Ratioì— ëŒ€ì‘í•˜ì—¬ ë¹¨ë¼ì§„ë‹¤. í•˜ì§€ë§Œ MemoryëŠ” Pruningëœ weightì˜ ìœ„ì¹˜ë¥¼ ê¸°ì–µí•˜ê¸° ìœ„í•œ memory ìš©ëŸ‰ì´ í•„ìš”í•˜ë¯€ë¡œ Pruning Ratioì— ë¹„ë¡€í•˜ì—¬ ì¤„ì§„ ì•ŠëŠ”ë‹¤.\nSparse Activation: Weightë¥¼ Pruningí•˜ëŠ” ê²ƒê³¼ ë‹¤ë¥´ê²Œ Activationì€ Test Inputì— ë”°ë¼ dynamic í•˜ë¯€ë¡œ Weightë¥¼ Pruningí•˜ëŠ” ê²ƒë³´ë‹¤ Computationì´ ëœ ì¤„ì–´ë“ ë‹¤.\nWeight Sharing: Quantization ë°©ë²•ìœ¼ë¡œ 32-bit dataë¥¼ 4-bit dataë¡œ ë³€ê²½í•¨ìœ¼ë¡œì¨ 8ë°°ì˜ memory ì ˆì•½ì„ í•  ìˆ˜ ìˆë‹¤.\n\n\n3.1 EIE\nEfficient Inference Engineì€ ê¸°ê³„ í•™ìŠµ ëª¨ë¸ì„ ì‹¤ì‹œê°„ìœ¼ë¡œ ì‹¤í–‰í•˜ê¸° ìœ„í•´ ìµœì í™”ëœ ì†Œí”„íŠ¸ì›¨ì–´ ë¼ì´ë¸ŒëŸ¬ë¦¬ë‚˜ í”„ë ˆì„ì›Œí¬ë¥¼ ë§í•œë‹¤. Processing Elements(PE)ì˜ êµ¬ì¡°\n\n\n\nPE ì—°ì‚° Logically / Physically ë¶„ì„\n\n\nì•„ë˜ ê·¸ë¦¼ì—ì„œ Inputë³„(\\(\\vec{a}\\)) ì—°ì‚°ì€ ì•„ë˜ì™€ ê°™ì´ Inputì´ 0ì¼ ë•ŒëŠ” skipë˜ê³  0ì´ ì•„ë‹ ë•ŒëŠ” prunning ë˜ì§€ ì•Šì€ weightì™€ ì—°ì‚°ì´ ì§„í–‰ëœë‹¤.\n\n\n\nInputë³„ ì—°ì‚° ê³¼ì •\n\n\nEIE ì‹¤í—˜ì€ ê°€ì¥ lossê°€ ì ì€ data ìë£Œí˜•ì¸ 16 bit Intí˜•ì„ ì‚¬ìš©í–ˆë‹¤.(0.5% loss) AlexNetì´ë‚˜ VGGì™€ ê°™ì´ ReLU Activationì´ ë§ì´ ì‚¬ìš©ë˜ëŠ” ëª¨ë¸ë“¤ì€ ê²½ëŸ‰í™”ê°€ ë§ì´ ëœ ë°˜ë©´, RNNì™€ LSTMì´ ì‚¬ìš©ëœ NeuralTalk ëª¨ë¸ë“¤ ê°™ì€ ê²½ìš°ì—ëŠ” ReLUë¥¼ ì‚¬ìš©í•˜ì§€ ì•Šì•„ ê²½ëŸ‰í™”ë  ìˆ˜ ìˆëŠ” ë¶€ë¶„ì´ ì—†ì–´ Activation Densityê°€ 100%ì¸ ê²ƒì„ í™•ì¸í•  ìˆ˜ ìˆë‹¤.\n\n\n\nEIE ì‹¤í—˜ ê²°ê³¼\n\n\n\n\n3.2 M:N Weight Sparsity\nì´ ë°©ë²•ì€ Nvidia í•˜ë“œì›¨ì–´ì˜ ì§€ì›ì´ í•„ìš”í•œ ë°©ë²•ìœ¼ë¡œ ë³´í†µ 2:4 Weight Sparsityë¥¼ ì‚¬ìš©í•œë‹¤. ì™¼ìª½ì˜ Sparse Matrixë¥¼ ì¬ë°°ì¹˜í•´ì„œ Non-zero data matrixì™€ ì¸ë±ìŠ¤ë¥¼ ì €ì¥í•˜ëŠ” Index matrixë¥¼ ë”°ë¡œ ë§Œë“¤ì–´ì„œ ì €ì¥í•œë‹¤.\n\n\n\n2:4 Weight Sparsity\n\n\nM:N Weight Sparsity ì ìš©í•˜ì§€ ì•Šì€ Dense GEMMê³¼ ì ìš©í•œ Sparse GEMMì„ ê³„ì‚°í•  ë•ŒëŠ” ì•„ë˜ì˜ ê·¸ë¦¼ê³¼ ê°™ì€ ê³¼ì •ìœ¼ë¡œ ì—°ì‚°ì´ ì§„í–‰ëœë‹¤.\n\n\n\nDense VS. Sparse GEMM\n\n\n\n\n3.3 Sparse Convolution\nSubmanifold Sparse Convolutional Networks (SSCN)ì€ ê³ ì°¨ì› ë°ì´í„°ì—ì„œ íš¨ìœ¨ì ì¸ ê³„ì‚°ì„ ê°€ëŠ¥í•˜ê²Œ í•˜ëŠ” ì‹ ê²½ë§ ì•„í‚¤í…ì²˜ì˜ í•œ í˜•íƒœì´ë‹¤. ì´ ê¸°ìˆ ì€ íŠ¹íˆ 3D í¬ì¸íŠ¸ í´ë¼ìš°ë“œ ë˜ëŠ” ê³ í•´ìƒë„ ì´ë¯¸ì§€ì™€ ê°™ì´ ëŒ€ê·œëª¨ ë° ê³ ì°¨ì› ë°ì´í„°ë¥¼ ì²˜ë¦¬í•  ë•Œ ì¤‘ìš”í•˜ë‹¤. SSCNì˜ í•µì‹¬ ì•„ì´ë””ì–´ëŠ” ë°ì´í„°ì˜ Sparcityì„ í™œìš©í•˜ì—¬ ê³„ì‚°ê³¼ ë©”ëª¨ë¦¬ ì‚¬ìš©ëŸ‰ì„ í¬ê²Œ ì¤„ì´ëŠ” ê²ƒì´ë‹¤.\n\n\n\nì¶œì²˜: Submanifold Sparse Convolutional Networks\n\n\nì´ëŸ¬í•œ Sparse Convolutionì€ ê¸°ë³¸ Convolutionê³¼ ë¹„êµí–ˆì„ ë•Œ ì•„ë˜ ê·¸ë¦¼ê³¼ ê°™ì´ ë‚˜íƒ€ë‚´ë³¼ ìˆ˜ ìˆë‹¤.\n\n\n\nConventional VS. Sparse Convolution\n\n\nì—°ì‚° ê³¼ì •ì„ ë¹„êµí•´ë³´ê¸° ìœ„í•´ Input Point Cloud(\\(P\\)), Feature Map(\\(W\\)), Ouput Point Cloud(\\(Q\\))ë¥¼ ì•„ë˜ì™€ ê°™ì´ ìˆë‹¤ê³  í•˜ì. ê¸°ì¡´ì˜ Convolutionê³¼ Sparse Convolutionì„ ë¹„êµí•´ë³´ë©´ ì—°ì‚°ëŸ‰ì´ 9:2ë¡œ ë§¤ìš° ì ì€ ì—°ì‚°ë§Œ í•„ìš”í•œ ê²ƒì„ ì•Œ ìˆ˜ ìˆë‹¤.\n\n\n\nConventional VS. Sparse ì—°ì‚°ëŸ‰ ë¹„êµ\n\n\nFeature Map(\\(W\\))ì„ ê¸°ì¤€ìœ¼ë¡œ ê° weight ë§ˆë‹¤ í•„ìš”í•œ Input dataì˜ í¬ê¸°ê°€ ë‹¤ë¥´ë‹¤. ì˜ˆë¥¼ ë“¤ì–´ \\(W_{-1, 0}\\)ì€ \\(P1\\)ê³¼ ë§Œì˜ ì—°ì‚°ì´ ì§„í–‰ë˜ë¯€ë¡œ \\(P1\\)ë§Œ ì—°ì‚°ì‹œ ë¶ˆëŸ¬ë‚´ê²Œ ëœë‹¤.\n\n\n\nSparse Convolution ê³„ì‚° ê³¼ì •\n\n\në”°ë¼ì„œ Feature Mapì˜ \\(W\\)ì— ë”°ë¼ í•„ìš”í•œ Input dataë¥¼ í‘œí˜„í•˜ê³  ë”°ë¡œ computationì„ ì§„í–‰í•˜ë©´ ì•„ë˜ì™€ ê°™ì´ ê³ ë¥´ì§€ ëª»í•œ ì—°ì‚°ëŸ‰ ë¶„ë°°ê°€ ì§„í–‰ë˜ëŠ”ë°(ì™¼ìª½ ê·¸ë¦¼) ì´ëŠ” computationì— overheadëŠ” ì—†ì§€ë§Œ regularityê°€ ì¢‹ì§€ ì•Šë‹¤. ë˜ëŠ” ê°€ì¥ computationì´ ë§ì€ ê²ƒì„ ê¸°ì¤€ìœ¼ë¡œ Batch ë‹¨ìœ„ë¡œ ê³„ì‚°í•˜ê²Œ ëœë‹¤ë©´(ê°€ìš´ë° ê·¸ë¦¼) ì ì€ computation weightì—ì„œì˜ ë¹„íš¨ìœ¨ì ì¸ ê³„ì‚° ëŒ€ê¸°ì‹œê°„ì´ ìƒê¸°ë¯€ë¡œ overheadê°€ ìƒê¸´ë‹¤. ë”°ë¼ì„œ ì ì ˆíˆ ë¹„ìŠ·í•œ ì—°ì‚°ëŸ‰ì„ ê°€ì§€ëŠ” groupingì„ ì§„í–‰í•œ ë’¤ batchë¡œ ë¬¶ìœ¼ë©´ ì ì ˆíˆ computationì„ ì§„í–‰í•  ìˆ˜ ìˆë‹¤.(ì˜¤ë¥¸ìª½ ê·¸ë¦¼)\n\n\n\nGrouping Computation\n\n\nì´ëŸ° Groupingì„ ì ìš©í•œ í›„ Sparse Convolutionì„ ì§„í–‰í•˜ë©´ Adaptive Groupingì´ ì ìš©ë˜ì–´ ì•„ë˜ì™€ ê°™ì´ ì§„í–‰ëœë‹¤.\n\n\n\nSparse Convolution ì˜ˆì‹œ\n\n\nì—¬ê¸°ê¹Œì§€ê°€ 2023ë…„ë„ ê°•ì˜ì—ì„œ ë§ˆì§€ë§‰ Sparse Convolutionì— ëŒ€í•´ ì„¤ëª…í•œ ë¶€ë¶„ì„ ì •ë¦¬í•œ ë¶€ë¶„ì´ë‹¤. í•˜ì§€ë§Œ ê°•ì˜ì—ì„œ ì„¤ëª…ì´ ë§ì´ ìƒëµë˜ì–´ ìˆìœ¼ë¯€ë¡œ ì¢€ ë” ìì„¸í•œ ë‚´ìš©ì€ Youtube ë°œí‘œ ì˜ìƒì´ë‚˜ 2022ë…„ë„ ê°•ì˜ë¥¼ ì°¸ê³ í•˜ëŠ” ê²ƒì„ ê¶Œì¥í•œë‹¤."
  },
  {
    "objectID": "posts/lecs/lec04.html#reference",
    "href": "posts/lecs/lec04.html#reference",
    "title": "ğŸ§‘â€ğŸ« Lecture 4",
    "section": "4. Reference",
    "text": "4. Reference\n\nMIT-TinyML-lecture04-Pruning-2\nAMC: Automl for Model Compression and Acceleration on Mobile Devices, 2018\nContinuous control with deep reinforcement learning\nFLOPsë€? ë”¥ëŸ¬ë‹ ì—°ì‚°ëŸ‰ì— ëŒ€í•´ì„œ\nNetAdapt: Platform-Aware Neural Network Adaptation for Mobile Applications\nThe Lottery Ticket Hypothesis: Finding Sparse, Trainable Neural Networks\nThe Lottery Ticket Hypothesis Finding Sparse, Trainable Neural Networks ë…¼ë¬¸ ë¦¬ë·°\nLLM Inference - HW/SW Optimizations\nAccelerating Sparse Deep Neural Networks\nSubmanifold Sparse Convolutional Networks\nTorchSparse: Efficient Point Cloud Inference Engine\nTorchSparse++: Efficient Training and Inference Framework for Sparse Convolution on GPUs\nmit-han-lab/torchsparse\nMLSysâ€™22 TorchSparse: Efficient Point Cloud Inference Engine"
  },
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "Study Archive",
    "section": "",
    "text": "ìŠ¤í„°ë”” ìë£Œì™€ ê´€ë ¨í•´ì„œ ì–´ë–¤ í† ì˜ë‚˜ ì˜ê²¬ ëª¨ë‘ ê°ì‚¬í•©ë‹ˆë‹¤! Github Discussionì— ê¸€ì„ ë‚¨ê²¨ì£¼ì…”ë„ ì¢‹ê³  ê° í¬ìŠ¤íŒ… í•˜ë‹¨ì— ìˆëŠ” Giscus ëŒ“ê¸€ì°½ì— ì½”ë©˜íŠ¸ë“¤ì„ ë‚¨ê²¨ì£¼ì‹œë©´ ë©ë‹ˆë‹¤.\nWelcome to any comments or opinions on our content! You can leave messages using the direct Discussion or the Giscus window on each post.\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nğŸ§‘â€ğŸ« Lecture 5-6\n\n\n\n\n\n\nTinyML\n\n\nEdgeAI\n\n\n\nQuantization\n\n\n\n\n\nMar 5, 2024\n\n\nSeunghyun Oh(ooshyun)\n\n\n\n\n\n\n\n\n\n\n\n\nğŸ§‘â€ğŸ« Lecture 4\n\n\n\n\n\n\npruning\n\n\nlecture\n\n\n\nPruning and Sparsity (Part II)\n\n\n\n\n\nFeb 18, 2024\n\n\nJung Yeon Lee\n\n\n\n\n\n\n\n\n\n\n\n\nğŸ§‘â€ğŸ« Lecture 3\n\n\n\n\n\n\npruning\n\n\nlecture\n\n\n\nPruning and Sparsity (Part I)\n\n\n\n\n\nJan 28, 2024\n\n\nSeunghyun Oh\n\n\n\n\n\n\n\n\n\n\n\n\nğŸ‘©â€ğŸ’» Lab 0\n\n\n\n\n\n\nlab\n\n\npytorch\n\n\nbasic\n\n\n\nPyTorch Tutorial\n\n\n\n\n\nJan 11, 2024\n\n\nJung Yeon Lee\n\n\n\n\n\n\nNo matching items"
  },
  {
    "objectID": "posts/lecs/lec05.html#reference",
    "href": "posts/lecs/lec05.html#reference",
    "title": "ğŸ§‘â€ğŸ« Lecture 5-6",
    "section": "7. Reference",
    "text": "7. Reference\n\nTinyML and Efficient Deep Learning Computing on MIT HAN LAB\nYoutube for TinyML and Efficient Deep Learning Computing on MIT HAN LAB\nDeep Compression [HanÂ et al., ICLR 2016]\nQuantization and Training of Neural Networks for Efficient Integer-Arithmetic-Only Inference [JacobÂ et al., CVPR 2018]\nWith Shared Microexponents, A Little Shifting Goes a Long Way [Bita Rouhani et al.]\nData-Free Quantization Through Weight Equalization and Bias Correction [Markus et al., ICCV 2019]\nEstimating or Propagating Gradients Through Stochastic Neurons for Conditional Computation [Bengio, arXiv 2013]\nBinarized Neural Networks: Training Deep Neural Networks with Weights and Activations Constrained to +1 orÂ âˆ’1. [CourbariauxÂ et al., Arxiv 2016]\nXNOR-Net: ImageNet Classification using Binary Convolutional Neural Networks [RastegariÂ et al., ECCV 2016]\nTernary Weight Networks [LiÂ et al., Arxiv 2016]\nTrained Ternary Quantization [ZhuÂ et al., ICLR 2017]\nDoReFa-Net: Training Low Bitwidth Convolutional Neural Networks with Low Bitwidth Gradients [ZhouÂ et al., arXiv 2016]\nWRPN: Wide Reduced-Precision Networks [MishraÂ et al., ICLR 2018]\nPACT: Parameterized Clipping Activation for Quantized Neural Networks [ChoiÂ et al., arXiv 2018]\nHAQ: Hardware-Aware Automated Quantization with Mixed Precision [WangÂ et al., CVPR 2019]"
  }
]