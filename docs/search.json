[
  {
    "objectID": "about.html",
    "href": "about.html",
    "title": "TinyML Study Group",
    "section": "",
    "text": "@Gueltto9th"
  },
  {
    "objectID": "about.html#lec-4lab",
    "href": "about.html#lec-4lab",
    "title": "TinyML Study Group",
    "section": "21Lec + 4Lab",
    "text": "21Lec + 4Lab\n\nLec1/2와 Lab0은 제외\n강의를 듣고 1명씩 돌아가면서 강의 복습 recap 발표\n다른 사람들은 질문/디스커션 토픽 가져오기\n주 1회 (약 16주 - 4개월 이내 완료 목표)\n\n\n\n[Chapter I: Efficient Inference]\n\nLecture 3: Pruning and Sparsity (Part I) @ooshyun\nLecture 4: Pruning and Sparsity (Part II) @curieuxjy\nLab 1 @CastleFlag\nLecture 5: Quantization (Part I) @ooshyun\nLecture 6: Quantization (Part II) @ooshyun\nLab 2 @curieuxjy\nLecture 7: Neural Architecture Search (Part I) @CastleFlag\nLecture 8: Neural Architecture Search (Part II) @CastleFlag\nLab 3 @ooshyun\nLecture 9: Knowledge Distillation @ooshyun\nLecture 10: MCUNet: TinyML on Microcontrollers @curieuxjy\nLecture 11: TinyEngine and Parallel Processing @curieuxjy\n\n\n\n[Chapter II: Domain-Specific Optimization]\n\nLecture 12: Transformer and LLM (Part I) @CastleFlag\nLecture 13: Transformer and LLM (Part II) @CastleFlag\nLecture 14: Vision Transformer @ooshyun\nLab 4 @ooshyun\nLecture 15: GAN, Video, and Point Cloud @curieuxjy\nLecture 16: Diffusion Model @curieuxjy\nLecture 17: Distributed Training (Part I) @CastleFlag\nLecture 18: Distributed Training (Part II) @CastleFlag\nLab 5 @ooshyun\nLecture 19: On-Device Training and Transfer Learning @ooshyun\nLecture 20: Efficient Fine-tuning and Prompt Engineering @curieuxjy\n\n\n\n[Chapter IV: Advanced Topics]\n\nLecture 21: Basics of Quantum Computing @curieuxjy\nLecture 22: Quantum Machine Learning @CastleFlag\nLecture 23: Noise Robust Quantum ML @CastleFlag"
  },
  {
    "objectID": "posts/lecs/lec09.html",
    "href": "posts/lecs/lec09.html",
    "title": "🧑‍🏫 Lecture 9",
    "section": "",
    "text": "이번 시간은 Knowledge Distillation 기법에 대해서 이야기 해볼까 해요. 지금까지 작은 크기의 모델을 만드는 방법에 대해서 알아봤지만, 여전히 작은 모델은 성능적으로 부족한 점이 많죠. 성능을 개선시키는 다른 방법에 대해서 고민하다가 “크기가 큰 모델을 이용해보자.” 에서 나온 아이디어가 바로 Knowledge Distillation 입니다."
  },
  {
    "objectID": "posts/lecs/lec09.html#what-is-knowledge-distillation",
    "href": "posts/lecs/lec09.html#what-is-knowledge-distillation",
    "title": "🧑‍🏫 Lecture 9",
    "section": "1. What is Knowledge Distillation?",
    "text": "1. What is Knowledge Distillation?\nKnowledge Distillation은 간단하게 Teach Network라고 불리는 크기가 큰 모델이 있어요. 이 Teacher Network가 먼저 Training을 합니다. 그런 다음 오늘의 주인공 Student Network로 불리는 크기가 작은 모델이 있죠. 이 모델은 두 가지 방식으로 학습을 하는데, 첫 번째는 기존에 학습하던 대로 Target 데이터로부터 학습이 있구요. 다른 한 가지는 Teacher Network를 따라가는 학습이 있습니다.\n\n\n\nReference. Knowledge Distillation: A Survey [Gou et al., IJCV 2020]\n\n\n\nThe goal of knowledge distillation is to align the class probability distributions from teacher and student networks.\n\n그럼 궁금한 점이 Teacher Network에 어떤 점을 배워야할까요? 강의에서는 총 6개로 Output logit, Intermediate weight, Intermediate feature, Gradient, Sparsity pattern, Relational information으로 나눠서 설명합니다. 설명하기에 앞서서 개념하나 소개하고 넘어갈께요.\n\n\n\nReference. MIT-TinyML-lecture10-Knowledge-Distillation in https://efficientml.ai\n\n\n만약 위 Teacher Network가 학습한 결과(T=1)로 고양이 사진일 확률 0.982, 강아지 사진일 확률이 0.017이라고 합시다. 그럼 Student Network는 Output logit을 학습한다고 가정하면, 이 두 확률을 따라 갈겁니다. 하지만 Student Network에 따라 이 수치까지 학습하기 어려울 수 있습니다. 이 때 “Temperature(T)”이라는 개념을 들여와 Teacher Network의 Cat과 Dog에 대한 확률을 좀 더 Smooth하게 만들죠.\n\\[\np(z_i,T) = \\dfrac{exp(z_j/T)}{\\sum_j exp(z_j/T)}\n\\]\n식으로 쓰면 위와 같이 될텐데, 보통은 1로 두고 한다고 강의에서 언급합니다. 왜 설명했냐구요? 혹시나 개념이 나오면 이해하시기 편하시라구요🙂 그럼, Teacher Network에서 어떤 부분을 Student Network에 학습시킬지 알아보시죠."
  },
  {
    "objectID": "posts/lecs/lec09.html#what-to-match-between-teacher-and-student-network",
    "href": "posts/lecs/lec09.html#what-to-match-between-teacher-and-student-network",
    "title": "🧑‍🏫 Lecture 9",
    "section": "2. What to match between Teacher and Student Network?",
    "text": "2. What to match between Teacher and Student Network?\n\n2.1 Output logits\n첫 번째는 Output logit입니다. loss로는 대표적으로 Cross entropy loss와 L2 loss가 있겠죠.\n\n\n\nReference. MIT-TinyML-lecture10-Knowledge-Distillation in https://efficientml.ai\n\n\n\n\n2.2 Intermediate weights\n두 번째는 Layer마다 Weight입니다. 하지만 Student Model은 Weight dimesion이 다를 수 밖에 없는데, 그럼 Linear Transformation을 이용해서 Dimension을 맞춰 학습하면 되겠네요.\n\n\n\nReference. MIT-TinyML-lecture10-Knowledge-Distillation in https://efficientml.ai\n\n\n\n\n\nReference. MIT-TinyML-lecture10-Knowledge-Distillation in https://efficientml.ai\n\n\n스터디 중에 나온 질문이 “그럼 Student Network에서 추가적인 레이어가 생기는데, 작게 만드는 의미가 없지 않느냐?” 였습니다. 제 생각은 Weight Dimension을 맞추기 위한 Linear Transformation을 위한 레이어는 추론시 사용하지 않을 수 있으니, Student Network의 성능을 높이기 더 제격아닐까요? 마치 추론 때 필요한 부품만 조립하듯 말이죠.\n\n\n2.3 Intermediate features\n세 번째는 Feature 입니다. 이전 경우가 Weight라고 하면, 이번은 Layer의 Output입니다. Teach Network과 Student Network의 Feature를 같게 학습시키는 방법은 여러가지가 있는데, 여기서는 Cosine of angle로 학습을 시키는 방법(Like What You Like: Knowledge Distill via Neuron Selectivity Transfer [Huang and Wang, arXiv 2017]) 과 Dimension을 줄여서 학습을 시키는 방법(Paraphrasing Complex Network: Network Compression via Factor Transfer [Kim et al., NeurIPS 2018])을 소개합니다.\n\n\n\nReference. Like What You Like: Knowledge Distill via Neuron Selectivity Transfer [Huang and Wang, arXiv 2017]\n\n\n\n\n\nReference. Paraphrasing Complex Network: Network Compression via Factor Transfer [Kim et al., NeurIPS 2018]\n\n\n\nThe paraphraser shrinks the output teacher feature map from m dimensions to m x k dimensions (called factor typically k=0.5) and then expands the dimensionality back to m.\nThe output of paraphraser is supervised with a reconstruction loss against the original m-dimensional output.\nStudent uses one layer of MLP to obtain a factor with the same dimensionality of m x k.\nFT minimizes the distance between teacher and student factors.\n\n\n\n2.4 Gradients\n네 번째는 Gradient 입니다. Gradient를 시각적으로 볼 수 있는 방법에는 Attention Map이 있는데요, 이 Attention Map은 이미지에서 특징적인 부분을 잡아낼 수 있죠.\n\nReference: Paying More Attention to Attention: Improving the Performance of Convolutional Neural Networks via Attention Transfer [Zagoruyko and Komodakis, ICLR 2017]\nGradients of feature maps are used to characterize attention of DNNs\nThe attention of a CNN feature map \\(x\\) is defined as \\(\\dfrac{\\partial L}{\\partial x}\\), where \\(L\\) is the learning objective.\nIntuition: If \\(\\dfrac{\\partial L}{\\partial x_{i,j}}\\) is large, a small perturbation at \\(i,j\\) will significantly impact the final output. As a result, the network is putting more attention on position \\(i, j\\)\n\n\n\n\nReference. MIT-TinyML-lecture10-Knowledge-Distillation in https://efficientml.ai\n\n\n아래 그림은 “Attention Map이 모델의 성능이 높다면 비슷한 패턴으로 나온다.” 는 예시로 나옵니다. Resnet34와 ResNet101의 Attention Map은 유사하게 보이는 반면 NIN인 경우는 많이 다른 것을 확인 할 수 있습니다.\n\nPerformant models have similar attention maps\nAttention makes of performant ImageNet models (ResNets) are indeed similar to each other, but the less performant model(NIN) has quite different attention maps\n\n\n\nReference. MIT-TinyML-lecture10-Knowledge-Distillation in https://efficientml.ai\n\n\n\n\n\n2.5 Sparsity patterns\n다섯 번째는 Sparsity Pattern 입니다. Layer마다 Output Acitivation을 같게 만드는 방법인데, Intermediate Feature와 유사하게 보이네요.\n\n\n\nReference. Knowledge Transfer via Distillation of Activation Boundaries Formed by Hidden Neurons [Heo et al., AAAI 2019]\n\n\n\nIntuition: the teacher and student networks should have similar sparsity patterns after the ReLU activation. A neuron is activated after ReLU if its value is larger than 0, denoted by the indicator function \\(\\rho(x) = 1 [x&gt;0]\\).\nWe want to minimize \\(\\mathscr{L}(I) = \\lvert\\lvert \\rho(T(I))-\\rho(S(I)) \\lvert\\lvert_1\\), where \\(S\\) and \\(T\\) corresponds to student and teacher networks, respectively\n\n\n\n2.6.1 Relational information: Different Layers\n마지막으로 모델내에서 나오는 텐서의 상호 연관성에 대해서도 같게 할 수 있다는 방법 두 가지가 나옵니다. 첫 번째는 각 레이어의 입력, 출력 텐서를 Inner product 하게 되면 하나의 Matrix를 얻을 수 있는데, 이 Matrix를 같게 학습시킨다는 아이디어죠.\n\n\n\nReference: A Gift from Knowledge Distillation: Fast Optimization, Network Minimization and Transfer Learning [Yim et al., CVPR 2017]\n\n\n\nUse inner product to extract relational information (a matrix of shape \\(C_{in} \\times C_{out}\\), reduction on the spatial dimensions) for both student and teacher networks. *Note: the student and teacher networks only differ in number of layers, not number of channels\nThen match the resulting dot products between teacher and student networks \\((G_1^T, G_1^S)\\)\n\n\n\n2.6.1 Relational information: Different Samples\n두 번째는 이전까지 저희는 학습데이터 하나하나마다 나오는 결과를 Teach와 Student를 같게끔 학습시켰는데, 이번엔 여러 학습데이터에서 나온 여러 Output을 하나의 Matrix 형태로 닮게 만드는 방법입니다.\n\nConventional KD focuses on matching features / logins for one input. Relation KD looks at the relations between intermediate features from multiple inputs.\n\n\n\n\nReference. Relational Knowledge Distillation [Park et al., CVPR 2019]\n\n\n\nRelation between different samples\n\n\n\nReference. Relational Knowledge Distillation [Park et al., CVPR 2019]\n\n\n\n지금까지 Student Network가 Teacher Network의 어떤 Output을 가지고 학습시킬지에 대해서 알아봤습니다. 그런데 저희가 TinyML을 하는 목적은 사실 “더 작은 네트워크를 사용해보자.” 이지 않았나요? 즉, Teacher Network없이 Student Network만으로는 할 수 있는 방법이 없을까요? 이런 생각에서 나온 아이디어가 Self and Online Distillation 입니다."
  },
  {
    "objectID": "posts/lecs/lec09.html#self-and-online-distillation",
    "href": "posts/lecs/lec09.html#self-and-online-distillation",
    "title": "🧑‍🏫 Lecture 9",
    "section": "3. Self and Online Distillation",
    "text": "3. Self and Online Distillation\n\nWhat is the disadvantage of fixed large teachers? Does it have to be the case that we need a fixed large teacher in KD?\n\n\n3.1 Self Distillation\n첫 번째 Self Distillation은 구조가 같은 네트워크를 계속해서 복사해 나갑니다. 그러면서 이전에 학습한 네트워크로 부터도 복사된 네트워크가 학습할 수 있도록 하며, k개 만큼 복사하며 학습한 후에 최종 Output으로는 복사한 네트워크들의 Output을 Ensemble한 결과를 이용하죠. 여기서 Accuracy는 k번째로 갈수록 늘어나겠죠?\n\n\n\nBorn-Again Neural Networks [Furlanello et al., ICML 2018]\n\n\n\nBorn-Again Networks generalizes defensive distillation by adding iterative training states and using both classification objective and distillation objective in subsequent stages.\nNetwork architecture \\(T = S_1=S_2=\\dots=S_k\\)\nNetwork accuracy \\(T &lt; S_1 &lt; S_2 &lt; \\dots &lt; S_k\\)\nCan alteratively ensemble \\(T,S_1, S_2, \\dots, S_k\\) to get even better performance\n\n\n\n3.2 Online Distillation\n두 번째는 Online Distillation인데, 여기 아이디어는 “같은 네트워크 구조를 쓰자.” 입니다. 그리고 Teacher network과 Student network는 처음부타 같이 학습하는데, Loss에 한 가지 항이 추가되죠. 바로 “KL Divergence” 입니다.\n\n\n\nReference: Deep Mutual Learning [Zhang et al., CVPR 2018]\n\n\n\nIdea: for both teach and student networks, we want to add a distillation objective that minimizes the output distribution of the other party.\n\\(\\mathscr{L}(S) = CrossEntropy(S(I), y)+KL(S(I), T(I))\\)\n\\(\\mathscr{L}(T) = CrossEntropy(T(I), y)+KL(S(I), T(I))\\)\nIt is not necessary to retrain \\(T\\) and \\(S=T\\) is allowed\n\n\n\nReference. Deep Mutual Learning [Zhang et al., CVPR 2018]\n\n\n\n\n\n3.3 Combined Distillation\n마지막은 Self 와 Online Distillation을 합친 연구들을 소개할게요.\n첫 번째는 On-the-Fly Native Ensemble 입니다. 구조를 보시면 Branch 마다 모델의 구조도 동일하게 Branch 0, Branch 1, … , Branch m 으로 나뉘는 게 Self Distillation를 보는 듯하죠. 그리고 각 Branch를 학습시 동시에 진행하는 것으로 보이네요.\n\n\n\nReference. Knowledge Distillation by On-the-Fly Native Ensemble [Lan et al., NeurlPS 2018]\n\n\n\nIdea: generating multiple output probability distributions and ensemble them as the target distribution for knowledge distillation.\nSimilar to DML(Deep Mutual Learning), ONE allows the teacher model to be exactly the same as the student model, and it does not require retraining the teach network first. It is also not necessary to train two models as in DML.\nResult\n\n\n\nReference. Knowledge Distillation by On-the-Fly Native Ensemble [Lan et al., NeurlPS 2018]\n\n\n\n두 번째 연구는 Be Your Own Teacher 라는 연구인데, 여기서는 각 레이어마다 나온 Feature map에 추가적인 레이어를 붙여서 Self Distillation의 방법을 이용합니다. Loss로는 Cross entropy(Output Logit), 추가적으로 붙여서 만든 각 모델마다 KL Divergence, 그리고 intermediate feature를 사용하네요. 흥미로웠던 점은 첫 번째, 두 번째 레이어에서는 성능이 거의 나오지 않을 것 같았는데 두 번째 레이어부터는 Ensemble까지 어느정도 성능이 나오는 것을 결과에서 볼 수 있어요.\n\n\n\nReference. Be Your Own Teacher: Improve the Performance of Convolutional Neural Networks via Self Distillation [Zhang et al., ICCV 2019]\n\n\n\nUse deeper layers to distill shallower layers.\nIntuition: Labels at later stages are more reliable, so the authors use them to supervise the predictions from the previous stages.\nResult\n\n\n\nReference. Be Your Own Teacher: Improve the Performance of Convolutional Neural Networks via Self Distillation [Zhang et al., ICCV 2019]"
  },
  {
    "objectID": "posts/lecs/lec09.html#distillation-for-different-tasks",
    "href": "posts/lecs/lec09.html#distillation-for-different-tasks",
    "title": "🧑‍🏫 Lecture 9",
    "section": "4. Distillation for different tasks",
    "text": "4. Distillation for different tasks\n이렇게 알아본 Knowledge Distillation은 어떤 Application에 사용될 수 있을까요? 강의는 Object Detection, Semantic Segmentation, GAN, Transformer 모델로 나눠서 이야기합니다. 각 부분마다 어떤 문제를 해결하는지 혹은 어떤 아이디어를 사용했는지만 짚고 넘어가볼게요(자세한 내용은 논문을 참조!).\n\n4.1 Object Detection\nObject Detection은 세 가지로 해결해야할 문제가 늘어났습니다. 하나는 Classification, 그동안 해왔던 부분이구요, 다른 두 개는 Background와 Foreground을 구분하는 것과 Bounding block 문제 입니다.\n\n\n\nReference. Object Detection: Learning Efficient Object Detection Models with Knowledge Distillation [Chen et al., NeurIPS 2017]\n\n\n이 연구는 Classifcation과 Background, Foreground 문제를 위해 세 가지 Loss를 사용합니다. 하나는 Feature, 그리고 Output Logit에서 Background, Foreground를 각각 다른 Weight를 준 Cross Entropy, 마지막은 Bounded 한 Regression Loss 입니다.\n\n\n\nReference. Localization: Localization Distillation for Dense Object Detection [Zheng et al., CVPR 2022]\n\n\n그럼 Bounding block 은 어떻게 해결할까요? 이 논문에서는 X축과 Y축으로 6개로 나눠진 구역에서 두 점으로 bounding block을 잡습니다. 이렇게 잡은 Bounding block의 분포를 Student Network가 학습하는 겁니다.\n\n\n\nReference. Localization: Localization Distillation for Dense Object Detection [Zheng et al., CVPR 2022]\n\n\n\n\n4.2 Semantic Segmentation\n두 번째 Task인 Semantic Segmentation에서는 Feature와 Output Logit에서 Pixel 단위로 Loss를 구한 다는 점, 그리고 Discriminator 모델을 가지고 학습을 시킨다는 점이 더해졌습니다.\n\n\n\nReference. Semantic Segmentation: Structured Knowledge Distillation for Semantic Segmentation [Liu et al., CVPR 2019]\n\n\n\n\n4.3 GAN\n세번째 Task는 GAN 입니다. 매 Task 마다 feature map을 KD-loss로 가져가고 기존에 Output Logit은 동일하게 가져가네요. 추가로 해당 연구에서는 각 레이어마다 채널 수 중 성능이 가장 좋은 케이스에 한해 Fine-Tuning을 진행한다는 점이 있습니다.\n\n\n\nReference. GAN: GAN Compression: Efficient Architectures for Interactive Conditional GANs [Li et al., CVPR 2020]\n\n\n\n\n4.4 Transformer\n마지막은 Transformer 모델에서 Knowledge Distillation 입니다. Transformer는 Feature Map, Attention Map을 안 볼 수가 없는데요, 아래 그림에서 보면 attention transfer를 하고 하지 않은 경우하고 확실히 Teacher와 Attention map가 비교가 되네요.\n\n\n\nRefernece. NLP: MobileBERT: a Compact Task-Agnostic BERT for Resource-Limited Devices [Sun et al., ACL 2020]"
  },
  {
    "objectID": "posts/lecs/lec09.html#network-augmentation-a-training-technique-for-tiny-machine-learning-models.",
    "href": "posts/lecs/lec09.html#network-augmentation-a-training-technique-for-tiny-machine-learning-models.",
    "title": "🧑‍🏫 Lecture 9",
    "section": "5. Network Augmentation, a training technique for tiny machine learning models.",
    "text": "5. Network Augmentation, a training technique for tiny machine learning models.\n지금까지 Task에 대해서 살펴봤는데요, 그럼 Tiny Model도 overfitting 문제가 있지 않을까요? 그레서 overfitting을 해결하는 방법에는 Data Augmentation이 있습니다. 아래 그림 처럼 Cutoff, Mixup, AutoAugment, Dropout과 같은 방법들이 있습니다.\n\n\n\nReference. Data Augmentation(AutoAugment: Learning Augmentation Policies from Data [Cubuk et al., CVPR 2019])\n\n\n\n\n\nReference. Dropout(DropBlock: A regularization method for convolutional networks [Ghiasi et al., NeurIPS 2018])\n\n\n하지만 Data Augmentation을 적용한 Tiny Model의 성능을 보시면 적용하는 방법마다 떨어지는 것을 확인할 수 있습니다. 그래서 여기서 제안한 아이디어가 “Network Augmentation” 입니다.\n\n\n\nReference. MIT-TinyML-lecture10-Knowledge-Distillation in https://efficientml.ai\n\n\n\nTiny Neural Network lacks capacity! → NetAug\n\n\n5.2 Network Augmentation\nNetwork Augmentation은 기존에 디자인한 모델을 가지고 학습을 시킨 후, 원 모델과 각 레이어마다 파라미터를 변경한 모델을 함께 재학습을 시키는 방법입니다. 파라미터를 변경하는 모델같은 경우 이전시간 실습에 있으니 궁금하시면 참고해주세요. 실험결과는 1.3 ~ 1.8 % Tiny 모델이 성능 개선이 이뤄진 것을 볼 수 있어요. 여기서 원 모델(ResNet50)이 Evaluation에서는 이미 가진 데이터로 모델을 충분히 훈련시켰기 때문에 더이상 늘어나지 않는 것도 확인할 수 있겠네요.\n\nTraining Process\n\n\n\nReference. Network Augmentation for Tiny Deep Learning [Cai et al., ICLR 2022]\n\n\n\\[\n  \\mathscr{L}_{aug} = \\mathscr{L}(W_{base}) + \\alpha \\mathscr{L}([W_{base}, W_{aug}])\n  \\]\n\n\\(\\mathscr{L}_{aug}\\) = base supervision + \\(\\alpha \\cdot\\)auxiliary supervision\n\nLearning Curve\n\n\n\nReference. Network Augmentation for Tiny Deep Learning [Cai et al., ICLR 2022]\n\n\nResult\n\n\n\nReference. Network Augmentation for Tiny Deep Learning [Cai et al., ICLR 2022]\n\n\nResult for Transfer Learning\n\n\n\nReference. Network Augmentation for Tiny Deep Learning [Cai et al., ICLR 2022]\n\n\n\n지금까지 Knowledge Distilation의 기법들 그리고 이를 이용한 Appllcation에 대해서 다뤄봤습니다. 다음 시간에는 TinyEngine을 위한 최적화 기법으로 다시 찾아올게요 🙂"
  },
  {
    "objectID": "posts/lecs/lec03.html",
    "href": "posts/lecs/lec03.html",
    "title": "🧑‍🏫 Lecture 3",
    "section": "",
    "text": "앞으로 총 5장에 걸쳐서 딥러닝 모델 경량화 기법들에 대해서 소개하려고 한다. 경량화 기법으로는 Pruning, Quantization, Neural Network Architecture Search, Knowledge Distillation, 그리고 Tiny Engine에서 돌리기 위한 방법을 진행할 예정인데 본 내용은 MIT에서 Song Han 교수님이 Fall 2022에 한 강의 TinyML and Efficient Deep Learning Computing 6.S965를 바탕으로 재정리한 내용이다. 강의 자료와 영상은 이 링크를 참조하자!\n첫 번째 내용으로 “가지치기”라는 의미를 가진 Pruning에 대해서 이야기, 시작!"
  },
  {
    "objectID": "posts/lecs/lec03.html#introduction-to-pruning",
    "href": "posts/lecs/lec03.html#introduction-to-pruning",
    "title": "🧑‍🏫 Lecture 3",
    "section": "1. Introduction to Pruning",
    "text": "1. Introduction to Pruning\nPruning이란 의미처럼 Neural Network에서 매개변수(노드)를 제거하는 방법입니다. 이는 Dropout하고 비슷한 의미로 볼 수 있는데, Dropout의 경우 모델 훈련 도중 랜덤적으로 특정 노드를 제외시키고 훈련시켜 모델의 Robustness를 높이는 방법으로 훈련을 하고나서도 모델의 노드는 그대로 유지가 된다. 반면 Pruning의 경우 훈련을 마친 후에, 특정 Threshold 이하의 매개변수(노드)의 경우 시 Neural Network에서 제외시켜 모델의 크기를 줄이면서 동시에 추론 속도 또한 높일 수 있다.\n\\[\n\\underset{W_p}{argmin}\\ L(x;W_p), \\text{ subject to } \\lvert\\lvert W_p\\lvert\\lvert_0\\ &lt; N\n\\]\n\nL represents the objective function for neural network training\n\\(x\\) is input, \\(W\\) is original weights, \\(W_p\\) is pruned weights\n\\(\\lvert\\lvert W_p\\lvert\\lvert_0\\) calcuates the #nonzeros in \\(W_p\\) and \\(N\\) is the target #nonzeros\n\n이는 위와 같은 식으로 표현할 수 있다. 특정 W 의 경우 0 으로 만들어 노드를 없애는 경우라고 볼 수 있겠습니다. 그렇게 Pruning한 Neural Network는 아래 그림 처럼 된다.\n\n\n\nReference. MIT-TinyML-lecture03-Pruning-1\n\n\n그럼 왜 Pruning을 하는 걸까? 강의에서 Pruning을 사용하면 Latency, Memeory와 같은 리소스를 확보할 수 있다고 관련된 아래같은 연구결과를 같이 보여준다.\n\n\n\nReference. MIT-TinyML-lecture03-Pruning-1\n\n\nSong Han 교수님은 Vision 딥러닝 모델 경량화 연구를 주로하셔서, CNN을 기반으로 한 모델을 예시로 보여주신다. 모두 Pruning이후에 모델 사이즈의 경우 최대 12배 줄어 들며 연산의 경우 6.3배까지 줄어 든 것을 볼 수 다.\n그렇다면 저렇게 “크기가 줄어든 모델이 성능을 유지할 수 있을까?“\n\n\n\nReference. MIT-TinyML-lecture03-Pruning-1\n\n\n그래프에서 모델의 Weight 분포도를 위 그림에서 보면, Pruning을 하고 난 이후에 Weight 분포도의 중심에 파라미터가 잘려나간 게 보인다. 이후 Fine Tuning을 하고 난 다음의 분포가 나와 있는데, 어느 정도 정확도는 떨어지지만 성능이 유지되는 걸 관찰할 수 있다.\n\n\n\nReference. MIT-TinyML-lecture03-Pruning-1\n\n\n그런 Fine tuning을 반복적으로 하게 된다면(Iterative Pruning and Fine tuning) 그래프에서는 최대 90프로 이상의 파라미터를 덜어낼 수 있다고 한다.\n물론 특정 모델에서, 특정 Task를 대상으로 한 것이라 일반화할 수는 없지만 리소스를 고려하는 상황이라면 충분히 시도해볼 만한 가치가 있어 보인다. 그럼 이렇게 성능을 유지하면서 Pruning을 하기 위해서 어떤 요소를 고려해야 할지 더 자세히 이야기해보자!\n소개하는 고려요소는 아래와 같다. Pruning 패턴부터 차례대로 시작!\n\nPruning Granularity → Pruning 패턴\nPruning Criterion → 얼마만큼에 파라미터를 Pruning 할 건가?\nPruning Ratio → 전체 파라미터에서 Pruning을 얼마만큼의 비율로?\nFine Turning → Pruning 이후에 어떻게 Fine-Tuning 할 건가?\nADMM → Pruning 이후, 어떻게 Convex가 된다고 할 수 있지?\nLottery Ticket Hypothesis → Training부터 Pruning까지 모델을 만들어 보자!\nSystem Support → 하드웨어나 소프트웨어적으로 Pruning을 지원하는 경우는?"
  },
  {
    "objectID": "posts/lecs/lec03.html#determine-the-pruning-granularity",
    "href": "posts/lecs/lec03.html#determine-the-pruning-granularity",
    "title": "🧑‍🏫 Lecture 3",
    "section": "2. Determine the Pruning Granularity",
    "text": "2. Determine the Pruning Granularity\n\n\n\nThe case of convolutional layers, red box is preserved and white one is pruned referred from MIT-TinyML-lecture03-Pruning-1\n\n\n여기서 고려요소는 “얼마만큼 뉴런을 그룹화하여 고려할 것인가?” 입니다. Regular한 정로도 분류하면서 Irregular한 경우와 Regular한 경우의 특징을 아래처럼 말합니다.\n\nFine-grained/Unstructured\n\nMore flexible pruning index choice\nHard to accelerate (irregular data expression)\nCan deliver speed up on some custom hardware\n\nCoarse-grained/Structured\n\nLess flexible pruning index choice (a subset of the fine-grained case)\nEasy to accelerate\n\n\nPruning을 한다고 모델 출력이 나오는 시간이 짧아지는 것이 아님도 언급합니다. Hardware Acceleration의 가능도가 있는데, 이 특징을 보면 알 수 있듯, Pruning의 자유도와 Hardware Acceleration이 trade-off, 즉 경량화 정도와 Latency사이에 trade-off 가 있을 것이 예측됩니다. 하나씩, 자료를 보면서 살펴 보겠습니다.\n\n2.1 Pattern-based Pruning\nIrregular에서도 Pattern-based Pruning은 연속적인 뉴런 M개 중 N개를 Pruning 하는 방법이다. 일반적으로는 N:M = 2:4 으로 한다고 소개한다.\n\n\n\nReference. Accelerating Inference with Sparsity Using the NVIDIA Ampere Architecture and NVIDIA TensorRT\n\n\nReference. Accelerating Inference with Sparsity Using the NVIDIA Ampere Architecture and NVIDIA TensorRT\n예시를 들어 보면, 위와 같은 Matrix에서 행을 보시면 8개의 Weight중 4개가 Non-zero인 것을 볼 수 있습니다. 여기서 Zero인 부분을 없애고 2bit index로 하여 Matrix 연산을 하면 Nvidia’s Ampere GPU에서 속도를 2배까지 높일 수 있다고 한다. 여기서 Sparsity는 “얼마만큼 경량화 됐는지?” 이라고 생각하면 된다.\n\nN:M sparsity means that in each contiguous M elements, N of them is pruned\nA classic case is 2:4 sparsity (50% sparsity)\nIt is supported by Nvidia’s Ampere GPU Architecture, which delivers up to 2x speed up and usually maintains accuracy.\n\n\n\n\nReference. https://developer.nvidia.com/blog/accelerating-inference-with-sparsity-using-ampere-and-tensorrt/\n\n\n\n\n2.2 Channel-level Pruning\n반대로 패턴이 상대적으로 regular 한 쪽인 Channel-level Pruning은 추론시간을 줄일 수 있는 반면에 경량화 비율이 적다고 말한다. 아래 그림을 보시면 Layer마다 Sparsity가 다른 걸 보실 수 있다.\n\nPro: Direct speed up!\nCon: smaller compression ratio\n\n\n\n\nReference AMC: Automl for Model Compression and Acceleration on Mobile Devices [He et al., ECCV 2018]\n\n\n아래에 자료에서는 Channel 별로 한 Pruning의 경우 전체 뉴련을 가지고 한 Pruning보다 추론 시간을 더 줄일 수 있다고 말한다.\n\n\n\nReference AMC: Automl for Model Compression and Acceleration on Mobile Devices [He et al., ECCV 2018]\n\n\n자료를 보면 Sparsity에서는 패턴화 돼 있으면 가속화가 용이해 Latency, 추론 시간을 줄일 수 있지만 그 만큼 Pruning하는 뉴런의 수가 적어 경량화 비율이 줄 것으로 보인다. 하지만 비교적 불규칙한 쪽에 속하는 Pattern-based Pruning의 경우가 하드웨어에서 지원해주는 경우, 모델 크기와 Latency를 둘 다 최적으로 잡을 수 있을 것으로 보인다."
  },
  {
    "objectID": "posts/lecs/lec03.html#determine-the-pruning-criterion",
    "href": "posts/lecs/lec03.html#determine-the-pruning-criterion",
    "title": "🧑‍🏫 Lecture 3",
    "section": "3. Determine the Pruning Criterion",
    "text": "3. Determine the Pruning Criterion\n그렇다면 어떤 파라미터를 가지는 뉴런을 우리는 잘라내야 할까요? Synapse와 Neuron으로 나눠서 살펴보자.\n\nWhich synapses? Which neurons? Which one is less important?\nHow to Select Synapses and Select Neurons to Prune\n\n\n3.1 Select of Synapses\n크게 세 가지로 분류하는데, 각 뉴런의 크기, 각 채널에 전체 뉴런에 대한 크기, 그리고 테일러 급수를 이용하여 gradient와 weight를 모두 고려한 크기를 소개한다. Song han 교수님이 방법들을 소개하기에 앞서서 유수의 기업들도 지난 5년 동안 주로 Magnitude-based Pruning만을 사용해왔다고 하는데, 2023년이 돼서 On-device AI가 각광받기 시작해서 점차적으로 관심을 받기 시작한 건가 싶기도 하다.\n3.1.1 Magnitude-based Pruning\n크기를 기준으로 하는 경우, “얼마만큼 뉴런 그룹에서 고려할 것인가?”와 “그룹내에서 어떤 정규화를 사용할 것인가?를 고려한다.\n\nHeuristic pruning criterion, Element-wise Pruning\n\\[\nImportance = \\lvert W \\lvert\n\\]\n\n\n\nReference. MIT-TinyML-lecture03-Pruning-1\n\n\nHeuristic pruning criterion, Row-wise Pruning, L1-norm magnitude\n\\[\nImportance = \\sum_{i\\in S}\\lvert w_i \\lvert, \\\\where\\ W^{(S)}\\ is\\ the\\ structural\\ set\\ S\\ of\\ parameters\\ W\n\\]\n\n\n\nReference. MIT-TinyML-lecture03-Pruning-1\n\n\nHeuristic pruning criterion, Row-wise Pruning, L2-norm magnitude\n\\[\nImportance = \\sum_{i\\in S}\\lvert w_i \\lvert, \\\\where\\ W^{(S)}\\ is\\ the\\ structural\\ set\\ S\\ of\\ parameters\\ W\n\\]\n\n\n\nReference. MIT-TinyML-lecture03-Pruning-1\n\n\nHeuristic pruning criterion, \\(L_p\\)- norm\n\\[\n\\lvert\\lvert W^{(S)}\\lvert\\lvert=\\huge( \\large\n   \\sum_{i\\in S} \\lvert w_i \\lvert^p\n\\huge) \\large^{\\frac{1}{p}}\n\\]\n\n3.1.2 Scaling-based Pruning\n두 번째로 Scaling을 하는 경우 채널마다 Scaling Factor를 둬서 Pruning을 한다. 그럼 Scaling Factor를 어떻게 둬야 할까? 강의에서 소개하는 이 논문에서는 Scaling factor \\(\\gamma\\) 파라미터를 trainable 파라미터로 두면서 batch normalization layer에 사용한다.\n\nScale factor is associated with each filter(i.e. output channel) in convolution layers.\nThe filters or output channels with small scaling factor magnitude will be pruned\nThe scaling factors can be reused from batch normalization layer\n\\[\n  z_o = \\gamma\\dfrac{z_i-\\mu_{B}}{\\sqrt{\\sigma_B^2+\\epsilon}}+\\beta\n  \\]\n\n\n\n\nReference. MIT-TinyML-lecture03-Pruning-1\n\n\n3.1.3 Talyor Expansion Analysis on Pruning Error\n세 번째 방법은 테일러 급수를 이용하여 Objective function을 최소화 하는 지점을 찾는 방법입니다. Talyor Series에 대한 자세한 내용은 여기서!\n\nEvaluate pruning error induced by pruning synapses.\nMinimize the objective function L(x; W)\nA Taylor series can approximate the induced error.\n\n\\[\n\\delta L = L(x;W)-L(x;W_p=W-\\delta W) \\\\ = \\sum_i g_i\\delta w_i + \\frac{1}{2} \\sum_i h_{ii}\\delta w_i^2 + \\frac{1}{2}\\sum_{i\\not=j}h_{ij}\\delta w_i \\delta w_j + O(\\lvert\\lvert \\delta W \\lvert\\lvert^3)\n\\] \\[\nwhere\\ g_i=\\dfrac{\\delta L}{\\delta w_i}, h_{i, j} = \\dfrac{\\delta^2 L}{\\delta w_i \\delta w_j}\n\\]\n\nSecond-Order-based Pruning\n\n\n\nReference. MIT-TinyML-lecture03-Pruning-1\n\n\n\n\n\nReference. MIT-TinyML-lecture03-Pruning-1\n\n\nOptimal Brain Damage[LeCun et al., NeurIPS 1989] 논문에서는 이 방법을 이용하기 위해 세 가지를 가정한다.\n\nObjective function L이 quadratic 이기 때문에 마지막 항이 무시된다(이는 Talyor Series의 Error 항을 알면 이해가 더 쉽다!)\n만약 신경망이 수렴하게되면, 첫 번째항도 무시된다.\n각 파라미터가 독립적이라면 Cross-term도 무시된다.\n\n그러면 식을 아래처럼 정리할 수 있는데, 중요한 부분은 Hessian Matrix H에 사용하는 Computation이 어렵다는 점!\n\\[\n\\delta L_i = L(x;W)-L(x;W_p\\lvert w_i=0)\\approx \\dfrac{1}{2} h_{ii}w_i^2,\\ where\\ h_{ii}=\\dfrac{\\partial^2 L}{\\partial w_i \\partial w_j}\n\\]\n\\[\nimportance_{w_i} = \\lvert \\delta L_i\\lvert = \\frac{1}{2}h_{ii}w_i^2\n\\] \\[\n*\\ h_{ii} \\text{ is non-negative}\n\\]\nFirst-Order-based Pruning\n\n참고로 이 방법은 2023년에는 소개하지 않는다.\n\n\n\n\nReference. MIT-TinyML-lecture03-Pruning-1\n\n\n\nIf only first-order expansion is considered under an i.i.d(Independent and identically distributed) assumption,\n\n\\[\n\\delta L_i = L(x;W) - L(x; W_P\\lvert w_i=0) \\approx g_iw_i,\\ where\\ g_i=\\dfrac{\\partial L}{\\partial w_i}\n\\] \\[\nimportance_{w_i} = \\lvert \\delta L_i \\lvert = \\lvert g_i w_i \\lvert \\ or \\ importance_{w_i} = \\lvert \\delta L_i \\lvert^2 = (g_i w_i)^2\n\\]\n\nFor coarse-grained pruning, we have,\n\\[\n  importance_{\\ W^{(S)}} = \\sum_{i \\in S}\\lvert \\delta L_i \\lvert^2 = \\sum_{i \\in S} (g_i w_i)^2,\\ where \\ W^{(S)}is\\ the\\ structural\\ set\\ of\\ parameters\n  \\]\n\n\n\n\n3.2 Select of Neurons\n어떤 Neuron을 없앨 지를 고려(Less useful → Remove) 한 이 방법은 Neuron의 경우도 있지만 아래 그림처럼 Channel로 고려할 수도 있다. 확실히 전에 소개했던 방법들보다 “Coarse-grained pruning”인 방법이다.\n\n\nPercentage-of-Zero-based Pruning\n첫번째는 Channel마다 0의 비율을 봐서 비율이 높은 Channel 을 없내는 방법이다. ReLU activation을 사용하면 Output이 0이 나오는데, 여기서 0의 비율, Average Percentage of Zero activations(APoZ)라고 부르는 것을 보고 가지치기할 Channel을 제거한다.\n\nReLU activation will generate zeros in the output activation\nSimilar to magnitude of weights, the Average Percentage of Zero activations(APoZ) can be exploited to measure the importance the neuron has\n\n\n\n\nReference. MIT-TinyML-lecture03-Pruning-1\n\n\nFirst-Order-based Pruning\n\n참고로 이 방법은 2023년에는 소개하지 않는 방법이다.\nMinimize the error on loss function introduced by pruning neurons\nSimilar to previous Taylor expansion on weights, the induced error of the objective function L(x; W) can be approximated by a Taylor series expanded on activations.\n\\[\n  \\delta L_i = L(x; W) - L(x\\lvert x_i = 0; W) \\approx \\dfrac{\\partial L}{\\partial x_i}x_i\n  \\]\nFor a structural set of neurons \\(x^{(S)}\\) (e.g., a channel plane),\n\\[\n  \\lvert \\delta L_{x^{(S)}} \\lvert\\ = \\Large\\lvert \\small\\sum_{i\\in S}\\dfrac{\\partial L}{\\partial x_i}x_i\\Large\\lvert\n  \\]\n\nRegression-based Pruning\n이 방법은 Quantized한 레이어의 output \\(\\hat Z\\)(construction error of the corresponding layer’s outputs)와 \\(Z\\)를 Training을 통해 차이를 줄이는 방법이다. 참고로 문제를 푸는 자세한 과정은 2022년 강의에만 나와 있다.\n\n\\[\nZ=XW^T=\\sum_{c=0}^{c_i-1}X_cW_c^T\n\\]\n\n\n\nReference. MIT-TinyML-lecture03-Pruning-1\n\n\n문제를 식으로 정의해보면 아래와 같은데,\n\n\\(\\beta\\) is the coefficient vector of length \\(c_i\\) for channel selection.\n\\(\\beta_c = 0\\) means channel \\(c\\) is pruned.\n\\(N_c\\) is the number of none zero channel\n\n우선 문제를 푸는 단계는 두 단계로 나눈다. Channel의 Scale \\(\\beta\\)를 우선 계산한 후에 \\(W\\)를 Quantized한 레이어의 output \\(\\hat Z\\)(construction error of the corresponding layer’s outputs)와 \\(Z\\)의 차이가 최소화되는 지점까지 Training시킨다.\nSolve the problem in two folds:\n\nFix W, solve \\(\\beta\\) for channel selection → NP(Nondeterministic polynomial)-hard\nFix \\(\\beta\\), solve W to minimize reconstruction error(Weight Reconstruction)\n\n각 문제를 푸는 과정을 조금 더 자세히 살펴봐보자. 본 내용은 2022년 강의에 있으니 참고!\nNP(Nondeterministic polynomial)-hard는 아래와 같이 식으로 정리할 수 있다.\n\\[\n\\underset{\\beta}{argmin} \\lvert\\lvert Z- \\sum_{c=0}^{c_i-1} \\beta_cX_cW_c^T \\lvert\\lvert_F^2 = \\lvert\\lvert \\sum_{c=0}^{c_i-1}X_cW_c^T - \\sum_{c=0}^{c_i-1} \\beta_cX_cW_c^T \\lvert\\lvert_F^2\n\\] \\[\n= \\lvert\\lvert\\sum_{c=0}^{c_i-1} (1-\\beta_c)X_cW_c^T \\lvert\\lvert_F^2, \\ s.t.\\ \\lvert\\lvert\\beta\\lvert\\lvert_0 \\ \\leq N_c\n\\]\n강의에서 소개하는 ThiNet이라는 논문에서는 greedy solution을 이용해서 채널 하나하나씩 Pruning 해보며 objective function의 l2-norm 최솟값을 구한다.\n1: S = []\n2: while len(S) &lt; N:\n3:   min_norm, min_c = +inf, 0\n4:   for c in range(c_i):\n5:     tmpS=S+[c]\n6:     Z = X[:,tmpS] * W[:,tmpS].t()\n7:     norm = Z.norm(2)\n8:     if norm &lt; min_norm:\n9:       min_norm, min_c = norm, c\n10:   S.append(min_c)\n11:   c_i.pop(min_c)\n여기서 더해서 \\(\\beta\\) 를 구하는 과정에서 일반화를 위해 LASSO 방식을 사용한다(LASSO에 대한 자세한 내용은 여기서). Relax the \\(l_0\\) to \\(l_1\\) regularization (LASSO):\n\\[\n\\underset{\\beta}{argmin}\\ \\lvert\\lvert Z- \\sum_{c=0}^{c_i-1}\\beta_cX_cW_c^T\\lvert\\lvert^2_F+\\lambda\\lvert\\lvert \\beta \\lvert\\lvert_1\n\\]\n\n\\(\\lambda\\) is a penalty coefficient. By increasing \\(\\lambda\\), there will be more zeros in \\(\\beta\\).\nGradually increase \\(\\lambda\\) and solve the LASSO regression for \\(\\beta\\), until \\(\\lvert\\lvert \\beta \\lvert\\lvert_0==N_c\\) is met.\nWhy \\(\\lvert\\lvert \\beta \\lvert\\lvert_0==N_c\\)?\n여기에 대해서는 따로 언급되지 않았지만, 의미상 scale 전체 N개 중에서 최적값을 찾아야한다면 전체를 N으로 유지하면서 최적값을 찾기 위해서가 아닐까?\n\n두 번째는 구한 \\(\\beta\\)를 고정한 상태로 Weight를 Quantized 전후의 차이를 최소화 하게 “Weight Reconstruction” 한다. 구하는 과정은 least square approach를 이용한 unique closed-form solution 이므로 아래를 참조하자.\n\\[\n\\underset{\\beta}{argmin}\\ \\lvert\\lvert Z- \\sum_{c=0}^{c_i-1}\\beta_cX_cW_c^T\\lvert\\lvert^2_F\n\\]\n\n\\(\\beta\\) is a coefficient vector from the previous step\nThis is a classic linear regression problem, which has a unique closed-form solution using the least square approach.\n\\[\n  \\underset{W}{argmin} \\lvert\\lvert Z-\\hat{Z} \\lvert\\lvert^2_F = \\lvert\\lvert Z-UW^T \\lvert\\lvert_F^2\n  \\]\nwhere\n\\[\n  U= \\Large[ \\small\\beta_0X_0\\ \\beta_1X_1 \\ \\cdots \\beta_cX_c \\cdots \\beta_{c_i-1}X_{c_i-1} \\Large]\n  \\]\nand thus,\n\\[\n  W^T = (U^TU)^{-1}U^T Z\n  \\]\n\nQ. How \\((U^TU)^{-1}\\) exists?\nLeast Square method, 임의의 벡터 \\(v = (v_0, v_1, \\dots, v_n)\\) 가 있을 때 \\(v^Tv\\) 의 역행렬은 항상 있을까? 가정에서 “a unique closed-form solution”라고 했으므로 이는 즉 linearly independen로 고려할 있고 역행렬이 있다(\\(v^Tv\\) is invertible)는 이야기이다."
  },
  {
    "objectID": "posts/lecs/lec03.html#discussion",
    "href": "posts/lecs/lec03.html#discussion",
    "title": "🧑‍🏫 Lecture 3",
    "section": "4. Discussion",
    "text": "4. Discussion\n\nPruning을 Dropout이랑 비교해서 어떤 차이점이 있는가?\n두 가지 방법은 분명히 Neuron과 Synapse를 없댄다는 측면에서는 비슷하다. 하지만 두 가지 측면에서 차이점이 있는데, 한 가지는 목적하는 바이고, 두 번째는 시점이다. Dropout은 목적하는 바가 훈련중에 overfitting을 방지하기 위함이 있고 Pruning의 경우는 훈련을 마친 모델의 크기를 줄이는 것에 있다. 그리고 두 번째 시점의 경우 Dropout은 훈련중에 이뤄지는 반면 Pruning은 훈련을 마치고, 그 크기를 줄인 후에 성능이 떨어지면 그에 맞게 Fine-tuning을 한다.\n스터디에서는 “왜 dropout을 통해 사이즈를 줄이지 않았는가? 그리고 구지 훈련을 마친 다음에 할 필요가 있나?” 라고 질문이 나왔었다. 물론 훈련 중에 모델의 사이즈를 작게 만들 수 있으면, 가능한 그렇게 하면 될 것이다. 하지만, 이 또한 두가지 측면을 고려할 필요가 있다. 하나는 “과연 모델의 사이즈를 훈련 중 혹은 전에 줄여나가면서 충분히 성능을 낼 수 있는가?”이고 다른 하나는 Pruning이나 모델 경량화는 최적화에 초점을 맞춘다고 생각한다. 그렇기 때문에 훈련 중간에 Channel pruning과 같은 기법을 사용할 수 있을 지는 미지수이고, 설령 Fine-grained Pruning과 같은 기법을 사용한다 하더라도 이는 모델의 사이즈만 줄어 들 뿐, 나머지 메모리(e.g. RAM)이나 Latency같은 성능은 좋게 가져갈 수 있을지도 미지수라고 생각한다.\n필자는 위와 같은 최적화를 통한 성능 개선을 이 글에서처럼 2022년 TinyML 강의에서 제공하는 실습을 통해 경험했었다. 앞선 예시는 OS를 가진 디바이스가 아닌 Bare-metal firmware로 환경이 조금 특수하기도 하고, 실제로 Torch나 Tensorflowlite에서 제공하는 모델 경량화를 직접적으로 분석해봐야 실질적인 예시를 알 수 있겠지만, 혹여 이해해 참고가 될까 덧붙여 놓는다."
  },
  {
    "objectID": "posts/lecs/lec03.html#reference",
    "href": "posts/lecs/lec03.html#reference",
    "title": "🧑‍🏫 Lecture 3",
    "section": "5. Reference",
    "text": "5. Reference\n\nMIT-TinyML-lecture03-Pruning-1\nAMC: Automl for Model Compression and Acceleration on Mobile Devices, 2018\nLearning Efficient Convolutional Networks through Network Slimming, 2017\nThiNet: A Filter Level Pruning Method for Deep Neural Network Compression, 2017\nChannel Pruning for Accelerating Very Deep Neural Networks"
  },
  {
    "objectID": "posts/lecs/lec04.html",
    "href": "posts/lecs/lec04.html",
    "title": "🧑‍🏫 Lecture 4",
    "section": "",
    "text": "이전 포스팅에서 Pruning에 대해서 배웠었다. 이번에는 Pruning에 대한 남은 이야기인 Pruning Ratio를 정하는 방법, Fine-tuning 과정에 대해 알아보고, 마지막으로 Sparsity를 위한 System Support에 대해 알아보고자 한다."
  },
  {
    "objectID": "posts/lecs/lec04.html#pruning-ratio",
    "href": "posts/lecs/lec04.html#pruning-ratio",
    "title": "🧑‍🏫 Lecture 4",
    "section": "1. Pruning Ratio",
    "text": "1. Pruning Ratio\nPruning을 하기 위해서 어느 정도 Pruning을 해야 할지 어떻게 정해야 할까?\n즉, 다시 말해서 몇 % 정도 그리고 어떻게 Pruning을 해야 좋을까?\n\n\n\nPruning 방식 비교\n\n\n우선 Channel 별 Pruning을 할 때, Channel 구분 없이 동일한 Pruning 비율(Uniform)을 적용하면 성능이 좋지 않다. 오른쪽 그래프에서 지향해야 하는 방향은 Latency는 적게, Accuracy는 높게이므로 왼쪽 상단의 영역이 되도록 Pruning을 진행해야 한다. 그렇다면 결론은 Channel 별 구분을 해서 어떤 Channel은 Pruning 비율을 높게, 어떤 Channel은 Pruning 비율을 낮게 해야 한다는 이야기가 된다.\n\n1.1 Sensitiviy Analysis\nChannel 별 구분을 해서 Pruning을 한다는 기본 아이디어는 아래와 같다.\n\nAccuracy에 영향을 많이 주는 Layer는 Pruning을 적게 해야 한다.\nAccuracy에 영향을 적게 주는 Layer는 Pruning을 많이 해야 한다.\n\nAccuracy를 되도록이면 원래의 모델보다 덜 떨어지게 만들면서 Pruning을 해서 모델을 가볍게 만드는 것이 목표이기 때문에 당연한 아이디어일 것이다. Accuracy에 영향을 많이 준다는 말은 Sensitive한 Layer이다라는 표현으로 다르게 말할 수 있다. 따라서 각 Layer의 Senstivity를 측정해서 Sensitive한 Layer는 Pruning Ratio를 낮게 설계하면 된다.\nLayer의 Sensitivity를 측정하기 위해 Sensitivity Analysis를 진행해보자. 당연히 특정 Layer의 Pruning Ratio가 높을 수록 weight가 많이 가지치기 된 것이므로 Accuracy는 떨어지게 된다.\n\n\n\nL0 Pruning Rate 그래프\n\n\n\n\n\n\n\n\nPruning을 하는 Weight는 어떻게 결정하나요?\n\n\n\n\n\nPruning Ratio에 의해 Pruned 되는 weight는 이전 강의에서 배운 “Importance(weight의 절댓값 크기)”에 따라 선택된다.\n\n\n\n위의 그림에서 처럼 Layer 0(L0)만을 가지고 Pruning Ratio를 높여가면서 관찰해보면, 약 70% 이후부터는 Accuracy가 급격하게 떨어지는 것을 볼 수 있다. L0에서 Ratio를 높여가며 Accuracy의 변화를 관찰한 것처럼 다른 Layer들도 관찰해보자.\n\n\n\nLayer별 Sensitivity 비교\n\n\nL1은 다른 Layer들에 비해 상대적으로 Pruning Ratio를 높여가도 Accuracy의 떨어지는 정도가 약한 반면, L0는 다른 Layer들에 비해 상대적으로 Pruning Ratio를 높여가면 Accuracy의 떨어지는 정도가 심한 것을 확인할 수 있다. 따라서 L1은 Sensitivity가 높다고 볼 수 있으며 Pruning을 적게해야 하고, L0은 Sensitivity가 낮다고 볼 수 있으며 Pruning을 많게해야 함을 알 수 있다.\n여기서 Sensitivity Analysis에서 고려해야할 몇가지 사항들에 대해서 짚고 넘어가자.\n\nSensitivity Analysis에서 모든 Layer들이 독립적으로 작동한다는 것을 전제로 한다. 즉, L0의 Pruning이 L1의 효과에 영향을 주지 않는 독립성을 가진다는 것을 전제로 한다.\nLayer의 Pruning Ratio가 동일하다고 해서 Pruned Weight수가 같음을 의미하지 않는다.\n\n100개의 weight가 있는 layer의 10% Pruning Ratio 적용은 10개의 weight가 pruned 되었음을 의미하고, 500개의 weight가 있는 layer의 10% Pruning Ratio 적용은 50개의 weight가 pruned 되었음을 의미한다.\nLayer의 전체 크기에 따라 Pruning Ratio의 적용 효과는 다를 수 있다.\n\n\nSensitivity Analysis까지 진행한 후에는 보통 사람이 Accuracy가 떨어지는 정도, threshold를 정해서 Pruning Ratio를 정한다.\n\n\n\nThreshold 정하기\n\n\n위 그래프에서는 Accuracy가 약 75%수준으로 유지되는 threhsold \\(T\\) 수평선을 기준으로 L0는 약 74%, L4는 약 80%, L3는 약 82%, L2는 90%까지 Pruning을 진행해야 겠다고 정한 예시를 보여준다. 민감한 layer인 L0는 상대적으로 Pruning을 적게, 덜 민감한 layer인 L2는 Pruning을 많게 하는 것을 확인할 수 있다.\n물론 사람이 정하는 threshold는 개선의 여지가 물론 있다. Pruning Ratio를 좀 더 Automatic하게 찾는 방법에 대해 알아보자.\n\n\n1.2 AMC\nAMC는 AutoML for Model Compression의 약자로, 강화학습(Reinforcement Learning) 방법으로 최적의 Pruning Ratio를 찾도록 하는 방법이다.\n\n\n\nAMC 전체 구조\n\n\nAMC의 구조는 위 그림과 같다. 강화학습 알고리즘 계열 중, Actor-Critic 계열의 알고리즘인 Deep Deterministic Policy Gradient(DDPG)을 활용하여 Pruning Ratio를 정하는 Action을 선택하도록 학습한다. 자세한 MDP(Markov Decision Process) 설계는 아래와 같다.\n\n\n\nAMC의 MDP\n\n\n강화학습 Agent의 학습 방향을 결정하는 중요한 Reward Function은 모델의 Accuracy를 고려해서 Error를 줄이도록 유도할 뿐만 아니라 Latency를 간접적으로 고려할 수 있도록 모델의 연산량을 나타내는 FLOP를 적게 하도록 유도하도록 설계한다. 오른쪽에 모델들의 연산량 별(Operations) Top-1 Accuracy 그래프를 보면 연산량이 많을수록 로그함수처럼 Accuracy가 증가하는 것을 보고 이를 보고 반영한 부분이라고 볼 수 있다.\n\n\n\nAMC의 Reward Function\n\n\n이렇게 AMC로 Pruning을 진행했을 때, Human Expert가 Pruning 한 것과 비교해보자. 아래 모델 섹션별 Density 히스토그램 그래프에서 Total을 보면, 동일 Accuracy가 나오도록 Pruning을 진행했을 때 AMC로 Pruning을 진행한 것(주황색)이 Human Expert Pruning 모델(파란색)보다 Density가 낮은 것을 확인할 수 있다. 즉, AMC로 Pruning 진행했을 때 더 많은 weight를 Pruning 더 가벼운 모델을 가지고도 Accuracy를 유지했다고 볼 수 있다.\n\n\n\nAMC의 Density Graph\n\n\n두번째 꺾은 선 그래프에서 AMC를 가지고 Pruning과 Fine-tuning을 번갈아 가며 여러 스텝으로 진행해가면서 관찰한 것을 조금 더 자세히 살펴보자. 각 Iteration(Pruning+Fine-tuning)을 stage1, 2, 3, 4로 나타내어 plot한 것을 보면, 1x1 conv보다 3x3 conv에서 Density가 더 낮은 것을 확인할 수 있다. 즉, 3x3 conv에서 1x1 conv보다 Pruning을 많이 한 것을 볼 수 있다. 이를 해석해보자면, AMC가 3x3 conv을 Pruning하면 9개의 weight를 pruning하고 이는 1x1 conv pruning해서 1개의 weight를 없애는 것보다 한번에 더 많은 weight 수를 줄일 수 있기 때문에 3x3 conv pruning을 적극 활용했을 것으로 볼 수 있다.\n\n\n\nAMC Result\n\n\n이 AMC 실험 결과표에서 보면, FLOP와 Time 각각 50%로 줄인 AMC 모델 둘다 Top-1 Accuracy가 기존의 1.0 MobileNet의 Accuracy보다 약 0.1~0.4% 정도만 줄고 Latency나 SpeedUp이 효율적으로 조정된 것을 확인할 수 있다.\n\n\n\n\n\n\nAMC 실험 결과표에서 0.75 MobileNet의 SpeedUp이 왜 1.7x 인가요?\n\n\n\n\n\n결과표에서 0.75 MobileNet은 25%의 weight를 감소시킨 것이기 때문에 SpeedUp이 \\(\\frac{4}{3} \\simeq 1.3\\)x이어야 한다고 생각할 수 있다. 하지만 연산량은 quadratic하게 감소하게 되기 때문에 \\(\\frac{4}{3} \\cdot \\frac{4}{3} \\simeq 1.7\\)x로 SpeedUp이 된다.\n\n\n\n\n\n1.3 NetAdapt\n또 다른 Pruning Ratio를 정하는 기법으로 NetAdapt이 있다. Latency Constraint를 가지고 layer마다 pruning을 적용해본다. 예를 들어, 줄일 목표 latency 량을 lms로 정하면, 10ms → 9ms로 줄 때까지 layer의 pruning ratio를 높여가는 방법이다.\n\n\n\nNetAdapt\n\n\nNetAdapt의 전체적인 과정은 아래와 같이 진행된다. 기존 모델에서 각 layer를 Latency Constraint에 도달하도록 Pruning하면서 Accuracy(\\(Acc_A\\)등)을 반복적으로 측정한다.\n\n각 layer의 pruning ratio를 조절한다.\nShort-term fine tuning을 진행한다.\nLatency Constraint에 도달했는지 확인한다.\nLatency Constraint 도달하면 해당 layer의 최적의 Pruning ratio로 판단한다.\n각 layer의 최적 Pruning ratio가 정해졌다면 마지막으로 Long-term fine tuning을 진행한다.\n\n\n\n\nNetAdapt 과정\n\n\n이와 같이 NetAdapt의 과정을 진행하면 아래와 같은 실험 결과를 볼 수 있다. Uniform하게 Pruning을 진행한 Multipilers보다 NetAdapt가 1.7x 더 빠르고 오히려 Accuracy는 약 0.3% 정도 높은 것을 알 수 있다.\n\n\n\nNetAdapt의 Latency / Top-1 Accuracy 그래프"
  },
  {
    "objectID": "posts/lecs/lec04.html#fine-tuningtrain",
    "href": "posts/lecs/lec04.html#fine-tuningtrain",
    "title": "🧑‍🏫 Lecture 4",
    "section": "2. Fine-tuning/Train",
    "text": "2. Fine-tuning/Train\nPrunned 모델의 퍼포먼스를 향상하기 위해서는 Pruning를 진행하고 나서 Fine-tuning 과정이 필요하다.\n\n2.1 Iterative Pruning\n보통 Pruned 모델의 Fine-tuning 과정에서는 기존에 학습했던 learning rate보다 작은 rate를 사용한다. 예를들어 기존의 모델을 학습할 때 사용한 learning rate의 \\(1/100\\) 또는 \\(1/10\\)을 사용한다. 또한 Pruning 과정과 Fine-tuning 과정은 1번만 진행하기보다 점차적으로 pruning ratio를 늘려가며 Pruning, Fine-tuning을 번갈아가며 여러번 진행하는게 더 좋다.\n\n\n\nIterative Pruning + Fine-tuning 비교 그래프\n\n\n\n\n2.2 Regularization\nTinyML의 목표는 가능한 많은 weight들을 0으로 만드는 것으로 생각할 수 있다. 그래야 모델을 가볍게 만들 수 있기 때문이다. 그래서 Regularization 기법을 이용해서 모델의 weight들을 0으로, 혹은 0과 가깝게 작은 값을 가지도록 만든다. 작은 값의 weight가 되도록 하는 이유는 0과 가까운 작은 값들은 다음 layer들로 넘어가면서 0이 될 가능성이 높아지기 때문이다. 기존의 딥러닝 모델들의 과적합(Overfitting)을 막기 위한 Regularization 기법들과 다르지 않으나 의도와 목적은 다른 것을 짚어볼 수 있다.\n\n\n\nPruning을 위한 Regularization\n\n\n\n\n2.3 The Lottery Ticket Hypothesis\n2019년 ICLR에서 발표된 논문에서 Jonathan Frankle과 Michael Carbin이 소개한 The Lottery Ticket Hypothesis(LTH)은 심층 신경망(DNN) 훈련에 대한 흥미로운 아이디어를 제안한다. 무작위로 초기화된 대규모 신경망 내에 더 작은 하위 네트워크(Winning Ticket)가 존재한다는 것을 말한다. 이 하위 네트워크는 처음부터 별도로 훈련할 때 원래 네트워크의 성능에 도달하거나 능가할 수 있다. 이 가설은 이러한 Winning Ticket이 학습하는 데 적합한 초기 가중치를 갖는다고 가정한다.\n\n\n\nLTH 설명 그림"
  },
  {
    "objectID": "posts/lecs/lec04.html#system-support-for-sparsity",
    "href": "posts/lecs/lec04.html#system-support-for-sparsity",
    "title": "🧑‍🏫 Lecture 4",
    "section": "3. System Support for Sparsity",
    "text": "3. System Support for Sparsity\nDNN을 가속화 시키는 방법은 크게 3가지, Sparse Weight, Sparse Activation, Weight Sharing이 있다. Sparse Weight, Sparse Activation은 Pruning이고 Weight Sharing은 Quantization의 방법이다.\n\n\n\nDNN을 가속화 시키는 방법\n\n\n\nSparse Weight: Weight를 Pruning하여 Computation은 Pruning Ratio에 대응하여 빨라진다. 하지만 Memory는 Pruning된 weight의 위치를 기억하기 위한 memory 용량이 필요하므로 Pruning Ratio에 비례하여 줄진 않는다.\nSparse Activation: Weight를 Pruning하는 것과 다르게 Activation은 Test Input에 따라 dynamic 하므로 Weight를 Pruning하는 것보다 Computation이 덜 줄어든다.\nWeight Sharing: Quantization 방법으로 32-bit data를 4-bit data로 변경함으로써 8배의 memory 절약을 할 수 있다.\n\n\n3.1 EIE\nEfficient Inference Engine은 기계 학습 모델을 실시간으로 실행하기 위해 최적화된 소프트웨어 라이브러리나 프레임워크를 말한다. Processing Elements(PE)의 구조\n\n\n\nPE 연산 Logically / Physically 분석\n\n\n아래 그림에서 Input별(\\(\\vec{a}\\)) 연산은 아래와 같이 Input이 0일 때는 skip되고 0이 아닐 때는 prunning 되지 않은 weight와 연산이 진행된다.\n\n\n\nInput별 연산 과정\n\n\nEIE 실험은 가장 loss가 적은 data 자료형인 16 bit Int형을 사용했다.(0.5% loss) AlexNet이나 VGG와 같이 ReLU Activation이 많이 사용되는 모델들은 경량화가 많이 된 반면, RNN와 LSTM이 사용된 NeuralTalk 모델들 같은 경우에는 ReLU를 사용하지 않아 경량화될 수 있는 부분이 없어 Activation Density가 100%인 것을 확인할 수 있다.\n\n\n\nEIE 실험 결과\n\n\n\n\n3.2 M:N Weight Sparsity\n이 방법은 Nvidia 하드웨어의 지원이 필요한 방법으로 보통 2:4 Weight Sparsity를 사용한다. 왼쪽의 Sparse Matrix를 재배치해서 Non-zero data matrix와 인덱스를 저장하는 Index matrix를 따로 만들어서 저장한다.\n\n\n\n2:4 Weight Sparsity\n\n\nM:N Weight Sparsity 적용하지 않은 Dense GEMM과 적용한 Sparse GEMM을 계산할 때는 아래의 그림과 같은 과정으로 연산이 진행된다.\n\n\n\nDense VS. Sparse GEMM\n\n\n\n\n3.3 Sparse Convolution\nSubmanifold Sparse Convolutional Networks (SSCN)은 고차원 데이터에서 효율적인 계산을 가능하게 하는 신경망 아키텍처의 한 형태이다. 이 기술은 특히 3D 포인트 클라우드 또는 고해상도 이미지와 같이 대규모 및 고차원 데이터를 처리할 때 중요하다. SSCN의 핵심 아이디어는 데이터의 Sparcity을 활용하여 계산과 메모리 사용량을 크게 줄이는 것이다.\n\n\n\n출처: Submanifold Sparse Convolutional Networks\n\n\n이러한 Sparse Convolution은 기본 Convolution과 비교했을 때 아래 그림과 같이 나타내볼 수 있다.\n\n\n\nConventional VS. Sparse Convolution\n\n\n연산 과정을 비교해보기 위해 Input Point Cloud(\\(P\\)), Feature Map(\\(W\\)), Ouput Point Cloud(\\(Q\\))를 아래와 같이 있다고 하자. 기존의 Convolution과 Sparse Convolution을 비교해보면 연산량이 9:2로 매우 적은 연산만 필요한 것을 알 수 있다.\n\n\n\nConventional VS. Sparse 연산량 비교\n\n\nFeature Map(\\(W\\))을 기준으로 각 weight 마다 필요한 Input data의 크기가 다르다. 예를 들어 \\(W_{-1, 0}\\)은 \\(P1\\)과 만의 연산이 진행되므로 \\(P1\\)만 연산시 불러내게 된다.\n\n\n\nSparse Convolution 계산 과정\n\n\n따라서 Feature Map의 \\(W\\)에 따라 필요한 Input data를 표현하고 따로 computation을 진행하면 아래와 같이 고르지 못한 연산량 분배가 진행되는데(왼쪽 그림) 이는 computation에 overhead는 없지만 regularity가 좋지 않다. 또는 가장 computation이 많은 것을 기준으로 Batch 단위로 계산하게 된다면(가운데 그림) 적은 computation weight에서의 비효율적인 계산 대기시간이 생기므로 overhead가 생긴다. 따라서 적절히 비슷한 연산량을 가지는 grouping을 진행한 뒤 batch로 묶으면 적절히 computation을 진행할 수 있다.(오른쪽 그림)\n\n\n\nGrouping Computation\n\n\n이런 Grouping을 적용한 후 Sparse Convolution을 진행하면 Adaptive Grouping이 적용되어 아래와 같이 진행된다.\n\n\n\nSparse Convolution 예시\n\n\n여기까지가 2023년도 강의에서 마지막 Sparse Convolution에 대해 설명한 부분을 정리한 부분이다. 하지만 강의에서 설명이 많이 생략되어 있으므로 좀 더 자세한 내용은 Youtube 발표 영상이나 2022년도 강의를 참고하는 것을 권장한다."
  },
  {
    "objectID": "posts/lecs/lec04.html#reference",
    "href": "posts/lecs/lec04.html#reference",
    "title": "🧑‍🏫 Lecture 4",
    "section": "4. Reference",
    "text": "4. Reference\n\nMIT-TinyML-lecture04-Pruning-2\nAMC: Automl for Model Compression and Acceleration on Mobile Devices, 2018\nContinuous control with deep reinforcement learning\nFLOPs란? 딥러닝 연산량에 대해서\nNetAdapt: Platform-Aware Neural Network Adaptation for Mobile Applications\nThe Lottery Ticket Hypothesis: Finding Sparse, Trainable Neural Networks\nThe Lottery Ticket Hypothesis Finding Sparse, Trainable Neural Networks 논문 리뷰\nLLM Inference - HW/SW Optimizations\nAccelerating Sparse Deep Neural Networks\nSubmanifold Sparse Convolutional Networks\nTorchSparse: Efficient Point Cloud Inference Engine\nTorchSparse++: Efficient Training and Inference Framework for Sparse Convolution on GPUs\nmit-han-lab/torchsparse\nMLSys’22 TorchSparse: Efficient Point Cloud Inference Engine"
  },
  {
    "objectID": "posts/lecs/lec07.html",
    "href": "posts/lecs/lec07.html",
    "title": "🧑‍🏫 Lecture 7-8",
    "section": "",
    "text": "이번 글에서는 Neural Architecture Search(이하 NAS)라는 기법을 소개한다. NAS는 최적의 신경망 구조를 자동으로 발견하는 과정이다.\n위 그림처럼, 기존에 모델을 개발할 때에는 레이어의 개수, 커널의 크기나 채널 개수등을 사람이 정해가며 만들었다면, NAS는 이를 자동화해서 가장 성능이 좋은 구조를 빠르게 찾는 것이다. 이를 위해 search space 등을 사용하고 이는 뒤에서 설명토록 하겠다."
  },
  {
    "objectID": "posts/lecs/lec07.html#what-is-nas",
    "href": "posts/lecs/lec07.html#what-is-nas",
    "title": "🧑‍🏫 Lecture 7-8",
    "section": "3.1 What is NAS?",
    "text": "3.1 What is NAS?\nNAS는 앞서 설명한 대로, 최적의 신경망 구조를 자동으로 발견하는 과정이다.  위의 그래프에서, 원의 안쪽에 별(*)로 표시된 모델이 NAS기반의 모델들이고, 실제로 사람이 제작한 모델보다 좋은 성능을 내는 것을 보여준다. (적은 연산수(MAC)으로 높은 Top-1 accuracy)  NAS는 기본적으로 위의 그림과 같은 실행 구조이다. 이를 순서대로 살펴보면 아래와 같다. 1. Search Space에서 특정한 연산/네트워크를 고른다 2. 그것들을 잘 조합해(Search Strategy) 적당한 구조를 만든다 3. 조합한 구조(architecture)의 성능을 평가한다(performance estimation) 4. 1~3의 과정을 반복하며 최적의 모델을 찾는다."
  },
  {
    "objectID": "posts/lecs/lec07.html#search-space",
    "href": "posts/lecs/lec07.html#search-space",
    "title": "🧑‍🏫 Lecture 7-8",
    "section": "3.2 Search space",
    "text": "3.2 Search space\n그렇다면 Search space란 무엇일까? Search space에는 두 가지 종류가 있다.\n\nCell-level\nNetwork-level cell-level은 convolution같은 연산, network-level은 bottleneck같은 네트워크를 대상으로 그 중 하나를 선택하는 것이다.\n\n\n3.2.1 Cell-level\n 위의 그림은 Cell-level의 선택을 RNN을 통해 진행하는 그림이다. 이는 우측과 같은 구조의 네트워크를 만들자고 할 때, 각각 어떤 연산/입력으로 채울지를 고르는 것이다. 연산/입력을 고르는 과정은 다음과 같다.\n\n어떤 입력을 사용할지(select one hidden state) 고른다\n각 입력에 어떤 연산을 취할지(select operation for first/second hidden state) 고른다\n연산을 취한 결과를 어떻게 합칠지(select method to combine 고른다\n\n이렇게 cell-level로 연산/입력을 바꿔가면서, 최적의 블록(에트워크)를 만드는 것이다.\n\n\n3.2.2 Network-level\nNetwork-level은 기존의 네트워크(ex. bottleneck)에서 여러 파라미터를 바꿔보며 최적화하는 것이다. 네트워크의 depth, resolution, width, kernel size, topology등을 바꿔가며 변형해, 최적의 성능을 가지는 파라미터를 찾는다.\n 위 그림은 depth를 변경하는 예시이다. 그림처럼 bottleneck 블록을 얼마나 깊게(2~4) 만들지 정하는 것이다."
  },
  {
    "objectID": "posts/lecs/lec07.html#design-the-search-space",
    "href": "posts/lecs/lec07.html#design-the-search-space",
    "title": "🧑‍🏫 Lecture 7-8",
    "section": "3.3 Design the search space",
    "text": "3.3 Design the search space\n그렇다면, search space를 어떻게 정의해야 할까? kernel size를 3~100, depth도 3~100 이렇게 모두 고려할 수는 없다. 이를 모두 테스트(performance 측정) 해보려면 엄청난 시간이 걸릴 것이다.  따라서 위 그림처럼 휴리스틱 방식등을 통해 더 좁은 search space에서 탐색한다면 시간을 줄일 수 있다.(search space optimization)  좋은 search space를 만들기 위해서, FLOPS(연산수)를 활용할 수 있다. 같은 메모리 조건에서 더 많은 연산을 진행하는 모델은, 더 나은 성능을 보일 것이기 때문이다. 위 그림에서 검정 색의 선보다 빨간 선의 search space를 고르는 것이 더 좋은 모델을 얻을 확률이 높다 (Larger FLOPS -&gt; Larger model capacity -&gt; More likey to give higher accuracy)"
  },
  {
    "objectID": "posts/lecs/lec07.html#search-strategy",
    "href": "posts/lecs/lec07.html#search-strategy",
    "title": "🧑‍🏫 Lecture 7-8",
    "section": "3.4 Search strategy",
    "text": "3.4 Search strategy\nsearch space를 정의했다면, 어떤 식으로 탐색해야 할까? 이 챕터에서는 5가지 방법을 다룬다. - Grid search - Random search - Reinforcement learning - Gradient descent - Evolutionary search\n\n3.4.1 Grid search\n grid search는 완전 탐색같은 방법이다. 위 그림처럼 가능한 모든 방법을 조합하며 최적을 찾는다.  좀더 일반화하면, 위 그림처럼 depth, width, resolution등의 파라미터에 우측처럼 식을 세워서 기존 모델과의 FLOPS가 얼마나 차이나도 되는지 범위를 설정하며 grid search를 진행할 수 있다.\n\n\n3.4.2 Random search\n Random search는 말그래도 무작위로 고르는 것이다. 최적화 측면에서 의미가 있다기 보다는, NAS 구현에 이상이 있는지 체크하는 sanity check 용도로 사용된다.\n\n\n3.4.3 Reinforcement learning\n Reinforcement learning은 말그대로 강화 학습(RL)을 이용한 방식이다. 이전에 cell-level을 설명했을 때 사용했던 RNN같은 controller를 사용해 모델(architecture)을 만들고, 이를 평가한 결과를 RNN에 반영해 최적화하는 방식이다.\n\n\n3.4.4 Gradient descent\n Gradient descent방식을 적용할 수도 있다. 그때마다 모델 평가를 하지 않아도 되기 때문에 더 computing efficient하다. 위 그림 (c)에서 3가지 path를 기억했다가, 확률값 높은 곳을 선택하고 그 결과를 통해 gradient를 역전파한다. 모든 선택(activation)을 메모리에 보관하고 있어야 한다는 단점이 있다.\n\n\n3.4.5 Evolutionary search\n Evelutionary search는 mutation-crossover 과정을 통해 모델을 변화시키며 최적의 모델을 찾아간다.  mutation은 기존의 모델 구조를 변형시키는 과정이다. 위 그림에서 모델의 depth를 임의로 변형시키는 것처럼, width나 kernel size를 랜덤하게 변화시켜 muation된 네트워크를 만든다.  그런 뒤, crossover를 통해 두 모델을 유전받는 것처럼 랜덤하게 합친다. 이렇게 만든 새 모델을 평가해가며 최적의 모델을 찾는다."
  },
  {
    "objectID": "posts/lecs/lec07.html#performanceaccuracy-estimation-strategy",
    "href": "posts/lecs/lec07.html#performanceaccuracy-estimation-strategy",
    "title": "🧑‍🏫 Lecture 7-8",
    "section": "4.1 Performance(Accuracy) estimation strategy",
    "text": "4.1 Performance(Accuracy) estimation strategy\n 앞의 NAS 구조 그림을 다시 보면, search space와 search strategy를 통해 만든 모델(구조)을 평가해야 한다. 이 때 이 모델은 단순히 구조만 잡혀 있는 것이기에, 간단히 생각해보면 처음부터 다시 학습을 해야 모델의 성능을 측정할 수 있다. 하지만 NAS 과정에서 굉장히 많은 구조를 만들게 되고, 이를 일일히 학습하면 엄청난 시간이 걸릴 것이다. 어떤 식으로 해결하는지 살펴보자.\n\n4.1.1 Train from scratch\n 앞서 말한 것처럼 가장 간단한 방법은 모든 구조를 학습해 보는 것이다. 그러나 이는 CIFAR10이라는 작은 문제를 해결하는 모델을 학습하는데도 12800개의 모델을 학습해야 하고, 이는 22,400 GPU-hour라는 어마어마한 시간이 걸린다.\n\n\n4.1.2 Inherit weight\n 그래서 등장한 방식이, 기존에 학습한 weight는 가만히 둔 채, 새로운 node를 추가하는 방식이다. 위 그림에서 Net2Wider는 기존 모델에서 width를 추가하고, Net2Deeper는 depth를 추가하고 기존 모델(parent model)의 weight를 적당히 계승한다. 이렇게 하면 새로 처음부터 학습하지 않아도 된다.\n\n\n4.1.3 Hypernetwork\n Hypernetwork는 모델의 구조를 보고 그 모델의 weight를 예측하는 방식을 사용한다. GNN을 통해 topology(모델 구조)에 대한 feature를 추출해 weight을 예측한다."
  },
  {
    "objectID": "posts/lecs/lec07.html#zero-shot-nas",
    "href": "posts/lecs/lec07.html#zero-shot-nas",
    "title": "🧑‍🏫 Lecture 7-8",
    "section": "4.2 Zero-shot NAS",
    "text": "4.2 Zero-shot NAS\n그렇다면 아예 train 하지 않고 NAS할 수 없을까? 1. ZenNAS 2. GradSign 두 가지 방법이 있지만, 그다지 좋은 방법은 아니다. ### 4.2.1 ZenNAS ZenNAS는 좋은 모델은 입력의 변화에 민감할 것이라는 가정을 통해 좋은 모델을 찾고자 한다. 과정은 아래와 같다.\n\n정규분포(N)을 따르는 임의의 입력 x를 만든다\nx를 살짝 변형한 x’을 만든다.\nx와 x’을 입력으로 사용한 모델의 결과를 비교해, 큰 차이가 없다면 나쁜 모델로 본다. 이 방법은 우리가 언뜻 생각하기에도 좋은 방법은 아니다.\n\n\n4.2.2 GradSign\n GradSign은 좋은 모델이 sample간에 local minima가 비슷한 위치에 분포할 것이라는 가정을 통해 좋은 모델을 찾고자 한다. local minima가 비슷한 위치라면(위 그림의 오른쪽),두 sample간에 gradient가 같은 부호(Sign)을 가질 것이고 이를 통해 좋은 모델인지 판단한다. (만약 부호가 같다면 모두 더했을 때 절댓값이 더 클 것이다)"
  },
  {
    "objectID": "posts/lecs/lec07.html#hardware-aware-nas",
    "href": "posts/lecs/lec07.html#hardware-aware-nas",
    "title": "🧑‍🏫 Lecture 7-8",
    "section": "4.3 Hardware-aware NAS",
    "text": "4.3 Hardware-aware NAS\n 오른쪽 그림처럼, 같은 모델을 여러 디바이스에 탑재하려면, 각각의 디바이스에 최적화덴 모델을 만들어야 하고, NAS 과정에서도 이에 대한 고려가 필요하다  그런데, 디바이스에 최적화 한다는 것은 MACs를 단순히 계산하는 것과는 다르다. 위 그림처럼 MACs는 더 적지만, latency가 더 늘어날 수도 있다. 디바이스 환경에서 가장 중요한 지표는 latency이다.  그런데 각각의 디바이스에서 모델의 latency를 평가하는 것은 굉장히 어렵고, 느리다.  따라서 사용할 수 있는 방법은, latency를 임의로 예측하는 것이다. latency를 예측하는 방법으로는 1. latency lookup table 2. network-wise latency profiling 등이 있다.\n\n4.3.1 latency lookup table\n 모델 구조와, 사용하는 연산(Op)의 latency(Lat)을 입력으로 주면, 해당 입력을 보고 산술적으로 +-해 계산하는 방식이다\n\n\n4.3.2 network-wise latency profiling\n kernel size와 width등을 입력으로하는 ML 모델을 만들어서 latency를 측정하는 방식이다  이렇게 latency를 예측하는 방법은, 꽤나 높은 정확도를 보인다"
  },
  {
    "objectID": "posts/lecs/lec07.html#neural-hardware-architecture-co-search",
    "href": "posts/lecs/lec07.html#neural-hardware-architecture-co-search",
    "title": "🧑‍🏫 Lecture 7-8",
    "section": "4.4 Neural-hardware architecture co-search",
    "text": "4.4 Neural-hardware architecture co-search\n NAS는 모델의 구조만 잘 바꾼다고 되는 것이 아니라, 하드웨어 또한 고려해야 한다.  Local Buffer Size, Global Buffer size, #PE처럼 단순히 숫자(sizing)에 관한 것도 있지만, Compute Array Size처럼 조금더 복합적으로 영향을 미치는 파라미터도 있다."
  },
  {
    "objectID": "posts/lecs/lec07.html#nlp-gan-point-cloud-pose",
    "href": "posts/lecs/lec07.html#nlp-gan-point-cloud-pose",
    "title": "🧑‍🏫 Lecture 7-8",
    "section": "5.1 NLP, GAN, point cloud, pose",
    "text": "5.1 NLP, GAN, point cloud, pose\nNAS의 활용 예시들이다. 강의 슬라이드만 간단히 첨부한다    얼굴 사진을 가지고 표정, 노화 정도등을 실시간으로 조정해보는 어플리케이션이다. NAS의 좋은 활용 예시인데, 작은 모델(small sub-net)으로는 실시간 변동에 대응하고, 큰 모델(large sub-net)로는 확정되는 이미지를 제작할 때 사용한다.\n여기까지 NAS 설명을 정리한 글입니다. 강의 내용을 많이 요약했기 때문에, 더 자세한 내용은 강의(https://hanlab.mit.edu/courses/2023-fall-65940)의 7-8강을 참조 부탁드립니다."
  },
  {
    "objectID": "posts/labs/lab04.html",
    "href": "posts/labs/lab04.html",
    "title": "👩‍💻 Lab 4",
    "section": "",
    "text": "이번 시간은 LLM Quantization을 실습하는 시간입니다. 데이터셋으로는 wikitext-2-raw-v1를 사용한고 Facebook에서 공개한 1.3 Billion 크기의 LLM 모델을 가지고 Activation Aware Quantization을 실습해볼 것입니다.\nTop 1%를 FP16으로 유지하면 Weight가 다른 데이터 타입으로 해야하는 데 이는 현실적으로는 힘들다고 하기에, Scaling하는 것 까지 실험할 예정입니다.\n실습 순서는 다음과 같습니다.\n이번 실습은 길지 않아 순서대로 진행하며, 각 단계별로 실험 결과를 확인해보면 되겠습니다."
  },
  {
    "objectID": "posts/labs/lab04.html#introduction",
    "href": "posts/labs/lab04.html#introduction",
    "title": "👩‍💻 Lab 4",
    "section": "Introduction",
    "text": "Introduction\nThis colab notebook provides code and a framework for Lab 4: LLM Quantization. You will learn how to quantize a large language model that can run efficiently. We will implement AWQ (activation aware weight only quantization) for 4 bit weight-only quantization.\nRunning large language models (LLMs) on the edge is of great importance, which not only enhances user experience but also addresses privacy concerns, as sensitive data remains localized and reduces the risk of potential breaches.\nHowever, deploying LLMs on the edge presents significant challenges. Edge devices operate under tight power constraints, setting them apart from workstations or cloud servers. This translates to restricted memory bandwidth and limited peak computation throughput on the edge. For instance, the NVIDIA Jetson Orin Nano, with its 8GB DRAM, cannot accommodate even the most compact LLaMA-2 model in half precision. Thankfully, AWQ presents a push-the-button solution for weight quantization, empowering LLM inference on edge devices with constrained memory.\nFurthermore, by using the AWQ 4-bit weight-only quantization algorithm, combined with an efficient 4-bit kernel, we can achieve the following acceleration on the RTX 4090. In the next lab section, we will also use TinyChatEnigne to achieve actual performance acceleration."
  },
  {
    "objectID": "posts/labs/lab04.html#setup",
    "href": "posts/labs/lab04.html#setup",
    "title": "👩‍💻 Lab 4",
    "section": "Setup",
    "text": "Setup\n\nprint('Installing packages...')\n!pip install torch transformers==4.31.0 accelerate==0.21.0 sentencepiece==0.1.99 tokenizers==0.13.3 tqdm zstandard\n# !pip install torch transformers==4.31.0 accelerate==0.21.0 sentencepiece==0.1.99 tokenizers==0.13.3 datasets==2.14.4 tqdm zstandard\n\n\n# ValueError: Invalid pattern: '**' can only be an entire path component\n# https://stackoverflow.com/questions/77671277/valueerror-invalid-pattern-can-only-be-an-entire-path-component\n# The solution works for datasets version 2.10.1 on Python 3.10, as it should update the package with a hotfix that was added for version &gt; 2.15.0.\n!pip install -U datasets\n\n\nimport tqdm\nimport torch\nfrom torch import nn\nfrom transformers import AutoModelForCausalLM, AutoTokenizer\nfrom datasets import load_dataset\nfrom functools import partial\nimport gc\n\nHere we use wikitext-2 dataset for evaluation. The dataset is automatically downloaded by the code.\n\ndef evaluate(model, tokenizer):\n    testenc = load_dataset('wikitext', 'wikitext-2-raw-v1', split='test')\n    # testenc = load_dataset('wikitext', 'wikitext-103-v1', split='train')\n    testenc = tokenizer(\"\\n\\n\".join(testenc['text']), return_tensors='pt')\n\n    testenc = testenc.input_ids.to(model.device)\n    nsamples = 40\n    model = model.eval()\n\n    nlls = []\n    for i in tqdm.tqdm(range(nsamples), desc=\"evaluating...\"):\n        batch = testenc[:, (i * 2048):((i + 1) * 2048)].to(model.device)\n        with torch.no_grad():\n            lm_logits = model(batch).logits\n        shift_logits = lm_logits[:, :-1, :].contiguous().float()\n        shift_labels = testenc[:, (i * 2048):((i + 1) * 2048)][:, 1:]\n        loss_fct = nn.CrossEntropyLoss()\n        loss = loss_fct(shift_logits.view(-1, shift_logits.size(-1)), shift_labels.view(-1))\n        neg_log_likelihood = loss.float() * 2048\n        nlls.append(neg_log_likelihood)\n\n    return torch.exp(torch.stack(nlls).sum() / (nsamples * 2048))\n\nThe following code is used to calculate the model size.\n\ndef get_model_size(model: nn.Module, data_width=16, group_size=-1):\n\n    if group_size != -1:\n        data_width += (16 + 4) / group_size\n\n    num_elements = 0\n    for param in model.parameters():\n        num_elements += param.numel()\n    return num_elements * data_width\n\nByte = 8\nKiB = 1024 * Byte\nMiB = 1024 * KiB\nGiB = 1024 * MiB\n\n\nmodel_path = \"facebook/opt-1.3b\"\ntokenizer = AutoTokenizer.from_pretrained(model_path, use_fast=False)\nmodel = AutoModelForCausalLM.from_pretrained(model_path, device_map=\"auto\")\n\nLet’s first evaluate the perplexity and model size of the FP32 Model.\n\n# Evaluate the model\nmodel_perplexity = evaluate(model, tokenizer)\nmodel_size = get_model_size(model, data_width=32, group_size=128)\nprint(f\"\\nmodel perplexity: {model_perplexity:.2f}\")\nprint(f\"model size: {model_size/MiB:.2f} MiB\")\n\nevaluating...: 100%|██████████| 40/40 [00:16&lt;00:00,  2.41it/s]\n\n\n\nmodel perplexity: 14.47\nmodel size: 5043.73 MiB\n\n\n\n# Check each layer in model \nfor n, m in model.named_modules():\n    print(f\"{n}: {m}\")\n    if isinstance(m, nn.Linear):\n        print(f\"{n}: {m.weight.shape}\")\n\nUniform quantization is to map real values in the range \\([\\beta, \\alpha]\\) to lie within \\([0, 2^{b} - 1]\\).\nNotation:\n\nQuantized Weight: \\(w_q\\)\nScale factor: \\(s_q\\)\nZero Point: \\(z\\)\n\n\\[\\begin{equation}\ns_q = \\frac{\\alpha - \\beta}{2^{b} - 1} \\tag{1},\n\\end{equation}\\]\n\\[\\begin{equation}\nz = -\\text{Round}(\\beta * scale) \\tag{2}\n\\end{equation}\\]\n\\[\\begin{equation}\nw_q = \\text{Clamp}(\\text{Round}(\\frac{w}{s_q}) + z) \\tag{3},\n\\end{equation}\\]\n\npseudo quantization\nThe following code is for pseudo quantization.\nPseudo Quantization is used to simulate the effects of quantization on a model without actually quantizing the model’s weights. (i.e. rounding to the nearest quantized value and then dequantizing back to a float.)\n\ntest_vector = torch.randn(10)\n# test_vector[0] = torch.nan\ntorch.isnan(test_vector).sum() == 0\n\ntensor(True)\n\n\n\n# core quantization method (simulated quantization)\ndef pseudo_quantize_tensor(w, n_bit=4, q_group_size=-1):\n    org_w_shape = w.shape\n    if q_group_size &gt; 0:\n        assert org_w_shape[-1] % q_group_size == 0\n        w = w.reshape(-1, q_group_size)\n\n    assert w.dim() == 2\n\n    # Calculate the maximum (\\alpha) and minimum values (\\beta) in the tensor.\n    max_val = w.amax(dim=1, keepdim=True)\n    assert max_val.dim() == 2 and max_val.size(0) == w.size(0) and max_val.size(1) == 1\n    min_val = w.amin(dim=1, keepdim=True)\n    assert min_val.dim() == 2 and min_val.size(0) == w.size(0) and min_val.size(1) == 1\n\n    # Calculate the scale factor and zero point.  (Formula 1 & 2)\n    max_int = 2 ** n_bit - 1\n    scales = (max_val - min_val).clamp(min=1e-5) / max_int\n    assert scales.shape == max_val.shape\n    zeros = (-torch.round(min_val / scales)).clamp_(0, max_int)\n    assert scales.shape == min_val.shape\n    assert torch.isnan(scales).sum() == 0\n    assert torch.isnan(w).sum() == 0\n\n    # Quantize W: Map values in the range [\\beta, \\alpha] to lie within [0, 2^b - 1] (Formula 3)\n    w = torch.clamp(torch.round(w / scales) + zeros, 0, max_int)\n    assert w.dim() == 2 and w.size(0) == scales.size(0) and w.size(1) == q_group_size\n\n    # Dequantize W (pseudo quantization, the inverse transformation of Formula 3)\n    w = (w - zeros) * scales\n    assert w.dim() == 2 and w.size(0) == scales.size(0) and w.size(1) == q_group_size\n\n    assert torch.isnan(w).sum() == 0\n\n    w = w.reshape(org_w_shape)\n    return w\n\n@torch.no_grad()\ndef pseudo_quantize_model_weight(\n    model, w_bit, q_group_size,\n):\n    for n, m in model.named_modules():\n        if isinstance(m, nn.Linear):\n            m.weight.data = pseudo_quantize_tensor(m.weight.data, n_bit=w_bit, q_group_size=q_group_size)\n\nLet’s evaluate the perplexity and model size of the quantized 3-bit Model.\n\ndel model\ngc.collect()\ntorch.cuda.empty_cache()\nmodel = AutoModelForCausalLM.from_pretrained(model_path, device_map=\"auto\")\npseudo_quantize_model_weight(model, w_bit=3, q_group_size=128)\n\n# Evaluate the model\nmodel_perplexity = evaluate(model, tokenizer)\nmodel_size = get_model_size(model, data_width=3, group_size=128)\nprint(f\"\\nmodel perplexity: {model_perplexity:.2f}\")\nprint(f\"model size: {model_size/MiB:.2f} MiB\")\n\nevaluating...: 100%|██████████| 40/40 [00:16&lt;00:00,  2.40it/s]\n\n\n\nmodel perplexity: 121.90\nmodel size: 495.06 MiB\n\n\nWe can see that the model size has decreased, but the perplexity has significantly increased.\nThere is a observation in LLM activations that outliers appear in a small fraction of the channels. If one channel has an outlier, it persistently appears in all tokens. The variance amongst the channels for a given token is large (the activations in some channels are very large, but most are small), but the variance between the magnitudes of a given channel across tokens is small (outlier channels are consistently large).\nAccording to the observation of AWQ, weight channels corresponding to activation outliers are more salient, and preserving those salient weights can lead to a significant performance improvement. Next, let’s try to find the salient weights and retain them as original values to observe the change in perplexity.\nThe following code is used to load the calibration dataset, so as to obtain activation outliers to identify salient weights.\n\ntest_vector = torch.randn(10, 20, 10, 10)\nprint(test_vector.shape)\ntest_vector = test_vector.view(-1, test_vector.shape[-1])\nprint(test_vector.shape)\ntest_vector = test_vector.mean(dim=0)\nprint(test_vector.shape)\n\ntorch.Size([10, 20, 10, 10])\ntorch.Size([2000, 10])\ntorch.Size([10])\n\n\n\ndef get_calib_dataset(tokenizer=None, n_samples=256, block_size=512):\n    dataset = load_dataset(\"mit-han-lab/pile-val-backup\", split=\"validation\")\n    dataset = dataset.shuffle(seed=42)\n    samples = []\n    n_run = 0\n    for data in dataset:\n        line = data[\"text\"]\n        line = line.strip()\n        line_encoded = tokenizer.encode(line)\n        if len(line_encoded) &gt; block_size:\n            continue\n        sample = torch.tensor([line_encoded])\n        if sample.numel() == 0:\n            continue\n        samples.append(sample)\n        n_run += 1\n        if n_run == n_samples:\n            break\n\n    # now concatenate all samples and split according to block size\n    cat_samples = torch.cat(samples, dim=1)\n    n_split = cat_samples.shape[1] // block_size\n    print(f\" * Split into {n_split} blocks\")\n    return [cat_samples[:, i*block_size:(i+1)*block_size] for i in range(n_split)]\n\n@torch.no_grad()\ndef get_calib_feat(model, tokenizer):\n    input_dict = dict()\n    def stat_input_max_hook(m, x, y, name):\n        if isinstance(x, tuple):\n            x = x[0]\n        x_max = x.view(-1, x.shape[-1]).abs().mean(dim=0).cpu().detach()  # each channel's mean in each block (127 in this case)\n        if name not in input_dict:\n            input_dict[name] = [x_max]\n        else:\n            input_dict[name] += [x_max]\n\n    hooks = []\n    for name, m in model.named_modules():\n        if isinstance(m, nn.Linear):\n            hooks.append(\n                m.register_forward_hook(\n                    partial(stat_input_max_hook, name=name)))\n\n    print(\"Collecting activation scales...\")\n    device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n\n    samples = get_calib_dataset(tokenizer)\n    pbar = tqdm.tqdm(samples)\n    for input_ids in pbar:\n        input_ids = input_ids.to(device)  # (1, 512)\n        model(input_ids) # gather input max\n\n    for hook in hooks:\n        hook.remove()\n    return input_dict\n\n\ndel model\ngc.collect()\ntorch.cuda.empty_cache()\nmodel = AutoModelForCausalLM.from_pretrained(model_path, device_map=\"auto\")\ninput_feat = get_calib_feat(model, tokenizer) # each block's mean, 127 blocks\n\n\ninput_feat.__len__()\n\n145\n\n\n\nfor k, v in input_feat.items():  # Max in each block(127 blocks)\n    print(f\"{k}: {v.__len__()} {v[0].shape}\") # each kayer, 127 blocks, each block has 2048 elements\n    break\n\nmodel.decoder.layers.0.self_attn.q_proj: 127 torch.Size([2048])\n\n\n\n\nQuestion 1 (50 pts)\n\nQuestion 1.1 (20 pts)\nNext, please add codes before and after the quantization to protect 1% of the salient weight channels (1% channels with highest importance), ensuring that their values remain unchanged after quantization. (The desired perplexity is 17.15)\n\n@torch.no_grad()\ndef pseudo_quantize_model_salient_weight_fp16(\n    model, w_bit, q_group_size, input_feat\n):\n    for n, m in model.named_modules():\n        if isinstance(m, nn.Linear):\n            importance = sum(input_feat[n]).float()\n\n            ############### YOUR CODE STARTS HERE ###############\n\n            # Step 1: Find 1% of the salient weight channels according to importance (hint: use torch.topk())\n            outlier_indices = torch.topk(importance, int(0.01 * importance.size(0)), largest=True).indices\n            assert outlier_indices.dim() == 1\n\n            ############### YOUR CODE ENDS HERE #################\n\n            # Back up the values of the salient weight channels\n            outlier = m.weight.data[:, outlier_indices].clone()\n\n            m.weight.data = pseudo_quantize_tensor(m.weight.data, n_bit=w_bit, q_group_size=q_group_size)\n\n            ############### YOUR CODE STARTS HERE ###############\n\n            # Step 2: Restore the 1% salient weight channels to their original FP16 values\n            m.weight.data[:, outlier_indices] = outlier\n\n            ############### YOUR CODE ENDS HERE #################\n\n\ndel model\ngc.collect()\ntorch.cuda.empty_cache()\nmodel = AutoModelForCausalLM.from_pretrained(model_path, device_map=\"auto\")\npseudo_quantize_model_salient_weight_fp16(model, w_bit=3, q_group_size=128, input_feat=input_feat)\n\n# Evaluate the model\nmodel_perplexity = evaluate(model, tokenizer)\nmodel_size = get_model_size(model, data_width=3, group_size=128)\nprint(f\"\\nmodel perplexity: {model_perplexity:.2f}\")\nprint(f\"model size: {model_size/MiB:.2f} MiB\")\n\nevaluating...: 100%|██████████| 40/40 [00:16&lt;00:00,  2.40it/s]\n\n\n\nmodel perplexity: 17.15\nmodel size: 495.06 MiB\n\n\n\n\nQuestion 1.2 (15 pts)\nLet’s conduct an ablation experiment: randomly protect 1% of the weight channels, ensuring that their values remain unchanged after quantization, and then observe the perplexity. (The desired perplexity is over 100)\n\nx = torch.randn(20, )\nprint(x)\ny = torch.randperm(x.size(0)) &lt; 5\nprint(y)\n\ntensor([ 0.5130,  3.1619, -1.9837, -1.1557, -0.4532, -0.4831,  0.7884, -0.1122,\n         0.6722, -0.2606, -1.2696,  0.3370,  0.0889,  0.2339,  0.5943,  1.2150,\n        -0.7380,  1.0870, -1.5264, -0.7932])\ntensor([False, False, False, False,  True, False, False, False,  True, False,\n        False, False, False, False,  True, False, False,  True, False,  True])\n\n\n\n@torch.no_grad()\ndef pseudo_quantize_model_random_weight_fp16(\n    model, w_bit, q_group_size, input_feat\n):\n    for n, m in model.named_modules():\n        if isinstance(m, nn.Linear):\n            importance = sum(input_feat[n]).float()\n\n            ############### YOUR CODE STARTS HERE ###############\n\n            # Step 1: Randomly choose 1% of the weight channels\n            outlier_mask = torch.randperm(importance.size(0)) &lt; int(0.01 * importance.size(0))\n            assert outlier_mask.dim() == 1\n\n            ############### YOUR CODE ENDS HERE #################\n\n            # Back up the values of the selected weight channels\n            outlier = m.weight.data[:, outlier_mask].clone()\n\n            m.weight.data = pseudo_quantize_tensor(m.weight.data, n_bit=w_bit, q_group_size=q_group_size)\n\n            ############### YOUR CODE STARTS HERE ###############\n\n            # Step 2: Restore the 1% selected weight channels to their original FP16 values\n            m.weight.data[:, outlier_mask] = outlier  \n\n            ############### YOUR CODE ENDS HERE #################\n\n\ndel model\ngc.collect()\ntorch.cuda.empty_cache()\nmodel = AutoModelForCausalLM.from_pretrained(model_path, device_map=\"auto\")\npseudo_quantize_model_random_weight_fp16(model, w_bit=3, q_group_size=128, input_feat=input_feat)\n\n# Evaluate the model\nmodel_perplexity = evaluate(model, tokenizer)\nmodel_size = get_model_size(model, data_width=3, group_size=128)\nprint(f\"\\nmodel perplexity: {model_perplexity:.2f}\")\nprint(f\"model size: {model_size/MiB:.2f} MiB\")\n\nevaluating...: 100%|██████████| 40/40 [00:16&lt;00:00,  2.41it/s]\n\n\n\nmodel perplexity: 116.94\nmodel size: 495.06 MiB\n\n\n\n\nQuestion 1.3 (15 pts)\nPlease provide a possible explanation for why the salient weight channels are so important.\n\n\nAnswser 1.3\nWhen the salient weight channels remains not randomly, the model perlexity contains as high performance. It seems that the salient weight channels are important for the model to maintain its performance, and those channels have most information than other less important channels.\n\n\n\nQuestion 2 (50 pts)\nDespite keeping 0.1% of weights in FP16 can improve the quantized performance without a noticeable increase in model size (measured in total bits), such a mixed-precision data type will make the system implementation difficult. We need to come up with a method to protect the important weights without actually keeping them as FP16.\nAccording to the methodology of AWQ, simply scaling up the salient weight channels can protect them. The principle is as follows:\n\nConsider a linear layer channel \\(\\mathbf{y} = \\mathbf{w}x\\) (from \\(\\mathbf{W}x\\)). We care about the quantization error from \\(Q(\\mathbf{w})x\\).\n\\(Err(Q(\\mathbf{w}) x) = Δ\\cdot RoundErr(\\frac{\\mathbf{w}}{Δ})\\cdot x\\), \\(Δ = \\frac{\\max(|w|)}{2^{N - 1}}\\).\nThe scaled version is \\(Err(Q(\\mathbf{w} \\cdot s)(\\frac{x}{s})) = Δ\\cdot RoundErr(\\frac{\\mathbf{w}}{Δ})\\cdot x\\cdot \\mathbf{\\frac{1}{s}}\\).\nThe \\(RoundErr\\) is always ~0.25 (average from 0-0.5).\nWhen the group size is relatively large (e.g., 128), scaling up one channel usually does not increase the maximum value in a group (i.e. \\(Δ\\) remains unchanged).\nThus, \\(Err(Q(\\mathbf{w} \\cdot s)(\\frac{x}{s})) = Δ\\cdot RoundErr(\\frac{\\mathbf{w}}{Δ})\\cdot x\\cdot \\mathbf{\\frac{1}{s}}\\) &lt; \\(Δ\\cdot RoundErr(\\frac{\\mathbf{w}}{Δ})\\cdot x = Err(Q(\\mathbf{w}) x)\\).\n\nTaking the following figure as an example, if we assume 3-bit int quantization, then the quantization error caused by the value in the last column of the second row of \\(W(+1.4)\\) should be \\(Err(Q(\\mathbf{w}) x) = Δ\\cdot RoundErr(\\frac{\\mathbf{w}}{Δ})\\cdot x\\) = \\(\\frac{4}{2^{3 - 1}} * |1.4 - 1.0| * (2 + 2 + 2) = 2.4\\).\nIf the second channel is scaled up by a factor of \\(2\\), the resulting quantization error would reduce to \\(\\frac{4}{2^{3 - 1}} * |2.8 - 3.0| * (2/2 + 2/2 + 2/2) = 0.6\\).\n\n\n\nscaleup.png\n\n\n\nQuestion 2.1 (20 pts)\nPlease write code to scale up the salient weight channels, then quantize it, and finally scale it back down, and observe the changes in perplexity. (The desired perplexity is 18.93)\n\n@torch.no_grad()\ndef pseudo_quantize_model_weight_scaleup(\n    model, w_bit, q_group_size, input_feat, scale_factor\n):\n    for n, m in model.named_modules():\n        if isinstance(m, nn.Linear):\n            importance = sum(input_feat[n]).float()\n\n            ############### YOUR CODE STARTS HERE ###############\n\n            # Step 1: Find 1% of the salient weight channels\n            outlier_mask = torch.topk(importance, int(0.01 * importance.size(0)), largest=True).indices\n            assert outlier_mask.dim() == 1\n\n            ############### YOUR CODE ENDS HERE #################\n\n            # To simulate applying the scale factor, we can simply multiply it before quantization, and then divide by the scale factor after quantization.\n            # Scale up the values of the salient weight channels\n            m.weight.data[:, outlier_mask] *= scale_factor\n\n            m.weight.data = pseudo_quantize_tensor(m.weight.data, n_bit=w_bit, q_group_size=q_group_size)\n\n            ############### YOUR CODE STARTS HERE ###############\n\n            # Step 2: Scale back down the values of the salient weight channels\n            m.weight.data[:, outlier_mask] /= scale_factor\n\n            ############### YOUR CODE ENDS HERE #################\n\n\ndel model\ngc.collect()\ntorch.cuda.empty_cache()\nmodel = AutoModelForCausalLM.from_pretrained(model_path, device_map=\"auto\")\npseudo_quantize_model_weight_scaleup(model, w_bit=3, q_group_size=128, input_feat=input_feat, scale_factor=2)\n\n# Evaluate the model\nmodel_perplexity = evaluate(model, tokenizer)\nmodel_size = get_model_size(model, data_width=3, group_size=128)\nprint(f\"\\nmodel perplexity: {model_perplexity:.2f}\")\nprint(f\"model size: {model_size/MiB:.2f} MiB\")\n\nevaluating...: 100%|██████████| 40/40 [00:16&lt;00:00,  2.40it/s]\n\n\n\nmodel perplexity: 18.93\nmodel size: 495.06 MiB\n\n\n\nscale_list = (1, 2, 3, 4, 5, 6, 7, 8)\nfor scale_factor in scale_list:\n    del model\n    gc.collect()\n    torch.cuda.empty_cache()\n    model = AutoModelForCausalLM.from_pretrained(model_path, device_map=\"auto\")\n    pseudo_quantize_model_weight_scaleup(model, w_bit=3, q_group_size=128, input_feat=input_feat, scale_factor=scale_factor)\n\n    # Evaluate the model\n    model_perplexity = evaluate(model, tokenizer)\n    model_size = get_model_size(model, data_width=3, group_size=128)\n    print(f\"\\n  Scale factor {scale_factor}\")\n    print(f\"\\nmodel perplexity: {model_perplexity:.2f}\")\n    print(f\"model size: {model_size/MiB:.2f} MiB\")\n\nevaluating...: 100%|██████████| 40/40 [00:16&lt;00:00,  2.40it/s]\nevaluating...: 100%|██████████| 40/40 [00:16&lt;00:00,  2.40it/s]\nevaluating...: 100%|██████████| 40/40 [00:16&lt;00:00,  2.39it/s]\nevaluating...: 100%|██████████| 40/40 [00:16&lt;00:00,  2.39it/s]\nevaluating...: 100%|██████████| 40/40 [00:16&lt;00:00,  2.39it/s]\nevaluating...: 100%|██████████| 40/40 [00:16&lt;00:00,  2.39it/s]\nevaluating...: 100%|██████████| 40/40 [00:16&lt;00:00,  2.39it/s]\nevaluating...: 100%|██████████| 40/40 [00:16&lt;00:00,  2.39it/s]\n\n\n\n  Scale factor 1\n\nmodel perplexity: 121.90\nmodel size: 495.06 MiB\n\n  Scale factor 2\n\nmodel perplexity: 18.93\nmodel size: 495.06 MiB\n\n  Scale factor 3\n\nmodel perplexity: 19.25\nmodel size: 495.06 MiB\n\n  Scale factor 4\n\nmodel perplexity: 21.26\nmodel size: 495.06 MiB\n\n  Scale factor 5\n\nmodel perplexity: 24.50\nmodel size: 495.06 MiB\n\n  Scale factor 6\n\nmodel perplexity: 30.36\nmodel size: 495.06 MiB\n\n  Scale factor 7\n\nmodel perplexity: 47.13\nmodel size: 495.06 MiB\n\n  Scale factor 8\n\nmodel perplexity: 100.99\nmodel size: 495.06 MiB\n\n\n\nscale_list = torch.arange(2, 3.1, 0.1)\nfor scale_factor in scale_list:\n    del model\n    gc.collect()\n    torch.cuda.empty_cache()\n    model = AutoModelForCausalLM.from_pretrained(model_path, device_map=\"auto\")\n    pseudo_quantize_model_weight_scaleup(model, w_bit=3, q_group_size=128, input_feat=input_feat, scale_factor=scale_factor)\n\n    # Evaluate the model\n    model_perplexity = evaluate(model, tokenizer)\n    model_size = get_model_size(model, data_width=3, group_size=128)\n    print(f\"\\n  Scale factor {scale_factor}\")\n    print(f\"\\nmodel perplexity: {model_perplexity:.2f}\")\n    print(f\"model size: {model_size/MiB:.2f} MiB\")\n\nevaluating...: 100%|██████████| 40/40 [00:16&lt;00:00,  2.40it/s]\nevaluating...: 100%|██████████| 40/40 [00:16&lt;00:00,  2.39it/s]\nevaluating...: 100%|██████████| 40/40 [00:16&lt;00:00,  2.39it/s]\nevaluating...: 100%|██████████| 40/40 [00:16&lt;00:00,  2.39it/s]\nevaluating...: 100%|██████████| 40/40 [00:16&lt;00:00,  2.39it/s]\nevaluating...: 100%|██████████| 40/40 [00:16&lt;00:00,  2.39it/s]\nevaluating...: 100%|██████████| 40/40 [00:16&lt;00:00,  2.39it/s]\nevaluating...: 100%|██████████| 40/40 [00:16&lt;00:00,  2.39it/s]\nevaluating...: 100%|██████████| 40/40 [00:16&lt;00:00,  2.39it/s]\nevaluating...: 100%|██████████| 40/40 [00:16&lt;00:00,  2.39it/s]\nevaluating...: 100%|██████████| 40/40 [00:16&lt;00:00,  2.39it/s]\n\n\n\n  Scale factor 2.0\n\nmodel perplexity: 18.93\nmodel size: 495.06 MiB\n\n  Scale factor 2.0999999046325684\n\nmodel perplexity: 18.86\nmodel size: 495.06 MiB\n\n  Scale factor 2.200000047683716\n\nmodel perplexity: 19.00\nmodel size: 495.06 MiB\n\n  Scale factor 2.299999952316284\n\nmodel perplexity: 18.96\nmodel size: 495.06 MiB\n\n  Scale factor 2.4000000953674316\n\nmodel perplexity: 18.97\nmodel size: 495.06 MiB\n\n  Scale factor 2.5\n\nmodel perplexity: 19.20\nmodel size: 495.06 MiB\n\n  Scale factor 2.5999999046325684\n\nmodel perplexity: 18.91\nmodel size: 495.06 MiB\n\n  Scale factor 2.700000047683716\n\nmodel perplexity: 19.03\nmodel size: 495.06 MiB\n\n  Scale factor 2.799999952316284\n\nmodel perplexity: 18.90\nmodel size: 495.06 MiB\n\n  Scale factor 2.9000000953674316\n\nmodel perplexity: 19.12\nmodel size: 495.06 MiB\n\n  Scale factor 3.0\n\nmodel perplexity: 19.25\nmodel size: 495.06 MiB\n\n\n\nscale_list = torch.arange(1, 2.1, 0.1)\nfor scale_factor in scale_list:\n    del model\n    gc.collect()\n    torch.cuda.empty_cache()\n    model = AutoModelForCausalLM.from_pretrained(model_path, device_map=\"auto\")\n    pseudo_quantize_model_weight_scaleup(model, w_bit=3, q_group_size=128, input_feat=input_feat, scale_factor=scale_factor)\n\n    # Evaluate the model\n    model_perplexity = evaluate(model, tokenizer)\n    model_size = get_model_size(model, data_width=3, group_size=128)\n    print(f\"\\n  Scale factor {scale_factor}\")\n    print(f\"\\nmodel perplexity: {model_perplexity:.2f}\")\n    print(f\"model size: {model_size/MiB:.2f} MiB\")\n\nevaluating...: 100%|██████████| 40/40 [00:16&lt;00:00,  2.40it/s]\nevaluating...: 100%|██████████| 40/40 [00:16&lt;00:00,  2.39it/s]\nevaluating...: 100%|██████████| 40/40 [00:16&lt;00:00,  2.39it/s]\nevaluating...: 100%|██████████| 40/40 [00:16&lt;00:00,  2.39it/s]\nevaluating...: 100%|██████████| 40/40 [00:16&lt;00:00,  2.39it/s]\nevaluating...: 100%|██████████| 40/40 [00:16&lt;00:00,  2.39it/s]\nevaluating...: 100%|██████████| 40/40 [00:16&lt;00:00,  2.39it/s]\nevaluating...: 100%|██████████| 40/40 [00:16&lt;00:00,  2.39it/s]\nevaluating...: 100%|██████████| 40/40 [00:16&lt;00:00,  2.39it/s]\nevaluating...: 100%|██████████| 40/40 [00:16&lt;00:00,  2.39it/s]\nevaluating...: 100%|██████████| 40/40 [00:16&lt;00:00,  2.39it/s]\n\n\n\n  Scale factor 1.0\n\nmodel perplexity: 121.90\nmodel size: 495.06 MiB\n\n  Scale factor 1.100000023841858\n\nmodel perplexity: 47.12\nmodel size: 495.06 MiB\n\n  Scale factor 1.2000000476837158\n\nmodel perplexity: 30.96\nmodel size: 495.06 MiB\n\n  Scale factor 1.2999999523162842\n\nmodel perplexity: 25.90\nmodel size: 495.06 MiB\n\n  Scale factor 1.399999976158142\n\nmodel perplexity: 21.14\nmodel size: 495.06 MiB\n\n  Scale factor 1.5\n\nmodel perplexity: 20.19\nmodel size: 495.06 MiB\n\n  Scale factor 1.600000023841858\n\nmodel perplexity: 19.71\nmodel size: 495.06 MiB\n\n  Scale factor 1.7000000476837158\n\nmodel perplexity: 19.01\nmodel size: 495.06 MiB\n\n  Scale factor 1.7999999523162842\n\nmodel perplexity: 18.97\nmodel size: 495.06 MiB\n\n  Scale factor 1.899999976158142\n\nmodel perplexity: 18.89\nmodel size: 495.06 MiB\n\n  Scale factor 2.0\n\nmodel perplexity: 18.93\nmodel size: 495.06 MiB\n\n\n\n\nQuestion 2.2 (15 pts)\nPlease try different scale factors (e.g. 1, 2, 3, and 4) in the code and observe the changes in perplexity.\nDid you observe the perplexity first decreasing and then increasing? Please explain why this would happen based on the principle mentioned above.\n\n\nAnswer 2.2\n\nYOUR ANSWER STARTS HERE\nFrom Scale 2, this perplexities are gradually increasing. At Scale 8, it reach out over the perplexity 100.\nFrom Scale 2 to 3, those perplexities are also gradually increasing.\nFrom Scale 1 to 2, those perplexities are also gradually decreasing and it reach out saturation at Scale 1.8\n\n\nYOUR ANSWER ENDS HERE\n\n\n\n\nQuestion 2.3 (15 pts)\nDue to the instability of fine-tuning, it would be a better choice to find the optimal \\(s\\) within a predefined search space. We can find the optimal scale in the search space to protect the salient weights while also considering other values. In practice, it can be observed that considering just the activations is sufficient to yield good results. Please add the code for search and run it to observe the perplexity. (The desired perplexity is 17.92)\n\\[\n𝐋(\\mathbf{s})=\\lVert Q(\\mathbf{W}\\cdot \\mathbf{s})  (\\mathbf{s^{-1}} \\cdot \\mathbf{X}) - \\mathbf{W}\\mathbf{X}  \\rVert,  \\quad\\mathbf{s}= \\mathbf{s_X}^{\\alpha}\n\\] \\[\n\\mathbf{s}^* = \\text{argmin}_{\\mathbf{s}} 𝐋(\\mathbf{s}),\\quad \\alpha^*=\\text{argmin}_{\\alpha} 𝐋(\\mathbf{s_X}^{\\alpha})\n\\]\n\n@torch.no_grad()\ndef scale_ln_fcs(ln, fcs, scales):\n    if not isinstance(fcs, list):\n        fcs = [fcs]\n\n    scales = scales.to(ln.weight.device)\n\n    ln.weight.div_(scales)\n    if hasattr(ln, 'bias') and ln.bias is not None:\n        ln.bias.div_(scales)\n\n    for fc in fcs:\n        fc.weight.mul_(scales.view(1, -1))\n\n    for p in ln.parameters():\n        assert torch.isnan(p).sum() == 0\n    for fc in fcs:\n        for p in fc.parameters():\n            assert torch.isnan(p).sum() == 0\n\n\n@torch.no_grad()\ndef scale_fc_fc(fc1, fc2, scales):\n    assert isinstance(fc1, nn.Linear)\n    assert isinstance(fc2, nn.Linear)\n\n    scales = scales.to(fc1.weight.device)\n\n    # fc1.weight.div_(scales.view(-1, 1))\n    fc1.weight[-scales.size(0):].div_(scales.view(-1, 1))\n    if fc1.bias is not None:\n        fc1.bias.div_(scales.view(-1))\n\n    fc2.weight.mul_(scales.view(1, -1))\n\n    for p in fc1.parameters():\n        assert torch.isnan(p).sum() == 0\n    for p in fc2.parameters():\n        assert torch.isnan(p).sum() == 0\n\n@torch.no_grad()\ndef auto_scale_block(module, name, w_bit,\n                     q_group_size,\n                     input_feat):\n\n    # find the best scale ratio\n    def _search_module_scale(block, linears2scale: list, x, kwargs={}):\n\n        x = x.to(next(block.parameters()).device)\n        with torch.no_grad():\n            org_out = block(x, **kwargs)\n            if isinstance(org_out, tuple):\n                org_out = org_out[0]\n\n        s_x = x.view(-1, x.shape[-1]).abs().mean(0)\n\n        ############### YOUR CODE STARTS HERE ###############\n\n        # Step 1: Initialize the best_error, best_ratio and best_scales\n        best_error = float('inf')\n        best_ratio = -1\n        best_scales = None\n\n        ############### YOUR CODE ENDS HERE #################\n\n        n_grid = 20\n        history = []\n\n        org_sd = {k: v.cpu() for k, v in block.state_dict().items()}\n        for ratio in range(n_grid):\n            # ratio is the \\alpha in the formula\n            ratio = ratio * 1 / n_grid\n\n            ############### YOUR CODE STARTS HERE ###############\n\n            # Step 2: Calculate the scales by the formula: scales = s_x^ratio\n            scales = torch.clamp(s_x, 1e-5).pow(ratio)\n            assert scales.shape == s_x.shape\n\n            ############### YOUR CODE ENDS HERE #################\n\n            scales = scales / (scales.max() * scales.min()).sqrt().view(1, -1)\n\n            for fc in linears2scale:\n\n                scales = scales.to(fc.weight.device)\n\n                # Scale up the values of the weight channels\n                fc.weight.mul_(scales)\n\n                fc.weight.data = pseudo_quantize_tensor(fc.weight.data, w_bit, q_group_size)\n\n                ############### YOUR CODE STARTS HERE ###############\n\n                # Step 3: Scale back down the values of the weight channels\n                fc.weight.div_(scales)\n\n                ############### YOUR CODE ENDS HERE #################\n\n            out = block(x, **kwargs)\n            if isinstance(out, tuple):\n                out = out[0]\n\n            loss = (org_out - out).float().pow(2).mean().item()  # float prevents overflow\n            history.append(loss)\n            is_best = loss &lt; best_error\n            if is_best:\n                best_error = loss\n                best_ratio = ratio\n                best_scales = scales\n            block.load_state_dict(org_sd)\n\n        if best_ratio == -1:\n            print(history)\n            raise Exception\n\n        best_scales = best_scales.view(-1)\n\n        assert torch.isnan(best_scales).sum() == 0, best_scales\n        return best_scales.detach()\n\n    # attention input\n    inp = input_feat[name + '.self_attn.out_proj']\n    inp = torch.cat([x.unsqueeze(0) for x in inp], dim=0).unsqueeze(0)\n    qkv = [module.self_attn.q_proj, module.self_attn.k_proj, module.self_attn.v_proj]\n    final_scales = _search_module_scale(module.self_attn, qkv, inp)\n    scale_ln_fcs(module.self_attn_layer_norm, qkv, final_scales)\n\n    # attn out\n    inp = input_feat[name + '.self_attn.out_proj']\n    inp = torch.cat([x.unsqueeze(0) for x in inp], dim=0)\n    final_scales = _search_module_scale(module.self_attn.out_proj, [module.self_attn.out_proj], inp)\n    scale_fc_fc(module.self_attn.v_proj, module.self_attn.out_proj, final_scales)\n\n    # fc1\n    inp = input_feat[name + '.fc1']\n    inp = torch.cat([x.unsqueeze(0) for x in inp], dim=0)\n    final_scales = _search_module_scale(module.fc1, [module.fc1], inp)\n    scale_ln_fcs(module.final_layer_norm, module.fc1, final_scales)\n\n    # fc2\n    inp = input_feat[name + '.fc2']\n    inp = torch.cat([x.unsqueeze(0) for x in inp], dim=0)\n    final_scales = _search_module_scale(module.fc2, [module.fc2], inp)\n    scale_fc_fc(module.fc1, module.fc2, final_scales)\n\n@torch.no_grad()\ndef pseudo_quantize_model_weight_auto_scale(\n    model, w_bit, q_group_size, input_feat\n):\n    from transformers.models.opt.modeling_opt import OPTDecoderLayer\n\n    for name, module in model.named_modules():\n        if isinstance(module, OPTDecoderLayer):\n            auto_scale_block(module, name, w_bit, q_group_size, input_feat)\n\n    for n, m in model.named_modules():\n        if isinstance(m, nn.Linear):\n            m.weight.data = pseudo_quantize_tensor(m.weight.data, n_bit=w_bit, q_group_size=q_group_size)\n\n\ndel model\ngc.collect()\ntorch.cuda.empty_cache()\nmodel = AutoModelForCausalLM.from_pretrained(model_path, device_map=\"auto\")\npseudo_quantize_model_weight_auto_scale(model, w_bit=3, q_group_size=128, input_feat=input_feat)\n\n# Evaluate the model\nmodel_perplexity = evaluate(model, tokenizer)\nmodel_size = get_model_size(model, data_width=3, group_size=128)\nprint(f\"\\nmodel perplexity: {model_perplexity:.2f}\")\nprint(f\"model size: {model_size/MiB:.2f} MiB\")\n\nevaluating...: 100%|██████████| 40/40 [00:16&lt;00:00,  2.40it/s]\n\n\n\nmodel perplexity: 17.93\nmodel size: 495.06 MiB"
  },
  {
    "objectID": "posts/labs/lab04.html#bonus-point",
    "href": "posts/labs/lab04.html#bonus-point",
    "title": "👩‍💻 Lab 4",
    "section": "Bonus point",
    "text": "Bonus point\nAny optimization techniques without mixed precision on your mind? Try to implement them to improve the perplexity further! If you can further improve the perplexity to \\(x\\), you can get \\(\\max(0, (17.92 - x) \\times 10)\\) bonus points here!\nIn conclusion, we can significantly reduce perplexity without using mixed-precision. Through an efficient kernel implementation, the 4-bit model can achieve decent acceleration for inference. Through learning about TinyChatEngine in the next section, we can run a LLaMA-7B model on our own laptops like the demo presented in the introduction."
  },
  {
    "objectID": "posts/labs/lab02.html",
    "href": "posts/labs/lab02.html",
    "title": "👩‍💻 Lab 2",
    "section": "",
    "text": "Lecture 5와 6을 통해 배운 Quantization 내용 중에 K-means Quantization과 Linear Quantization에 대해 실습하며 배워보는 Lab2에 대한 풀이와 설명에 대한 포스팅이다. 기존의 실습 노트는 Original 강의의 링크를, 한국어 번역과 Solution은 이 링크를 참고하면 됩니다. 아래 Colaboratory 버튼을 누르면 실습노트를 바로 실행시키는 Colab Notebook을 실행시킬 수 있습니다."
  },
  {
    "objectID": "posts/labs/lab02.html#goals",
    "href": "posts/labs/lab02.html#goals",
    "title": "👩‍💻 Lab 2",
    "section": "Goals",
    "text": "Goals\n이번 실습에서는 모델 크기와 지연 시간을 줄이기 위해 클래식한 neural network model을 quantizing하는 연습을 할 것입니다. 이 실습의 목표는 다음과 같습니다:\n\nQuantization의 기본 개념을 이해합니다.\nk-means quantization을 구현하고 적용합니다.\nk-means quantization에 대해 quantization-aware training을 구현하고 적용합니다.\nlinear quantization을 구현하고 적용합니다.\nlinear quantization에 대해 integer-only inference를 구현하고 적용합니다.\nQuantization에서의 성능 개선(예: 속도 향상)에 대한 기본적인 이해를 얻습니다.\n이러한 quantization 접근 방식 사이의 차이점과 트레이드오프를 이해합니다."
  },
  {
    "objectID": "posts/labs/lab02.html#contents",
    "href": "posts/labs/lab02.html#contents",
    "title": "👩‍💻 Lab 2",
    "section": "Contents",
    "text": "Contents\n주요 섹션은 K-Means Quantization 과 Linear Quantization 2가지로 구성되어 있습니다.\n이번 실습 노트에서 총 10개의 질문을 통해 학습하게 됩니다.:\n\nK-Means Quantization에 대해서는 3개의 질문이 있습니다 (Question 1-3).\nLinear Quantization에 대해서는 6개의 질문이 있습니다 (Question 4-9).\nQuestion 10은 k-means quantization과 linear quantization을 비교합니다.\n\n\n실습노트에 대한 설정 부분(Setup)은 Colaboratory Note를 열면 확인하실 수 있습니다. 포스팅에서는 보다 실습내용에 집중할 수 있도록 생략되어 있습니다."
  },
  {
    "objectID": "posts/labs/lab02.html#question-1-10-pts",
    "href": "posts/labs/lab02.html#question-1-10-pts",
    "title": "👩‍💻 Lab 2",
    "section": "Question 1 (10 pts)",
    "text": "Question 1 (10 pts)\n아래의 K-Means quantization function을 완성하세요.\n\nfrom fast_pytorch_kmeans import KMeans\n\ndef k_means_quantize(fp32_tensor: torch.Tensor, bitwidth=4, codebook=None):\n    \"\"\"\n    quantize tensor using k-means clustering\n    :param fp32_tensor:\n    :param bitwidth: [int] quantization bit width, default=4\n    :param codebook: [Codebook] (the cluster centroids, the cluster label tensor)\n    :return:\n        [Codebook = (centroids, labels)]\n            centroids: [torch.(cuda.)FloatTensor] the cluster centroids\n            labels: [torch.(cuda.)LongTensor] cluster label tensor\n    \"\"\"\n    if codebook is None:\n        ############### YOUR CODE STARTS HERE ###############\n        # get number of clusters based on the quantization precision\n        n_clusters = 2 ** bitwidth  # Calculate number of clusters as 2^bitwidth\n        ############### YOUR CODE ENDS HERE #################\n        # use k-means to get the quantization centroids\n        kmeans = KMeans(n_clusters=n_clusters, mode='euclidean', verbose=0)\n        labels = kmeans.fit_predict(fp32_tensor.view(-1, 1)).to(torch.long)\n        centroids = kmeans.centroids.to(torch.float).view(-1)\n        codebook = Codebook(centroids, labels)\n\n    ############### YOUR CODE STARTS HERE ###############\n    # decode the codebook into k-means quantized tensor for inference\n    # hint: one line of code\n    quantized_tensor = codebook.centroids[codebook.labels].view_as(fp32_tensor)\n    ############### YOUR CODE ENDS HERE #################\n    fp32_tensor.set_(quantized_tensor.view_as(fp32_tensor))\n    return codebook\n\n위에서 작성한 k-means quantization function을 더미 텐서에 적용하여 확인해봅시다.\n\ntest_k_means_quantize()\n\ntensor([[-0.3747,  0.0874,  0.3200, -0.4868,  0.4404],\n        [-0.0402,  0.2322, -0.2024, -0.4986,  0.1814],\n        [ 0.3102, -0.3942, -0.2030,  0.0883, -0.4741],\n        [-0.1592, -0.0777, -0.3946, -0.2128,  0.2675],\n        [ 0.0611, -0.1933, -0.4350,  0.2928, -0.1087]])\n* Test k_means_quantize()\n    target bitwidth: 2 bits\n        num unique values before k-means quantization: 25\n        num unique values after  k-means quantization: 4\n* Test passed."
  },
  {
    "objectID": "posts/labs/lab02.html#question-2-10-pts",
    "href": "posts/labs/lab02.html#question-2-10-pts",
    "title": "👩‍💻 Lab 2",
    "section": "Question 2 (10 pts)",
    "text": "Question 2 (10 pts)\n마지막 코드 셀은 2비트 k-means quantization을 수행하고 quantization 전후의 텐서를 플롯합니다. 각 클러스터는 고유한 색상으로 렌더링되며, quantized 텐서들이 4(\\(2^2\\))가지 고유한 색상으로 표시됩니다.\n이러한 현상을 관찰한 것을 바탕으로 질문들에 답하세요.\n\nQuestion 2.1 (5 pts)\n4비트로 k-means quantization이 수행되면, quantized 텐서에는 몇 개의 고유한 색상이 렌더링될까요?\nYour Answer:\n4비트 k-means quantization이 수행되면, quantized 텐서에 \\((2^4 = 16)\\)개의 고유한 색상이 렌더링됩니다. 이는 4비트로 0000부터 1111까지의 16가지 다른 상태나 조합을 나타낼 수 있으며, 이는 텐서 값이 그룹화될 수 있는 16개의 고유한 클러스터에 해당합니다.\n\n\nQuestion 2.2 (5 pts)\nn-비트 k-means quantization이 수행되면, quantized 텐서에 몇 개의 고유한 색상이 렌더링될까요?\nYour Answer:\nn-비트 k-means quantization이 수행되면, quantized 텐서에는 \\((2^n)\\)개의 고유한 색상이 렌더링 됩니다. 이는 n비트를 사용하여 \\((2^n)\\)개의 다른 상태나 조합을 나타낼 수 있으며, 이는 텐서 값이 그룹화될 수 있는 \\((2^n)\\)개의 고유한 클러스터에 해당합니다."
  },
  {
    "objectID": "posts/labs/lab02.html#k-means-quantization-on-whole-model",
    "href": "posts/labs/lab02.html#k-means-quantization-on-whole-model",
    "title": "👩‍💻 Lab 2",
    "section": "K-Means Quantization on Whole Model",
    "text": "K-Means Quantization on Whole Model\nlab 1에서 했던 것과 유사하게, 이제 전체 모델을 quantizing하기 위해 k-means quantization 함수를 클래스로 래핑합니다. KMeansQuantizer 클래스에서는 모델 가중치가 변경될 때마다 codebooks(i.e., centroids와 labels)을 적용하거나 업데이트할 수 있도록 codebooks의 변화를 기록해야 합니다.\n\nfrom torch.nn import parameter\nclass KMeansQuantizer:\n    def __init__(self, model : nn.Module, bitwidth=4):\n        self.codebook = KMeansQuantizer.quantize(model, bitwidth)\n\n    @torch.no_grad()\n    def apply(self, model, update_centroids):\n        for name, param in model.named_parameters():\n            if name in self.codebook:\n                if update_centroids:\n                    update_codebook(param, codebook=self.codebook[name])\n                self.codebook[name] = k_means_quantize(\n                    param, codebook=self.codebook[name])\n\n    @staticmethod\n    @torch.no_grad()\n    def quantize(model: nn.Module, bitwidth=4):\n        codebook = dict()\n        if isinstance(bitwidth, dict):\n            for name, param in model.named_parameters():\n                if name in bitwidth:\n                    codebook[name] = k_means_quantize(param, bitwidth=bitwidth[name])\n        else:\n            for name, param in model.named_parameters():\n                if param.dim() &gt; 1:\n                    codebook[name] = k_means_quantize(param, bitwidth=bitwidth)\n        return codebook\n\n이제 K-Means Quantization을 사용하여 모델을 8비트, 4비트, 2비트로 quantize해봅시다. 모델 크기를 계산할 때 codebooks의 저장 공간은 무시한다는 점을 유의하세요.\n\nprint('Note that the storage for codebooks is ignored when calculating the model size.')\nquantizers = dict()\nfor bitwidth in [8, 4, 2]:\n    recover_model()\n    print(f'k-means quantizing model into {bitwidth} bits')\n    quantizer = KMeansQuantizer(model, bitwidth)\n    quantized_model_size = get_model_size(model, bitwidth)\n    print(f\"    {bitwidth}-bit k-means quantized model has size={quantized_model_size/MiB:.2f} MiB\")\n    quantized_model_accuracy = evaluate(model, dataloader['test'])\n    print(f\"    {bitwidth}-bit k-means quantized model has accuracy={quantized_model_accuracy:.2f}%\")\n    quantizers[bitwidth] = quantizer\n\nNote that the storage for codebooks is ignored when calculating the model size.\nk-means quantizing model into 8 bits\n    8-bit k-means quantized model has size=8.80 MiB\n    8-bit k-means quantized model has accuracy=92.76%\nk-means quantizing model into 4 bits\n    4-bit k-means quantized model has size=4.40 MiB\n    4-bit k-means quantized model has accuracy=79.07%\nk-means quantizing model into 2 bits\n    2-bit k-means quantized model has size=2.20 MiB\n    2-bit k-means quantized model has accuracy=10.00%"
  },
  {
    "objectID": "posts/labs/lab02.html#trained-k-means-quantization",
    "href": "posts/labs/lab02.html#trained-k-means-quantization",
    "title": "👩‍💻 Lab 2",
    "section": "Trained K-Means Quantization",
    "text": "Trained K-Means Quantization\n마지막 셀의 결과에서 볼 수 있듯이, 모델을 적은 비트로 quantize할 때 정확도가 크게 떨어집니다. 따라서, 정확도를 회복하기 위해 quantization-aware training을 해야 합니다.\nk-means quantization-aware 훈련 동안, centroids도 업데이트됩니다. 이는 Deep Compression: Compressing Deep Neural Networks With Pruning, Trained Quantization And Huffman Coding에서 제안되었습니다.\ncentroids에 대한 그래디언트는 다음과 같이 계산됩니다,\n\n\\(\\frac{\\partial \\mathcal{L} }{\\partial C_k} = \\sum_{j} \\frac{\\partial \\mathcal{L} }{\\partial W_{j}} \\frac{\\partial W_{j} }{\\partial C_k} = \\sum_{j} \\frac{\\partial \\mathcal{L} }{\\partial W_{j}} \\mathbf{1}(I_{j}=k)\\)\n\n여기서 \\(\\mathcal{L}\\)은 손실, \\(C_k\\)는 k-번째 centroid, \\(I_{j}\\)는 가중치 \\(W_{j}\\)의 라벨입니다.\n\\(\\mathbf{1}()\\)은 지시 함수이며, \\(\\mathbf{1}(I_{j}=k)\\)는 \\(1\\;\\mathrm{if}\\;I_{j}=k\\;\\mathrm{else}\\;0\\), 즉, \\(I_{j}==k\\)를 의미합니다.\nlab에서는 간단히 최신 가중치에 따라 centroids를 직접 업데이트합니다:\n\n\\(C_k = \\frac{\\sum_{j}W_{j}\\mathbf{1}(I_{j}=k)}{\\sum_{j}\\mathbf{1}(I_{j}=k)}\\)\n\n\nQuestion 3 (10 pts)\n아래의 codebook update function을 완성하세요.\nHint:\n위의 centroids를 업데이트하는 방정식은 실제로 동일한 클러스터에 있는 가중치의 평균(mean)을 업데이트된 centroid 값으로 사용하고 있습니다.\n\ndef update_codebook(fp32_tensor: torch.Tensor, codebook: Codebook):\n    \"\"\"\n    update the centroids in the codebook using updated fp32_tensor\n    :param fp32_tensor: [torch.(cuda.)Tensor]\n    :param codebook: [Codebook] (the cluster centroids, the cluster label tensor)\n    \"\"\"\n    n_clusters = codebook.centroids.numel()\n    fp32_tensor = fp32_tensor.view(-1)\n    for k in range(n_clusters):\n    ############### YOUR CODE STARTS HERE ###############\n        codebook.centroids[k] = fp32_tensor[codebook.labels == k].mean()\n    ############### YOUR CODE ENDS HERE #################\n\n이제 다음 코드 셀을 실행하여 k-means quantized 모델을 finetuning하여 정확도를 회복해봅시다. 정확도 하락이 0.5보다 작으면 finetuning을 중단합니다.\n\naccuracy_drop_threshold = 0.5\nquantizers_before_finetune = copy.deepcopy(quantizers)\nquantizers_after_finetune = quantizers\n\nfor bitwidth in [8, 4, 2]:\n    recover_model()\n    quantizer = quantizers[bitwidth]\n    print(f'k-means quantizing model into {bitwidth} bits')\n    quantizer.apply(model, update_centroids=False)\n    quantized_model_size = get_model_size(model, bitwidth)\n    print(f\"    {bitwidth}-bit k-means quantized model has size={quantized_model_size/MiB:.2f} MiB\")\n    quantized_model_accuracy = evaluate(model, dataloader['test'])\n    print(f\"    {bitwidth}-bit k-means quantized model has accuracy={quantized_model_accuracy:.2f}% before quantization-aware training \")\n    accuracy_drop = fp32_model_accuracy - quantized_model_accuracy\n    if accuracy_drop &gt; accuracy_drop_threshold:\n        print(f\"        Quantization-aware training due to accuracy drop={accuracy_drop:.2f}% is larger than threshold={accuracy_drop_threshold:.2f}%\")\n        num_finetune_epochs = 5\n        optimizer = torch.optim.SGD(model.parameters(), lr=0.01, momentum=0.9)\n        scheduler = torch.optim.lr_scheduler.CosineAnnealingLR(optimizer, num_finetune_epochs)\n        criterion = nn.CrossEntropyLoss()\n        best_accuracy = 0\n        epoch = num_finetune_epochs\n        while accuracy_drop &gt; accuracy_drop_threshold and epoch &gt; 0:\n            train(model, dataloader['train'], criterion, optimizer, scheduler,\n                  callbacks=[lambda: quantizer.apply(model, update_centroids=True)])\n            model_accuracy = evaluate(model, dataloader['test'])\n            is_best = model_accuracy &gt; best_accuracy\n            best_accuracy = max(model_accuracy, best_accuracy)\n            print(f'        Epoch {num_finetune_epochs-epoch} Accuracy {model_accuracy:.2f}% / Best Accuracy: {best_accuracy:.2f}%')\n            accuracy_drop = fp32_model_accuracy - best_accuracy\n            epoch -= 1\n    else:\n        print(f\"        No need for quantization-aware training since accuracy drop={accuracy_drop:.2f}% is smaller than threshold={accuracy_drop_threshold:.2f}%\")\n\nk-means quantizing model into 8 bits\n    8-bit k-means quantized model has size=8.80 MiB\n    8-bit k-means quantized model has accuracy=92.76% before quantization-aware training \n        No need for quantization-aware training since accuracy drop=0.19% is smaller than threshold=0.50%\nk-means quantizing model into 4 bits\n    4-bit k-means quantized model has size=4.40 MiB\n    4-bit k-means quantized model has accuracy=79.07% before quantization-aware training \n        Quantization-aware training due to accuracy drop=13.88% is larger than threshold=0.50%\n        Epoch 0 Accuracy 92.47% / Best Accuracy: 92.47%\nk-means quantizing model into 2 bits\n    2-bit k-means quantized model has size=2.20 MiB\n    2-bit k-means quantized model has accuracy=10.00% before quantization-aware training \n        Quantization-aware training due to accuracy drop=82.95% is larger than threshold=0.50%\n        Epoch 0 Accuracy 90.21% / Best Accuracy: 90.21%\n        Epoch 1 Accuracy 90.82% / Best Accuracy: 90.82%\n        Epoch 2 Accuracy 91.00% / Best Accuracy: 91.00%\n        Epoch 3 Accuracy 91.12% / Best Accuracy: 91.12%\n        Epoch 4 Accuracy 91.17% / Best Accuracy: 91.17%"
  },
  {
    "objectID": "posts/labs/lab02.html#n-bit-integer",
    "href": "posts/labs/lab02.html#n-bit-integer",
    "title": "👩‍💻 Lab 2",
    "section": "n-bit Integer",
    "text": "n-bit Integer\nn-비트 signed integer는 보통 two’s complement 표기법으로 표현됩니다.\nn-비트 signed integer는 범위 \\([-2^{n-1}, 2^{n-1}-1]\\) 내의 정수를 인코딩할 수 있습니다. 예를 들어, 8비트 정수는 [-128, 127] 범위에 속합니다.\n\ndef get_quantized_range(bitwidth):\n    quantized_max = (1 &lt;&lt; (bitwidth - 1)) - 1\n    quantized_min = -(1 &lt;&lt; (bitwidth - 1))\n    return quantized_min, quantized_max"
  },
  {
    "objectID": "posts/labs/lab02.html#question-4-15-pts",
    "href": "posts/labs/lab02.html#question-4-15-pts",
    "title": "👩‍💻 Lab 2",
    "section": "Question 4 (15 pts)",
    "text": "Question 4 (15 pts)\n아래의 linear quantization function을 완성하세요.\nHint:\n\n\\(r=S(q-Z)\\)에서, \\(q = r/S + Z\\)으로 바꿔서 볼 수 있습니다.\n\\(r\\)과 \\(S\\) 모두 부동 소수점 숫자(floating number)이므로, 정수 \\(Z\\)를 직접 \\(r/S\\)에 더할 수 없습니다. 따라서 \\(q = \\mathrm{int}(\\mathrm{round}(r/S)) + Z\\)입니다.\ntorch.FloatTensor를 torch.IntTensor로 변환하기 위해서, torch.round(), torch.Tensor.round(), torch.Tensor.round_()을 사용하여 모든 값을 부동 소수점 정수로 먼저 변환합니다.\n그 다음 torch.Tensor.to(torch.int8)를 사용하여 데이터 타입을 torch.float에서 torch.int8로 변환할 수 있습니다.\n\n\ndef linear_quantize(fp_tensor, bitwidth, scale, zero_point, dtype=torch.int8) -&gt; torch.Tensor:\n    \"\"\"\n    linear quantization for single fp_tensor\n      from\n        fp_tensor = (quantized_tensor - zero_point) * scale\n      we have,\n        quantized_tensor = int(round(fp_tensor / scale)) + zero_point\n    :param tensor: [torch.(cuda.)FloatTensor] floating tensor to be quantized\n    :param bitwidth: [int] quantization bit width\n    :param scale: [torch.(cuda.)FloatTensor] scaling factor\n    :param zero_point: [torch.(cuda.)IntTensor] the desired centroid of tensor values\n    :return:\n        [torch.(cuda.)FloatTensor] quantized tensor whose values are integers\n    \"\"\"\n    assert(fp_tensor.dtype == torch.float)\n    assert(isinstance(scale, float) or\n           (scale.dtype == torch.float and scale.dim() == fp_tensor.dim()))\n    assert(isinstance(zero_point, int) or\n           (zero_point.dtype == dtype and zero_point.dim() == fp_tensor.dim()))\n\n    ############### YOUR CODE STARTS HERE ###############\n    # Step 1: scale the fp_tensor\n    scaled_tensor = fp_tensor / scale\n    # Step 2: round the floating value to integer value\n    rounded_tensor = torch.round(scaled_tensor)\n    ############### YOUR CODE ENDS HERE #################\n\n    rounded_tensor = rounded_tensor.to(dtype)\n\n    ############### YOUR CODE STARTS HERE ###############\n    # Step 3: shift the rounded_tensor to make zero_point 0\n    shifted_tensor = rounded_tensor + zero_point\n    ############### YOUR CODE ENDS HERE #################\n\n    # Step 4: clamp the shifted_tensor to lie in bitwidth-bit range\n    quantized_min, quantized_max = get_quantized_range(bitwidth)\n    quantized_tensor = shifted_tensor.clamp_(quantized_min, quantized_max)\n    return quantized_tensor\n\n위에서 작성한 linear quantization 기능을 더미 텐서에 적용하여 기능을 검증해봅시다.\n\ntest_linear_quantize()\n\n* Test linear_quantize()\n    target bitwidth: 2 bits\n        scale: 0.3333333333333333\n        zero point: -1\n* Test passed."
  },
  {
    "objectID": "posts/labs/lab02.html#question-5-10-pts",
    "href": "posts/labs/lab02.html#question-5-10-pts",
    "title": "👩‍💻 Lab 2",
    "section": "Question 5 (10 pts)",
    "text": "Question 5 (10 pts)\n이제 linear quantization을 위한 스케일링 인자 \\(S\\)와 제로 포인트 \\(Z\\)를 결정해야 합니다.\nlinear quantization은 \\(r = S(q-Z)\\) 로 표현될 수 있다는 것을 기억하세요.\n\nScale\nLinear quantization은 부동 소수점 범위 [fp_min, fp_max]를 양자화된 범위 [quantized_min, quantized_max]로 투영(projection)합니다. 즉,\n\n\\(r_{\\mathrm{max}} = S(q_{\\mathrm{max}}-Z)\\)\n\\(r_{\\mathrm{min}} = S(q_{\\mathrm{min}}-Z)\\)\n\n이 두 방정식을 빼면, 우리는 다음을 얻습니다,\n\nQuestion 5.1 (1 pts)\n다음 텍스트 셀에서 올바른 답을 선택하고 잘못된 답을 삭제해주세요.\n\n\\(S=r_{\\mathrm{max}} / q_{\\mathrm{max}}\\)\n\n\n\\(S=(r_{\\mathrm{max}} + r_{\\mathrm{min}}) / (q_{\\mathrm{max}} + q_{\\mathrm{min}})\\)\n\n\n✅\\(S=(r_{\\mathrm{max}} - r_{\\mathrm{min}}) / (q_{\\mathrm{max}} - q_{\\mathrm{min}})\\)\n\n\n\\(S=r_{\\mathrm{max}} / q_{\\mathrm{max}} - r_{\\mathrm{min}} / q_{\\mathrm{min}}\\)\n\nfp_tensor의 \\(r_{\\mathrm{min}}\\)과 \\(r_{\\mathrm{max}}\\)를 결정하는 다양한 방법이 있습니다.\n\n가장 흔한 방법은 fp_tensor의 최소값과 최대값을 사용하는 것입니다.\n또 다른 널리 사용되는 방법은 Kullback-Leibler-J 발산을 최소화하여 fp_max를 결정하는 것입니다.\n\n\n\n\nzero point\n스케일링 인자 \\(S\\)를 결정하면, \\(r_{\\mathrm{min}}\\)과 \\(q_{\\mathrm{min}}\\) 사이의 관계를 사용하여 제로 포인트 \\(Z\\)를 계산할 수 있습니다.\n\nQuestion 5.2 (1 pts)\n다음 텍스트 셀에서 올바른 답을 선택하고 잘못된 답을 삭제해주세요.\n\n\\(Z = \\mathrm{int}(\\mathrm{round}(r_{\\mathrm{min}} / S - q_{\\mathrm{min}})\\)\n\n\n\\(Z = \\mathrm{int}(\\mathrm{round}(q_{\\mathrm{min}} - r_{\\mathrm{min}} / S))\\)\n\n\n✅\\(Z = q_{\\mathrm{min}} - r_{\\mathrm{min}} / S\\)\n\n\n\\(Z = r_{\\mathrm{min}} / S - q_{\\mathrm{min}}\\)\n\n\n\n\nQuestion 5.3 (8 pts)\nfloating point tensor \\(r\\)로부터 scale \\(S\\)와 zero point \\(Z\\)를 계산하는 아래의 함수를 완성하세요.\n\ndef get_quantization_scale_and_zero_point(fp_tensor, bitwidth):\n    \"\"\"\n    get quantization scale for single tensor\n    :param fp_tensor: [torch.(cuda.)Tensor] floating tensor to be quantized\n    :param bitwidth: [int] quantization bit width\n    :return:\n        [float] scale\n        [int] zero_point\n    \"\"\"\n    quantized_min, quantized_max = get_quantized_range(bitwidth)\n    fp_max = fp_tensor.max().item()\n    fp_min = fp_tensor.min().item()\n\n    ############### YOUR CODE STARTS HERE ###############\n    # Calculate scale\n    scale = (fp_max - fp_min) / (quantized_max - quantized_min)\n    # Calculate zero_point\n    zero_point = quantized_min - round(fp_min / scale)\n    ############### YOUR CODE ENDS HERE #################\n\n    # clip the zero_point to fall in [quantized_min, quantized_max]\n    if zero_point &lt; quantized_min:\n        zero_point = quantized_min\n    elif zero_point &gt; quantized_max:\n        zero_point = quantized_max\n    else: # convert from float to int using round()\n        zero_point = round(zero_point)\n    return scale, int(zero_point)\n\n이제 Question 4의 linear_quantize()와 Question 5의 get_quantization_scale_and_zero_point()을 하나의 함수로 래핑합니다.\n\ndef linear_quantize_feature(fp_tensor, bitwidth):\n    \"\"\"\n    linear quantization for feature tensor\n    :param fp_tensor: [torch.(cuda.)Tensor] floating feature to be quantized\n    :param bitwidth: [int] quantization bit width\n    :return:\n        [torch.(cuda.)Tensor] quantized tensor\n        [float] scale tensor\n        [int] zero point\n    \"\"\"\n    scale, zero_point = get_quantization_scale_and_zero_point(fp_tensor, bitwidth)\n    quantized_tensor = linear_quantize(fp_tensor, bitwidth, scale, zero_point)\n    return quantized_tensor, scale, zero_point"
  },
  {
    "objectID": "posts/labs/lab02.html#special-case-linear-quantization-on-weight-tensor",
    "href": "posts/labs/lab02.html#special-case-linear-quantization-on-weight-tensor",
    "title": "👩‍💻 Lab 2",
    "section": "Special case: linear quantization on weight tensor",
    "text": "Special case: linear quantization on weight tensor\n먼저 가중치 값의 분포를 살펴봅시다.\n\ndef plot_weight_distribution(model, bitwidth=32):\n    # bins = (1 &lt;&lt; bitwidth) if bitwidth &lt;= 8 else 256\n    if bitwidth &lt;= 8:\n        qmin, qmax = get_quantized_range(bitwidth)\n        bins = np.arange(qmin, qmax + 2)\n        align = 'left'\n    else:\n        bins = 256\n        align = 'mid'\n    fig, axes = plt.subplots(3,3, figsize=(10, 6))\n    axes = axes.ravel()\n    plot_index = 0\n    for name, param in model.named_parameters():\n        if param.dim() &gt; 1:\n            ax = axes[plot_index]\n            ax.hist(param.detach().view(-1).cpu(), bins=bins, density=True,\n                    align=align, color = 'blue', alpha = 0.5,\n                    edgecolor='black' if bitwidth &lt;= 4 else None)\n            if bitwidth &lt;= 4:\n                quantized_min, quantized_max = get_quantized_range(bitwidth)\n                ax.set_xticks(np.arange(start=quantized_min, stop=quantized_max+1))\n            ax.set_xlabel(name)\n            ax.set_ylabel('density')\n            plot_index += 1\n    fig.suptitle(f'Histogram of Weights (bitwidth={bitwidth} bits)')\n    fig.tight_layout()\n    fig.subplots_adjust(top=0.925)\n    plt.show()\n\nrecover_model()\nplot_weight_distribution(model)\n\n\n\n\n\n\n\n\n위의 히스토그램에서 볼 수 있듯이, 가중치 값의 분포는 (이 경우에는 classifier를 제외하고) 거의 0을 중심으로 대칭적입니다 . 따라서 가중치를 양자화할 때 보통 제로 포인트 \\(Z=0\\)으로 설정합니다.\n\\(r = S(q-Z)\\)에서,\n\n\\(r_{\\mathrm{max}} = S \\cdot q_{\\mathrm{max}}\\)\n\n\n\\(S = r_{\\mathrm{max}} / q_{\\mathrm{max}}\\)\n\n가중치 값의 최대 절댓값을 \\(r_{\\mathrm{max}}\\)로 이용합니다.\n\ndef get_quantization_scale_for_weight(weight, bitwidth):\n    \"\"\"\n    get quantization scale for single tensor of weight\n    :param weight: [torch.(cuda.)Tensor] floating weight to be quantized\n    :param bitwidth: [integer] quantization bit width\n    :return:\n        [floating scalar] scale\n    \"\"\"\n    # we just assume values in weight are symmetric\n    # we also always make zero_point 0 for weight\n    fp_max = max(weight.abs().max().item(), 5e-7)\n    _, quantized_max = get_quantized_range(bitwidth)\n    return fp_max / quantized_max\n\n\nPer-channel Linear Quantization\n2D convolution의 경우, 가중치 텐서는 (num_output_channels, num_input_channels, kernel_height, kernel_width) 모양의 4차원 텐서입니다.\n많은 실험들을 통해, 서로 다른 출력 채널에 대해 서로 다른 스케일링 인자 \\(S\\)와 제로 포인트 \\(Z\\)를 사용하는 것이 더 나은 성능을 발휘한다는 것을 알 수 있었습니다. 따라서 각 출력 채널의 서브텐서에 대한 스케일링 인자 \\(S\\)와 제로 포인트 \\(Z\\)를 독립적으로 정해야 합니다.\n\ndef linear_quantize_weight_per_channel(tensor, bitwidth):\n    \"\"\"\n    linear quantization for weight tensor\n        using different scales and zero_points for different output channels\n    :param tensor: [torch.(cuda.)Tensor] floating weight to be quantized\n    :param bitwidth: [int] quantization bit width\n    :return:\n        [torch.(cuda.)Tensor] quantized tensor\n        [torch.(cuda.)Tensor] scale tensor\n        [int] zero point (which is always 0)\n    \"\"\"\n    dim_output_channels = 0\n    num_output_channels = tensor.shape[dim_output_channels]\n    scale = torch.zeros(num_output_channels, device=tensor.device)\n    for oc in range(num_output_channels):\n        _subtensor = tensor.select(dim_output_channels, oc)\n        _scale = get_quantization_scale_for_weight(_subtensor, bitwidth)\n        scale[oc] = _scale\n    scale_shape = [1] * tensor.dim()\n    scale_shape[dim_output_channels] = -1\n    scale = scale.view(scale_shape)\n    quantized_tensor = linear_quantize(tensor, bitwidth, scale, zero_point=0)\n    return quantized_tensor, scale, 0\n\n\n\nA Quick Peek at Linear Quantization on Weights\n이제 가중치에 대해 linear quantization를 적용할 때 가중치 분포와 모델 크기를 서로 다른 bitwidths로 살펴보겠습니다.\n\n@torch.no_grad()\ndef peek_linear_quantization():\n    for bitwidth in [4, 2]:\n        for name, param in model.named_parameters():\n            if param.dim() &gt; 1:\n                quantized_param, scale, zero_point = \\\n                    linear_quantize_weight_per_channel(param, bitwidth)\n                param.copy_(quantized_param)\n        plot_weight_distribution(model, bitwidth)\n        recover_model()\n\npeek_linear_quantization()"
  },
  {
    "objectID": "posts/labs/lab02.html#quantized-inference",
    "href": "posts/labs/lab02.html#quantized-inference",
    "title": "👩‍💻 Lab 2",
    "section": "Quantized Inference",
    "text": "Quantized Inference\n양자화 후, convolution 및 fully-connected layer의 추론도 변경됩니다.\n\\(r = S(q-Z)\\)를 상기해 보면, 다음과 같습니다.\n\n\\(r_{\\mathrm{input}} = S_{\\mathrm{input}}(q_{\\mathrm{input}}-Z_{\\mathrm{input}})\\)\n\\(r_{\\mathrm{weight}} = S_{\\mathrm{weight}}(q_{\\mathrm{weight}}-Z_{\\mathrm{weight}})\\)\n\\(r_{\\mathrm{bias}} = S_{\\mathrm{bias}}(q_{\\mathrm{bias}}-Z_{\\mathrm{bias}})\\)\n\n\\(Z_{\\mathrm{weight}}=0\\)이므로, \\(r_{\\mathrm{weight}} = S_{\\mathrm{weight}}q_{\\mathrm{weight}}\\)입니다.\n부동 소수점 convolution은 다음과 같이 작성할 수 있습니다.\n\n\\(r_{\\mathrm{output}} = \\mathrm{CONV}[r_{\\mathrm{input}}, r_{\\mathrm{weight}}] + r_{\\mathrm{bias}}\\) \\(\\;\\;\\;\\;\\;\\;\\;\\;= \\mathrm{CONV}[S_{\\mathrm{input}}(q_{\\mathrm{input}}-Z_{\\mathrm{input}}), S_{\\mathrm{weight}}q_{\\mathrm{weight}}] + S_{\\mathrm{bias}}(q_{\\mathrm{bias}}-Z_{\\mathrm{bias}})\\) \\(\\;\\;\\;\\;\\;\\;\\;\\;= \\mathrm{CONV}[q_{\\mathrm{input}}-Z_{\\mathrm{input}}, q_{\\mathrm{weight}}]\\cdot (S_{\\mathrm{input}} \\cdot S_{\\mathrm{weight}}) + S_{\\mathrm{bias}}(q_{\\mathrm{bias}}-Z_{\\mathrm{bias}})\\)\n\n계산을 더 간단하게 하기 위해\n\n\\(Z_{\\mathrm{bias}} = 0\\)\n\\(S_{\\mathrm{bias}} = S_{\\mathrm{input}} \\cdot S_{\\mathrm{weight}}\\)\n\n로 설정하여,\n\n\\(r_{\\mathrm{output}} = (\\mathrm{CONV}[q_{\\mathrm{input}}-Z_{\\mathrm{input}}, q_{\\mathrm{weight}}] + q_{\\mathrm{bias}})\\cdot (S_{\\mathrm{input}} \\cdot S_{\\mathrm{weight}})\\) \\(\\;\\;\\;\\;\\;\\;\\;\\;= (\\mathrm{CONV}[q_{\\mathrm{input}}, q_{\\mathrm{weight}}] - \\mathrm{CONV}[Z_{\\mathrm{input}}, q_{\\mathrm{weight}}] + q_{\\mathrm{bias}})\\cdot (S_{\\mathrm{input}}S_{\\mathrm{weight}})\\)\n\n이며,\n\n\\(r_{\\mathrm{output}} = S_{\\mathrm{output}}(q_{\\mathrm{output}}-Z_{\\mathrm{output}})\\)\n\n이므로\n\n\\(S_{\\mathrm{output}}(q_{\\mathrm{output}}-Z_{\\mathrm{output}}) = (\\mathrm{CONV}[q_{\\mathrm{input}}, q_{\\mathrm{weight}}] - \\mathrm{CONV}[Z_{\\mathrm{input}}, q_{\\mathrm{weight}}] + q_{\\mathrm{bias}})\\cdot (S_{\\mathrm{input}} S_{\\mathrm{weight}})\\)\n\n따라서\n\n\\(q_{\\mathrm{output}} = (\\mathrm{CONV}[q_{\\mathrm{input}}, q_{\\mathrm{weight}}] - \\mathrm{CONV}[Z_{\\mathrm{input}}, q_{\\mathrm{weight}}] + q_{\\mathrm{bias}})\\cdot (S_{\\mathrm{input}}S_{\\mathrm{weight}} / S_{\\mathrm{output}}) + Z_{\\mathrm{output}}\\)\n\n\\(Z_{\\mathrm{input}}\\), \\(q_{\\mathrm{weight}}\\), \\(q_{\\mathrm{bias}}\\)는 추론 전에 결정되므로,\n\n\\(Q_{\\mathrm{bias}} = q_{\\mathrm{bias}} - \\mathrm{CONV}[Z_{\\mathrm{input}}, q_{\\mathrm{weight}}]\\)\n\n로 설정하면,\n\n\\(q_{\\mathrm{output}} = (\\mathrm{Linear}[q_{\\mathrm{input}}, q_{\\mathrm{weight}}] + Q_{\\mathrm{bias}})\\cdot (S_{\\mathrm{input}} \\cdot S_{\\mathrm{weight}} / S_{\\mathrm{output}}) + Z_{\\mathrm{output}}\\)\n\n\nQuestion 6 (5 pts)\nbias를 linear quantizing하는 함수를 완성하세요.\nHint:\n위의 추론과정에서 아래와 같은 수식을 얻었습니다.\n\n\\(Z_{\\mathrm{bias}} = 0\\)\n\\(S_{\\mathrm{bias}} = S_{\\mathrm{input}} \\cdot S_{\\mathrm{weight}}\\)\n\n\ndef linear_quantize_bias_per_output_channel(bias, weight_scale, input_scale):\n    \"\"\"\n    linear quantization for single bias tensor\n        quantized_bias = fp_bias / bias_scale\n    :param bias: [torch.FloatTensor] bias weight to be quantized\n    :param weight_scale: [float or torch.FloatTensor] weight scale tensor\n    :param input_scale: [float] input scale\n    :return:\n        [torch.IntTensor] quantized bias tensor\n    \"\"\"\n    assert(bias.dim() == 1)\n    assert(bias.dtype == torch.float)\n    assert(isinstance(input_scale, float))\n    if isinstance(weight_scale, torch.Tensor):\n        assert(weight_scale.dtype == torch.float)\n        weight_scale = weight_scale.view(-1)\n        assert(bias.numel() == weight_scale.numel())\n\n    ############### YOUR CODE STARTS HERE ###############\n    bias_scale = weight_scale * input_scale\n    ############### YOUR CODE ENDS HERE #################\n\n    quantized_bias = linear_quantize(bias, 32, bias_scale,\n                                     zero_point=0, dtype=torch.int32)\n    return quantized_bias, bias_scale, 0\n\n\n\nQuantized Fully-Connected Layer\n양자화된 fully-connected layer의 경우, \\(Q_{\\mathrm{bias}}\\)를 먼저 계산합니다. \\(Q_{\\mathrm{bias}} = q_{\\mathrm{bias}} - \\mathrm{Linear}[Z_{\\mathrm{input}}, q_{\\mathrm{weight}}]\\)를 기억하세요.\n\ndef shift_quantized_linear_bias(quantized_bias, quantized_weight, input_zero_point):\n    \"\"\"\n    shift quantized bias to incorporate input_zero_point for nn.Linear\n        shifted_quantized_bias = quantized_bias - Linear(input_zero_point, quantized_weight)\n    :param quantized_bias: [torch.IntTensor] quantized bias (torch.int32)\n    :param quantized_weight: [torch.CharTensor] quantized weight (torch.int8)\n    :param input_zero_point: [int] input zero point\n    :return:\n        [torch.IntTensor] shifted quantized bias tensor\n    \"\"\"\n    assert(quantized_bias.dtype == torch.int32)\n    assert(isinstance(input_zero_point, int))\n    return quantized_bias - quantized_weight.sum(1).to(torch.int32) * input_zero_point\n\n\nQuestion 7 (15 pts)\n아래의 양자화된 fully-connected layer inference function를 완성하세요.\nHint:\n\n\\(q_{\\mathrm{output}} = (\\mathrm{Linear}[q_{\\mathrm{input}}, q_{\\mathrm{weight}}] + Q_{\\mathrm{bias}})\\cdot (S_{\\mathrm{input}} S_{\\mathrm{weight}} / S_{\\mathrm{output}}) + Z_{\\mathrm{output}}\\)\n\n\ndef quantized_linear(input, weight, bias, feature_bitwidth, weight_bitwidth,\n                     input_zero_point, output_zero_point,\n                     input_scale, weight_scale, output_scale):\n    \"\"\"\n    quantized fully-connected layer\n    :param input: [torch.CharTensor] quantized input (torch.int8)\n    :param weight: [torch.CharTensor] quantized weight (torch.int8)\n    :param bias: [torch.IntTensor] shifted quantized bias or None (torch.int32)\n    :param feature_bitwidth: [int] quantization bit width of input and output\n    :param weight_bitwidth: [int] quantization bit width of weight\n    :param input_zero_point: [int] input zero point\n    :param output_zero_point: [int] output zero point\n    :param input_scale: [float] input feature scale\n    :param weight_scale: [torch.FloatTensor] weight per-channel scale\n    :param output_scale: [float] output feature scale\n    :return:\n        [torch.CharIntTensor] quantized output feature (torch.int8)\n    \"\"\"\n    assert(input.dtype == torch.int8)\n    assert(weight.dtype == input.dtype)\n    assert(bias is None or bias.dtype == torch.int32)\n    assert(isinstance(input_zero_point, int))\n    assert(isinstance(output_zero_point, int))\n    assert(isinstance(input_scale, float))\n    assert(isinstance(output_scale, float))\n    assert(weight_scale.dtype == torch.float)\n\n    # Step 1: integer-based fully-connected (8-bit multiplication with 32-bit accumulation)\n    if 'cpu' in input.device.type:\n        # use 32-b MAC for simplicity\n        output = torch.nn.functional.linear(input.to(torch.int32), weight.to(torch.int32), bias)\n    else:\n        # current version pytorch does not yet support integer-based linear() on GPUs\n        output = torch.nn.functional.linear(input.float(), weight.float(), bias.float())\n\n    ############### YOUR CODE STARTS HERE ###############\n    # Step 2: scale the output\n    #         hint: 1. scales are floating numbers, we need to convert output to float as well\n    #               2. the shape of weight scale is [oc, 1, 1, 1] while the shape of output is [batch_size, oc]\n    real_scale = input_scale * weight_scale.view(-1) / output_scale\n    output = output.float() * real_scale\n\n    # Step 3: Shift output by output_zero_point\n    output += output_zero_point\n    ############### YOUR CODE STARTS HERE ###############\n\n    # Make sure all value lies in the bitwidth-bit range\n    output = output.round().clamp(*get_quantized_range(feature_bitwidth)).to(torch.int8)\n    return output\n\nLet’s verify the functionality of defined quantized fully connected layer.\n\ntest_quantized_fc()\n\n* Test quantized_fc()\n    target bitwidth: 2 bits\n      batch size: 4\n      input channels: 8\n      output channels: 8\n* Test passed.\n\n\n\n\n\n\n\n\n\n\n\n\nQuantized Convolution\n양자화된 컨볼루션 레이어의 경우, 먼저 \\(Q_{\\mathrm{bias}}\\)를 계산합니다. \\(Q_{\\mathrm{bias}} = q_{\\mathrm{bias}} - \\mathrm{CONV}[Z_{\\mathrm{input}}, q_{\\mathrm{weight}}]\\)를 기억하세요.\n\ndef shift_quantized_conv2d_bias(quantized_bias, quantized_weight, input_zero_point):\n    \"\"\"\n    shift quantized bias to incorporate input_zero_point for nn.Conv2d\n        shifted_quantized_bias = quantized_bias - Conv(input_zero_point, quantized_weight)\n    :param quantized_bias: [torch.IntTensor] quantized bias (torch.int32)\n    :param quantized_weight: [torch.CharTensor] quantized weight (torch.int8)\n    :param input_zero_point: [int] input zero point\n    :return:\n        [torch.IntTensor] shifted quantized bias tensor\n    \"\"\"\n    assert(quantized_bias.dtype == torch.int32)\n    assert(isinstance(input_zero_point, int))\n    return quantized_bias - quantized_weight.sum((1,2,3)).to(torch.int32) * input_zero_point\n\n\nQuestion 8 (15 pts)\n아래의 quantized convolution function을 완성하세요.\nHint: &gt; \\(q_{\\mathrm{output}} = (\\mathrm{CONV}[q_{\\mathrm{input}}, q_{\\mathrm{weight}}] + Q_{\\mathrm{bias}}) \\cdot (S_{\\mathrm{input}}S_{\\mathrm{weight}} / S_{\\mathrm{output}}) + Z_{\\mathrm{output}}\\)\n\ndef quantized_conv2d(input, weight, bias, feature_bitwidth, weight_bitwidth,\n                     input_zero_point, output_zero_point,\n                     input_scale, weight_scale, output_scale,\n                     stride, padding, dilation, groups):\n    \"\"\"\n    quantized 2d convolution\n    :param input: [torch.CharTensor] quantized input (torch.int8)\n    :param weight: [torch.CharTensor] quantized weight (torch.int8)\n    :param bias: [torch.IntTensor] shifted quantized bias or None (torch.int32)\n    :param feature_bitwidth: [int] quantization bit width of input and output\n    :param weight_bitwidth: [int] quantization bit width of weight\n    :param input_zero_point: [int] input zero point\n    :param output_zero_point: [int] output zero point\n    :param input_scale: [float] input feature scale\n    :param weight_scale: [torch.FloatTensor] weight per-channel scale\n    :param output_scale: [float] output feature scale\n    :return:\n        [torch.(cuda.)CharTensor] quantized output feature\n    \"\"\"\n    assert(len(padding) == 4)\n    assert(input.dtype == torch.int8)\n    assert(weight.dtype == input.dtype)\n    assert(bias is None or bias.dtype == torch.int32)\n    assert(isinstance(input_zero_point, int))\n    assert(isinstance(output_zero_point, int))\n    assert(isinstance(input_scale, float))\n    assert(isinstance(output_scale, float))\n    assert(weight_scale.dtype == torch.float)\n\n    # Step 1: calculate integer-based 2d convolution (8-bit multiplication with 32-bit accumulation)\n    input = torch.nn.functional.pad(input, padding, 'constant', input_zero_point)\n    if 'cpu' in input.device.type:\n        # use 32-b MAC for simplicity\n        output = torch.nn.functional.conv2d(input.to(torch.int32), weight.to(torch.int32), None, stride, 0, dilation, groups)\n    else:\n        # current version pytorch does not yet support integer-based conv2d() on GPUs\n        output = torch.nn.functional.conv2d(input.float(), weight.float(), None, stride, 0, dilation, groups)\n        output = output.round().to(torch.int32)\n    if bias is not None:\n        output = output + bias.view(1, -1, 1, 1)\n\n    ############### YOUR CODE STARTS HERE ###############\n    # hint: this code block should be the very similar to quantized_linear()\n\n    # Step 2: scale the output\n    #         hint: 1. scales are floating numbers, we need to convert output to float as well\n    #               2. the shape of weight scale is [oc, 1, 1, 1] while the shape of output is [batch_size, oc, height, width]\n    real_scale = input_scale * weight_scale.view(-1) / output_scale\n    output = output.float() * real_scale.unsqueeze(1).unsqueeze(2)\n\n    # Step 3: shift output by output_zero_point\n    #         hint: one line of code\n    output += output_zero_point\n    ############### YOUR CODE STARTS HERE ###############\n\n    # Make sure all value lies in the bitwidth-bit range\n    output = output.round().clamp(*get_quantized_range(feature_bitwidth)).to(torch.int8)\n    return output"
  },
  {
    "objectID": "posts/labs/lab02.html#question-9-10-pts",
    "href": "posts/labs/lab02.html#question-9-10-pts",
    "title": "👩‍💻 Lab 2",
    "section": "Question 9 (10 pts)",
    "text": "Question 9 (10 pts)\n마지막으로 모든 것을 종합하여 모델에 대한 훈련 후 int8 양자화를 수행합니다. 모델의 컨볼루션 레이어와 선형 레이어를 하나씩 양자화된 버전으로 변환합니다.\n\n먼저, BatchNorm 계층을 이전 convolutional layer에 융합할 것이며, 이는 양자화 전에 하는 표준 관행입니다. BatchNorm을 융합하면 추론 중에 추가 곱셈이 줄어듭니다.\n\n융합 모델인 model_fused가 원래 모델과 동일한 정확도를 갖는지도 검증할 예정입니다(BN fusion은 네트워크 기능을 변경하지 않는 동등한 변환입니다).\n\ndef fuse_conv_bn(conv, bn):\n    # modified from https://mmcv.readthedocs.io/en/latest/_modules/mmcv/cnn/utils/fuse_conv_bn.html\n    assert conv.bias is None\n\n    factor = bn.weight.data / torch.sqrt(bn.running_var.data + bn.eps)\n    conv.weight.data = conv.weight.data * factor.reshape(-1, 1, 1, 1)\n    conv.bias = nn.Parameter(- bn.running_mean.data * factor + bn.bias.data)\n\n    return conv\n\nprint('Before conv-bn fusion: backbone length', len(model.backbone))\n#  fuse the batchnorm into conv layers\nrecover_model()\nmodel_fused = copy.deepcopy(model)\nfused_backbone = []\nptr = 0\nwhile ptr &lt; len(model_fused.backbone):\n    if isinstance(model_fused.backbone[ptr], nn.Conv2d) and \\\n        isinstance(model_fused.backbone[ptr + 1], nn.BatchNorm2d):\n        fused_backbone.append(fuse_conv_bn(\n            model_fused.backbone[ptr], model_fused.backbone[ptr+ 1]))\n        ptr += 2\n    else:\n        fused_backbone.append(model_fused.backbone[ptr])\n        ptr += 1\nmodel_fused.backbone = nn.Sequential(*fused_backbone)\n\nprint('After conv-bn fusion: backbone length', len(model_fused.backbone))\n# sanity check, no BN anymore\nfor m in model_fused.modules():\n    assert not isinstance(m, nn.BatchNorm2d)\n\n#  the accuracy will remain the same after fusion\nfused_acc = evaluate(model_fused, dataloader['test'])\nprint(f'Accuracy of the fused model={fused_acc:.2f}%')\n\nBefore conv-bn fusion: backbone length 29\nAfter conv-bn fusion: backbone length 21\nAccuracy of the fused model=92.95%\n\n\n\n\n\n\n각 특징 맵의 범위를 얻기 위해 일부 샘플 데이터로 모델을 실행하여 특징 맵의 범위를 얻고, 해당 스케일링 팩터와 제로 포인트를 계산할 수 있습니다.\n\n\n# add hook to record the min max value of the activation\ninput_activation = {}\noutput_activation = {}\n\ndef add_range_recoder_hook(model):\n    import functools\n    def _record_range(self, x, y, module_name):\n        x = x[0]\n        input_activation[module_name] = x.detach()\n        output_activation[module_name] = y.detach()\n\n    all_hooks = []\n    for name, m in model.named_modules():\n        if isinstance(m, (nn.Conv2d, nn.Linear, nn.ReLU)):\n            all_hooks.append(m.register_forward_hook(\n                functools.partial(_record_range, module_name=name)))\n    return all_hooks\n\nhooks = add_range_recoder_hook(model_fused)\nsample_data = iter(dataloader['train']).__next__()[0]\nmodel_fused(sample_data.cuda())\n\n# remove hooks\nfor h in hooks:\n    h.remove()\n\n\n마지막으로 모델 양자화를 해보겠습니다. 다음과 같은 매핑으로 모델을 변환합니다.\n\nnn.Conv2d: QuantizedConv2d,\nnn.Linear: QuantizedLinear,\n# the following twos are just wrappers, as current\n# torch modules do not support int8 data format;\n# we will temporarily convert them to fp32 for computation\nnn.MaxPool2d: QuantizedMaxPool2d,\nnn.AvgPool2d: QuantizedAvgPool2d,\n\nclass QuantizedConv2d(nn.Module):\n    def __init__(self, weight, bias,\n                 input_zero_point, output_zero_point,\n                 input_scale, weight_scale, output_scale,\n                 stride, padding, dilation, groups,\n                 feature_bitwidth=8, weight_bitwidth=8):\n        super().__init__()\n        # current version Pytorch does not support IntTensor as nn.Parameter\n        self.register_buffer('weight', weight)\n        self.register_buffer('bias', bias)\n\n        self.input_zero_point = input_zero_point\n        self.output_zero_point = output_zero_point\n\n        self.input_scale = input_scale\n        self.register_buffer('weight_scale', weight_scale)\n        self.output_scale = output_scale\n\n        self.stride = stride\n        self.padding = (padding[1], padding[1], padding[0], padding[0])\n        self.dilation = dilation\n        self.groups = groups\n\n        self.feature_bitwidth = feature_bitwidth\n        self.weight_bitwidth = weight_bitwidth\n\n\n    def forward(self, x):\n        return quantized_conv2d(\n            x, self.weight, self.bias,\n            self.feature_bitwidth, self.weight_bitwidth,\n            self.input_zero_point, self.output_zero_point,\n            self.input_scale, self.weight_scale, self.output_scale,\n            self.stride, self.padding, self.dilation, self.groups\n            )\n\nclass QuantizedLinear(nn.Module):\n    def __init__(self, weight, bias,\n                 input_zero_point, output_zero_point,\n                 input_scale, weight_scale, output_scale,\n                 feature_bitwidth=8, weight_bitwidth=8):\n        super().__init__()\n        # current version Pytorch does not support IntTensor as nn.Parameter\n        self.register_buffer('weight', weight)\n        self.register_buffer('bias', bias)\n\n        self.input_zero_point = input_zero_point\n        self.output_zero_point = output_zero_point\n\n        self.input_scale = input_scale\n        self.register_buffer('weight_scale', weight_scale)\n        self.output_scale = output_scale\n\n        self.feature_bitwidth = feature_bitwidth\n        self.weight_bitwidth = weight_bitwidth\n\n    def forward(self, x):\n        return quantized_linear(\n            x, self.weight, self.bias,\n            self.feature_bitwidth, self.weight_bitwidth,\n            self.input_zero_point, self.output_zero_point,\n            self.input_scale, self.weight_scale, self.output_scale\n            )\n\nclass QuantizedMaxPool2d(nn.MaxPool2d):\n    def forward(self, x):\n        # current version PyTorch does not support integer-based MaxPool\n        return super().forward(x.float()).to(torch.int8)\n\nclass QuantizedAvgPool2d(nn.AvgPool2d):\n    def forward(self, x):\n        # current version PyTorch does not support integer-based AvgPool\n        return super().forward(x.float()).to(torch.int8)\n\n# we use int8 quantization, which is quite popular\nfeature_bitwidth = weight_bitwidth = 8\nquantized_model = copy.deepcopy(model_fused)\nquantized_backbone = []\nptr = 0\nwhile ptr &lt; len(quantized_model.backbone):\n    if isinstance(quantized_model.backbone[ptr], nn.Conv2d) and \\\n        isinstance(quantized_model.backbone[ptr + 1], nn.ReLU):\n        conv = quantized_model.backbone[ptr]\n        conv_name = f'backbone.{ptr}'\n        relu = quantized_model.backbone[ptr + 1]\n        relu_name = f'backbone.{ptr + 1}'\n\n        input_scale, input_zero_point = \\\n            get_quantization_scale_and_zero_point(\n                input_activation[conv_name], feature_bitwidth)\n\n        output_scale, output_zero_point = \\\n            get_quantization_scale_and_zero_point(\n                output_activation[relu_name], feature_bitwidth)\n\n        quantized_weight, weight_scale, weight_zero_point = \\\n            linear_quantize_weight_per_channel(conv.weight.data, weight_bitwidth)\n        quantized_bias, bias_scale, bias_zero_point = \\\n            linear_quantize_bias_per_output_channel(\n                conv.bias.data, weight_scale, input_scale)\n        shifted_quantized_bias = \\\n            shift_quantized_conv2d_bias(quantized_bias, quantized_weight,\n                                        input_zero_point)\n\n        quantized_conv = QuantizedConv2d(\n            quantized_weight, shifted_quantized_bias,\n            input_zero_point, output_zero_point,\n            input_scale, weight_scale, output_scale,\n            conv.stride, conv.padding, conv.dilation, conv.groups,\n            feature_bitwidth=feature_bitwidth, weight_bitwidth=weight_bitwidth\n        )\n\n        quantized_backbone.append(quantized_conv)\n        ptr += 2\n    elif isinstance(quantized_model.backbone[ptr], nn.MaxPool2d):\n        quantized_backbone.append(QuantizedMaxPool2d(\n            kernel_size=quantized_model.backbone[ptr].kernel_size,\n            stride=quantized_model.backbone[ptr].stride\n            ))\n        ptr += 1\n    elif isinstance(quantized_model.backbone[ptr], nn.AvgPool2d):\n        quantized_backbone.append(QuantizedAvgPool2d(\n            kernel_size=quantized_model.backbone[ptr].kernel_size,\n            stride=quantized_model.backbone[ptr].stride\n            ))\n        ptr += 1\n    else:\n        raise NotImplementedError(type(quantized_model.backbone[ptr]))  # should not happen\nquantized_model.backbone = nn.Sequential(*quantized_backbone)\n\n# finally, quantized the classifier\nfc_name = 'classifier'\nfc = model.classifier\ninput_scale, input_zero_point = \\\n    get_quantization_scale_and_zero_point(\n        input_activation[fc_name], feature_bitwidth)\n\noutput_scale, output_zero_point = \\\n    get_quantization_scale_and_zero_point(\n        output_activation[fc_name], feature_bitwidth)\n\nquantized_weight, weight_scale, weight_zero_point = \\\n    linear_quantize_weight_per_channel(fc.weight.data, weight_bitwidth)\nquantized_bias, bias_scale, bias_zero_point = \\\n    linear_quantize_bias_per_output_channel(\n        fc.bias.data, weight_scale, input_scale)\nshifted_quantized_bias = \\\n    shift_quantized_linear_bias(quantized_bias, quantized_weight,\n                                input_zero_point)\n\nquantized_model.classifier = QuantizedLinear(\n    quantized_weight, shifted_quantized_bias,\n    input_zero_point, output_zero_point,\n    input_scale, weight_scale, output_scale,\n    feature_bitwidth=feature_bitwidth, weight_bitwidth=weight_bitwidth\n)\n\n양자화 과정이 완료되었습니다! 모델 아키텍처를 인쇄하고 시각화하며 양자화된 모델의 정확성도 검증해 보겠습니다.\n\nQuestion 9.1 (5 pts)\n양자화된 모델을 실행하기 위해서는 (0, 1) 범위의 입력 데이터를 (-128, 127) 범위의 int8 범위로 매핑하는 추가적인 전처리가 필요합니다. 이 전처리를 진행하는 아래 코드를 완성하세요.\nHint: 양자화된 모델은 fp32 모델과 거의 동일한 정확도를 가지고 있습니다.\n\nprint(quantized_model)\n\ndef extra_preprocess(x):\n    # hint: you need to convert the original fp32 input of range (0, 1)\n    #  into int8 format of range (-128, 127)\n    ############### YOUR CODE STARTS HERE ###############\n    x_scaled = x * 255\n    x_shifted = x_scaled - 128\n    return x_shifted.clamp(-128, 127).to(torch.int8)\n    ############### YOUR CODE ENDS HERE #################\n\nint8_model_accuracy = evaluate(quantized_model, dataloader['test'],\n                               extra_preprocess=[extra_preprocess])\nprint(f\"int8 model has accuracy={int8_model_accuracy:.2f}%\")\n\nVGG(\n  (backbone): Sequential(\n    (0): QuantizedConv2d()\n    (1): QuantizedConv2d()\n    (2): QuantizedMaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n    (3): QuantizedConv2d()\n    (4): QuantizedConv2d()\n    (5): QuantizedMaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n    (6): QuantizedConv2d()\n    (7): QuantizedConv2d()\n    (8): QuantizedMaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n    (9): QuantizedConv2d()\n    (10): QuantizedConv2d()\n    (11): QuantizedMaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n    (12): QuantizedAvgPool2d(kernel_size=2, stride=2, padding=0)\n  )\n  (classifier): QuantizedLinear()\n)\nint8 model has accuracy=92.90%"
  },
  {
    "objectID": "posts/labs/lab02.html#question-9.2-bonus-question-5-pts",
    "href": "posts/labs/lab02.html#question-9.2-bonus-question-5-pts",
    "title": "👩‍💻 Lab 2",
    "section": "Question 9.2 (Bonus Question; 5 pts)",
    "text": "Question 9.2 (Bonus Question; 5 pts)\nlinear quantized model에 ReLU 층이 없는 이유를 설명하세요.\nYour Answer:\n선형(Linear) 양자화 모델에서 ReLU(Rectified Linear Unit) 층이 없는 이유는 주로 양자화 과정에서의 데이터 표현 방식과 연산의 효율성과 관련이 있습니다. 양자화는 모델의 가중치나 활성화를 고정된 비트 너비(예: 8비트)의 정수로 제한하여 저장하고 계산하는 기술입니다. 이러한 제한은 모델의 크기를 줄이고, 계산 속도를 향상시키며, 저전력 장치에서의 실행을 용이하게 합니다. 그러나 이 과정에서 데이터의 정밀도가 손실될 수 있으며, 이는 모델 성능에 영향을 미칠 수 있습니다.\nReLU 활성화 함수는 입력이 양수일 경우 그대로 출력하고, 음수일 경우 0으로 만드는 간단하고 효율적인 비선형 함수입니다. ReLU는 딥러닝 모델에서 널리 사용되며, 특히 은닉층에서 비선형성을 추가하여 모델의 표현력을 향상시키는 데 중요한 역할을 합니다.\n선형 양자화 모델에서 ReLU 층이 없는 주된 이유는 다음과 같습니다:\n\n양자화된 데이터의 범위 제한: 정수 양자화 과정에서는 데이터가 특정 범위 내의 값으로 제한됩니다. 예를 들어, 8비트 양자화에서는 값이 -128부터 127까지의 정수 범위를 가집니다. 이러한 범위 내에서 ReLU를 적용하면 음수 값이 모두 0으로 변환되어, 양수 값만 남게 됩니다. 이 과정에서 데이터의 범위가 더욱 제한되어, 양자화된 모델의 표현력이 더욱 감소할 수 있습니다.\n효율성: 양자화된 모델은 가능한 한 계산을 간단하게 유지하여 빠른 추론 속도와 낮은 전력 소모를 달성하려고 합니다. ReLU와 같은 비선형 함수를 추가하면, 추론 과정에서 추가적인 계산이 필요하게 됩니다. 어떤 경우에는 모델의 구조나 목적에 따라 이러한 추가 계산 없이도 충분한 성능을 달성할 수 있으므로, ReLU 층을 생략할 수 있습니다.\n모델 설계와 목적: 특정 양자화 모델에서는 성능 유지를 위해 ReLU 대신 다른 기법이나 활성화 함수를 사용할 수 있습니다. 예를 들어, 양자화 전 모델에서 ReLU를 사용하는 대신, 양자화 과정에서 최적화된 활성화 함수를 선택하거나, ReLU의 효과를 모방할 수 있는 다른 방법을 모색할 수 있습니다.\n\n결론적으로, 선형 양자화 모델에서 ReLU 층의 부재는 데이터의 범위 제한, 계산 효율성, 그리고 특정 모델 설계와 목적에 기인할 수 있습니다. 모델의 설계자는 성능, 속도, 크기 등의 요구 사항을 균형 있게 고려하여 최적의 모델 구조를 결정해야 합니다."
  },
  {
    "objectID": "posts/labs/lab01.html",
    "href": "posts/labs/lab01.html",
    "title": "👩‍💻 Lab 1",
    "section": "",
    "text": "이번 Lab 1 Pruning은 다음과 같은 목표와 내용으로 구성되어 있습니다. 아래 버튼을 눌러 Colaboratory에서 바로 실행할 수 있습니다.\n\n\n\n\npruning의 기본 개념을 이해합니다.\nfine-grained pruning을 구현하고 적용합니다.\nchannel pruning을 구현하고 적용합니다.\npruning으로부터의 성능 개선(예: 속도 향상)에 대한 기본적인 이해를 얻습니다.\n이러한 pruning 접근 방식 간의 차이점과 tradeoffs를 이해합니다.\n\n\n\n\n이 실습에는 Fine-grained Pruning과 Channel Pruning의 두 가지 주요 섹션이 있습니다.\n총 9개의 질문이 있습니다:\n\nFine-grained Pruning에 대해서는 5개의 질문이 있습니다 (Question 1-5).\nChannel Pruning에 대해서는 3개의 질문이 있습니다 (Question 6-8).\nQuestion 9는 fine-grained pruning과 channel pruning을 비교합니다.\n\n\n실습노트에 대한 설정 부분(Setup)은 Colaboratory Note를 열면 확인하실 수 있습니다. 포스팅에서는 보다 실습내용에 집중할 수 있도록 생략되어 있습니다."
  },
  {
    "objectID": "posts/labs/lab01.html#goals",
    "href": "posts/labs/lab01.html#goals",
    "title": "👩‍💻 Lab 1",
    "section": "",
    "text": "pruning의 기본 개념을 이해합니다.\nfine-grained pruning을 구현하고 적용합니다.\nchannel pruning을 구현하고 적용합니다.\npruning으로부터의 성능 개선(예: 속도 향상)에 대한 기본적인 이해를 얻습니다.\n이러한 pruning 접근 방식 간의 차이점과 tradeoffs를 이해합니다."
  },
  {
    "objectID": "posts/labs/lab01.html#contents",
    "href": "posts/labs/lab01.html#contents",
    "title": "👩‍💻 Lab 1",
    "section": "",
    "text": "이 실습에는 Fine-grained Pruning과 Channel Pruning의 두 가지 주요 섹션이 있습니다.\n총 9개의 질문이 있습니다:\n\nFine-grained Pruning에 대해서는 5개의 질문이 있습니다 (Question 1-5).\nChannel Pruning에 대해서는 3개의 질문이 있습니다 (Question 6-8).\nQuestion 9는 fine-grained pruning과 channel pruning을 비교합니다.\n\n\n실습노트에 대한 설정 부분(Setup)은 Colaboratory Note를 열면 확인하실 수 있습니다. 포스팅에서는 보다 실습내용에 집중할 수 있도록 생략되어 있습니다."
  },
  {
    "objectID": "posts/labs/lab01.html#question-1-10-pts",
    "href": "posts/labs/lab01.html#question-1-10-pts",
    "title": "👩‍💻 Lab 1",
    "section": "Question 1 (10 pts)",
    "text": "Question 1 (10 pts)\n위 weight 히스토그램들을 보고 다음 질문에 답해 주세요.\n\nQuestion 1.1 (5 pts)\n각기 다른 계층에서 weight 분포들의 공통적인 특성은 무엇인가요?\nYour Answer:\nmean이 0인 normal 분포를 따르고 있다 (backbone의 경우, classifier 제외)\n\n\nQuestion 1.2 (5 pts)\n이러한 특성들이 pruning에 어떻게 도움이 되나요?\nYour Answer:\n0이 많으므로, 계산하지 않거나 없앨 수 있다."
  },
  {
    "objectID": "posts/labs/lab01.html#magnitude-based-pruning",
    "href": "posts/labs/lab01.html#magnitude-based-pruning",
    "title": "👩‍💻 Lab 1",
    "section": "Magnitude-based Pruning",
    "text": "Magnitude-based Pruning\nFine-grained pruning에 있어서 널리 사용되는 importance(중요도)는 weight 값의 크기, 즉,\n\\(Importance=|W|\\)\n입니다. Magnitude-based Pruning으로 알려져 있습니다 (Learning both Weights and Connections for Efficient Neural Networks 참조).\n\n\nQuestion 2 (15 pts)\n다음 magnitude-based fine-grained pruning 함수를 완성해 주세요.\nHint:\n\n1단계에서는 pruning 후에 0의 개수(num_zeros)를 계산합니다. num_zeros는 정수여야 합니다. 부동 소수점 숫자를 정수로 변환하기 위해 round() 또는 int()를 사용할 수 있습니다. 여기서는 round()를 사용합니다.\n2단계에서는 가중치 텐서의 importance를 계산합니다. Pytorch는 torch.abs(), torch.Tensor.abs(), torch.Tensor.abs_() API를 제공합니다.\n3단계에서는 threshold를 계산하여 threshold보다 중요도가 낮은 모든 synapses가 제거되도록 합니다. Pytorch는 torch.kthvalue(), torch.Tensor.kthvalue(), torch.topk() API를 제공합니다.\n4단계에서는 threshold를 기반으로 pruning mask를 계산합니다. mask에서 1은 synapse가 유지됨을 나타내고, 0은 synapse가 제거됨을 나타냅니다. mask = importance &gt; threshold. Pytorch는 torch.gt() API를 제공합니다.\n\n\ndef fine_grained_prune(tensor: torch.Tensor, sparsity : float) -&gt; torch.Tensor:\n    \"\"\"\n    magnitude-based pruning for single tensor\n    :param tensor: torch.(cuda.)Tensor, weight of conv/fc layer\n    :param sparsity: float, pruning sparsity\n        sparsity = #zeros / #elements = 1 - #nonzeros / #elements\n    :return:\n        torch.(cuda.)Tensor, mask for zeros\n    \"\"\"\n    sparsity = min(max(0.0, sparsity), 1.0)\n    if sparsity == 1.0:\n        tensor.zero_()\n        return torch.zeros_like(tensor)\n    elif sparsity == 0.0:\n        return torch.ones_like(tensor)\n\n    num_elements = tensor.numel()\n\n    ##################### YOUR CODE STARTS HERE #####################\n    # Step 1: calculate the #zeros (please use round())\n    num_zeros = round(num_elements * sparsity)\n    # Step 2: calculate the importance of weight\n    importance = torch.abs(tensor)\n    # Step 3: calculate the pruning threshold\n    threshold = torch.kthvalue(torch.flatten(importance), num_zeros)[0]\n    # Step 4: get binary mask (1 for nonzeros, 0 for zeros)\n    mask = importance &gt; threshold\n\n    ##################### YOUR CODE ENDS HERE #######################\n\n    # Step 5: apply mask to prune the tensor\n    tensor.mul_(mask)\n\n    return mask\n\n위에서 정의한 fine-grained pruning 기능을 확인하기 위해, 더미 텐서에 위 함수를 적용해 봅시다.\n\ntest_fine_grained_prune()\n\n\n\n\n\n\n\n\n* Test fine_grained_prune()\n    target sparsity: 0.75\n        sparsity before pruning: 0.04\n        sparsity after pruning: 0.76\n        sparsity of pruning mask: 0.76\n* Test passed.\n\n\n\n\nQuestion 3 (5 pts)\n마지막 셀은 pruning 전후의 텐서를 그립니다. 0이 아닌 값은 파란색으로, 0은 회색으로 표시됩니다. 다음 코드 셀에서 target_sparsity의 값을 수정하여 pruning 후 sparse 텐서에 0이 아닌 값이 10개만 남도록 해 주세요.\n\n##################### YOUR CODE STARTS HERE #####################\n# sparsity:=#Zeros/#𝑊=1−#Nonzeros/#𝑊\n# 1 - 10/25\ntarget_sparsity = 0.6 # please modify the value of target_sparsit\n##################### YOUR CODE ENDS HERE #####################\ntest_fine_grained_prune(target_sparsity=target_sparsity, target_nonzeros=10)\n\n\n\n\n\n\n\n\n* Test fine_grained_prune()\n    target sparsity: 0.60\n        sparsity before pruning: 0.04\n        sparsity after pruning: 0.60\n        sparsity of pruning mask: 0.60\n* Test passed.\n\n\n이제 fine-grained pruning 함수를 전체 모델을 pruning하는 클래스로 래핑합니다. FineGrainedPruner 클래스에서는 모델 가중치가 변경될 때마다 마스크를 적용하여 모델이 항상 sparse 상태를 유지할 수 있도록 pruning 마스크 기록을 가지고 있어야 합니다.\n\nclass FineGrainedPruner:\n    def __init__(self, model, sparsity_dict):\n        self.masks = FineGrainedPruner.prune(model, sparsity_dict)\n\n    @torch.no_grad()\n    def apply(self, model):\n        for name, param in model.named_parameters():\n            if name in self.masks:\n                param *= self.masks[name]\n\n    @staticmethod\n    @torch.no_grad()\n    def prune(model, sparsity_dict):\n        masks = dict()\n        for name, param in model.named_parameters():\n            if param.dim() &gt; 1: # we only prune conv and fc weights\n                masks[name] = fine_grained_prune(param, sparsity_dict[name])\n        return masks"
  },
  {
    "objectID": "posts/labs/lab01.html#sensitivity-scan",
    "href": "posts/labs/lab01.html#sensitivity-scan",
    "title": "👩‍💻 Lab 1",
    "section": "Sensitivity Scan",
    "text": "Sensitivity Scan\n각 레이어는 모델 성능에 대해 각각 다르게 기여합니다. 각 레이어에 적절한 sparsity를 결정하는 것은 어려운 일입니다. 널리 사용되는 접근 방식은 sensitivity scan입니다.\nsensitivity scan 동안, 각 시간마다 하나의 레이어만을 prune하여 accuracy 저하를 관찰합니다. 다양한 sparsities를 스캔함으로써, 해당 레이어의 sensitivity curve (즉, 정확도 대비 sparsity)를 그릴 수 있습니다.\n다음은 sensitivity curves의 예시 그림입니다. x축은 sparsity 또는 #parameters가 감소한 비율 (즉, sparsity)입니다. y축은 검증 정확도입니다. (Learning both Weights and Connections for Efficient Neural Networks의 Figure 6)\n\n다음 코드 셀은 스캔된 sparsities와 각 가중치가 prune될 때의 정확도 리스트를 반환하는 sensitivity scan 함수를 정의합니다.\n\n@torch.no_grad()\ndef sensitivity_scan(model, dataloader, scan_step=0.1, scan_start=0.4, scan_end=1.0, verbose=True):\n    sparsities = np.arange(start=scan_start, stop=scan_end, step=scan_step)\n    accuracies = []\n    named_conv_weights = [(name, param) for (name, param) \\\n                          in model.named_parameters() if param.dim() &gt; 1]\n    for i_layer, (name, param) in enumerate(named_conv_weights):\n        param_clone = param.detach().clone()\n        accuracy = []\n        for sparsity in tqdm(sparsities, desc=f'scanning {i_layer}/{len(named_conv_weights)} weight - {name}'):\n            fine_grained_prune(param.detach(), sparsity=sparsity)\n            acc = evaluate(model, dataloader, verbose=False)\n            if verbose:\n                print(f'\\r    sparsity={sparsity:.2f}: accuracy={acc:.2f}%', end='')\n            # restore\n            param.copy_(param_clone)\n            accuracy.append(acc)\n        if verbose:\n            print(f'\\r    sparsity=[{\",\".join([\"{:.2f}\".format(x) for x in sparsities])}]: accuracy=[{\", \".join([\"{:.2f}%\".format(x) for x in accuracy])}]', end='')\n        accuracies.append(accuracy)\n    return sparsities, accuracies\n\n다음 셀들을 실행하여 sensitivity curves를 그려주세요. 완료하는 데 약 2분 정도 걸릴 것입니다.\n\nsparsities, accuracies = sensitivity_scan(\n    model, dataloader['test'], scan_step=0.1, scan_start=0.4, scan_end=1.0)\n\n\n\n\n    sparsity=[0.40,0.50,0.60,0.70,0.80,0.90]: accuracy=[92.42%, 91.19%, 87.55%, 83.39%, 69.41%, 31.81%]    sparsity=[0.40,0.50,0.60,0.70,0.80,0.90]: accuracy=[92.93%, 92.88%, 92.71%, 92.40%, 91.32%, 84.78%]    sparsity=[0.40,0.50,0.60,0.70,0.80,0.90]: accuracy=[92.94%, 92.64%, 92.46%, 91.77%, 89.85%, 78.56%]    sparsity=[0.40,0.50,0.60,0.70,0.80,0.90]: accuracy=[92.86%, 92.72%, 92.23%, 91.09%, 85.35%, 51.31%]    sparsity=[0.40,0.50,0.60,0.70,0.80,0.90]: accuracy=[92.88%, 92.68%, 92.22%, 89.47%, 76.86%, 38.78%]    sparsity=[0.40,0.50,0.60,0.70,0.80,0.90]: accuracy=[92.92%, 92.71%, 92.63%, 91.88%, 89.90%, 82.19%]    sparsity=[0.40,0.50,0.60,0.70,0.80,0.90]: accuracy=[92.94%, 92.86%, 92.65%, 92.10%, 90.58%, 83.65%]    sparsity=[0.40,0.50,0.60,0.70,0.80,0.90]: accuracy=[92.94%, 92.92%, 92.88%, 92.81%, 92.63%, 91.34%]    sparsity=[0.40,0.50,0.60,0.70,0.80,0.90]: accuracy=[92.91%, 92.83%, 92.81%, 92.97%, 92.68%, 92.52%]\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\ndef plot_sensitivity_scan(sparsities, accuracies, dense_model_accuracy):\n    lower_bound_accuracy = 100 - (100 - dense_model_accuracy) * 1.5\n    fig, axes = plt.subplots(3, int(math.ceil(len(accuracies) / 3)),figsize=(15,8))\n    axes = axes.ravel()\n    plot_index = 0\n    for name, param in model.named_parameters():\n        if param.dim() &gt; 1:\n            ax = axes[plot_index]\n            curve = ax.plot(sparsities, accuracies[plot_index])\n            line = ax.plot(sparsities, [lower_bound_accuracy] * len(sparsities))\n            ax.set_xticks(np.arange(start=0.4, stop=1.0, step=0.1))\n            ax.set_ylim(80, 95)\n            ax.set_title(name)\n            ax.set_xlabel('sparsity')\n            ax.set_ylabel('top-1 accuracy')\n            ax.legend([\n                'accuracy after pruning',\n                f'{lower_bound_accuracy / dense_model_accuracy * 100:.0f}% of dense model accuracy'\n            ])\n            ax.grid(axis='x')\n            plot_index += 1\n    fig.suptitle('Sensitivity Curves: Validation Accuracy vs. Pruning Sparsity')\n    fig.tight_layout()\n    fig.subplots_adjust(top=0.925)\n    plt.show()\n\nplot_sensitivity_scan(sparsities, accuracies, dense_model_accuracy)\n\n\n\n\n\n\n\n\n\nQuestion 4 (15 pts)\n위 sensitivity curves의 정보를 사용하여 다음 질문에 답해 주세요.\n\nQuestion 4.1 (5 pts)\npruning sparsity와 모델 정확도 사이의 관계는 무엇인가요? (즉, sparsity가 높아질 때 정확도가 증가하나요, 아니면 감소하나요?)\nYour Answer:\npruning sparsity가 높아질 수록, model accuracy는 감소하는 경향을 보인다\n\n\nQuestion 4.2 (5 pts)\n모든 레이어가 같은 sensitivity를 가지고 있나요?\nYour Answer:\n어떤 레이어는 sensitive하지 않고(classifier), 어떤 레이어는 sensitive하다(conv0) 대체로, 앞쪽 레이어(0~1..)이 민감해보인다\n\n\nQuestion 4.3 (5 pts)\n어떤 레이어가 pruning sparsity에 가장 민감한가요?\nYour Answer:\nconv0 layer"
  },
  {
    "objectID": "posts/labs/lab01.html#parameters-of-each-layer",
    "href": "posts/labs/lab01.html#parameters-of-each-layer",
    "title": "👩‍💻 Lab 1",
    "section": "#Parameters of each layer",
    "text": "#Parameters of each layer\n정확도뿐만 아니라 각 레이어의 매개변수(parameter) 수도 sparsity 선택에 영향을 미칩니다. 매개변수가 더 많은 레이어는 더 큰 sparsities를 요구합니다.\n다음 코드 셀을 실행하여 전체 모델에서 #parameters의 분포를 그려주세요.\n\ndef plot_num_parameters_distribution(model):\n    num_parameters = dict()\n    for name, param in model.named_parameters():\n        if param.dim() &gt; 1:\n            num_parameters[name] = param.numel()\n    fig = plt.figure(figsize=(8, 6))\n    plt.grid(axis='y')\n    plt.bar(list(num_parameters.keys()), list(num_parameters.values()))\n    plt.title('#Parameter Distribution')\n    plt.ylabel('Number of Parameters')\n    plt.xticks(rotation=60)\n    plt.tight_layout()\n    plt.show()\n\nplot_num_parameters_distribution(model)"
  },
  {
    "objectID": "posts/labs/lab01.html#sensitivity-curves와-parameters-분포를-기반으로-sparsity-선택하기",
    "href": "posts/labs/lab01.html#sensitivity-curves와-parameters-분포를-기반으로-sparsity-선택하기",
    "title": "👩‍💻 Lab 1",
    "section": "Sensitivity Curves와 #Parameters 분포를 기반으로 Sparsity 선택하기",
    "text": "Sensitivity Curves와 #Parameters 분포를 기반으로 Sparsity 선택하기\n\nQuestion 5 (10 pts)\nsensitivity curves와 모델의 #parameters 분포를 기반으로 각 레이어의 sparsity를 선택해 주세요.\npruned 모델의 전체 압축 비율은 대체로 #parameters가 큰 레이어에 주로 의존하며, 다른 레이어는 pruning에 대한 sensitivity가 다릅니다(Question 4 참조).\npruning 후에 sparse 모델이 dense 모델의 크기의 25%이며, finetuning 후에 검증 정확도가 92.5% 이상인지 확인하세요.\nHint:\n\n#parameters가 더 많은 레이어는 더 큰 sparsity를 가져야 합니다. (Figure #Parameter Distribution 참조)\npruning sparsity에 민감한 레이어(즉, sparsity가 높아질수록 정확도가 빠르게 떨어지는 레이어)는 더 작은 sparsity를 가져야 합니다. (Figure Sensitivity Curves 참조)\n\n\nrecover_model()\n\nsparsity_dict = {\n##################### YOUR CODE STARTS HERE #####################\n    # please modify the sparsity value of each layer\n    # please DO NOT modify the key of sparsity_dict\n    'backbone.conv0.weight': 0,\n    'backbone.conv1.weight': 0.5,\n    'backbone.conv2.weight': 0.5,\n    'backbone.conv3.weight': 0.5,\n    'backbone.conv4.weight': 0.5,\n    'backbone.conv5.weight': 0.8,\n    'backbone.conv6.weight': 0.8,\n    'backbone.conv7.weight': 0.9,\n    'classifier.weight': 0\n##################### YOUR CODE ENDS HERE #######################\n}\n\n정의된 sparsity_dict에 따라 모델을 prune하고 sparse 모델의 정보를 출력하기 위해 다음 셀을 실행해 주세요.\n\npruner = FineGrainedPruner(model, sparsity_dict)\nprint(f'After pruning with sparsity dictionary')\nfor name, sparsity in sparsity_dict.items():\n    print(f'  {name}: {sparsity:.2f}')\nprint(f'The sparsity of each layer becomes')\nfor name, param in model.named_parameters():\n    if name in sparsity_dict:\n        print(f'  {name}: {get_sparsity(param):.2f}')\n\nsparse_model_size = get_model_size(model, count_nonzero_only=True)\nprint(f\"Sparse model has size={sparse_model_size / MiB:.2f} MiB = {sparse_model_size / dense_model_size * 100:.2f}% of dense model size\")\nsparse_model_accuracy = evaluate(model, dataloader['test'])\nprint(f\"Sparse model has accuracy={sparse_model_accuracy:.2f}% before fintuning\")\n\nplot_weight_distribution(model, count_nonzero_only=True)\n\nAfter pruning with sparsity dictionary\n  backbone.conv0.weight: 0.00\n  backbone.conv1.weight: 0.50\n  backbone.conv2.weight: 0.50\n  backbone.conv3.weight: 0.50\n  backbone.conv4.weight: 0.50\n  backbone.conv5.weight: 0.80\n  backbone.conv6.weight: 0.80\n  backbone.conv7.weight: 0.90\n  classifier.weight: 0.00\nThe sparsity of each layer becomes\n  backbone.conv0.weight: 0.00\n  backbone.conv1.weight: 0.50\n  backbone.conv2.weight: 0.50\n  backbone.conv3.weight: 0.50\n  backbone.conv4.weight: 0.50\n  backbone.conv5.weight: 0.80\n  backbone.conv6.weight: 0.80\n  backbone.conv7.weight: 0.90\n  classifier.weight: 0.00\nSparse model has size=8.63 MiB = 24.50% of dense model size\nSparse model has accuracy=87.00% before fintuning"
  },
  {
    "objectID": "posts/labs/lab01.html#finetune-the-fine-grained-pruned-model",
    "href": "posts/labs/lab01.html#finetune-the-fine-grained-pruned-model",
    "title": "👩‍💻 Lab 1",
    "section": "Finetune the fine-grained pruned model",
    "text": "Finetune the fine-grained pruned model\n이전 셀의 출력에서 볼 수 있듯이, fine-grained pruning이 모델 가중치의 대부분을 줄이지만 모델의 정확도도 떨어졌습니다. 따라서, sparse 모델의 정확도를 회복하기 위해 finetune해야 합니다.\nsparse 모델을 finetune하기 위해 다음 셀을 실행해 주세요. 완료하는 데 약 3분 정도 걸릴 것입니다.\n\nnum_finetune_epochs = 5\noptimizer = torch.optim.SGD(model.parameters(), lr=0.01, momentum=0.9, weight_decay=1e-4)\nscheduler = torch.optim.lr_scheduler.CosineAnnealingLR(optimizer, num_finetune_epochs)\ncriterion = nn.CrossEntropyLoss()\n\nbest_sparse_model_checkpoint = dict()\nbest_accuracy = 0\nprint(f'Finetuning Fine-grained Pruned Sparse Model')\nfor epoch in range(num_finetune_epochs):\n    # At the end of each train iteration, we have to apply the pruning mask\n    #    to keep the model sparse during the training\n    train(model, dataloader['train'], criterion, optimizer, scheduler,\n          callbacks=[lambda: pruner.apply(model)])\n    accuracy = evaluate(model, dataloader['test'])\n    is_best = accuracy &gt; best_accuracy\n    if is_best:\n        best_sparse_model_checkpoint['state_dict'] = copy.deepcopy(model.state_dict())\n        best_accuracy = accuracy\n    print(f'    Epoch {epoch+1} Accuracy {accuracy:.2f}% / Best Accuracy: {best_accuracy:.2f}%')\n\nFinetuning Fine-grained Pruned Sparse Model\n    Epoch 1 Accuracy 92.66% / Best Accuracy: 92.66%\n    Epoch 2 Accuracy 92.77% / Best Accuracy: 92.77%\n    Epoch 3 Accuracy 92.80% / Best Accuracy: 92.80%\n    Epoch 4 Accuracy 92.68% / Best Accuracy: 92.80%\n    Epoch 5 Accuracy 92.77% / Best Accuracy: 92.80%\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nbest finetuned sparse 모델의 정보를 보기 위해 다음 셀을 실행해 주세요.\n\n# load the best sparse model checkpoint to evaluate the final performance\nmodel.load_state_dict(best_sparse_model_checkpoint['state_dict'])\nsparse_model_size = get_model_size(model, count_nonzero_only=True)\nprint(f\"Sparse model has size={sparse_model_size / MiB:.2f} MiB = {sparse_model_size / dense_model_size * 100:.2f}% of dense model size\")\nsparse_model_accuracy = evaluate(model, dataloader['test'])\nprint(f\"Sparse model has accuracy={sparse_model_accuracy:.2f}% after fintuning\")\n\nSparse model has size=8.63 MiB = 24.50% of dense model size\nSparse model has accuracy=92.80% after fintuning"
  },
  {
    "objectID": "posts/labs/lab01.html#remove-channel-weights",
    "href": "posts/labs/lab01.html#remove-channel-weights",
    "title": "👩‍💻 Lab 1",
    "section": "Remove Channel Weights",
    "text": "Remove Channel Weights\nFine-grained pruning과 달리, channel pruning에서는 텐서에서 가중치를 완전히 제거할 수 있습니다. 즉, 출력 채널의 수가 줄어듭니다:\n\n\\(\\#\\mathrm{out\\_channels}_{\\mathrm{new}} = \\#\\mathrm{out\\_channels}_{\\mathrm{origin}} \\cdot (1 - \\mathrm{sparsity})\\)\n\nChannel pruning 후에도 가중치 텐서 \\(W\\)는 여전히 dense합니다. 따라서, sparsity를 prune ratio라고 합니다.\nFine-grained pruning처럼, 다른 레이어에 대해 다른 pruning 비율을 사용할 수 있습니다. 하지만 지금은 모든 레이어에 대해 균일한 pruning 비율을 사용합니다. 우리는 대략 30%의 균일한 pruning 비율로 2배의 계산 감소를 목표로 합니다(왜 그런지 생각해 보세요).\n이 섹션의 끝에서 레이어별로 다른 pruning 비율을 시도해 볼 수 있습니다. channel_prune 함수에 비율 리스트를 전달할 수 있습니다.\n\nQuestion 6 (10 pts)\nChannel pruning을 위한 다음 함수를 완성해 주세요.\n여기서 우리는 첫 번째 \\(\\#\\mathrm{out\\_channels}_{\\mathrm{new}}\\) 채널을 제외한 모든 출력 채널을 단순히 prune합니다.\n\ndef get_num_channels_to_keep(channels: int, prune_ratio: float) -&gt; int:\n    \"\"\"A function to calculate the number of layers to PRESERVE after pruning\n    Note that preserve_rate = 1. - prune_ratio\n    \"\"\"\n    ##################### YOUR CODE STARTS HERE #####################\n    return int(round((1-prune_ratio)*channels))\n    ##################### YOUR CODE ENDS HERE #####################\n\n@torch.no_grad()\ndef channel_prune(model: nn.Module,\n                  prune_ratio: Union[List, float]) -&gt; nn.Module:\n    \"\"\"Apply channel pruning to each of the conv layer in the backbone\n    Note that for prune_ratio, we can either provide a floating-point number,\n    indicating that we use a uniform pruning rate for all layers, or a list of\n    numbers to indicate per-layer pruning rate.\n    \"\"\"\n    # sanity check of provided prune_ratio\n    assert isinstance(prune_ratio, (float, list))\n    n_conv = len([m for m in model.backbone if isinstance(m, nn.Conv2d)])\n    # note that for the ratios, it affects the previous conv output and next\n    # conv input, i.e., conv0 - ratio0 - conv1 - ratio1-...\n    if isinstance(prune_ratio, list):\n        assert len(prune_ratio) == n_conv - 1\n    else:  # convert float to list\n        prune_ratio = [prune_ratio] * (n_conv - 1)\n\n    # we prune the convs in the backbone with a uniform ratio\n    model = copy.deepcopy(model)  # prevent overwrite\n    # we only apply pruning to the backbone features\n    all_convs = [m for m in model.backbone if isinstance(m, nn.Conv2d)]\n    all_bns = [m for m in model.backbone if isinstance(m, nn.BatchNorm2d)]\n    # apply pruning. we naively keep the first k channels\n    assert len(all_convs) == len(all_bns)\n    for i_ratio, p_ratio in enumerate(prune_ratio):\n        prev_conv = all_convs[i_ratio]\n        prev_bn = all_bns[i_ratio]\n        next_conv = all_convs[i_ratio + 1]\n        original_channels = prev_conv.out_channels  # same as next_conv.in_channels\n        n_keep = get_num_channels_to_keep(original_channels, p_ratio)\n\n        # prune the output of the previous conv and bn\n        prev_conv.weight.set_(prev_conv.weight.detach()[:n_keep])\n        prev_bn.weight.set_(prev_bn.weight.detach()[:n_keep])\n        prev_bn.bias.set_(prev_bn.bias.detach()[:n_keep])\n        prev_bn.running_mean.set_(prev_bn.running_mean.detach()[:n_keep])\n        prev_bn.running_var.set_(prev_bn.running_var.detach()[:n_keep])\n\n        # prune the input of the next conv (hint: just one line of code)\n        ##################### YOUR CODE STARTS HERE #####################\n        next_conv.weight.set_(next_conv.weight.detach()[:, :n_keep])\n        ##################### YOUR CODE ENDS HERE #####################\n\n    return model\n\n구현이 올바른지 확인하기 위해 다음 셀을 실행하여 확인하세요.\n\ndummy_input = torch.randn(1, 3, 32, 32).cuda()\npruned_model = channel_prune(model, prune_ratio=0.3)\npruned_macs = get_model_macs(pruned_model, dummy_input)\nassert pruned_macs == 305388064\nprint('* Check passed. Right MACs for the pruned model.')\n\n* Check passed. Right MACs for the pruned model.\n\n\n이제 30% pruning 비율을 가진 균일 channel pruning 후 모델의 성능을 평가해 봅시다.\n직접적으로 30%의 채널을 제거하는 것은 낮은 정확도로 이어집니다.\n\npruned_model_accuracy = evaluate(pruned_model, dataloader['test'])\nprint(f\"pruned model has accuracy={pruned_model_accuracy:.2f}%\")\n\n\n\n\npruned model has accuracy=28.14%"
  },
  {
    "objectID": "posts/labs/lab01.html#ranking-channels-by-importance",
    "href": "posts/labs/lab01.html#ranking-channels-by-importance",
    "title": "👩‍💻 Lab 1",
    "section": "Ranking Channels by Importance",
    "text": "Ranking Channels by Importance\n보시다시피, 모든 레이어에서 첫 30%의 채널을 제거하면 정확도가 크게 감소합니다. 이 문제를 해결하는 한 가지 가능한 방법은 덜 중요한 채널 가중치를 찾아서 제거하는 것입니다. 중요도를 평가하는 인기 있는 기준은 각 입력 채널에 해당하는 가중치의 Frobenius norm을 사용하는 것입니다:\n\n\\(importance_{i} = \\|W_{i}\\|_2, \\;\\; i = 0, 1, 2,\\cdots, \\#\\mathrm{in\\_channels}-1\\)\n\n우리는 채널 가중치를 더 중요한 것에서 덜 중요한 것으로 정렬한 다음, 각 레이어에 대해 처음 \\(k\\)개의 채널을 유지할 수 있습니다.\n\nQuestion 7 (15 pts)\nFrobenius norm에 기반하여 가중치 텐서를 정렬하는 다음 함수를 완성해 주세요.\nHint:\n\n텐서의 Frobenius norm을 계산하기 위해, Pytorch는 torch.norm API를 제공합니다.\n\n\n# function to sort the channels from important to non-important\ndef get_input_channel_importance(weight):\n    in_channels = weight.shape[1]\n    importances = []\n    # compute the importance for each input channel\n    for i_c in range(weight.shape[1]):\n        channel_weight = weight.detach()[:, i_c]\n        ##################### YOUR CODE STARTS HERE #####################\n        importance = torch.norm(channel_weight, p=2)\n        ##################### YOUR CODE ENDS HERE #####################\n        importances.append(importance.view(1))\n    return torch.cat(importances)\n\n@torch.no_grad()\ndef apply_channel_sorting(model):\n    model = copy.deepcopy(model)  # do not modify the original model\n    # fetch all the conv and bn layers from the backbone\n    all_convs = [m for m in model.backbone if isinstance(m, nn.Conv2d)]\n    all_bns = [m for m in model.backbone if isinstance(m, nn.BatchNorm2d)]\n    # iterate through conv layers\n    for i_conv in range(len(all_convs) - 1):\n        # each channel sorting index, we need to apply it to:\n        # - the output dimension of the previous conv\n        # - the previous BN layer\n        # - the input dimension of the next conv (we compute importance here)\n        prev_conv = all_convs[i_conv]\n        prev_bn = all_bns[i_conv]\n        next_conv = all_convs[i_conv + 1]\n        # note that we always compute the importance according to input channels\n        importance = get_input_channel_importance(next_conv.weight)\n        # sorting from large to small\n        sort_idx = torch.argsort(importance, descending=True)\n\n        # apply to previous conv and its following bn\n        prev_conv.weight.copy_(torch.index_select(\n            prev_conv.weight.detach(), 0, sort_idx))\n        for tensor_name in ['weight', 'bias', 'running_mean', 'running_var']:\n            tensor_to_apply = getattr(prev_bn, tensor_name)\n            tensor_to_apply.copy_(\n                torch.index_select(tensor_to_apply.detach(), 0, sort_idx)\n            )\n\n        # apply to the next conv input (hint: one line of code)\n        ##################### YOUR CODE STARTS HERE #####################\n        next_conv.weight.copy_(torch.index_select(next_conv.weight.detach(), 1, sort_idx))\n        ##################### YOUR CODE ENDS HERE #####################\n\n    return model\n\n이제 다음 셀을 실행하여 결과가 올바른지 확인하세요.\n\nprint('Before sorting...')\ndense_model_accuracy = evaluate(model, dataloader['test'])\nprint(f\"dense model has accuracy={dense_model_accuracy:.2f}%\")\n\nprint('After sorting...')\nsorted_model = apply_channel_sorting(model)\nsorted_model_accuracy = evaluate(sorted_model, dataloader['test'])\nprint(f\"sorted model has accuracy={sorted_model_accuracy:.2f}%\")\n\n# make sure accuracy does not change after sorting, since it is\n# equivalent transform\nassert abs(sorted_model_accuracy - dense_model_accuracy) &lt; 0.1\nprint('* Check passed.')\n\nBefore sorting...\ndense model has accuracy=92.95%\nAfter sorting...\nsorted model has accuracy=92.95%\n* Check passed.\n\n\n\n\n\n\n\n\n마지막으로 프루닝된 모델의 정확도를 정렬할 때와 그렇지 않을 때를 비교합니다.\n\nchannel_pruning_ratio = 0.3  # pruned-out ratio\n\nprint(\" * Without sorting...\")\npruned_model = channel_prune(model, channel_pruning_ratio)\npruned_model_accuracy = evaluate(pruned_model, dataloader['test'])\nprint(f\"pruned model has accuracy={pruned_model_accuracy:.2f}%\")\n\n\nprint(\" * With sorting...\")\nsorted_model = apply_channel_sorting(model)\npruned_model = channel_prune(sorted_model, channel_pruning_ratio)\npruned_model_accuracy = evaluate(pruned_model, dataloader['test'])\nprint(f\"pruned model has accuracy={pruned_model_accuracy:.2f}%\")\n\n * Without sorting...\npruned model has accuracy=28.14%\n * With sorting...\npruned model has accuracy=36.81%\n\n\n\n\n\n\n\n\n보시다시피 channel sorting은 pruned model의 정확도를 약간 향상시킬 수 있지만 여전히 channel pruning에 매우 일반적인 큰 저하가 있는 걸 알 수 있습니다. 이러한 정확도 저하를 회복하기 위해 fine-tuning을 수행할 수 있습니다.\n\nnum_finetune_epochs = 5\noptimizer = torch.optim.SGD(pruned_model.parameters(), lr=0.01, momentum=0.9, weight_decay=1e-4)\nscheduler = torch.optim.lr_scheduler.CosineAnnealingLR(optimizer, num_finetune_epochs)\ncriterion = nn.CrossEntropyLoss()\n\nbest_accuracy = 0\nfor epoch in range(num_finetune_epochs):\n    train(pruned_model, dataloader['train'], criterion, optimizer, scheduler)\n    accuracy = evaluate(pruned_model, dataloader['test'])\n    is_best = accuracy &gt; best_accuracy\n    if is_best:\n        best_accuracy = accuracy\n    print(f'Epoch {epoch+1} Accuracy {accuracy:.2f}% / Best Accuracy: {best_accuracy:.2f}%')\n\n\n\n\n\n\n\nEpoch 1 Accuracy 91.66% / Best Accuracy: 91.66%\nEpoch 2 Accuracy 92.10% / Best Accuracy: 92.10%\nEpoch 3 Accuracy 92.01% / Best Accuracy: 92.10%\nEpoch 4 Accuracy 92.18% / Best Accuracy: 92.18%\nEpoch 5 Accuracy 92.16% / Best Accuracy: 92.18%"
  },
  {
    "objectID": "posts/labs/lab01.html#measure-acceleration-from-pruning",
    "href": "posts/labs/lab01.html#measure-acceleration-from-pruning",
    "title": "👩‍💻 Lab 1",
    "section": "Measure acceleration from pruning",
    "text": "Measure acceleration from pruning\nfine-tuning이 끝나면 모델은 정확도를 거의 회복합니다. channel pruning는 fine-grained pruning에 비해 일반적으로 정확도를 회복하기가 더 어렵다는 것을 이미 알고 계실 수도 있습니다. 그러나 specialized model format이 없으면 직접적으로 모델 크기가 작아지고 계산이 작아집니다. GPU에서도 더 빠르게 실행될 수 있습니다.\n이제 pruned model의 모델 크기, 계산 및 지연 시간을 비교해봅시다.\n\n# helper functions to measure latency of a regular PyTorch models.\n#   Unlike fine-grained pruning, channel pruning\n#   can directly leads to model size reduction and speed up.\n@torch.no_grad()\ndef measure_latency(model, dummy_input, n_warmup=20, n_test=100):\n    model.eval()\n    # warmup\n    for _ in range(n_warmup):\n        _ = model(dummy_input)\n    # real test\n    t1 = time.time()\n    for _ in range(n_test):\n        _ = model(dummy_input)\n    t2 = time.time()\n    return (t2 - t1) / n_test  # average latency\n\ntable_template = \"{:&lt;15} {:&lt;15} {:&lt;15} {:&lt;15}\"\nprint (table_template.format('', 'Original','Pruned','Reduction Ratio'))\n\n# 1. measure the latency of the original model and the pruned model on CPU\n#   which simulates inference on an edge device\ndummy_input = torch.randn(1, 3, 32, 32).to('cpu')\npruned_model = pruned_model.to('cpu')\nmodel = model.to('cpu')\n\npruned_latency = measure_latency(pruned_model, dummy_input)\noriginal_latency = measure_latency(model, dummy_input)\nprint(table_template.format('Latency (ms)',\n                            round(original_latency * 1000, 1),\n                            round(pruned_latency * 1000, 1),\n                            round(original_latency / pruned_latency, 1)))\n\n# 2. measure the computation (MACs)\noriginal_macs = get_model_macs(model, dummy_input)\npruned_macs = get_model_macs(pruned_model, dummy_input)\nprint(table_template.format('MACs (M)',\n                            round(original_macs / 1e6),\n                            round(pruned_macs / 1e6),\n                            round(original_macs / pruned_macs, 1)))\n\n# 3. measure the model size (params)\noriginal_param = get_num_parameters(model)\npruned_param = get_num_parameters(pruned_model)\nprint(table_template.format('Param (M)',\n                            round(original_param / 1e6, 2),\n                            round(pruned_param / 1e6, 2),\n                            round(original_param / pruned_param, 1)))\n\n# put model back to cuda\npruned_model = pruned_model.to('cuda')\nmodel = model.to('cuda')\n\n                Original        Pruned          Reduction Ratio\nLatency (ms)    24.2            13.0            1.9            \nMACs (M)        606             305             2.0            \nParam (M)       9.23            5.01            1.8            \n\n\n\nQuestion 8 (10 pts)\n이전 코드셀의 정보를 이용하여 다음 질문에 답변해 주시기 바랍니다.\n\nQuestion 8.1 (5 pts)\n30%의 채널을 제거하면 대략 50%의 계산 절감 효과가 발생하는 이유를 설명하세요.\nYour Answer:\nMAC은 2배, Param은 1.8배 줄어들었지만, latency는 1.7배만 더 빨라졌다 메모리 관련된 이유라고 추정됨\n\n\nQuestion 8.2 (5 pts)\n지연 시간 감소 비율(latency reduction ratio)이 계산 감소(computation reduction)보다 약간 작은 이유를 설명하세요.\nYour Answer:\n0.7^2 = 0.49 니까, 얼추 2배. 파라미터 수가 줄어들수록 latency는 quadratic하게 줄어든다"
  },
  {
    "objectID": "posts/labs/lab01.html#question-9-10-pts",
    "href": "posts/labs/lab01.html#question-9-10-pts",
    "title": "👩‍💻 Lab 1",
    "section": "Question 9 (10 pts)",
    "text": "Question 9 (10 pts)\n이번 랩에서 모든 실험을 한 후에는 fine-grained pruning와 channel pruning에 익숙해질 수 있습니다.\nlecture와 이번 lab에서 배운 내용을 활용하여 다음 질문에 답변해 주시기 바랍니다.\n\nQuestion 9.1 (5 pts)\nfine-grained pruning와 channel pruning의 장단점은 무엇입니까?\ncompression ratio, accuracy, latency, hardware support(i.e., 전문 하드웨어 가속기 필요) 등의 관점에서 논의할 수 있습니다.\nYour Answer:\n\nfine-grained\n\n\n장점\n\n정확도가 높음\nUsually larger compression ratio since we can flexibly find “redundant” weights\n\n단점\n\ncpu overhead\nmemory overhead\nhardware support 필요(eie…)\n\n\n\nchannel pruning\n\n\n장점\n\n빠른 inference\n\n단점\n\nsmaller compression ratio\n\n\n\n\nQuestion 9.2 (5 pts)\n스마트폰에서 모델을 더 빨리 실행시키고 싶다면, 어떤 가지치기 방법을 사용할 것인가요? 그 이유는 무엇인가요?\nYour Answer:\n특별한 하드웨어 서포트가 필요하지않고, inference time이 빠른 channel pruning."
  },
  {
    "objectID": "posts/labs/lab03.html",
    "href": "posts/labs/lab03.html",
    "title": "👩‍💻 Lab 3",
    "section": "",
    "text": "이번 시간은 Neural Architecture Search(NAS)에서 실습을 해보는 시간이였어요. 파라미터를 가지고 네트워크를 더 깊게 만들거나, 채널을 더 크게 만드는 방법에 대해 실제로 코드 예시가 친절하게 돼 있어, 실험결과를 자세히 보기 좋았던 예제입니다. 영어로 된 설명은 NAS 강의에 나오는 자료라 꼭 읽으실 필요는 없어요. 그리고 중간중간에 이해를 돕기 위한 다이어그램이나 설명이 Getting Started 부분에 있어서 참고하시면 좋을 것 같아요.\n그럼 시작해보시죠!"
  },
  {
    "objectID": "posts/labs/lab03.html#introduction",
    "href": "posts/labs/lab03.html#introduction",
    "title": "👩‍💻 Lab 3",
    "section": "Introduction",
    "text": "Introduction\n처음에는 여러 연구들과 연구에 해당하는 모델을 언급합니다. 저희가 오늘 실습할 모델은 Once for All(OFA) MCUNet 이니 참고해주세요.\n\n\n오늘 OFA MCUNet에서는 아래 그림처럼 이미 훈련한 모델을 가지고 채널 수를 줄이거나, 레이어의 수를 조절하는 것과 같이 파라미터를 조정한 “subset”들을 가지고 메모리와 연산속도(MAC)를 평가할 겁니다. 그리고 저희가 원하는 MAC과 Peak Memory를 가진 모델을 찾아보는 것이죠.\n\n어떻게 Constraint에 맞는 모델을 찾을 것이냐하면, 바로 Accuracy Predictor 모델을 만드는 겁니다. 모델구조와 Accuracy에 대한 데이터를 OFA MCUNet에서 모은 다음, 그 데이터를 가지고 모델을 만듭니다. 그 모델에 모델 파라미터를 넣으면 모델 정확도가 나오는, 그런 모델이 되는거죠. 마지막으로 Accuracy Predictor를 가지고 임의로 모델 파라미터에 대한 샘플을 모아 원하는 Constraint에 맞는 모델을 찾는 겁니다. 강의에서 NAS를 소개하는 목적은 “큰 모델에서 작은 디바이스에 넣기 위해서 Sub-Network를 원하는 스펙에 맞게 찾아 넣는다.” 인 겁니다. 그런 작은 디바이스에 대한 예시로 MCU, Alexa, Google Home을 아래 그림처럼 보여주죠.\n\nBut the tight memory budget (50,000x smaller than GPUs) makes deep learning deployment difficult.\n\nThere are 2 main sections: accuracy & efficiency predictors and architecture search.\n\nFor predictors, there are 4 questions in total. There is one question (5 pts) in the Getting Started section and the other three questions (30 pts) are in the Predictors section.\nFor architecture search, there are 6 questions in total.\n\n이제 각설하고 하나씩 실험해볼게요! 패키지는 아래와 같이 설치하시면 됩니다.\nFirst, install the required packages and download the Visual Wake Words dataset that will be used in this lab.\n\n# print(\"Cleanning up workspace ...\")\n# # !rm -rf *\n# print(\"Installing graphviz ...\")\n# # !sudo apt-get install graphviz 1&gt;/adev/null\n# print(\"Downloading MCUNet codebase ...\")\n# !wget https://www.dropbox.com/s/3y2n2u3mfxczwcb/mcunetv2-dev-main.zip?dl=0 &gt;/dev/null\n# !unzip mcunetv2-dev-main.zip* 1&gt;/dev/null\n# !mv mcunetv2-dev-main/* . 1&gt;/dev/null\n# print(\"Downloading VWW dataset ...\")\n# !wget https://www.dropbox.com/s/169okcuuv64d4nn/data.zip?dl=0 &gt;/dev/null\n# print(\"Unzipping VWW dataset ...\")\n# !unzip data.zip* 1&gt;/dev/null\n# print(\"Installing thop and onnx ...\")\n# !pip install thop 1&gt;/dev/null\n# !pip install onnx 1&gt;/dev/null\n\n\nimport argparse\nimport json\nfrom PIL import Image\nfrom tqdm import tqdm\nimport copy\nimport math\nimport numpy as np\nimport os\nimport random\nimport torch\nfrom torch import nn\nfrom torchvision import datasets, transforms\nfrom mcunet.tinynas.search.accuracy_predictor import (\n    AccuracyDataset,\n    MCUNetArchEncoder,\n)\n\nfrom mcunet.tinynas.elastic_nn.networks.ofa_mcunets import OFAMCUNets\nfrom mcunet.utils.mcunet_eval_helper import calib_bn, validate\nfrom mcunet.utils.arch_visualization_helper import draw_arch\n\n\n%matplotlib inline\nfrom matplotlib import pyplot as plt\nimport warnings\nwarnings.filterwarnings('ignore')"
  },
  {
    "objectID": "posts/labs/lab03.html#getting-started-super-network-and-the-vww-dataset",
    "href": "posts/labs/lab03.html#getting-started-super-network-and-the-vww-dataset",
    "title": "👩‍💻 Lab 3",
    "section": "Getting Started: Super Network and the VWW dataset",
    "text": "Getting Started: Super Network and the VWW dataset\n실험에서는 이미 훈련한 MCUNetV2 super network 를 가져옵니다. 데이터셋을 가져오는 부분, OFA MCUNet 클래스를 가져오는 부분, 그리고 Sub-network를 가져와 모델 구조를 시각화하고 저희가 원하는 Constraint(메모리, 연산속도)에 맞는 모델을 찾는 코드 예제입니다.\n\nMCUNetV2 is a family of efficiency neural networks tailored for resource-constrained microntrollers. It utilizes patch-based inference, receptive field redistribution and system-NN co-design and greatly improves the accuracy-efficiency tradeoff of MCUNet.\n\n\ndef build_val_data_loader(data_dir, resolution, batch_size=128, split=0):\n    # split = 0: real val set, split = 1: holdout validation set\n    assert split in [0, 1]\n    normalize = transforms.Normalize(mean=[0.5, 0.5, 0.5], std=[0.5, 0.5, 0.5])\n    kwargs = {\"num_workers\": min(8, os.cpu_count()), \"pin_memory\": False}\n\n    val_transform = transforms.Compose(\n        [\n            transforms.Resize(\n                (resolution, resolution)\n            ),  # if center crop, the person might be excluded\n            transforms.ToTensor(),\n            normalize,\n        ]\n    )\n    val_dataset = datasets.ImageFolder(data_dir, transform=val_transform)\n\n    val_dataset = torch.utils.data.Subset(\n        val_dataset, list(range(len(val_dataset)))[split::2]\n    )\n        \n    val_loader = torch.utils.data.DataLoader(\n        val_dataset, batch_size=batch_size, shuffle=False, **kwargs\n    )\n    return val_loader\n\n\ndata_dir = \"data/vww-s256/val\"\n\nval_data_loader = build_val_data_loader(data_dir, resolution=128, batch_size=1)\n\nvis_x, vis_y = 2, 3\nfig, axs = plt.subplots(vis_x, vis_y)\n\nnum_images = 0\nfor data, label in val_data_loader:\n    img = np.array((((data + 1) / 2) * 255).numpy(), dtype=np.uint8)\n    img = img[0].transpose(1, 2, 0)\n    if label.item() == 0:\n        label_text = \"No person\"\n    else:\n        label_text = \"Person\"\n    axs[num_images // vis_y][num_images % vis_y].imshow(img)\n    axs[num_images // vis_y][num_images % vis_y].set_title(f\"Label: {label_text}\")\n    axs[num_images // vis_y][num_images % vis_y].set_xticks([])\n    axs[num_images // vis_y][num_images % vis_y].set_yticks([])\n    num_images += 1\n    if num_images &gt; vis_x * vis_y - 1:\n        break\n\nplt.show()\n\n\n\n\n\n\n\n\n여기서 OFA MCUNet의 Design Space가 \\(&gt;10^{19}\\) 나 된다고 하네요. 어마어마하죠? Subnet은 inverted MobileNet blocks로 구성돼 있으면서 모델 구조를 바꾸는 파라미터로는 kernel sizes (3, 5, 7), expand ratios (3, 4, 6), depth, global channel scaling (0.5x, 0.75x, 1.0x) (specified by width_mult_list) 가 있습니다. 자세한 설명은 이따가 계속할게요.\n\ndevice = \"cuda:0\"\nofa_network = OFAMCUNets(\n    n_classes=2,\n    bn_param=(0.1, 1e-3),\n    dropout_rate=0.0,\n    base_stage_width=\"mcunet384\",\n    width_mult_list=[0.5, 0.75, 1.0],\n    ks_list=[3, 5, 7],\n    expand_ratio_list=[3, 4, 6],\n    depth_list=[0, 1, 2],\n    base_depth=[1, 2, 2, 2, 2],\n    fuse_blk1=True,\n    se_stages=[False, [False, True, True, True], True, True, True, False],\n)\n\nofa_network.load_state_dict(\n    torch.load(\"vww_supernet.pth\", map_location=\"cpu\")[\"state_dict\"], strict=True\n)\n\nofa_network = ofa_network.to(device)\n\n\nfrom mcunet.utils.pytorch_utils import count_peak_activation_size, count_net_flops, count_parameters\n\ndef evaluate_sub_network(ofa_network, cfg, image_size=None):\n    if \"image_size\" in cfg:\n        image_size = cfg[\"image_size\"]\n    batch_size = 128\n    # step 1. sample the active subnet with the given config.\n    ofa_network.set_active_subnet(**cfg)\n    # step 2. extract the subnet with corresponding weights.\n    subnet = ofa_network.get_active_subnet().to(device)\n    # step 3. calculate the efficiency stats of the subnet.\n    peak_memory = count_peak_activation_size(subnet, (1, 3, image_size, image_size))\n    macs = count_net_flops(subnet, (1, 3, image_size, image_size))\n    params = count_parameters(subnet)\n    # step 4. perform BN parameter re-calibration.\n    calib_bn(subnet, data_dir, batch_size, image_size)\n    # step 5. define the validation dataloader.\n    val_loader = build_val_data_loader(data_dir, image_size, batch_size)\n    # step 6. validate the accuracy.\n    acc = validate(subnet, val_loader)\n    return acc, peak_memory, macs, params\n\nWe also provide a handly helper function to visualize the architecture of the subnets. The function takes in the configuration of the subnet and returns an image representing the architecture.\n\ndef visualize_subnet(cfg):\n    draw_arch(cfg[\"ks\"], cfg[\"e\"], cfg[\"d\"], cfg[\"image_size\"], out_name=\"viz/subnet\")\n    im = Image.open(\"viz/subnet.png\")\n    im = im.rotate(90, expand=1)\n    fig = plt.figure(figsize=(im.size[0] / 250, im.size[1] / 250))\n    plt.axis(\"off\")\n    plt.imshow(im)\n    plt.show()\n\n위 코드를 이용해서 모델구조 시각화도 할텐데요, MBConv3-3x3과 같은 이름이 나올거에요. 각각 expand ratio e와 kernel size of the depthwise convolution layer k로 MBConv{e}-{k}x{k}가 나타나니 참고하시면 좋을 것 같아요.\n\nMore Explanation to understand OFA-MCUNet\n과제를 들어가기 앞서 OFA-MCUNet에 대해서 조금 설명할까해요. 내려가면서 모델 구조에 파라미터들이 나오는 데 각 의미를 알면 이해하기가 더 수월할 겁니다.\n모델은 총 first_conv, blocks, feature_mix_layer, classifier 으로 구성해요. block에서도 첫 번째, 마지막 block을 제외한 총 6개의 block에서 kernel size, expand ratio, depth, width multiply를 파라미터로 해서 모델을 키우거나, 줄이죠. 각각의 파라미터를 좀 더 살펴보죠!\n\n1. Kernel size\nkernel size는 Convolution에 나오는 그 kernel이 맞습니다. 예제에서는 3x3, 5x5, 7x7로 가질 수 있어요.\n\n\n2. Width multiply, Depth\nOFA MCUNet을 블럭으로 표현하면 아래와 같죠. 그중 초록색으로 칠해진 Block을 보시면, Block으로 들어오는 Input Channel과 Output Channel이 있어요. 바로 그 둘을 얼마나 줄일 것인가, 유지할 것인가가 Width multiply입니다.\n두 번째로 하나의 Block은 MBConv(MobileNet Conv)로 구성됩니다. 그러면 이 MBConv가 몇 개가 들어 갈 것이냐가 관건일 텐데요, 이걸 정하는 것이 Depth입니다. 파라미터에서는 depth_list와 base_depth로 나눠서 각 block별로 base_depth를 기준으로 depth_list에 나오는 개수 만큼 더 MBConv이 추가되죠.\n\n마지막은 expand ratio 입니다. 이 파라미터는 MBConv 내에서 있어요, 역시나 그림을 보시죠. MBConv는 MobileNet Convolution, Separable Convolution,\nSE-Block, 그리고 다시 MobileNet Convolution으로 구성되요. 그 중, 처음 입력의 채널과 첫 MobileNet Convolution을 거치고 나온 출력 채널의 비를 Expand ratio라고 합니다.\n\n\n\n-\n\n\n\n# OFAMCUNets\n# constitutes: first_conv, blocks, feature_mix_layer, classifier\n# total 9 block (first_conv, first block, blocks, last block)\n# 1. first_conv = 1x1 channel inc conv (3 -&gt; X)\n# 2. first block = MB InvertedConvLayer\n\n# 3. blocks\n# - depth = num block\n# - 1 block = MobileInvertedResidualBlock = MBConvLayer + Residual\n#############################################################\n# Dynamic MBConvLayer = 2 times channel expansion           #\n#                  fuse_blk1    se_stage                    #\n# MBConvLayer + SeparableConv + SEBlock + MBConvLayer       #\n#############################################################\n# SEblock: conv 1x1 (reduce) -&gt; act -&gt; conv 1x1 (expand) -&gt; h_sigmoid\n# -&gt; SENet(Squeeze-and-Excitation Network)\n\n# 4. Last block = Mobile Inverted Residual Blcok\n# 5. feature_mix_layer = 1x1 channel dec conv\n# 6. classifier = linear layer\n\n# Parameters (sample_active_subnet)\n# kernel size, expand ratio, depth, width multiplY\n\n코드 중 make_divisible이라는 메서드가 있습니다. 여기선 채널을 늘리거나 줄일 때 8로 나눌 수 있게 합니다. tensorflow에서도 사용한다고 하는데, 이유는 아직 모르겠네요!\ndef make_divisible(v, divisor, min_val=None):\n    \"\"\"\n    This function is taken from the original tf repo.\n    It ensures that all layers have a channel number that is divisible by 8\n    It can be seen here:\n    https://github.com/tensorflow/models/blob/master/research/slim/nets/mobilenet/mobilenet.py\n    :param v:\n    :param divisor:\n    :param min_val:\n    :return:\n    \"\"\"\n    if min_val is None:\n        min_val = divisor\n    new_v = max(min_val, int(v + divisor / 2) // divisor * divisor)\n    # Make sure that round down does not go down by more than 10%.\n    if new_v &lt; 0.9 * v:\n        new_v += divisor\n    return new_v\n\n\n\nTL;DR. Summary\n실험은 총 4 단계로 나뉩니다. 여기서 파라미터는 kernel size, expand ratio, depth, width multiply죠.\n\n1. OFA-MCUNet\n처음은 훈련된 vww_supernet을 가지고 파라미터마다 accuracy 조합을 구합니다. 그리고 각 결과마다 이후에 constraint 범위 내에 들어오는 모델구조를 찾기 위해 MAC과 Peak memory 또한 구할 겁니다.\n\n\n2. Accuracy Predictor\n앞서서 구한 파라미터마다 Accuracy를 가지고, 이번엔 반대로 이 데이터를 가지고 Accuracy를 예측하는 모델을 훈련시킬겁니다. 모델은 Linear Layer가 세층으로 쌓여있는 간단한 모델을 사용하죠. 하지만 파타미터 조합을 Embedding vector로 만들기 위해 encoder가 들어갑니다.\n\n\n3. Encoding: MCUNetArchEncoder\n그 과정에서 파라미터 조합을 Embedding vector로 Encoding을 합니다. 예를 들어, Kenral size가 3x3, 5x5, 7x7 이 있는 경우 각각을 (0, 0, 1), (0, 1, 0), (1, 0, 0) 이렇게 encoding 하는 거죠. 이 encoding이 들어간 Accuracy Predictor 모델을 훈련시킵니다. 훈련시킨 모델의 Prediction과 Label 간의 상관관계가 Linear하게 나오는 것 또한 보여줄 겁니다.\n\n\n4. Random Search and Evolutionary Search\n마지막 단계는 Constraint, 즉 메모리와 MAC에 해당하는 모델의 파라미터를 찾는 단계입니다. Random Mutate 방식과 Crossover 방식을 사용하는데, 자세한 내용은 코드를 참고하시는게 이해하기 더 수월할 겁니다! 참고로 마지막에 Question 10에서 “The activation size of the subnet is at most 64 KB” 의 조건을 가진 모델의 구조는 못찾았습니다. 혹시 찾게 된다면, 혹은 찾지 못하는 이유를 아시게된다면 공유해주세요!\n\n\n\nOFA_network’s forward\n모델 실험하기에 앞서서, 채널을 만약 줄인다면 어떤식으로 할지 Convolution Network에서 나온 코드를 가져와봤어요. 파라미터에 맞게 결정한 out_channel, in_channel을 아래 코드 처럼 잘라 active subnet이라고 부를 거에요. 실험은 제가 임의로 이미지 사이즈를 48, 96, 128, 256, 384, 512로 키워나가면서 했고, sub network로 샘플링하는 방법으로는 random, max, min으로 했습니다.\nfilters = self.conv.weight[:out_channel, :in_channel, :, :].contiguous()\npadding = get_same_padding(self.kernel_size)\ny = F.conv2d(x, filters, None, self.stride, padding, self.dilation, 1)\n흥미로웠던 건 이미지가 커지면 커질수록 Accuracy는 계속 올라가다가 512에서 부터 떨어지더라구요. 실험결과는 아래를 참고바랍니다.\n\n# sample_active_subnet\n# kernel size, expand ratio, depth, width mult\n\nimage_size = 48\n\ncfg = ofa_network.sample_active_subnet(sample_function=random.choice, image_size=image_size)\nacc, _, _, params = evaluate_sub_network(ofa_network, cfg)\nvisualize_subnet(cfg)\nprint(f\"The accuracy of the sampled subnet: #params={params/1e6: .1f}M, accuracy={acc: .1f}%.\")\n\nlargest_cfg = ofa_network.sample_active_subnet(sample_function=max, image_size=image_size)\nacc, _, _, params = evaluate_sub_network(ofa_network, largest_cfg)\nvisualize_subnet(largest_cfg)\nprint(f\"The largest subnet: #params={params/1e6: .1f}M, accuracy={acc: .1f}%.\")\n\nsmallest_cfg = ofa_network.sample_active_subnet(sample_function=min, image_size=image_size)\nacc, peak_memory, macs, params = evaluate_sub_network(ofa_network, smallest_cfg)\nvisualize_subnet(smallest_cfg)\nprint(f\"The smallest subnet: #params={params/1e6: .1f}M, accuracy={acc: .1f}%.\")\n\nValidate: 100%|██████████| 32/32 [00:00&lt;00:00, 51.09it/s, loss=0.603, top1=65.9]\nValidate: 100%|██████████| 32/32 [00:00&lt;00:00, 53.97it/s, loss=0.625, top1=64.2]\nValidate: 100%|██████████| 32/32 [00:00&lt;00:00, 51.76it/s, loss=0.718, top1=59.3]\n\n\n\n\n\n\n\n\n\nThe accuracy of the sampled subnet: #params= 1.6M, accuracy= 65.9%.\nThe largest subnet: #params= 2.5M, accuracy= 64.2%.\nThe smallest subnet: #params= 0.3M, accuracy= 59.3%.\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nimage_size = 96\n\ncfg = ofa_network.sample_active_subnet(sample_function=random.choice, image_size=image_size)\nacc, _, _, params = evaluate_sub_network(ofa_network, cfg)\nvisualize_subnet(cfg)\nprint(f\"The accuracy of the sampled subnet: #params={params/1e6: .1f}M, accuracy={acc: .1f}%.\")\n\nlargest_cfg = ofa_network.sample_active_subnet(sample_function=max, image_size=image_size)\nacc, _, _, params = evaluate_sub_network(ofa_network, largest_cfg)\nvisualize_subnet(largest_cfg)\nprint(f\"The largest subnet: #params={params/1e6: .1f}M, accuracy={acc: .1f}%.\")\n\nsmallest_cfg = ofa_network.sample_active_subnet(sample_function=min, image_size=image_size)\nacc, peak_memory, macs, params = evaluate_sub_network(ofa_network, smallest_cfg)\nvisualize_subnet(smallest_cfg)\nprint(f\"The smallest subnet: #params={params/1e6: .1f}M, accuracy={acc: .1f}%.\")\n\nValidate: 100%|██████████| 32/32 [00:00&lt;00:00, 35.68it/s, loss=0.321, top1=86.4]\nValidate: 100%|██████████| 32/32 [00:00&lt;00:00, 42.76it/s, loss=0.29, top1=88.6] \nValidate: 100%|██████████| 32/32 [00:00&lt;00:00, 44.92it/s, loss=0.379, top1=83.4]\n\n\n\n\n\n\n\n\n\nThe accuracy of the sampled subnet: #params= 0.6M, accuracy= 86.4%.\nThe largest subnet: #params= 2.5M, accuracy= 88.6%.\nThe smallest subnet: #params= 0.3M, accuracy= 83.4%.\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nimage_size = 128\n\n# sample_active_subnet\n# kernel size, expand ratio, depth, width mult\n\ncfg = ofa_network.sample_active_subnet(sample_function=random.choice, image_size=image_size)\nacc, _, _, params = evaluate_sub_network(ofa_network, cfg)\nvisualize_subnet(cfg)\nprint(f\"The accuracy of the sampled subnet: #params={params/1e6: .1f}M, accuracy={acc: .1f}%.\")\n\nlargest_cfg = ofa_network.sample_active_subnet(sample_function=max, image_size=image_size)\nacc, _, _, params = evaluate_sub_network(ofa_network, largest_cfg)\nvisualize_subnet(largest_cfg)\nprint(f\"The largest subnet: #params={params/1e6: .1f}M, accuracy={acc: .1f}%.\")\n\nsmallest_cfg = ofa_network.sample_active_subnet(sample_function=min, image_size=image_size)\nacc, peak_memory, macs, params = evaluate_sub_network(ofa_network, smallest_cfg)\nvisualize_subnet(smallest_cfg)\nprint(f\"The smallest subnet: #params={params/1e6: .1f}M, accuracy={acc: .1f}%.\")\n\nValidate: 100%|██████████| 32/32 [00:00&lt;00:00, 39.53it/s, loss=0.228, top1=91.3]\nValidate: 100%|██████████| 32/32 [00:01&lt;00:00, 30.92it/s, loss=0.21, top1=92.3] \nValidate: 100%|██████████| 32/32 [00:00&lt;00:00, 40.69it/s, loss=0.307, top1=87.3]\n\n\n\n\n\n\n\n\n\nThe accuracy of the sampled subnet: #params= 1.3M, accuracy= 91.3%.\nThe largest subnet: #params= 2.5M, accuracy= 92.3%.\nThe smallest subnet: #params= 0.3M, accuracy= 87.3%.\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nimage_size = 256\n\n# sample_active_subnet\n# kernel size, expand ratio, depth, width mult\n\ncfg = ofa_network.sample_active_subnet(sample_function=random.choice, image_size=image_size)\nacc, _, _, params = evaluate_sub_network(ofa_network, cfg)\nvisualize_subnet(cfg)\nprint(f\"The accuracy of the sampled subnet: #params={params/1e6: .1f}M, accuracy={acc: .1f}%.\")\n\nlargest_cfg = ofa_network.sample_active_subnet(sample_function=max, image_size=image_size)\nacc, _, _, params = evaluate_sub_network(ofa_network, largest_cfg)\nvisualize_subnet(largest_cfg)\nprint(f\"The largest subnet: #params={params/1e6: .1f}M, accuracy={acc: .1f}%.\")\n\nsmallest_cfg = ofa_network.sample_active_subnet(sample_function=min, image_size=image_size)\nacc, peak_memory, macs, params = evaluate_sub_network(ofa_network, smallest_cfg)\nvisualize_subnet(smallest_cfg)\nprint(f\"The smallest subnet: #params={params/1e6: .1f}M, accuracy={acc: .1f}%.\")\n\nValidate: 100%|██████████| 32/32 [00:01&lt;00:00, 19.93it/s, loss=0.187, top1=93.5]\nValidate: 100%|██████████| 32/32 [00:03&lt;00:00, 10.12it/s, loss=0.177, top1=93.9] \nValidate: 100%|██████████| 32/32 [00:01&lt;00:00, 25.67it/s, loss=0.258, top1=90.2]\n\n\n\n\n\n\n\n\n\nThe accuracy of the sampled subnet: #params= 0.6M, accuracy= 93.5%.\nThe largest subnet: #params= 2.5M, accuracy= 93.9%.\nThe smallest subnet: #params= 0.3M, accuracy= 90.2%.\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nimage_size = 256+128\n\n# sample_active_subnet\n# kernel size, expand ratio, depth, width mult\n\ncfg = ofa_network.sample_active_subnet(sample_function=random.choice, image_size=image_size)\nacc, _, _, params = evaluate_sub_network(ofa_network, cfg)\nvisualize_subnet(cfg)\nprint(f\"The accuracy of the sampled subnet: #params={params/1e6: .1f}M, accuracy={acc: .1f}%.\")\n\nlargest_cfg = ofa_network.sample_active_subnet(sample_function=max, image_size=image_size)\nacc, _, _, params = evaluate_sub_network(ofa_network, largest_cfg)\nvisualize_subnet(largest_cfg)\nprint(f\"The largest subnet: #params={params/1e6: .1f}M, accuracy={acc: .1f}%.\")\n\nsmallest_cfg = ofa_network.sample_active_subnet(sample_function=min, image_size=image_size)\nacc, peak_memory, macs, params = evaluate_sub_network(ofa_network, smallest_cfg)\nvisualize_subnet(smallest_cfg)\nprint(f\"The smallest subnet: #params={params/1e6: .1f}M, accuracy={acc: .1f}%.\")\n\nValidate: 100%|██████████| 32/32 [00:03&lt;00:00,  8.16it/s, loss=0.241, top1=91.1] \nValidate: 100%|██████████| 32/32 [00:06&lt;00:00,  4.60it/s, loss=0.263, top1=90.5] \nValidate: 100%|██████████| 32/32 [00:02&lt;00:00, 12.13it/s, loss=0.34, top1=85.4] \n\n\n\n\n\n\n\n\n\nThe accuracy of the sampled subnet: #params= 1.1M, accuracy= 91.1%.\nThe largest subnet: #params= 2.5M, accuracy= 90.5%.\nThe smallest subnet: #params= 0.3M, accuracy= 85.4%.\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nimage_size = 512\n\n# sample_active_subnet\n# kernel size, expand ratio, depth, width mult\n\ncfg = ofa_network.sample_active_subnet(sample_function=random.choice, image_size=image_size)\nacc, _, _, params = evaluate_sub_network(ofa_network, cfg)\nvisualize_subnet(cfg)\nprint(f\"The accuracy of the sampled subnet: #params={params/1e6: .1f}M, accuracy={acc: .1f}%.\")\n\nlargest_cfg = ofa_network.sample_active_subnet(sample_function=max, image_size=image_size)\nacc, _, _, params = evaluate_sub_network(ofa_network, largest_cfg)\nvisualize_subnet(largest_cfg)\nprint(f\"The largest subnet: #params={params/1e6: .1f}M, accuracy={acc: .1f}%.\")\n\nsmallest_cfg = ofa_network.sample_active_subnet(sample_function=min, image_size=image_size)\nacc, peak_memory, macs, params = evaluate_sub_network(ofa_network, smallest_cfg)\nvisualize_subnet(smallest_cfg)\nprint(f\"The smallest subnet: #params={params/1e6: .1f}M, accuracy={acc: .1f}%.\")\n\nValidate: 100%|██████████| 32/32 [00:06&lt;00:00,  5.31it/s, loss=0.376, top1=83.1]\nValidate: 100%|██████████| 32/32 [00:11&lt;00:00,  2.67it/s, loss=0.413, top1=81]   \nValidate: 100%|██████████| 32/32 [00:04&lt;00:00,  7.23it/s, loss=0.489, top1=76.1]\n\n\n\n\n\n\n\n\n\nThe accuracy of the sampled subnet: #params= 0.5M, accuracy= 83.1%.\nThe largest subnet: #params= 2.5M, accuracy= 81.0%.\nThe smallest subnet: #params= 0.3M, accuracy= 76.1%.\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nQuestion 1: Design space exploration.\nTry manually sample different subnets by running the cell above multiple times. You can also vary the input resolution. Talk about your findings.\nHint: which dimension plays the most important role for the accuracy?\nAnswer: Image resolution plays the most important role for classification accuracy.\n네, 질문에서 사실 힌트를 얻어 실험을 했습니다. “Image resolution에 따른 Accuracy 변화”를 알 수 있었습니다."
  },
  {
    "objectID": "posts/labs/lab03.html#part-1.-predictors",
    "href": "posts/labs/lab03.html#part-1.-predictors",
    "title": "👩‍💻 Lab 3",
    "section": "Part 1. Predictors",
    "text": "Part 1. Predictors\n이제 두번째 단계는 앞서서 모델을 통해 얻은 VWW dataset으로 Accuracy를 예측하는 모델을 만들겁니다. 모델은 생각모다 간단해요, Linear 세 층으로 구성돼 있죠. 아래 그램은 궁극적으로 Constraint에 해당하는 모델을 우리는 구할거다, 이런 내용입니다.\n\nEfficiency predictor는 모델 구조가 결정되면 Accuracy와 함께 나올거에요. 앞선 예제에서 했으니 기억이 안나신다면 이전 예제로!\n\nQuestion 2: Implement the efficiency predictor.\n처음은 “AnalyticalEfficiencyPredictor”라는 클래스를 만듭니다. 이미지 크기에 따라 MAC과 메모리를 계산해주고(get_efficiency), 이 두가지가 타겟하고 부합하는지도 알려주는 함수(satisfy_constraint)도 만듭니다. FLOP과 메모리 계산은 교수님이 친절하게 만들어 놓으신 count_net_flops과 count_peak_activation_size를 사용하면 됩니다.\n\nclass AnalyticalEfficiencyPredictor:\n    def __init__(self, net):\n        self.net = net\n\n    def get_efficiency(self, spec: dict):\n        self.net.set_active_subnet(**spec)\n        subnet = self.net.get_active_subnet()\n        if torch.cuda.is_available():\n            subnet = subnet.cuda()\n        ############### YOUR CODE STARTS HERE ###############\n        # Hint: take a look at the `evaluate_sub_network` function above.\n        # Hint: the data shape is (batch_size, input_channel, image_size, image_size)\n        data_shape = (1, 3, spec[\"image_size\"], spec[\"image_size\"])\n        macs = count_net_flops(subnet, data_shape)\n        peak_memory = count_peak_activation_size(subnet, data_shape)\n        ################ YOUR CODE ENDS HERE ################\n\n        return dict(millionMACs=macs / 1e6, KBPeakMemory=peak_memory / 1024)\n\n    def satisfy_constraint(self, measured: dict, target: dict):\n        for key in measured:\n            # if the constraint is not specified, we just continue\n            if key not in target:\n                continue\n            # if we exceed the constraint, just return false.\n            if measured[key] &gt; target[key]:\n                return False\n        # no constraint violated, return true.\n        return True\n\nLet’s test your implementation for the analytical efficiency predictor by examining the returned values for the smallest and largest subnets we just evaluated a while ago. The results from the efficiency predictor should match with the previous results.\n\nefficiency_predictor = AnalyticalEfficiencyPredictor(ofa_network)\n\nimage_size = 96\n# Print out the efficiency of the smallest subnet.\nsmallest_cfg = ofa_network.sample_active_subnet(sample_function=min, image_size=image_size)\neff_smallest = efficiency_predictor.get_efficiency(smallest_cfg)\n\n# Print out the efficiency of the largest subnet.\nlargest_cfg = ofa_network.sample_active_subnet(sample_function=max, image_size=image_size)\neff_largest = efficiency_predictor.get_efficiency(largest_cfg)\n\nprint(\"Efficiency stats of the smallest subnet:\", eff_smallest)\nprint(\"Efficiency stats of the largest subnet:\", eff_largest)\n\nEfficiency stats of the smallest subnet: {'millionMACs': 8.302128, 'KBPeakMemory': 72.0}\nEfficiency stats of the largest subnet: {'millionMACs': 79.416432, 'KBPeakMemory': 270.0}\n\n\n\n\nQuestion 3: Implement the accuracy predictor.\n이제 Accuracy predictor를 만들어야죠? 그전에, 데이터셋으로 주어진 걸 살펴보니 파라미터간 조합으로 보입니다. 이를 데이터로써 쓰기 위해 임베딩을 해야하는데 그 역할을 바로 MCUNetArchEncoder가 합니다. 역시나 교수님이 친절하게 만들어주셨군요. 그리고 Accuracy predictor 모델 구조는 MLP (multi-layer perception)를 사용할겁니다.\nThe accuracy predictor takes in the architecture of a sub-network and predicts its accuracy on the VWW dataset. Since it is an MLP network, the sub-network must be encoded into a vector. In this lab, we provide a class MCUNetArchEncoder to perform such conversion from sub-network architecture to a binary vector.\n\nimage_size_list = [96, 112, 128, 144, 160]\narch_encoder = MCUNetArchEncoder(\n    image_size_list=image_size_list,\n    base_depth=ofa_network.base_depth,\n    depth_list=ofa_network.depth_list,\n    expand_list=ofa_network.expand_ratio_list,\n    width_mult_list=ofa_network.width_mult_list,\n)\n\nWe generated an accuracy dataset beforehand, which is a collection of [architecture, accuracy] pairs stored under the acc_datasets folder.\nWith the architecture encoder, you are now required define the accuracy predictor, which is a multi-layer perception (MLP) network with 400 channels per intermediate layer. For simplicity, we fix the number of layers to be 3. Please implement this MLP network in the following cell.\n\nclass AccuracyPredictor(nn.Module):\n    def __init__(\n        self,\n        arch_encoder,\n        hidden_size=400,\n        n_layers=3,\n        checkpoint_path=None,\n        device=\"cuda:0\",\n    ):\n        super(AccuracyPredictor, self).__init__()\n        self.arch_encoder = arch_encoder\n        self.hidden_size = hidden_size\n        self.n_layers = n_layers\n        self.device = device\n\n        layers = []\n        \n        ############### YOUR CODE STARTS HERE ###############\n        # Let's build an MLP with n_layers layers. \n        # Each layer (nn.Linear) has hidden_size channels and \n        # uses nn.ReLU as the activation function.\n        # Hint: You can assume that n_layers is fixed to be 3, for simplicity.\n        # Hint: the input dimension of the first layer is not hidden_size.\n        for i in range(self.n_layers):\n            layers.append(\n                nn.Sequential(\n                    nn.Linear(\n                        self.arch_encoder.n_dim if i == 0 else self.hidden_size,\n                        self.hidden_size,\n                    ),\n                    nn.ReLU(inplace=True),\n                )\n            )\n        ################ YOUR CODE ENDS HERE ################\n        layers.append(nn.Linear(self.hidden_size, 1, bias=False))\n        self.layers = nn.Sequential(*layers)\n        self.base_acc = nn.Parameter(\n            torch.zeros(1, device=self.device), requires_grad=False\n        )\n\n        if checkpoint_path is not None and os.path.exists(checkpoint_path):\n            checkpoint = torch.load(checkpoint_path, map_location=\"cpu\")\n            if \"state_dict\" in checkpoint:\n                checkpoint = checkpoint[\"state_dict\"]\n            self.load_state_dict(checkpoint)\n            print(\"Loaded checkpoint from %s\" % checkpoint_path)\n\n        self.layers = self.layers.to(self.device)\n\n    def forward(self, x):\n        y = self.layers(x).squeeze()\n        return y + self.base_acc\n\n    def predict_acc(self, arch_dict_list):\n        X = [self.arch_encoder.arch2feature(arch_dict) for arch_dict in arch_dict_list]\n        X = torch.tensor(np.array(X)).float().to(self.device)\n        return self.forward(X)\n\nLet’s print out the architecture of the AccuracyPredictor you just defined.\n\nos.makedirs(\"pretrained\", exist_ok=True)\nacc_pred_checkpoint_path = (\n    f\"pretrained/{ofa_network.__class__.__name__}_acc_predictor.pth\"\n)\nacc_predictor = AccuracyPredictor(\n    arch_encoder,\n    hidden_size=400,\n    n_layers=3,\n    checkpoint_path=None,\n    device=device,\n)\nprint(acc_predictor)\n\nAccuracyPredictor(\n  (layers): Sequential(\n    (0): Sequential(\n      (0): Linear(in_features=128, out_features=400, bias=True)\n      (1): ReLU(inplace=True)\n    )\n    (1): Sequential(\n      (0): Linear(in_features=400, out_features=400, bias=True)\n      (1): ReLU(inplace=True)\n    )\n    (2): Sequential(\n      (0): Linear(in_features=400, out_features=400, bias=True)\n      (1): ReLU(inplace=True)\n    )\n    (3): Linear(in_features=400, out_features=1, bias=False)\n  )\n)\n\n\n데이터 셋은 총 4만개의 훈련데이터와 만개의 테스트 데이터로 있고, Accuracy는 모델 파라미터(architecture)와 쌍을 이룰거라는, 내용입니다. 하나 더, 파라미터를 one-hot representation 로 바꾸는 과정도 잊지마시죠! 다음 결과를 보시면 “kernel size embedding: [0 1 0] =&gt; kernel size: 5; expand ratio embedding: [0 1 0] =&gt; expand ratio: 4” 이러면서 모델 구조가 임베딩된 걸 확인하실 수 있어요\nLet’s first visualize some samples in the accuracy dataset in the following cell.\nThe accuracy dataset is composed of 50,000 [architecture, accuracy] pairs, where 40,000 of them are used as the training set and the rest 10,000 are used as validation set.\nFor accuracy, We calculate the average accuracy of all [architecture, accuracy] pairs on the accuracy dataset and define it as base_acc. For the accuracy predictor, instead of directly regressing the accuracy of each architecture, its training target is accuracy - base_acc. Since accuracy - base_acc is usually much smaller than accuracy itself, this can make training easier.\nFor architecture, each subnet within the design space is uniquely represented by a binary vector. The binary vector is a concatenation of the one-hot representation for both global parameters (e.g. input resolution, width multiplier) and parameters of each inverted MobileNet block (e.g. kernel sizes and expand ratios). Note that we prefer one-hot representations over numerical representations because all design hyperparameters are discrete values.\nFor example, our design space supports\nkernel_size = [3, 5, 7]\nexpand_ratio = [3, 4, 6]\nThen, we represent kernel_size=3 as [1, 0, 0], kernel_size=5 as [0, 1, 0], and kernel_size=7 as [0, 0, 1]. Similarly, for expand_ratio=3, it is written as [1, 0, 0]; expand_ratio=4 is written as [0, 1, 0] and expand_ratio=6 is written as [0, 0, 1]. The representation for each inverted MobileNet block is obtained by concatenating the kernel size embedding with the expand ratio embedding. Note that for skipped blocks, we use [0, 0, 0] to represent their kernel sizes and expand ratios. You will see a detailed explanation of the architecture-embedding correspondence after running the following cell.\n\nacc_dataset = AccuracyDataset(\"acc_datasets\")\ntrain_loader, valid_loader, base_acc = acc_dataset.build_acc_data_loader(\n    arch_encoder=arch_encoder\n)\n\nprint(f\"The basic accuracy (mean accuracy of all subnets within the dataset is: {(base_acc * 100): .1f}%.\")\n\n# Let's print one sample in the training set\nsampled = 0\nfor (data, label) in train_loader:\n    data = data.to(device)\n    label = label.to(device)\n    print(\"=\" * 100)\n    # dummy pass to print the divided encoding\n    arch_encoding = arch_encoder.feature2arch(data[0].int().cpu().numpy(), verbose=False)\n    # print out the architecture encoding process in detail\n    arch_encoding = arch_encoder.feature2arch(data[0].int().cpu().numpy(), verbose=True)\n    visualize_subnet(arch_encoding)\n    print(f\"The accuracy of this subnet on the holdout validation set is: {(label[0] * 100): .1f}%.\")\n    sampled += 1\n    if sampled == 1:\n        break\n\nLoading data: 100%|██████████| 50000/50000 [00:00&lt;00:00, 228025.66it/s]\n\n\nTrain Size: 40000, Valid Size: 10000\nThe basic accuracy (mean accuracy of all subnets within the dataset is:  90.3%.\n====================================================================================================\nnetwork embedding: [1 0 0 0 0 | 0 1 0 | 0 1 0 | 0 1 0 | 1 0 0 | 0 0 1 | 1 0 0 | 1 0 0 | 0 0 1 | 1 0 0 | 0 1 0 | 0 1 0 | 0 0 1 | 0 0 1 | 0 0 0 | 0 0 0 | 0 1 0 | 0 0 1 | 0 1 0 | 0 0 1 | 0 1 0 | 0 1 0 | 0 1 0 | 0 0 1 | 1 0 0 | 1 0 0 | 0 1 0 | 0 1 0 | 0 0 1 | 0 0 1 | 0 1 0 | 0 0 1 | 0 0 1 | 1 0 0 | 0 1 0 | 0 0 1 | 0 0 0 | 0 0 0 | 0 0 0 | 0 0 0 | 0 1 0 | 0 0 1]\nimage resolution embedding: [1 0 0 0 0] =&gt; image resolution: 96\nwidth multiplier embedding: [0 1 0] =&gt; width multiplier: 0.75\n**************************************************Stage1**************************************************\nkernel size embedding: [0 1 0] =&gt; kernel size: 5; expand ratio embedding: [0 1 0] =&gt; expand ratio: 4\nkernel size embedding: [1 0 0] =&gt; kernel size: 3; expand ratio embedding: [0 0 1] =&gt; expand ratio: 6\nkernel size embedding: [1 0 0] =&gt; kernel size: 3; expand ratio embedding: [1 0 0] =&gt; expand ratio: 3\n**************************************************Stage2**************************************************\nkernel size embedding: [0 0 1] =&gt; kernel size: 7; expand ratio embedding: [1 0 0] =&gt; expand ratio: 3\nkernel size embedding: [0 1 0] =&gt; kernel size: 5; expand ratio embedding: [0 1 0] =&gt; expand ratio: 4\nkernel size embedding: [0 0 1] =&gt; kernel size: 7; expand ratio embedding: [0 0 1] =&gt; expand ratio: 6\nkernel size embedding: [0 0 0] expand ratio embedding: [0 0 0] =&gt; layer skipped.\n**************************************************Stage3**************************************************\nkernel size embedding: [0 1 0] =&gt; kernel size: 5; expand ratio embedding: [0 0 1] =&gt; expand ratio: 6\nkernel size embedding: [0 1 0] =&gt; kernel size: 5; expand ratio embedding: [0 0 1] =&gt; expand ratio: 6\nkernel size embedding: [0 1 0] =&gt; kernel size: 5; expand ratio embedding: [0 1 0] =&gt; expand ratio: 4\nkernel size embedding: [0 1 0] =&gt; kernel size: 5; expand ratio embedding: [0 0 1] =&gt; expand ratio: 6\n**************************************************Stage4**************************************************\nkernel size embedding: [1 0 0] =&gt; kernel size: 3; expand ratio embedding: [1 0 0] =&gt; expand ratio: 3\nkernel size embedding: [0 1 0] =&gt; kernel size: 5; expand ratio embedding: [0 1 0] =&gt; expand ratio: 4\nkernel size embedding: [0 0 1] =&gt; kernel size: 7; expand ratio embedding: [0 0 1] =&gt; expand ratio: 6\nkernel size embedding: [0 1 0] =&gt; kernel size: 5; expand ratio embedding: [0 0 1] =&gt; expand ratio: 6\n**************************************************Stage5**************************************************\nkernel size embedding: [0 0 1] =&gt; kernel size: 7; expand ratio embedding: [1 0 0] =&gt; expand ratio: 3\nkernel size embedding: [0 1 0] =&gt; kernel size: 5; expand ratio embedding: [0 0 1] =&gt; expand ratio: 6\nkernel size embedding: [0 0 0] expand ratio embedding: [0 0 0] =&gt; layer skipped.\nkernel size embedding: [0 0 0] expand ratio embedding: [0 0 0] =&gt; layer skipped.\n**************************************************Stage6**************************************************\nkernel size embedding: [0 1 0] =&gt; kernel size: 5; expand ratio embedding: [0 0 1] =&gt; expand ratio: 6\nThe accuracy of this subnet on the holdout validation set is:  88.7%.\n\n\n\n\n\n\n\n\n\n\n\nQuestion 4: Complete the code for accuracy predictor training.\n훈련할 시간입니다!\n\ncriterion = torch.nn.L1Loss().to(device)\noptimizer = torch.optim.Adam(acc_predictor.parameters())\n# the default value is zero\nacc_predictor.base_acc.data += base_acc\nfor epoch in tqdm(range(10)):\n    acc_predictor.train()\n    for (data, label) in tqdm(train_loader, desc=\"Epoch%d\" % (epoch + 1), position=0, leave=True):\n        # step 1. Move the data and labels to device (cuda:0).\n        data = data.to(device)\n        label = label.to(device)\n        ############### YOUR CODE STARTS HERE ###############\n        # step 2. Run forward pass.\n        pred = acc_predictor(data)\n        # step 3. Calculate the loss.\n        loss = criterion(pred, label)\n        # step 4. Perform the backward pass.\n        optimizer.zero_grad()\n        loss.backward()\n        optimizer.step()\n        ################ YOUR CODE ENDS HERE ################\n\n    acc_predictor.eval()\n    with torch.no_grad():\n        with tqdm(total=len(valid_loader), desc=\"Val\", position=0, leave=True) as t:\n            for (data, label) in valid_loader:\n                # step 1. Move the data and labels to device (cuda:0).\n                data = data.to(device)\n                label = label.to(device)\n                ############### YOUR CODE STARTS HERE ###############\n                # step 2. Run forward pass.\n                pred = acc_predictor(data)\n                # step 3. Calculate the loss.\n                loss = criterion(pred, label)\n                ############### YOUR CODE ENDS HERE ###############\n                t.set_postfix({\"loss\": loss.item()})\n                t.update(1)\n\nif not os.path.exists(acc_pred_checkpoint_path):\n    torch.save(acc_predictor.cpu().state_dict(), acc_pred_checkpoint_path)\n\nEpoch1: 100%|██████████| 157/157 [00:00&lt;00:00, 362.86it/s]\nVal: 100%|██████████| 40/40 [00:00&lt;00:00, 109.00it/s, loss=0.00374]\nEpoch2: 100%|██████████| 157/157 [00:00&lt;00:00, 262.66it/s]\nVal: 100%|██████████| 40/40 [00:00&lt;00:00, 141.77it/s, loss=0.0026]\nEpoch3: 100%|██████████| 157/157 [00:00&lt;00:00, 241.87it/s]\nVal: 100%|██████████| 40/40 [00:00&lt;00:00, 118.13it/s, loss=0.00251]\nEpoch4: 100%|██████████| 157/157 [00:00&lt;00:00, 336.42it/s]\nVal: 100%|██████████| 40/40 [00:00&lt;00:00, 119.41it/s, loss=0.00259]\nEpoch5: 100%|██████████| 157/157 [00:00&lt;00:00, 331.75it/s]\nVal: 100%|██████████| 40/40 [00:00&lt;00:00, 117.39it/s, loss=0.00242]\nEpoch6: 100%|██████████| 157/157 [00:00&lt;00:00, 341.96it/s]\nVal: 100%|██████████| 40/40 [00:00&lt;00:00, 96.35it/s, loss=0.00235]\nEpoch7: 100%|██████████| 157/157 [00:00&lt;00:00, 321.68it/s]\nVal: 100%|██████████| 40/40 [00:00&lt;00:00, 122.19it/s, loss=0.0023]\nEpoch8: 100%|██████████| 157/157 [00:00&lt;00:00, 307.33it/s]\nVal: 100%|██████████| 40/40 [00:00&lt;00:00, 121.72it/s, loss=0.00178]\nEpoch9: 100%|██████████| 157/157 [00:00&lt;00:00, 329.76it/s]\nVal: 100%|██████████| 40/40 [00:00&lt;00:00, 119.59it/s, loss=0.00203]\nEpoch10: 100%|██████████| 157/157 [00:00&lt;00:00, 308.76it/s]\nVal: 100%|██████████| 40/40 [00:00&lt;00:00, 99.72it/s, loss=0.00195] \n100%|██████████| 10/10 [00:08&lt;00:00,  1.17it/s]\n\n\n훈련한 모델의 Prediction과 실제 수치와 Corrleation이 그래프로 보이네요. “Linear” 합니다.\n\npredicted_accuracies = []\nground_truth_accuracies = []\nacc_predictor = acc_predictor.to(\"cuda:0\")\nacc_predictor.eval()\nwith torch.no_grad():\n    with tqdm(total=len(valid_loader), desc=\"Val\") as t:\n        for (data, label) in valid_loader:\n            data = data.to(device)\n            label = label.to(device)\n            pred = acc_predictor(data)\n            predicted_accuracies += pred.cpu().numpy().tolist()\n            ground_truth_accuracies += label.cpu().numpy().tolist()\n            if len(predicted_accuracies) &gt; 200:\n                break\nplt.scatter(predicted_accuracies, ground_truth_accuracies)\n# draw y = x\nmin_acc, max_acc = min(predicted_accuracies), max(predicted_accuracies)\nprint(min_acc, max_acc)\nplt.plot([min_acc, max_acc], [min_acc, max_acc], c=\"red\", linewidth=2)\nplt.xlabel(\"Predicted accuracy\")\nplt.ylabel(\"Measured accuracy\")\nplt.title(\"Correlation between predicted accuracy and real accuracy\")\n\nVal:   0%|          | 0/40 [00:00&lt;?, ?it/s]\n\n\n0.8604847192764282 0.9356203079223633\n\n\nText(0.5, 1.0, 'Correlation between predicted accuracy and real accuracy')"
  },
  {
    "objectID": "posts/labs/lab03.html#part-2.-neural-architecture-search",
    "href": "posts/labs/lab03.html#part-2.-neural-architecture-search",
    "title": "👩‍💻 Lab 3",
    "section": "Part 2. Neural Architecture Search",
    "text": "Part 2. Neural Architecture Search\n드디어 마지막 단계입니다. 원하는 모델을 찾아보죠! 두 가지 Search 방법을 이용할 건데, 하나는 Random Search이고, 다른 하나는 evolutionary Search 이용한 Neural Architecture Search입니다(드디어 NAS!).\n\n\nQuestion 5: Complete the following random search agent.\nRandom Search는 열심히 constraint에 해당하는 Sample을 모아서 최고의 Accuracy를 가진 모델 구조를 고르면 됩니다.\n\nclass RandomSearcher:\n    def __init__(self, efficiency_predictor, accuracy_predictor):\n        self.efficiency_predictor = efficiency_predictor\n        self.accuracy_predictor = accuracy_predictor\n\n    def random_valid_sample(self, constraint):\n        # randomly sample subnets until finding one that satisfies the constraint \n        while True:\n            sample = self.accuracy_predictor.arch_encoder.random_sample_arch()\n            efficiency = self.efficiency_predictor.get_efficiency(sample)\n            if self.efficiency_predictor.satisfy_constraint(efficiency, constraint):\n                return sample, efficiency\n\n    def run_search(self, constraint, n_subnets=100):\n        subnet_pool = []\n        # sample subnets\n        for _ in tqdm(range(n_subnets)):\n            sample, efficiency = self.random_valid_sample(constraint)\n            subnet_pool.append(sample)\n        # predict the accuracy of subnets\n        accs = self.accuracy_predictor.predict_acc(subnet_pool)\n        ############### YOUR CODE STARTS HERE ###############\n        # hint: one line of code\n        # get the index of the best subnet\n        best_idx = accs.argmax()\n        ############### YOUR CODE ENDS HERE #################\n        # return the best subnet\n        return accs[best_idx], subnet_pool[best_idx]\n\n\n\nQuestion 6: Complete the following function.\nNote: MACs 100M results lower than MACs 50M, Prof. Han says this might not be intuitive.\n\ndef search_and_measure_acc(agent, constraint, **kwargs):\n    ############### YOUR CODE STARTS HERE ###############\n    # hint: call the search function\n    best_info = agent.run_search(constraint=constraint, **kwargs)\n    ############### YOUR CODE ENDS HERE #################\n    # get searched subnet\n    \n    print(\"Best info: \", best_info)\n\n    ofa_network.set_active_subnet(**best_info[1])\n    subnet = ofa_network.get_active_subnet().to(device)\n    # calibrate bn\n    calib_bn(subnet, data_dir, 128, best_info[1][\"image_size\"])  # ?\n    # build val loader\n    val_loader = build_val_data_loader(data_dir, best_info[1][\"image_size\"], 128)\n    # measure accuracy\n    acc = validate(subnet, val_loader)\n    # print best_info\n    print(f\"Accuracy of the selected subnet: {acc}\")\n    # visualize model architecture\n    visualize_subnet(best_info[1])\n    return acc, subnet\n\n\nrandom.seed(1)\nnp.random.seed(1)\nnas_agent = RandomSearcher(efficiency_predictor, acc_predictor)\n# MACs-constrained search\nsubnets_rs_macs = {}\nfor millonMACs in [50, 100]:\n    search_constraint = dict(millonMACs=millonMACs)\n    print(f\"Random search with constraint: MACs &lt;= {millonMACs}M\")\n    subnets_rs_macs[millonMACs] = search_and_measure_acc(nas_agent, search_constraint, n_subnets=300)\n\n# memory-constrained search\nsubnets_rs_memory = {}\nfor KBPeakMemory in [256, 512]:\n    search_constraint = dict(KBPeakMemory=KBPeakMemory)\n    print(f\"Random search with constraint: Peak memory &lt;= {KBPeakMemory}KB\")\n    subnets_rs_memory[KBPeakMemory] = search_and_measure_acc(nas_agent, search_constraint, n_subnets=300)\n\nRandom search with constraint: MACs &lt;= 50M\nBest info:  (tensor(0.9327, device='cuda:0', grad_fn=&lt;SelectBackward0&gt;), {'ks': [5, 7, 5, 3, 3, 7, 5, 3, 7, 3, 3, 3, 7, 5, 5, 5, 7, 5, 3, 7], 'e': [4, 3, 3, 6, 4, 3, 6, 6, 4, 3, 3, 4, 4, 6, 6, 4, 3, 4, 4, 3], 'd': [2, 2, 1, 1, 2, 0], 'image_size': 160, 'wid': 1})\nAccuracy of the selected subnet: 93.27543427346657\nRandom search with constraint: MACs &lt;= 100M\nBest info:  (tensor(0.9329, device='cuda:0', grad_fn=&lt;SelectBackward0&gt;), {'ks': [3, 3, 5, 7, 7, 5, 5, 7, 7, 3, 5, 5, 5, 3, 7, 5, 7, 5, 5, 3], 'e': [4, 3, 6, 3, 3, 6, 4, 4, 4, 4, 3, 3, 4, 4, 4, 4, 4, 6, 6, 3], 'd': [2, 2, 1, 1, 2, 2], 'image_size': 160, 'wid': 1})\nAccuracy of the selected subnet: 93.47394543971969\nRandom search with constraint: Peak memory &lt;= 256KB\nBest info:  (tensor(0.9248, device='cuda:0', grad_fn=&lt;SelectBackward0&gt;), {'ks': [7, 7, 5, 7, 7, 5, 3, 5, 3, 5, 3, 5, 7, 3, 3, 5, 7, 7, 5, 3], 'e': [4, 4, 6, 4, 6, 3, 6, 4, 6, 6, 6, 4, 4, 4, 4, 6, 3, 6, 4, 4], 'd': [1, 1, 1, 1, 1, 1], 'image_size': 160, 'wid': 0})\nAccuracy of the selected subnet: 92.8287840963889\nRandom search with constraint: Peak memory &lt;= 512KB\nBest info:  (tensor(0.9328, device='cuda:0', grad_fn=&lt;SelectBackward0&gt;), {'ks': [3, 3, 3, 3, 5, 5, 5, 7, 5, 7, 5, 5, 7, 5, 7, 7, 7, 7, 3, 3], 'e': [4, 3, 6, 4, 6, 6, 4, 6, 4, 4, 4, 6, 4, 4, 3, 6, 4, 4, 3, 6], 'd': [2, 1, 2, 0, 2, 0], 'image_size': 160, 'wid': 1})\nAccuracy of the selected subnet: 93.15136479455839\n\n\n100%|██████████| 300/300 [00:19&lt;00:00, 15.43it/s]\nValidate: 100%|██████████| 32/32 [00:01&lt;00:00, 30.42it/s, loss=0.187, top1=93.3]\n100%|██████████| 300/300 [00:19&lt;00:00, 15.05it/s]\nValidate: 100%|██████████| 32/32 [00:01&lt;00:00, 30.58it/s, loss=0.186, top1=93.5]\n100%|██████████| 300/300 [00:43&lt;00:00,  6.83it/s]\nValidate: 100%|██████████| 32/32 [00:00&lt;00:00, 34.29it/s, loss=0.204, top1=92.8]\n100%|██████████| 300/300 [00:22&lt;00:00, 13.54it/s]\nValidate: 100%|██████████| 32/32 [00:01&lt;00:00, 29.29it/s, loss=0.19, top1=93.2] \n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nQuestion 7: Complete the following evolutionary search agent.\nEvolutionary Search는 여기서 한 단계가 더 들어가요. 바로 “Crossover”이라는 단계인데, 랜덤으로 뽑은 두 샘플에서 또 다시 모델 구조에서 부분을 나눠 랜덤으로 둘 중에 하나를 뽑아서 합치는 과정입니다. sub-network가 뽑히는 개수를 population이라고 하면서 랜덤 샘플링을 하는 횟수를 generation, 그리고 max_time_budget을 둬서 몇번의 generation을 거칠 것인가도 제한합니다. 매 generation마다 모이는 population을 정렬할 거거든요.\n\nNow you have succesfully implemented the random search algorithm. In this part, we will implement a more sample-efficient search algorithm, evolutionary search. Evolutionary search is inspired by the evolution algorithm (or genetic algorithm). A population of sub-networks are first sampled from the design space. Then, in each generation, we perform random mutation and crossover operations as is shown in the figure above. The sub-networks with highest accuracy will be kept, and this process will be repeated until the number of generations reaches max_time_budget. Similar to the random search, throughout the search process, all sub-networks that cannot satisfy the efficiency constraint will be discarded.\n\nclass EvolutionSearcher:\n    def __init__(self, efficiency_predictor, accuracy_predictor, **kwargs):\n        self.efficiency_predictor = efficiency_predictor\n        self.accuracy_predictor = accuracy_predictor\n\n        # evolution hyper-parameters\n        self.arch_mutate_prob = kwargs.get(\"arch_mutate_prob\", 0.1)\n        self.resolution_mutate_prob = kwargs.get(\"resolution_mutate_prob\", 0.5)\n        self.population_size = kwargs.get(\"population_size\", 100)\n        self.max_time_budget = kwargs.get(\"max_time_budget\", 500)\n        self.parent_ratio = kwargs.get(\"parent_ratio\", 0.25)\n        self.mutation_ratio = kwargs.get(\"mutation_ratio\", 0.5)\n\n    def update_hyper_params(self, new_param_dict):\n        self.__dict__.update(new_param_dict)\n\n    def random_valid_sample(self, constraint):\n        # randomly sample subnets until finding one that satisfies the constraint \n        while True:\n            sample = self.accuracy_predictor.arch_encoder.random_sample_arch()\n            efficiency = self.efficiency_predictor.get_efficiency(sample)\n            if self.efficiency_predictor.satisfy_constraint(efficiency, constraint):\n                return sample, efficiency\n\n    def mutate_sample(self, sample, constraint):\n        while True:\n            new_sample = copy.deepcopy(sample)\n\n            self.accuracy_predictor.arch_encoder.mutate_resolution(new_sample, self.resolution_mutate_prob)\n            self.accuracy_predictor.arch_encoder.mutate_width(new_sample, self.arch_mutate_prob)\n            self.accuracy_predictor.arch_encoder.mutate_arch(new_sample, self.arch_mutate_prob)\n\n            efficiency = self.efficiency_predictor.get_efficiency(new_sample)\n            if self.efficiency_predictor.satisfy_constraint(efficiency, constraint):\n                return new_sample, efficiency\n\n    def crossover_sample(self, sample1, sample2, constraint):\n        while True:\n            new_sample = copy.deepcopy(sample1)\n            for key in new_sample.keys():\n                if not isinstance(new_sample[key], list):\n                    ############### YOUR CODE STARTS HERE ###############\n                    # hint: randomly choose the value from sample1[key] and sample2[key], random.choice\n                    new_sample[key] = random.choice([sample1[key], sample2[key]])\n                    ############### YOUR CODE ENDS HERE #################\n                else:\n                    for i in range(len(new_sample[key])):\n                        ############### YOUR CODE STARTS HERE ###############\n                        new_sample[key][i] = random.choice([sample1[key][i], sample2[key][i]])\n                        ############### YOUR CODE ENDS HERE #################\n\n            efficiency = self.efficiency_predictor.get_efficiency(new_sample)\n            if self.efficiency_predictor.satisfy_constraint(efficiency, constraint):\n                return new_sample, efficiency\n\n    def run_search(self, constraint, **kwargs):\n        self.update_hyper_params(kwargs)\n\n        mutation_numbers = int(round(self.mutation_ratio * self.population_size))\n        parents_size = int(round(self.parent_ratio * self.population_size))\n\n        best_valids = [-100]\n        population = []  # (acc, sample) tuples\n        child_pool = []\n        best_info = None\n        # generate random population\n        for _ in range(self.population_size):\n            sample, efficiency = self.random_valid_sample(constraint)\n            child_pool.append(sample)\n\n        accs = self.accuracy_predictor.predict_acc(child_pool)\n        for i in range(self.population_size):\n            population.append((accs[i].item(), child_pool[i]))\n\n        # evolving the population\n        with tqdm(total=self.max_time_budget) as t:\n            for i in range(self.max_time_budget):\n                ############### YOUR CODE STARTS HERE ###############\n                # hint: sort the population according to the acc (descending order)\n                population = sorted(population, key=lambda x: x[0], reverse=True)\n                ############### YOUR CODE ENDS HERE #################\n\n                ############### YOUR CODE STARTS HERE ###############\n                # hint: keep topK samples in the population, K = parents_size\n                # the others are discarded.\n                population = population[:parents_size]\n                ############### YOUR CODE ENDS HERE #################\n\n                # update best info\n                acc = population[0][0]\n                if acc &gt; best_valids[-1]:\n                    best_valids.append(acc)\n                    best_info = population[0]\n                else:\n                    best_valids.append(best_valids[-1])\n\n                child_pool = []\n                for j in range(mutation_numbers):\n                    # randomly choose a sample\n                    par_sample = population[np.random.randint(parents_size)][1]\n                    # mutate this sample\n                    new_sample, efficiency = self.mutate_sample(par_sample, constraint)\n                    child_pool.append(new_sample)\n\n                for j in range(self.population_size - mutation_numbers):\n                    # randomly choose two samples\n                    par_sample1 = population[np.random.randint(parents_size)][1]\n                    par_sample2 = population[np.random.randint(parents_size)][1]\n                    # crossover\n                    new_sample, efficiency = self.crossover_sample(\n                        par_sample1, par_sample2, constraint\n                    )\n                    child_pool.append(new_sample)\n                # predict accuracy with the accuracy predictor\n                accs = self.accuracy_predictor.predict_acc(child_pool)\n                for j in range(self.population_size):\n                    population.append((accs[j].item(), child_pool[j]))\n\n                t.update(1)\n\n        return best_info\n\n\n\nQuestion 8: Run evolutionary search and tune evo_params to optimize the results. Describe your findings.\n남은 부분은 실험, 실험, 실험입니다. 관찰해보시죠!\nAnswer: - The default population size and time budget are too small. Increasing them can effectively improves the final results. But it also increases the search cost. - Increasing the probability of resolution mutation can improve the final results. (hint: Question 1)\n\nrandom.seed(1)\nnp.random.seed(1)\n\nevo_params = {\n    'arch_mutate_prob': 0.1, # The probability of architecture mutation in evolutionary search\n    'resolution_mutate_prob': 0.1, # The probability of resolution mutation in evolutionary search\n    'population_size': 10,  # The size of the population\n    'max_time_budget': 10,  \n    'parent_ratio': 0.1,\n    'mutation_ratio': 0.1,\n}\n\nnas_agent = EvolutionSearcher(efficiency_predictor, acc_predictor, **evo_params)\n# MACs-constrained search\nsubnets_evo_macs = {}\nfor millonMACs in [50, 100]:\n    search_constraint = dict(millionMACs=millonMACs)\n    print(f\"Evolutionary search with constraint: MACs &lt;= {millonMACs}M\")\n    subnets_evo_macs[millonMACs] = search_and_measure_acc(nas_agent, search_constraint)\n\n# memory-constrained search\nsubnets_evo_memory = {}\nfor KBPeakMemory in [256, 512]:\n    search_constraint = dict(KBPeakMemory=KBPeakMemory)\n    print(f\"Evolutionary search with constraint: Peak memory &lt;= {KBPeakMemory}KB\")\n    subnets_evo_memory[KBPeakMemory] = search_and_measure_acc(nas_agent, search_constraint)\n\nEvolutionary search with constraint: MACs &lt;= 50M\nAccuracy of the selected subnet: 92.28287844220107\nEvolutionary search with constraint: MACs &lt;= 100M\nAccuracy of the selected subnet: 92.33250616939725\nEvolutionary search with constraint: Peak memory &lt;= 256KB\nAccuracy of the selected subnet: 92.45657571267253\nEvolutionary search with constraint: Peak memory &lt;= 512KB\nAccuracy of the selected subnet: 93.05210921143184\n\n\n100%|██████████| 10/10 [00:05&lt;00:00,  1.72it/s]\nValidate: 100%|██████████| 32/32 [00:00&lt;00:00, 35.68it/s, loss=0.214, top1=92.3]\n100%|██████████| 10/10 [00:07&lt;00:00,  1.42it/s]\nValidate: 100%|██████████| 32/32 [00:00&lt;00:00, 36.25it/s, loss=0.206, top1=92.3]\n100%|██████████| 10/10 [00:07&lt;00:00,  1.35it/s]\nValidate: 100%|██████████| 32/32 [00:00&lt;00:00, 35.39it/s, loss=0.21, top1=92.5] \n100%|██████████| 10/10 [00:05&lt;00:00,  1.74it/s]\nValidate: 100%|██████████| 32/32 [00:00&lt;00:00, 32.62it/s, loss=0.193, top1=93.1]\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nQuestion 9: Run evolutionary search under real-world constraints.\nIn real-world applications, we may have multiple efficiency constraints: https://blog.tensorflow.org/2019/10/visual-wake-words-with-tensorflow-lite_30.html. Use evolutionary search to find models that satisfy the following constraints: - [15 pts] 250 KB, 60M MACs (acc &gt;= 92.5% to get the full credit) - [10 pts, bonus] 200KB, 30M MACs (acc &gt;= 90% to get the full credit)\nHint: You do not have to use the same evo_params for these two tasks.\n\nrandom.seed(1)\nnp.random.seed(1)\nevo_params = {\n    'arch_mutate_prob': 0.1, # The probability of architecture mutation in evolutionary search\n    'resolution_mutate_prob': 0.5, # The probability of resolution mutation in evolutionary search\n    'population_size': 50,  # The size of the population\n    'max_time_budget': 20,  \n    'parent_ratio': 0.25,\n    'mutation_ratio': 0.3,\n}\n\nnas_agent = EvolutionSearcher(efficiency_predictor, acc_predictor, **evo_params)\n\n(millionMACs, KBPeakMemory) = [60, 250]\nprint(f\"Evolution search with constraint: MACs &lt;= {millionMACs}M, peak memory &lt;= {KBPeakMemory}KB\")\nsearch_and_measure_acc(nas_agent, dict(millionMACs=millionMACs, KBPeakMemory=KBPeakMemory))\nprint(\"Evolution search finished!\")\n\nEvolution search with constraint: MACs &lt;= 60M, peak memory &lt;= 250KB\nAccuracy of the selected subnet: 92.60545903435415\nEvolution search finished!\n\n\n100%|██████████| 20/20 [01:13&lt;00:00,  3.66s/it]\nValidate: 100%|██████████| 32/32 [00:00&lt;00:00, 36.77it/s, loss=0.201, top1=92.6]\n\n\n\n\n\n\n\n\n\n\nrandom.seed(1)\nnp.random.seed(1)\nevo_params = {\n    'arch_mutate_prob': 0.1, # The probability of architecture mutation in evolutionary search\n    'resolution_mutate_prob': 0.5, # The probability of resolution mutation in evolutionary search\n    'population_size': 50,  # The size of the population\n    'max_time_budget': 30,  \n    'parent_ratio': 0.25,\n    'mutation_ratio': 0.3,\n}\nnas_agent = EvolutionSearcher(efficiency_predictor, acc_predictor, **evo_params)\n\n(millionMACs, KBPeakMemory) = [30, 200]\nprint(f\"Evolution search with constraint: MACs &lt;= {millionMACs}M, peak memory &lt;= {KBPeakMemory}KB\")\nsearch_and_measure_acc(nas_agent, dict(millionMACs=millionMACs, KBPeakMemory=KBPeakMemory))\nprint(\"Evolution search finished!\")\n\nEvolution search with constraint: MACs &lt;= 30M, peak memory &lt;= 200KB\nAccuracy of the selected subnet: 90.54590573748644\nEvolution search finished!\n\n\n100%|██████████| 30/30 [01:45&lt;00:00,  3.52s/it]\nValidate: 100%|██████████| 32/32 [00:00&lt;00:00, 37.25it/s, loss=0.241, top1=90.5]\n\n\n\n\n\n\n\n\n\n\nrandom.seed(1)\nnp.random.seed(1)\nevo_params = {\n    'arch_mutate_prob': 0.1, # The probability of architecture mutation in evolutionary search\n    'resolution_mutate_prob': 0.5, # The probability of resolution mutation in evolutionary search\n    'population_size': 50,  # The size of the population\n    'max_time_budget': 20,  \n    'parent_ratio': 0.25,\n    'mutation_ratio': 0.3,\n}\n\nnas_agent = EvolutionSearcher(efficiency_predictor, acc_predictor, **evo_params)\n\n(millionMACs, KBPeakMemory) = [15, 256]\nprint(f\"Evolution search with constraint: MACs &lt;= {millionMACs}M, peak memory &lt;= {KBPeakMemory}KB\")\nsearch_and_measure_acc(nas_agent, dict(millionMACs=millionMACs, KBPeakMemory=KBPeakMemory))\nprint(\"Evolution search finished!\")\n\nEvolution search with constraint: MACs &lt;= 15M, peak memory &lt;= 256KB\nBest info:  (0.8773346543312073, {'ks': [7, 5, 5, 3, 5, 3, 5, 7, 5, 5, 7, 5, 3, 7, 3, 5, 3, 3, 7, 5], 'e': [4, 6, 6, 6, 4, 3, 3, 4, 3, 4, 4, 6, 6, 4, 6, 4, 3, 4, 3, 3], 'd': [0, 1, 0, 1, 1, 1], 'image_size': 96, 'wid': 0})\nAccuracy of the selected subnet: 86.35235731631296\nEvolution search finished!\n\n\n100%|██████████| 20/20 [01:29&lt;00:00,  4.48s/it]\nValidate: 100%|██████████| 32/32 [00:00&lt;00:00, 45.67it/s, loss=0.319, top1=86.4]\n\n\n\n\n\n\n\n\n\n\nrandom.seed(1)\nnp.random.seed(1)\nevo_params = {\n    'arch_mutate_prob': 0.1, # The probability of architecture mutation in evolutionary search\n    'resolution_mutate_prob': 0.5, # The probability of resolution mutation in evolutionary search\n    'population_size': 50,  # The size of the population\n    'max_time_budget': 20,  \n    'parent_ratio': 0.25,\n    'mutation_ratio': 0.3,\n}\n\nnas_agent = EvolutionSearcher(efficiency_predictor, acc_predictor, **evo_params)\n\n(millionMACs, KBPeakMemory) = [60, 64]\nprint(f\"Evolution search with constraint: MACs &lt;= {millionMACs}M, peak memory &lt;= {KBPeakMemory}KB\")\nsearch_and_measure_acc(nas_agent, dict(millionMACs=millionMACs, KBPeakMemory=KBPeakMemory))\nprint(\"Evolution search finished!\")\n\nEvolution search with constraint: MACs &lt;= 60M, peak memory &lt;= 64KB\n\n\nKeyboardInterrupt: \n\n\n\nrandom.seed(1)\nnp.random.seed(1)\nevo_params = {\n    'arch_mutate_prob': 0.1, # The probability of architecture mutation in evolutionary search\n    'resolution_mutate_prob': 0.5, # The probability of resolution mutation in evolutionary search\n    'population_size': 50,  # The size of the population\n    'max_time_budget': 20,  \n    'parent_ratio': 0.25,\n    'mutation_ratio': 0.3,\n}\n\nnas_agent = EvolutionSearcher(efficiency_predictor, acc_predictor, **evo_params)\n\n(millionMACs, KBPeakMemory) = [10, 64]\nprint(f\"Evolution search with constraint: MACs &lt;= {millionMACs}M, peak memory &lt;= {KBPeakMemory}KB\")\nsearch_and_measure_acc(nas_agent, dict(millionMACs=millionMACs, KBPeakMemory=KBPeakMemory))\nprint(\"Evolution search finished!\")\n\nEvolution search with constraint: MACs &lt;= 10M, peak memory &lt;= 64KB\n\n\n\n\nQuestion 10: Is it possible to find a subnet with the following efficiency constraints in the current design space?\n\nA: The activation size of the subnet is at most 256KB and the MACs of the subnet is at most 15M.\nB: The activation size of the subnet is at most 64 KB.\n\nAnswer:\n\nA: yes\nB: no"
  },
  {
    "objectID": "posts/labs/lab00.html",
    "href": "posts/labs/lab00.html",
    "title": "👩‍💻 Lab 0",
    "section": "",
    "text": "Lab 0 PyTorch Tutorial\nIn this tutorial, we will explore how to train a neural network with PyTorch.\n\nSetup\nWe will first install a few packages that will be used in this tutorial:\n\n!pip install torchprofile 1&gt;/dev/null\n\nWe will then import a few libraries:\n\nimport random\nfrom collections import OrderedDict, defaultdict\n\nimport numpy as np\nimport torch\nfrom matplotlib import pyplot as plt\nfrom torch import nn\nfrom torch.optim import *\nfrom torch.optim.lr_scheduler import *\nfrom torch.utils.data import DataLoader\nfrom torchprofile import profile_macs\nfrom torchvision.datasets import *\nfrom torchvision.transforms import *\nfrom tqdm.auto import tqdm\n\nTo ensure the reproducibility, we will control the seed of random generators:\n\nrandom.seed(0)\nnp.random.seed(0)\ntorch.manual_seed(0)\n\n\n\nData\nIn this tutorial, we will use CIFAR-10 as our target dataset. This dataset contains images from 10 classes, where each image is of size 3x32x32, i.e. 3-channel color images of 32x32 pixels in size.\n\ntransforms = {\n  \"train\": Compose([\n    RandomCrop(32, padding=4),\n    RandomHorizontalFlip(),\n    ToTensor(),\n  ]),\n  \"test\": ToTensor(),\n}\n\ndataset = {}\nfor split in [\"train\", \"test\"]:\n  dataset[split] = CIFAR10(\n    root=\"data/cifar10\",\n    train=(split == \"train\"),\n    download=True,\n    transform=transforms[split],\n  )\n\nWe can visualize a few images in the dataset and their corresponding class labels:\n\nsamples = [[] for _ in range(10)]\nfor image, label in dataset[\"test\"]:\n  if len(samples[label]) &lt; 4:\n    samples[label].append(image)\n\nplt.figure(figsize=(20, 9))\nfor index in range(40):\n  label = index % 10\n  image = samples[label][index // 10]\n\n  # Convert from CHW to HWC for visualization\n  image = image.permute(1, 2, 0)\n\n  # Convert from class index to class name\n  label = dataset[\"test\"].classes[label]\n\n  # Visualize the image\n  plt.subplot(4, 10, index + 1)\n  plt.imshow(image)\n  plt.title(label)\n  plt.axis(\"off\")\nplt.show()\n\nTo train a neural network, we will need to feed data in batches. We create data loaders with batch size of 512:\n\ndataflow = {}\nfor split in ['train', 'test']:\n  dataflow[split] = DataLoader(\n    dataset[split],\n    batch_size=512,\n    shuffle=(split == 'train'),\n    num_workers=0,\n    pin_memory=True,\n  )\n\nWe can print the data type and shape from the training data loader:\n\nfor inputs, targets in dataflow[\"train\"]:\n  print(\"[inputs] dtype: {}, shape: {}\".format(inputs.dtype, inputs.shape))\n  print(\"[targets] dtype: {}, shape: {}\".format(targets.dtype, targets.shape))\n  break\n\n\n\nModel\nIn this tutorial, we will use a variant of VGG-11 (with fewer downsamples and a smaller classifier) as our model.\n\nclass VGG(nn.Module):\n  ARCH = [64, 128, 'M', 256, 256, 'M', 512, 512, 'M', 512, 512, 'M']\n\n  def __init__(self) -&gt; None:\n    super().__init__()\n\n    layers = []\n    counts = defaultdict(int)\n\n    def add(name: str, layer: nn.Module) -&gt; None:\n      layers.append((f\"{name}{counts[name]}\", layer))\n      counts[name] += 1\n\n    in_channels = 3\n    for x in self.ARCH:\n      if x != 'M':\n        # conv-bn-relu\n        add(\"conv\", nn.Conv2d(in_channels, x, 3, padding=1, bias=False))\n        add(\"bn\", nn.BatchNorm2d(x))\n        add(\"relu\", nn.ReLU(True))\n        in_channels = x\n      else:\n        # maxpool\n        add(\"pool\", nn.MaxPool2d(2))\n\n    self.backbone = nn.Sequential(OrderedDict(layers))\n    self.classifier = nn.Linear(512, 10)\n\n  def forward(self, x: torch.Tensor) -&gt; torch.Tensor:\n    # backbone: [N, 3, 32, 32] =&gt; [N, 512, 2, 2]\n    x = self.backbone(x)\n\n    # avgpool: [N, 512, 2, 2] =&gt; [N, 512]\n    x = x.mean([2, 3])\n\n    # classifier: [N, 512] =&gt; [N, 10]\n    x = self.classifier(x)\n    return x\n\nmodel = VGG().cuda()\n\nIts backbone is composed of eight conv-bn-relu blocks interleaved with four maxpool’s to downsample the feature map by 2^4 = 16 times:\n\nprint(model.backbone)\n\nAfter the feature map is pooled, its classifier predicts the final output with a linear layer:\n\nprint(model.classifier)\n\nAs this course focuses on efficiency, we will then inspect its model size and (theoretical) computation cost.\n\nThe model size can be estimated by the number of trainable parameters:\n\n\nnum_params = 0\nfor param in model.parameters():\n  if param.requires_grad:\n    num_params += param.numel()\nprint(\"#Params:\", num_params)\n\n\nThe computation cost can be estimated by the number of multiply–accumulate operations (MACs) using TorchProfile:\n\n\nnum_macs = profile_macs(model, torch.zeros(1, 3, 32, 32).cuda())\nprint(\"#MACs:\", num_macs)\n\nThis model has 9.2M parameters and requires 606M MACs for inference. We will work together in the next few labs to improve its efficiency.\n\n\nOptimization\nAs we are working on a classification problem, we will apply cross entropy as our loss function to optimize the model:\n\ncriterion = nn.CrossEntropyLoss()\n\nOptimization will be carried out using stochastic gradient descent (SGD) with momentum:\n\noptimizer = SGD(\n  model.parameters(),\n  lr=0.4,\n  momentum=0.9,\n  weight_decay=5e-4,\n)\n\nThe learning rate will be modulated using the following scheduler (which is adapted from this blog series):\n\nnum_epochs = 20\nsteps_per_epoch = len(dataflow[\"train\"])\n\n# Define the piecewise linear scheduler\nlr_lambda = lambda step: np.interp(\n  [step / steps_per_epoch],\n  [0, num_epochs * 0.3, num_epochs],\n  [0, 1, 0]\n)[0]\n\n# Visualize the learning rate schedule\nsteps = np.arange(steps_per_epoch * num_epochs)\nplt.plot(steps, [lr_lambda(step) * 0.4 for step in steps])\nplt.xlabel(\"Number of Steps\")\nplt.ylabel(\"Learning Rate\")\nplt.grid(\"on\")\nplt.show()\n\nscheduler = LambdaLR(optimizer, lr_lambda)\n\n\n\nTraining\nWe first define the training function that optimizes the model for one epoch (i.e., a pass over the training set):\n\ndef train(\n  model: nn.Module,\n  dataflow: DataLoader,\n  criterion: nn.Module,\n  optimizer: Optimizer,\n  scheduler: LambdaLR,\n) -&gt; None:\n  model.train()\n\n  for inputs, targets in tqdm(dataflow, desc='train', leave=False):\n    # Move the data from CPU to GPU\n    inputs = inputs.cuda()\n    targets = targets.cuda()\n\n    # Reset the gradients (from the last iteration)\n    optimizer.zero_grad()\n\n    # Forward inference\n    outputs = model(inputs)\n    loss = criterion(outputs, targets)\n\n    # Backward propagation\n    loss.backward()\n\n    # Update optimizer and LR scheduler\n    optimizer.step()\n    scheduler.step()\n\nWe then define the evaluation function that calculates the metric (i.e., accuracy in our case) on the test set:\n\n@torch.inference_mode()\ndef evaluate(\n  model: nn.Module,\n  dataflow: DataLoader\n) -&gt; float:\n  model.eval()\n\n  num_samples = 0\n  num_correct = 0\n\n  for inputs, targets in tqdm(dataflow, desc=\"eval\", leave=False):\n    # Move the data from CPU to GPU\n    inputs = inputs.cuda()\n    targets = targets.cuda()\n\n    # Inference\n    outputs = model(inputs)\n\n    # Convert logits to class indices\n    outputs = outputs.argmax(dim=1)\n\n    # Update metrics\n    num_samples += targets.size(0)\n    num_correct += (outputs == targets).sum()\n\n  return (num_correct / num_samples * 100).item()\n\nWith training and evaluation functions, we can finally start training the model! This will take around 10 minutes.\n\nfor epoch_num in tqdm(range(1, num_epochs + 1)):\n  train(model, dataflow[\"train\"], criterion, optimizer, scheduler)\n  metric = evaluate(model, dataflow[\"test\"])\n  print(f\"epoch {epoch_num}:\", metric)\n\nIf everything goes well, your trained model should be able to achieve &gt;92.5% of accuracy!\n\n\nVisualization\nWe can visualize the model’s prediction to see how the model truly performs:\n\nplt.figure(figsize=(20, 10))\nfor index in range(40):\n  image, label = dataset[\"test\"][index]\n\n  # Model inference\n  model.eval()\n  with torch.inference_mode():\n    pred = model(image.unsqueeze(dim=0).cuda())\n    pred = pred.argmax(dim=1)\n\n  # Convert from CHW to HWC for visualization\n  image = image.permute(1, 2, 0)\n\n  # Convert from class indices to class names\n  pred = dataset[\"test\"].classes[pred]\n  label = dataset[\"test\"].classes[label]\n\n  # Visualize the image\n  plt.subplot(4, 10, index + 1)\n  plt.imshow(image)\n  plt.title(f\"pred: {pred}\" + \"\\n\" + f\"label: {label}\")\n  plt.axis(\"off\")\nplt.show()"
  },
  {
    "objectID": "posts/lecs/lec12.html",
    "href": "posts/lecs/lec12.html",
    "title": "🧑‍🏫 Lecture 12-13",
    "section": "",
    "text": "이번 글에서는 현대 NLP의 핵심 도구인 transformer와 LLM, 그리고 이들을 최적화하는 여러 방법에 대해서 소개한다. transformer를 최적화 하기 위한 방법으로 positional embedding의 변화나 llama2에 사용된 grouped-query-attention등을 다룬다. 더 나아가, LLMs의 효율적인 추론(inference)와 조정(fine-tuning) 알고리즘과 시스템에 대해서도 다룬다. inference에 대해서는 vLLM, StreamingLLM, fine-tuning은 LoRA, QLoRA, Adapter기법 등을 소개한다."
  },
  {
    "objectID": "posts/lecs/lec12.html#absolute-positional-encoding---relative-positional-encoding",
    "href": "posts/lecs/lec12.html#absolute-positional-encoding---relative-positional-encoding",
    "title": "🧑‍🏫 Lecture 12-13",
    "section": "Absolute positional encoding -> Relative positional encoding",
    "text": "Absolute positional encoding -&gt; Relative positional encoding\n원본 transformer 모델에서는 positional embedding으로 sinusoid embedding을 사용한다. 이는 position마다 독립적이면서도 연속적인 embedding vector를 만들어낸다. \n그러나 이런 index에 의존적인 absolute positional encoding 방식에는, training중에 보지 못한 길이를 대응하기 어렵다는 문제점이 있다. 예를 들어, 250 token까지만 학습했는데 251 token의 데이터가 들어오는 상황 말이다.\nRelative positional encoding을 사용하면 train short, test long을 달성할 수 있다. 이외에도 absolute positional encoding은 위치 정보를 input embedding에 더해 Q/K/V 전체에 영향을 미치지만, relative positional encoding은 Q,K에 bias를 더하는 방식으로 attention score에 영향을 준다(V에는 영향을 미치지 않는다)\n\nAttention with Linear Biases (ALiBi)\n가장 간단한 방법으로는 ALiBi가 있다. 이 방법은 단순히 attention matrix에 query와 key의 거리에 대한 offset을 더해준다. \n\n\nRotary Positional Embedding (RoPE)\n다른 방법으로는 llama2에도 사용될 정도로 널리 사용되고 있는 RoPE이다. RoPE의 아이디어는 입력 데이터의 위치 정보를 회전(rotation)을 통해 인코딩하는 것이다. d차원의 word embedding으로 d/2개의 짝을 만들고, 각각의 pair를 2d 좌표로 가정하고, position에 따라 회전시키는 것이다.  위 그림에서 x1,x2에 m * theta를 곱해서 회전시키고 있다.  RoPE를 수식으로 나타내면 위와 같다\nLLM은 보통 학습할 때 context 길이에 제한이 있다. 예를들어 llama는 2k, llama2는 4k로 제한된 데이터로 학습한다. 그러나 RoPE 방식 덕에 더 큰 context 길이도 다룰 수 있다.  더 작은 theta를 사용하면, 더 촘촘하게 interpolate 하면서 context길이를 늘릴 수 있다. 위 그림에서 단순히 4096 context 길이는 unseen이라 실패하지만, theta를 절반으로 줄인 아래 그래프에서는 원래 context 길이 안에 들어오기 때문에 성공하는 모습을 볼 수 있다."
  },
  {
    "objectID": "posts/lecs/lec12.html#kv-cache-optimizationsmulti-head-attention-mha---multi-query-attention-mqa---grouped-query-attentiongqa",
    "href": "posts/lecs/lec12.html#kv-cache-optimizationsmulti-head-attention-mha---multi-query-attention-mqa---grouped-query-attentiongqa",
    "title": "🧑‍🏫 Lecture 12-13",
    "section": "KV cache optimizations(Multi-Head Attention (MHA) -> Multi-Query Attention (MQA) -> Grouped-Query Attention(GQA)",
    "text": "KV cache optimizations(Multi-Head Attention (MHA) -&gt; Multi-Query Attention (MQA) -&gt; Grouped-Query Attention(GQA)\nKV cache란 attention 매커니즘의 Key, Value를 저장해놓는 것을 말한다. transfomer를 decode(gpt-style. decoder 모델 사용해 생성하는 것을 의미함)할 때는 지금 시점 토큰의 attention을 계산하기 위해 이전 토큰들의 Key, Value값을 모두 저장하고 있어야 한다.  위 그림에서 “trainium” 토큰을 생성하기 위해선 이전 “I”, “love”의 K,V가 필요하다(Query는 필요하지 않음) 단순히 상상해봐도, KV cache를 저장하기 위해 사용되는 메모리가 너무 많이 필요하다. llama2-7b 모델에서 KV cache 크기는 batch_size * 32(layers) * 128(n_emd) * N(length) * 2(K,V니까 2개) * 2byte(fp16) = 512KB * BS * N 만큼 필요하다. llama2-70B 모델을 이런 식으로 계산해보면, batch size 16일 때 4096번째 token을 처리할 때 KV cache의 용량은 160GB에 달한다.  따라서 KV cache의 사이즈를 줄일 필요가 있고, 그 방법이 multi-query-attention(MQA), grouped-query-attention(GQA)이다. 이중 GQA는 llama2에도 적용될 정도로 많이 사용되는 방식이다. 각각의 방식을 살펴보면\nMQA : 모든 value와 key를 하나로 평균낸다\nGQA : 모든 value와 key를 G개로 평균낸다(보통 G는 N/8)\n 위 그림처럼 MQA, GQA를 사용하면 KV cache 크기를 많이 줄일 수 있다."
  },
  {
    "objectID": "posts/lecs/lec12.html#ffn-glu",
    "href": "posts/lecs/lec12.html#ffn-glu",
    "title": "🧑‍🏫 Lecture 12-13",
    "section": "FFN->GLU",
    "text": "FFN-&gt;GLU\ninverted bottleneck, relu를 사용하던 기존 FFN 계층을, GLU(Gated Linear Unit)과 swish 활성화 함수를 사용하면 성능이 더 나아진다고 한다.   이때 성능은 PPL(perplexity)로 측정한다."
  },
  {
    "objectID": "posts/lecs/lec12.html#quantization-smoothquant-awq-tinychat",
    "href": "posts/lecs/lec12.html#quantization-smoothquant-awq-tinychat",
    "title": "🧑‍🏫 Lecture 12-13",
    "section": "5.1. Quantization: SmoothQuant, AWQ, TinyChat",
    "text": "5.1. Quantization: SmoothQuant, AWQ, TinyChat\n 하지만, 단순히 W8A8과 같이 quantize하는 것은, 굉장히 큰 성능 저하를 보여준다 이유는, LLM에서는 activation의 outlier가 중요한 역할을 하기 때문이다  위 그림의 오른쪽처럼 activation에는 굉장히 큰 outlier가 존재하고 weight는 비교적 편차가 없다. 따라서 activation을 10으로 나누고, weight에 10을 곱하면 수식의 값은 변하지 않지만, activation을 좀 더 편하게 quantize할 수 있게 된다(오른쪽 그림). 이런 방식을 activation을 좀더 평평하게 만든다고 해서 smoothQuant라고 한다.  smoothQuant방식은 llama 모델에서도 매우 잘 동작한다.\n 위 그림의 x축인 compute intensity는 FLOPs / MemoryBandwith를 나타낸다. 즉, 데이터 하나당 연산을 얼마나 효율적으로 할 수 있느냐에 대한 지표이다. 위 그림에서 batch size가 1일 때 낮은 TFLOPS를 보이는 이유는 메모리 때문이다. LLM에서 매 토큰을 생성하기 위해서는 큰 메모리 fetch가 필요하다(parameter fetch.). activation과 weight 중에서는 weight가 훨씬 더 크므로, weight를 줄이는데 더 집중해야 한다.\n위에서 살펴본 W8A8 방식의 quantization은 batch serving(한번에 여러 batch를 처리하는 일)에서는 잘 동작한다. 하나만 처리하는 작업은(single-batch) memory-bounded(메모리가 부족하면 bottleneck이 된다)이다.  당연히 weight를 바로 quantize 하면 위 그림처럼 성능 저하가 발생한다 오른쪽 그림을 살펴보면, RTN방식을 단순히 적용한 경우 Perplexity가 많이 상승한 것을 볼 수 있다.  더 나은 방법은, 중요한(salient) weight들만 quantize 하지 않고 두는 것인데, salient 하다고 판단하는 기준을 “activation”값을 기반으로 판단할 때(magnitude-base, 단순히 절댓값이 크면 중요하다고 판단) 좋은 성능을 보인다. 이런 방식을 AWQ(Activation-aware Weight Quantization) 라고 한다.\nSmoothQuant와 AWQ는 오늘날 널리 사용되는 방식이다."
  },
  {
    "objectID": "posts/lecs/lec12.html#pruningsparsity-spatten-h2o-moe",
    "href": "posts/lecs/lec12.html#pruningsparsity-spatten-h2o-moe",
    "title": "🧑‍🏫 Lecture 12-13",
    "section": "5.2. Pruning/sparsity: SpAtten, H2O, MoE",
    "text": "5.2. Pruning/sparsity: SpAtten, H2O, MoE\nquantization을 했으면, pruning도 해 봐야 한다.  Wanda는 AWQ처럼 Weight와 Activation을 고려해서 pruning 하는 방식이다   SpAtten은 중요하지 않은 토큰 자체를 삭제하는 방식이다. 오른쪽 attention 맵 기반으로, 가장 낮은 attention 합계를 가진 토큰을 삭제한다.  H2O는 Heavy Hitter Token(H2)를 중심으로 남기고, 나머지를 pruning하는 방식이다. 여기서 말하는 Heavy Hitter란 attention 기반으로 선정한다. 이해하기로는, SpAtten의 방식과 비슷하다고 느꼈다.  DejaVu는 입력에 영향을 받지 않는 attention head들이 존재하고, 이것을 contextual sparsity라고 부르며, 이 패턴을 MLP를 통해 예상할 수 있다는 가설을 세웠다. 이런 contextual sparsity를 제거하는 방식을 DejaVu는 사용했다.  MoE(Mixture of Experts) 는 FFN을 N개로 나누고, Expert를 사용해 그중에 하나를 고르는 개념을 도입한다. 그림 중간에 있는 Router로부터 확률적으로 어떤 FFN을 사용할지 MoE방식은 GPT-4에서 사용하고 있다고 알려져 있다."
  },
  {
    "objectID": "posts/lecs/lec12.html#vllmpaged-attention",
    "href": "posts/lecs/lec12.html#vllmpaged-attention",
    "title": "🧑‍🏫 Lecture 12-13",
    "section": "6.1. vLLM(Paged Attention)",
    "text": "6.1. vLLM(Paged Attention)\n다수의 사용자가 LLM을 사용하는 환경에선 무엇이 문제가 될까?  위 그림처럼 우리는 LLM의 출력이 얼마나 길어질 지 모르기 떄문에, 얼마나 메모리를 할당해야 할 지 알 수 없다. 따라서 &lt;resv&gt; 처럼 내부 단편화, 혹은 다른 요청간의 간격으로 인해 외부 단편화가 발생하게 된다. 마치 실제 운영체제의 메모리같다. 그렇다면, 재밌게도 운영체제에서 사용했던 방법으로 이를 해결할 수 있고, 그 방법이 바로 Page를 사용하는 방식이다.\n OS에서 다른 프로세스간 메모리를 사용할 때 page단위로 사용했듯이, LLM에서도 다른 요청들 간에 KV cache를 page 단위로 사용하면 된다.  위처럼 다른 요청을 page단위로 받을 수 있다.\n 더 놀라운 점은, 하나의 KV Cache를 공유할 수 있다는 점이다. 앞 문장을 공유하거나, 아니면 Prompt같이 많이 사용되는 문장의 KV cache를 공유해 효율적으로 대량의 inference가 가능하다.\n이런 방식을 Paged Attention 이라고 하고, 이 방법을 사용한 것이 vLLM이라는 방법론이다."
  },
  {
    "objectID": "posts/lecs/lec12.html#streamingllm",
    "href": "posts/lecs/lec12.html#streamingllm",
    "title": "🧑‍🏫 Lecture 12-13",
    "section": "6.2. StreamingLLM",
    "text": "6.2. StreamingLLM\nLLM 배포시 또 다른 문제는 길이 문제이다.  엄청나게 긴 문장이나, 혹은 챗봇에서 엄청 예전에 이야기했던 내용까지 기억하려면 메모리가 매우 많이 필요하다. 단순히 transformer 방식을 사용하면(노란색 그래프) 메모리는 선형적으로 증가하고, perplexity는 입력 길이 4K 이후로 폭발적으로 증가한다(training에서 보지 못한 길이이기 때문에) windowed attention(일정 context만 기억, 녹색)은 메모리 사용량은 일정하지만 입력 길이가 window길이를 벗어나는 순간(그림에서는 1K정도) perplexity가 급증한다(첫 몇 토큰이 매우 중요하기 때문에)  a가 위에서 말한 단순 transformer방식, b가 windowed attention이다. c는 sliding window방식인데, 이전 토큰을 메모리에 두는게 아니라 다시 계산하는 방법이다. 이 방법은 perplexity는 괜찮지만, 연산하는데 너무 많은 시간이 든다.\n 이런 문제를 해결하기 위해 찾아내기 위한 아이디어를 Attention Sink에서 찾았다. 위 그림에서 보면 첫번째 토큰의 attention score가 매우 높은 것을 알 수 있다. 그런데, 그들이 문맥적으로(semantically)중요하지 않은 경우에도 그렇다. 이런 현상을 Attention Sink라고 하는데, 왜 일어나는 현상일까?  attention을 구할 때 softmax를 사용하게 되는데, decoding을 하면서 첫번째 토큰은 모든 토큰을 decode 할 때 등장하게 되므로, 당연히 어느정도의 값을 계속 더해가서 생기는 현상이라는 것이다.  그래서 이런 attention sink가 일어나는 첫 토큰은 무조건 남겨두고, windowed attention을 사용하면 더 괜찮은 결과를 얻을 수 있다는 것이다. 이런 현상에 대한 논리적인 설명은 찾지 못했지만, 아마도 첫 토큰이 문맥적으로 중요하지 않더라도 “sink(쌓아두는)”의 역할을 하는 것이라고 생각된다.  ablation study에서는 하나의 토큰이 아니라, 4개의 token을 sink로 하는게 평균적으로 좋다는 결과가 있다"
  },
  {
    "objectID": "posts/lecs/lec12.html#flashattention",
    "href": "posts/lecs/lec12.html#flashattention",
    "title": "🧑‍🏫 Lecture 12-13",
    "section": "6.3. FlashAttention",
    "text": "6.3. FlashAttention\n FlashAttention은 좀 더 하드웨어적인 접근이다. HBM(High Bandwith Memory)에 접근하는 횟수를 줄이는 아이디어이다. 행렬 연산을 할 때 전체 메모리를 불러오는 것이 아니라 하나씩 불러와서(Copy Block to SRAM부분) GPU의 SRAM 내에서 연산을 최대한 마치겠다는 아이디어이다. 앞에서 다뤘던 MQA, GQA등을 적용한 FlashAttention-2라는 논문도 있다."
  },
  {
    "objectID": "posts/lecs/lec12.html#speculative-decoding",
    "href": "posts/lecs/lec12.html#speculative-decoding",
    "title": "🧑‍🏫 Lecture 12-13",
    "section": "6.4. Speculative decoding",
    "text": "6.4. Speculative decoding\nLLM의 decoding은 매우 memory-bounded하다. 하나하나의 토큰을 생성할 때마다 매우 많은 메모리 연산이 필요하다. Speculative Decoding은 이런 문제를 해결하기 위해 작은 모델로 K개의 토큰을 생성한 뒤 큰 모델로 이 토큰이 좋은지 아닌지 판단하고 대안을 생성한다(큰 모델에서는 batch size가 1일 때나 K일때나 비슷하므로)  K개의 token을 생성할 때, 큰 모델을 K번 호출하는 것이 아니라 작은 모델을 K번, 큰 모델은 1번만 호출하면 되므로 decoding 시간을 절약할 수 있다(대략 2~3배)"
  },
  {
    "objectID": "posts/lecs/lec12.html#loraqlora",
    "href": "posts/lecs/lec12.html#loraqlora",
    "title": "🧑‍🏫 Lecture 12-13",
    "section": "7.1. LoRA/QLoRA",
    "text": "7.1. LoRA/QLoRA\n LoRA는 모델 전체를 update하는게 아니라, 작은 bypass branch의 weight만 update하는 방법이다. LLM을 pretrain한 가중치 W가 있고, full-fine tuning했을 때 달라지는 가중치를 delta W라고 하자. 그러면 그 delta W를 low-rank 행렬인 A와 B의 곱(위 그림의 주황색 행렬)으로 나타내자는 아이디어이다.\n QLoRA는 간단히 말하면 LoRA에 quantization을 더한 것이다. NF4(NormalFloat4)라는 normal distribution에 최적화된 데이터 타입, Double Quantization, paged optimizer등의 기법을 사용한다."
  },
  {
    "objectID": "posts/lecs/lec12.html#adapter",
    "href": "posts/lecs/lec12.html#adapter",
    "title": "🧑‍🏫 Lecture 12-13",
    "section": "7.2. Adapter",
    "text": "7.2. Adapter\nAdapter는 transformer 블록에 learnable한 작은 블록을 하나 끼워 넣는 것이다.  위 그림에서 어댑터는 오른쪽 노란색 구조를 넣은 것이다. 하지만 새로운 layer가 추가되는 것이라, inference시 시간이 조금 더 늘어날 수 있다는 문제점이 있다."
  },
  {
    "objectID": "posts/lecs/lec12.html#prompt-tuning",
    "href": "posts/lecs/lec12.html#prompt-tuning",
    "title": "🧑‍🏫 Lecture 12-13",
    "section": "7.3. Prompt Tuning",
    "text": "7.3. Prompt Tuning\n 위의 방법들과는 다르게, tuning없이 prompt만 입력해서 특정한 task에 대한 성능을 높이는 방법이다. 예를 들어, “뒤에 문장을 요약해줘 :” 라는 문장을 입력에 추가하면 요약 task에 대한 성능이 올라간다. 이를 활요하면, 하나의 모델로 여러가지 task에 대응할 수 있게 되며, 모델이 커질수록 해당 task에 대해서만 fine-tuning한 모델이랑 비슷한 성능을 내게 된다."
  },
  {
    "objectID": "posts/lecs/lec05.html",
    "href": "posts/lecs/lec05.html",
    "title": "🧑‍🏫 Lecture 5-6",
    "section": "",
    "text": "이번 글에서는 MIT HAN LAB에서 강의하는 TinyML and Efficient Deep Learning Computing에 나오는 Quantization 방법을 소개하려 한다. Quantization(양자화) 신호와 이미지에서 아날로그를 디지털로 변환하는 과정에서 사용하는 개념이다. 아래 그림과 같이 연속적인 센서로 부터 들어오는 아날로그 데이터 나 이미지를 표현하기 위해 단위 시간에 대해서 데이터를 샘플링하여 데이터를 수집한다.\n디지털로 데이터를 변환하기 위해 데이터 타입을 정하면서 이를 하나씩 양자화한다. 양수와 음수를 표현하기 위해 Unsigned Integer 에서 Signed Integer, Signed에서도 Sign-Magnitude 방식과 Two’s Complement방식으로, 그리고 더 많은 소숫점 자리를 표현하기 위해 Fixed-point에서 Floating point로 데이터 타입에서 수의 범주를 확장시킨다. 참고로 Device의 Computationality와 ML 모델의 성능지표중 하나인 FLOP이 바로 floating point operations per second이다.\n이 글에서 floating point를 이해하면, fixed point를 사용하는 것이 매모리에서, 그리고 연산에서 더 효율적일 것이라고 예상해볼 있 수 있다. ML모델을 클라우드 서버에서 돌릴 때는 크게 문제되지 않았지만 아래 두 가지 표를 보면 에너지소모, 즉 배터리 효율에서 크게 차이가 보인다. 그렇기 때문에 모델에서 Floating point를 fixed point로 더 많이 바꾸려고 하는데 이 방법으로 나온 것이 바로 Quatization이다.\n이번 글에서는 Quntization 중에서 Quantization 방법과 그 중 Linear한 방법에 대해 더 자세하게, 그리고 Post-training Quantization까지 다루고, 다음 글에서는 Quantization-Aware Training, Binary/Tenary Quantization, Mixed Precision Quantization까지 다루려고 한다."
  },
  {
    "objectID": "posts/lecs/lec05.html#common-network-quantization",
    "href": "posts/lecs/lec05.html#common-network-quantization",
    "title": "🧑‍🏫 Lecture 5-6",
    "section": "1. Common Network Quantization",
    "text": "1. Common Network Quantization\n앞서서 소개한 것처럼 Neural Netowork를 위한 Quantization은 다음과 같이 나눌 수 있다. Quantization 방법을 하나씩 알아보자.\n\n\n\nReference. MIT-TinyML-lecture5-Quantization-1\n\n\n\n\n\n Reference. MIT-TinyML-lecture5-Quantization-1 in https://efficientml.ai \n\n\n\n1.1 K-Means-based Quantization\n그 중 첫 번째로 K-means-based Quantization이 있다. Deep Compression [Han et al., ICLR 2016] 논문에 소개했다는 이 방법은 중심값을 기준으로 clustering을 하는 방법이다. 예제를 봐보자.\n\n\n\nReference. MIT-TinyML-lecture5-Quantization-1\n\n\n위 예제는 weight를 codebook에서 -1, 0, 1.5, 2로 나눠 각각에 맞는 인덱스로 표기한다. 이렇게 연산을 하면 기존에 64bytes를 사용했던 weight가 20bytes로 줄어든다. codebook으로 예제는 2bit로 나눴지만, 이를 N-bit만큼 줄인다면 우리는 총 32/N배의 메모리를 줄일 수 있다. 하지만 이 과정에서 quantizatio error, 즉 quantization을 하기 전과 한 후에 오차가 생기는 것을 위 예제에서 볼 수 있다. 메모리 사용량을 줄이는 것도 좋지만, 이 때문에 성능에 오차가 생기지 않게 하기위해 이 오차를 줄이는 것 또한 중요하다.\n\n\n\nReference. MIT-TinyML-lecture5-Quantization-1\n\n\n이를 보완하기 위해 Quantized한 Weight를 위에 그림처럼 Fine-tuning하기도 한다. centroid를 fine-tuning한다고 생각하면 되는데, 각 centroid에서 생기는 오차를 평균내 tuning하는 방법이다. 이 방법을 제안한 논문 에서는 Convolution 레이어에서는 4bit까지 centroid를 가졌을 때, Full-Connected layer에서는 2 bit까지 centroid를 가졌을 때 성능에 하락이 없다고 말하고 있었다.\n\n\n\nReference. Deep Compression [Han et al., ICLR 2016]\n\n\n이렇게 Quantization 된 Weight는 위처럼 연속적인 값에서 아래처럼 Discrete한 값으로 바뀐다.\n\n\n\nReference. Deep Compression [Han et al., ICLR 2016]\n\n\n논문은 이렇게 Quantization한 weight를 한 번 더 Huffman coding를 이용해 최적화시킨다. 짧게 설명하자면, 빈도수가 높은 문자는 짧은 이진코드를, 빈도 수가 낮은 문자에는 긴 이진코드를 쓰는 방법이다. 압축 결과로 General한 모델과 압축 비율이 꽤 큰 SqueezeNet을 예로 든다. 자세한 내용은 논문을 참고하는 걸로.\n\n\n\nReference. Deep Compression [Han et al., ICLR 2016]\n\n\n\n\n\nReference. Deep Compression [Han et al., ICLR 2016]\n\n\ninference를 위해 weight를 Decoding하는 과정은 inference과정에서 저장한 cluster의 인덱스를 이용해 codebook에서 해당하는 값을 찾아내는 것이다. 이 방법은 저장 공간을 줄일 수는 있지만, floating point Computation이나 메모리 접근하는 방식으로 centroid를 쓰는 한계가 있을 수 밖에 없다.\n\n\n\nReference. MIT-TinyML-lecture5-Quantization-1\n\n\n\n\n\n Reference. Deep Compression [Han et al., ICLR 2016] \n\n\n\n\n1.2 Linear Quantization\n두 번째 방법은 Linear Quatization이다. floating-point인 weight를 N-bit의 정수로 affine mapping을 시키는 방법이다. 간단하게 식으로 보는 게 더 이해가 쉽다.\n\n\n\nReference. MIT-TinyML-lecture5-Quantization-1\n\n\n여기서 S(Scale of Linear Quantization)와 Z(Zero point of Linear Quantization)가 있는데 이 둘이 quantization parameter 로써 tuning을 할 수 있는 값인 것이다.\n\n\n\nReference. MIT-TinyML-lecture5-Quantization-1\n\n\n\n\n1.3 Scale and Zero point\n\n\n\nReference. MIT-TinyML-lecture5-Quantization-1\n\n\n이 Scale과 Zero point 두 파라미터를 이용해서 affine mapping은 위 그림과 같다. Bit 수(Bit Width)가 낮아지면 낮아질 수록, floating point에서 표현할 있는 수 또한 줄어들 것이다. 그렇다면 Scale와 Zero point는 각각 어떻게 계산할까?\n우선 floating-point 인 숫자의 범위 중 최대값과 최솟값에 맞게 두 식을 세우고 이를 연립방정식으로 Scale과 Zero point을 구할 수 있다.\n\nScale point \\[\n  r_{max} = S(q_{max}-Z)\n  \\] \\[\n  r_{min} = S(q_{min}-Z)\n  \\]\n\\[\n  r_{max} - r_{min} = S(q_{max} - q_{min})\n  \\]\n\\[\n  S = \\dfrac{r_{max}-r_{min}}{q_{max}-q_{min}}\n  \\]\nZero point \\[\n  r_{min} = S(q_{min}-Z)\n  \\]\n\\[\n  Z=q_{min}-\\dfrac{r_{min}}{S}\n  \\]\n\\[\n  Z = round\\Big(q_{min}-\\dfrac{r_{min}}{S}\\Big)\n  \\]\n\n예를 들어, 아래와 같은 예제에서 \\(r_{max}\\) 는\\(2.12\\) 이고 \\(r_{min}\\) 은 \\(-1.08\\) 로 Scale을 계산하면 아래 그림처럼 된다. Zero point는 \\(-1\\) 로 계산할 수 있다.\n\n\n\n\nReference. MIT-TinyML-lecture5-Quantization-1\n\n\n그럼 Symmetric하게 r의 범위를 제한하는 것과 같은 다른 Linear Quantization은 없을까? 이를 앞서, Quatized된 값들이 Matrix Multiplication을 하면서 미리 계산될 수 있는 수 (Quantized Weight, Scale, Zero point)가 있으니 inference시 연산량을 줄이기 위해 미리 계산할 수 있는 파라미터는 없을까?\n\n\n1.4 Quantized Matrix Multiplication\n입력 X, Weight W, 결과 Y가 Matrix Multiplication을 했다고 할 때 식을 계산해보자.\n\\[\nY=WX\n\\]\n\\[\nS_Y(q_Y-Z_Y) = S_W(q_W-Z_W) \\cdot S_X(q_X-Z_X\n\\]\n\\[\n\\vdots\n\\]\n\n\n\nReference. MIT-TinyML-lecture5-Quantization-1\n\n\n여기서 마지막 정리한 식을 살펴보면,\n\\(Z_x\\) 와 \\(q_w, Z_w, Z_X\\) 의 경우는 미리 연산이 가능하다. 또 \\(S_wS_X/S_Y\\) 의 경우 항상 수의 범위가 \\((0, 1)\\) 로 \\(2^{-n}M_0\\) , \\(M_0 \\in [0.5, 1)\\) 로 변형하면 N-bit Integer로 Fixed-point 형태로 표현 가능하다. 여기에 \\(Z_w\\)가 0이면 어떨까? 또 미리 계산할 수 있는 항이 보인다.\n\n\n1.5 Symmetric Linear Quantization\n\n\n\nReference. MIT-TinyML-lecture5-Quantization-1\n\n\n\\(Z_w = 0\\) 이라고 함은 바로 위와 같은 Weight 분포인데, 바로 Symmetric한 Linear Quantization으로 \\(Z_w\\)를 0으로 만들어 \\(Z_w q_x\\)항을 0으로 둘 수 있어 연산을 또 줄일 수 있을 것이다.\nSymmetric Linear Quantization은 주어진 데이터에서 Full range mode와 Restrict range mode로 나뉜다.\n첫 번째 Full range mode 는 Scale을 real number(데이터, weight)에서 범위가 넓은 쪽에 맞추는 것이다. 예를 들어 아래의 경우, r_min이 r_max보다 절댓값이 더 크기 때문에 r_min에 맞춰 q_min을 가지고 Scale을 구한다. 이 방법은 Pytorch native quantization과 ONNX에서 사용된다고 강의에서 소개한다.\n\n\n\nReference. MIT-TinyML-lecture5-Quantization-1\n\n\n두 번째 Restrict range mode는 Scale을 real number(데이터, weight)에서 범위가 좁은 쪽에 맞추는 것이다. 예를 들어 아래의 경우, r_min가 r_max보다 절댓값이 더 크기 때문에 r_min에 맞추면서 q_max에 맞도록 Scale을 구한다. 이 방법은 TensorFlow, NVIDIA TensorRT, Intel DNNL에서 사용된다고 강의에서 소개한다.\n\n\n\nReference. MIT-TinyML-lecture5-Quantization-1\n\n\n그렇다면 왜 Symmetric 써야할까? Asymmetric 방법과 Symmetric 방법의 차이는 뭘까? (feat. Neural Network Distiller) 아래 그림을 참고하면 되지만, 가장 큰 차이로 보이는 것은 Computation vs Compactful quantized range로 이해간다.\n\n\n\nReference. MIT-TinyML-lecture5-Quantization-1\n\n\n\n\n1.6 Linear Quantization examples\n그럼 Quatization 방법에 대해 알아봤으니 이를 Full-Connected Layer, Convolution Layer에 적용해보고 어떤 효과가 있는지 알아보자.\n\n1.6.1 Full-Connected Layer\n아래처럼 식을 전개해보면 미리 연산할 계산할 수 있는 항과 N-bit integer로 표현할 있는 항으로 나눌 수 있다(전개하는 이유는 아마 미리 계산할 수 있는 항을 알아보기 위함이 아닐까 싶다).\n\\[\nY=WX+b\n\\]\n\\[\n\\downarrow\n\\]\n\\[\nS_Y(q_Y - Z_Y) = S_W(q_W - Z_W) \\cdot S_X(q_X - Z_X) + S_b(q_b - Z_b)\n\\]\n\\[\n\\downarrow \\ Z_w=0\n\\]\n\\[\nS_Y(q_Y - Z_Y) = S_WS_X(q_Wq_X - Z_xq_W) + S_b(q_b - Z_b)\n\\]\n\\[\n\\downarrow \\ Z_b=0, S_b=S_WS_X\n\\]\n\\[\nS_Y(q_Y - Z_Y) = S_WS_X(q_Wq_X - Z_xq_W+q_b)\n\\]\n\\[\n\\downarrow\n\\]\n\\[\nq_Y = \\dfrac{S_WS_X}{S_Y}(q_Wq_X+q_b - Z_Xq_W) + Z_Y\n\\]\n\\[\n\\downarrow \\ q_{bias}=q_b-Z_xq_W\\\\\n\\]\n\\[\nq_Y = \\dfrac{S_WS_X}{S_Y}(q_Wq_X+ q_{bias}) + Z_Y\\\\\n\\]\n간단히 표기하기 위해 \\(Z_W=0, Z_b=0, S_b = S_W S_X\\) 이라고 가정한다.\n\n\n\nReference. MIT-TinyML-lecture5-Quantization-1\n\n\n\n\n1.6.2 Convolutional Layer\nConvolution Layer의 경우는 Weight와 X의 곱의 경우를 Convolution으로 바꿔서 생각해보면 된다. 그도 그럴 것이 Convolution은 Kernel과 Input의 곱의 합으로 이루어져 있기 때문에 Full-Connected와 거의 유사하게 전개될 수 있을 것이다.\n\n\n\nReference. MIT-TinyML-lecture5-Quantization-1"
  },
  {
    "objectID": "posts/lecs/lec05.html#post-training-quantization-ptq",
    "href": "posts/lecs/lec05.html#post-training-quantization-ptq",
    "title": "🧑‍🏫 Lecture 5-6",
    "section": "2. Post-training Quantization (PTQ)",
    "text": "2. Post-training Quantization (PTQ)\n그럼 앞서서 Quantizaed한 Layer를 Fine tuning할 없을까? “How should we get the optimal linear quantization parameters (S, Z)?” 이 질문에 대해서 Weight, Activation, Bias 세 가지와 그에 대하여 논문에서 보여주는 결과까지 알아보자.\n\n2.1 Weight quantization\nTL;DR. 이 강의에서 소개하는 Weight quantization은 Grandularity에 따라 Whole(Per-Tensor), Channel, 그리고 Layer로 들어간다.\n\n2.1.1 Granularity\nWeight quantization에서 Granularity에 따라서 Per-Tensor, Per-Channel, Group, 그리고 Generalized 하는 방법으로 확장시켜 Shared Micro-exponent(MX) data type을 차례로 보여준다. Scale을 몇 개나 둘 것이냐, 그 Scale을 적용하는 범위를 어떻게 둘 것이냐, 그리고 Scale을 얼마나 디테일하게(e.g. floating-point)할 것이냐에 초점을 둔다.\n\n\n\nReference. MIT-TinyML-lecture5-Quantization-2\n\n\n첫 번째는 Per-Tensor Quantization 특별하게 설명할 것 없이 이전까지 설명했던 하나의 Scale을 사용하는 Linear Quantization이라고 생각하면 되겠다. 특징으로는 Large model에 대해서는 성능이 괜찮지만 작은 모델로 떨어지면 성능이 급격하게 떨어진다고 설명한다. Channel별로 weight 범주가 넓은 경우나 outlier weight가 있는 경우 quantization 이후에 성능이 하락했다고 말한다.\n\n\n\nReference. MIT-TinyML-lecture5-Quantization-2\n\n\n그래서 그 해결방안으로 나오는 것이 두 번째 방법인 Per-Channel Quantization이다. 위 예제에서 보면 Channel 마다 최대값과 각각에 맞는 Scale을 따로 가지는 것을 볼 수 있다. 그리고 적용한 결과인 아래 그림을 보면 Per-Channel과 Per-Tensor를 비교해보면 Per-Channel이 기존에 floating point weight와의 차이가 더 적다. 하지만, 만약 하드웨어에서 Per-Channel Quantization을 지원하지 않는다면 불필요한 연산을 추가로 해야하기 때문에 이는 적합한 방법이 될 수 없다는 점도 고려해야할 것이다(이는 이전 Tiny Engine에 대한 글에서 Channel내에 캐싱을 이용한 최적화와 연관이 있다). 그럼 또 다른 방법은 없을까?\n\n\n\nReference. MIT-TinyML-lecture5-Quantization-2\n\n\n세 번째 방법은 Group Quantization으로 소개하는 Per-vector Scaled Quantization와 Shared Micro-exponent(MX) data type 이다. Per-vector Scaled Quantization은 2023년도 강의부터 소개하는데, 이 방법은 Scale factor를 그룹별로 하나, Per-Tensor로 하나로 두개를 두는 방법이다. 아래의 그림을 보면,\n\n\n\nReference. MIT-TinyML-lecture5-Quantization-2\n\n\n\\[\nr=S(q-Z) \\rightarrow r=\\gamma \\cdot S_{q}(q-Z)\n\\]\n\\(S_q\\) 로 vector별 스케일링을 하나, \\(\\gamma\\) 로 Tensor에 스케일링을 하며 감마는 floating point로 하는 것을 볼 수있다. 아무래도 vector단위로 스케일링을 하게되면 channel과 비교해서 하드웨어 플랫폼에 맞게 accuracy의 trade-off를 조절하기 더 수월할 것으로 보인다.\n여기서 강의는 지표인 Memory Overhead로 “Effective Bit Width”를 소개한다. 이는 Microsoft에서 제공하는 Quantization Approach MX4, MX6, MX9과 연결돼 있는데, 이 데이터타입은 조금 이후에 더 자세히 설명할 것이다. Effective Bit Width? 예시 하나를 들어 이해해보자. 만약 4-bit Quatization을 4-bit per-vector scale을 16 elements(4개의 weight가 각각 4bit를 가진다고 생각하면 16 element로 계산된다 유추할 있다) 라면, Effective Bit Width는 4(Scale bit) + 4(Vector Scale bit) / 16(Vector Size) = 4.25가 된다. Element당 Scale bit라고 간단하게 생각할 수도 있을 듯 싶다.\n마지막 Per-vector Scaled Quantization을 이해하다보면 이전에 Per-Tensor, Per-Channel도 그룹으로 얼마만큼 묶는 차이가 있고, 이는 이들을 일반화할 수 있어 보인다. 강의에서 바로 다음에 소개하는 방법이 바로 Multi-level scaling scheme이다. Per-Channel Quantization와 Per-Vector Quantization(VSQ, Vector-Scale Quantization)부터 봐보자.\n\n\n\nReference. With Shared Microexponents, A Little Shifting Goes a Long Way [Bita Rouhani et al.]\n\n\nPer-Channel Quantization는 Scale factor가 하나로 Effective Bit Width는 4가 된다. 그리고 VSQ는 이전에 계산했 듯 4.25가 될 것이다(참고로 Per Channel로 적용되는 Scale의 경우 element의 수가 많아서 그런지 따로 Effective Bit Width로 계산하지는 않는다). VSQ까지 보면서 Effective Bit Width는,\nEffective Bit Width = Scale bit + Group 0 Scale bit / Group 0 Size +...\ne.g. VSQ Data type int4 = Scale bit (4) + Group 0 Scale bit(4) / Group 0 Size(16) = 4.25\n이렇게 계산할 수 있다. 그리고, MX4, MX6, MX9가 나온다. 참고로 S는 Sign bit, M은 Mantissa bit, E는 Exponent bit를 의미한다(Mantissa나 Exponent에 대한 자세한 내용은 floating point vs fixed point 글을 참고하자). 아래는 Microsoft에서 제공하는 Quantization Approach MX4, MX6, MX9에 대한 표이다.\n\n\n\nReference. MIT-TinyML-lecture5-Quantization-1\n\n\n\n\n2.1.2 Weight Equalization\n여기까지 Weight Quatization에서 그룹으로 얼마만큼 묶는지에 따라(강의에서는 Granularity) Quatization을 하는 여러 방법을 소개했다. 다음으로 소개 할 방법은 Weight Equalization이다. 2022년에 소개해준 내용인데, 이는 i번째 layer의 output channel를 scaling down 하면서 i+1번째 layer의 input channel을 scaling up 해서 Scale로 인해 Quantization 전후로 생기는 Layer간 차이를 줄이는 방법이다.\n\n\n\nReference. Data-Free Quantization Through Weight Equalization and Bias Correction [Markus et al., ICCV 2019]\n\n\n예를 들어 위에 그림처럼 Layer i의 output channel과 Layer i+1의 input channel이 있다. 여기서 식을 전개하면 아래와 같은데,\n\\[\n\\begin{aligned}\ny^{(i+1)}&=f(W^{(i+1)}x^{(i+1)}+b^{(i+1)}) \\\\\n         &=f(W^{(i+1)} \\cdot f(W^{(i)}x^{(i)}+b^{(i)}) +b^{(i+1)}) \\\\\n         &=f(W^{(i+1)}S \\cdot f(S^{-1}(W^{(i)}x^{(i)}+S^{-1}b^{(i)})) +b^{(i+1)})\n\\end{aligned}\n\\]\nwhere \\(S = diag(s)\\) , \\(s_j\\) is the weight equalization scale factor of output channel \\(j\\)\n여기서 Scale(S)가 i+1번째 layer의 weight에, i번째 weight에 1/S 로 Scale될 떄 기존에 Scale 하지 않은 식과 유사하게 유지할 있는 것을 볼 수 있다. 즉,\n\\[\nr^{(i)}_{oc_j} / s = r^{(i+1)}_{ic_j} \\cdot s\n\\]\n\\[\ns_j = \\dfrac{1}{r^{(i+1)}_{ic=j}}\\sqrt{r_{oc=j}^{(i)}\\cdot r_{ic=j}^{(i+1)}}\n\\]\n\\[\nr^{(i)}_{oc_j} = r^{(i)}_{oc_j} / s = \\sqrt{r_{oc=j}^{(i)}\\cdot r_{ic=j}^{(i+1)}}\n\\]\n\\[\nr^{(i)}_{ic_j} =r^{(i)}_{ic_j} \\cdot s = \\sqrt{r_{oc=j}^{(i)}\\cdot r_{ic=j}^{(i+1)}}\n\\]\n이렇게 하면 i번째 layer의 output channel과 i+1번째 layer의 input channel의 Scale을 각각 \\(S\\) 와 \\(1/S\\) 로하며 weight간의 격차를 줄일 수 있다.\n\n\n2.1.3 Adaptive rounding\n\n\n\nReference. MIT-TinyML-lecture5-Quantization-1\n\n\n마지막 소개하는 방법은 Adaptive rounding 이다. 반올림은 Round-to-nearest으로 불리는 일반적인 반올림을 생각할 수 있고, 하나의 기준을 가지고 반올림을 하는 Adaptive Round를 생각할 할 수 있다. 강의에서는 Round-to-nearest가 최적의 방법이 되지 않는다고 말하며, Adaptive round로 weight에 0부터 1 사이의 값을 더해 수식처럼 \\(\\tilde{w} = \\lfloor\\lfloor  w\\rfloor + \\delta\\rceil, \\delta \\in [0, 1]\\) 최적의 Optimal한 반올림 값을 구한다. $$\n\\[\\begin{aligned}\n&argmin_V\\lvert\\lvert Wx-\\tilde Wx\\lvert\\lvert ^2_F + \\lambda f_{reg}(V) \\\\\n\n\\rightarrow & argmin_V\\lvert\\lvert Wx-\\lfloor\\lfloor W \\rfloor + h(V) \\rceil x\\lvert\\lvert ^2_F + \\lambda f_{reg}(V)\n\\end{aligned}\\]\n$$ ### 2.2 Activation quantization 두 번째로 Activation quantization이 있다. 모델결과로 나오는 결과를 직접적으로 결정하는 Activation Quatization에서는 두 가지를 고려한 방법을 소개한다. 하나는 Activation 레이어에서 결과값을 Smoothing한 분포를 가지게 하기 위해 Exponential Moving Average(EMA)를 사용하는 방법이고, 다른 하나는 다양한 입력값을 고려해 batch samples을 FP32 모델과 calibration하는 방법이다.\nExponential Moving Average (EMA)은 아래 식에서 \\(\\alpha\\) 를 구하는 방법이다. \\[\n\\hat r^{(t)}_{max, min} = \\alpha r^{(t)}_{max, min} + (1-\\alpha) \\hat r^{(t)}_{max, min}  \n\\] Calibration의 컨셉은 많은 input의 min/max 평균을 이용하자는 것이다. 그래서 trained FP32 model과 sample batch를 가지고 quantized한 모델의 결과와 calibration을 돌리면서 그 차이를 최소화 시키는데, 여기에 이용하는 지표는 loss of information와 Newton-Raphson method를 사용한 Mean Square Error(MSE)가 있다. \\[\nMSE = \\underset{\\lvert r \\lvert_{max}}{min}\\ \\mathbb{E}[(X-Q(X))^2]\n\\] \\[\nKL\\ divergence=D_{KL}(P\\lvert\\lvert Q) = \\sum_i^N P(x_i)log\\dfrac{P(x_i)}{Q(x_i)}\n\\] ### 2.3 Quanization Bias Correction\n마지막으로 Quatization으로 biased error를 잡는다는 것을 소개한다. \\(\\epsilon = Q(W)-W\\) 이라고 두고 아래처럼 식이 전개시키면 마지막 항에서 보이는 \\(-\\epsilon\\mathbb{E}[x]\\) 부분이 bias를 quatization을 할 때 제거 된다고 한다(이 부분은 2023년에는 소개하진 않는데, 당연한 것이어서 안하는지, 혹은 영향이 크지 않아서 그런지는 모르겠다. Bias Quatization이후에 MobileNetV2에서 한 레이어의 output을 보면 어느정도 제거되는 것처럼 보인다). \\[\n\\begin{aligned}\n\\mathbb{E}[y] &= \\mathbb{E}[Wx] + \\mathbb{E}[\\epsilon x] - \\mathbb{E}[\\epsilon x],\\ \\mathbb{E}[Q(W)x] = \\mathbb{E}[Wx] + \\mathbb{E}[\\epsilon x] \\\\\n\\mathbb{E}[y] &= \\mathbb{E}[Q(W)x] - \\epsilon\\mathbb{E}[x]\n\\end{aligned}\n\\]\n\n\n\nReference. MIT-TinyML-lecture5-Quantization-2\n\n\n\n\n\n2.4 Post-Training INT8 Linear Quantization Result\n앞선 Post-Training Quantization을 적용한 결과를 보여준다. 이미지계열 모델을 모두 사용했으며, 성능하락폭은 지표로 보여준다. 비교적 큰 모델들의 경우 준수한 성능을 보여주지만 MobileNetV1, V2와 같은 작은 모델은 생각보다 Quantization으로 떨어지는 성능폭(-11.8%, -2.1%) 이 큰 것을 볼 수 있다. 그럼 작은 크기의 모델들은 어떻게 Training 해야할까?\n\n\n\nReference. MIT-TinyML-lecture5-Quantization-2"
  },
  {
    "objectID": "posts/lecs/lec05.html#quantization-aware-trainingqat",
    "href": "posts/lecs/lec05.html#quantization-aware-trainingqat",
    "title": "🧑‍🏫 Lecture 5-6",
    "section": "3. Quantization-Aware Training(QAT)",
    "text": "3. Quantization-Aware Training(QAT)\n\n3.1 Quantization-Aware Training\n\n\n\nReference. MIT-TinyML-lecture06-Quantization-2\n\n\n\nUsually, fine-tuning a pre-trained floating point model provides better accuracy than training from scratch.\n\n이전에 K-mean Quantization에서 Fine-tuning때 Centroid에 gradient를 반영했었다. Quantization-Aware Training은 이와 유사하게 Quantization - Reconstruction을 통해 만들어진 Weight로 Training을 하는 방법을 말한다. 예시를 들어서 자세히 살펴보자.\n\n\n\nReference. MIT-TinyML-lecture06-Quantization-2\n\n\n\nA full precision copy of the weights W is maintained throughout the training.\nThe small gradients are accumulated without loss of precision\nOnce the model is trained, only the quantized weights are used for inference\n\n위 그림에서 Layer N이 보인다. 이 Layer N은 weights를 파라미터로 가지지만, 실제로 Training 과정에서 쓰이는 weight는 “weight quantization”을 통해 Quantization - Reconstruction을 통해 만들어진 Weight를 가지고 훈련을 할 것이다.\n\n\n3.2 Straight-Through Estimator(STE)\n\n\n\nReference. MIT-TinyML-lecture06-Quantization-2\n\n\n그럼 훈련에서 gradient는 어떻게 전달할 수 있을까? Quantization의 개념상, weight quantization에서 weight로 넘어가는 gradient는 없을 수 밖에 없다. 그렇게 되면 사실상 weight로 back propagation이 될 수 없게 되고, 그래서 소개하는 개념이 Straight-Through Estimator(STE) 입니다. 말이 거창해서 그렇지, Q(W)에서 받은 gradient를 그대로 weights 로 넘겨주는 방식이다.\n\nQuantization is discrete-valued, and thus the derivative is 0 almost everywhere → NN will learn nothing!\nStraight-Through Estimator(STE) simply passes the gradients through the quantization as if it had been the identity function.\n\\[\n  g_W = \\dfrac{\\partial L}{\\partial  W} = \\dfrac{\\partial L}{\\partial  Q(W)}\n  \\]\n\n\n\n\nReference. MIT-TinyML-lecture06-Quantization-2\n\n\n\nReference\n\nNeural Networks for Machine Learning [Hinton et al., Coursera Video Lecture, 2012]\nEstimating or Propagating Gradients Through Stochastic Neurons for Conditional Computation [Bengio, arXiv 2013]\n\n\n이 훈련의 결과가 궁금하시다면 이 논문을 참고하자. 참고로 논문에서는 MobileNetV1, V2 그리고 NASNet-Mobile을 이용해 Post-Training Quantization과 Quantization-Aware Training을 비교하고 있다."
  },
  {
    "objectID": "posts/lecs/lec05.html#binary-and-ternary-quantization",
    "href": "posts/lecs/lec05.html#binary-and-ternary-quantization",
    "title": "🧑‍🏫 Lecture 5-6",
    "section": "4. Binary and Ternary Quantization",
    "text": "4. Binary and Ternary Quantization\n자, 그럼 Quantization을 궁극적으로 2bit로 할 수는 없을까? 바로 Binary(1, -1)과 Tenary(1, 0, -1) 이다.\n\nCan we push the quantization precision to 1 bit?\n\n\n\nReference. MIT-TinyML-lecture06-Quantization-2\n\n\nReference\n\nBinaryConnect: Training Deep Neural Networks with Binary Weights during Propagations [Courbariaux et al., NeurIPS 2015]\nXNOR-Net: ImageNet Classification using Binary Convolutional Neural Networks [Rastegari et al., ECCV 2016]\n\n\n먼저 Weight를 2bit로 Quantization을 하게 되면, 메모리에서는 32bit를 1bit로 줄이니 32배나 줄일 수 있고, Computation도 (8x5)+(-3x2)+(5x0)+(-1x1)에서 5-2+0-1 로 절반을 줄일 수 있다.\n\n4.1 Binarization: Deterministic Binarization\n그럼 Binarization에서 +1과 -1을 어떤 기준으로 해야할까? 가장 쉬운 방법은 threhold를 기준으로 +-1로 나누는 것이다.\nDirectly computes the bit value base on a threshold, usually 0 resulting in a sign function.\n\\[\nq = sign(r) = \\begin{dcases}\n+1, &r \\geq 0 \\\\\n-1, &r &lt; 0\n\\end{dcases}\n\\]\n\n\n4.2 Binarization: Stochastic Binarization\n다른 방법으로는 output에서 hard-sigmoid function을 거쳐서 나온 값만큼 확률적으로 +-1이 나오도록 하는 것이다. 하지만 이 방법은 무작위로 비트를 생성하는 하드웨어를 하는 것이 어렵기 때문에 사용하진 않는다고 언급한다.\n\nUse global statistics or the value of input data to determine the probability of being -1 or +1\nIn Binary Connect(BC), probability is determined by hard sigmoid function \\(\\sigma(r)\\)\n\\[\n  q=\\begin{dcases}\n  +1, &\\text{with probability } p=\\sigma(r)\\\\\n  -1, & 1-p\n  \\end{dcases}\n  \\\\\n  where\\ \\sigma(r)=min(max(\\dfrac{r+1}{2}, 0), 1)\n  \\]\n\n\n\nReference. MIT-TinyML-lecture06-Quantization-2\n\n\nHarder to implement as it requires the hardware to generate random bits when quantizing.\n\n\n\n4.3 Binarization: Use Scale\n앞선 방법을 이용해서 ImageNet Top-1 을 평가해보면 Quantization이후 -21.2%나 성능이 하락하는 걸 볼 수 있다. “어떻게 보완할 수 있을까?” 한 것이 linear qunatization에서 사용했던 Scale 개념이다.\n\nUsing Scale, Minimizing Quantization Error in Binarization\n\n\n\nReference. MIT-TinyML-lecture06-Quantization-2\n\n\n\n여기서 Scale은 \\(\\dfrac{1}{n}\\lvert\\lvert W \\lvert\\lvert_1\\) 로 계산할 수 있고, 성능은 하락이 거의 없는 것도 확인할 수 있다. 왜 \\(\\dfrac{1}{n}\\lvert\\lvert W \\lvert\\lvert_1\\)인지는 아래 증명과정을 참고하자!\n\nWhy \\(\\alpha\\) is \\(\\dfrac{1}{n}\\lvert\\lvert W \\lvert\\lvert_1\\)?\n\\[\n  \\begin{aligned}\n  &J(B, \\alpha)=\\lvert\\lvert W-\\alpha B\\lvert\\lvert^2 \\\\\n  &\\alpha^*, B^*= \\underset{\\alpha, B}{argmin}\\ J(B, \\alpha) \\\\\n  &J(B,\\alpha) = \\alpha^2B^TB-2\\alpha W^T B + W^TW\\ since\\ B \\in \\{+1, -1\\}^n \\\\\n  &B^TB=n(constant), W^TW= constant(a \\ known\\ variable) \\\\\n  &J(B,\\alpha) = \\alpha^2n-2\\alpha W^T B + C \\\\\n  &B^* = \\underset{B}{argmax} \\{W^T B\\}\\ s.t.\\ B\\in \\{+1,-1 \\}^n \\\\\n  &\\alpha^*=\\dfrac{W^TB^*}{n} \\\\\n  &\\alpha^*=\\dfrac{W^Tsign(W)}{n} = \\dfrac{\\lvert W_i \\lvert}{n} = \\dfrac{1}{n}\\lvert\\lvert W\\lvert\\lvert_{l1}\n  \\end{aligned}\n  \\]\n\nReference. XNOR-Net: ImageNet Classification using Binary Convolutional Neural Networks [Rastegari et al., ECCV 2016]\nB*는 J(B,\\(\\alpha\\))에서 최솟값을 구해야하므로 \\(W^T\\)B 가 최대여야하고 그러기 위해서는 W가 양수일때는 B도 양수, W가 음수일 때는 B도 음수여야 \\(W^TB=\\sum\\lvert W \\lvert\\) 이 되면서 최댓값이 될 수 있다.\n\n\n\n\n4.4 Binarization: Activation\n그럼 Activation까지 Quantization을 해봅시다.\n4.4.1 Activation\n\n\n\nUntitled\n\n\n여기서 조금 더 연산을 최적화 할 수 있어보이는 것이 Matrix Muliplication이 XOR 연산과 비슷하게 보인다.\n4.4.2 XNOR bit count\n\n\n\nReference. MIT-TinyML-lecture06-Quantization-2\n\n\n\n\\(y_i=-n+ popcount(W_i\\ xnor\\ x) &lt;&lt; 1\\) → popcount returns the number of 1\n\n그래서 popcount과 XNOR을 이용해서 Computation에서 좀 더 최적화를 진행합니다. 이렇게 최적화를 진행하게 되면, 메모리는 32배, Computation은 58배가량 줄어들 수 있다고 말한다.\n\n\n\nReference. MIT-TinyML-lecture06-Quantization-2\n\n\n이렇게 Weight, Scale factor, Activation, 그리고 XNOR-Bitcout 까지. 총 네 가지 단계로 Binary Quantization을 나눈다. 다음으로는 Ternary Quantization은 알아보자.\n\n\n\nReference. XNOR-Net: ImageNet Classification using Binary Convolutional Neural Networks [Rastegari et al., ECCV 2016]\n\n\n\n\nBinarizing Input 의 경우는 average를 모든 channel에 같이 적용할 것이기 때문에 그 c만큼을 average filter로 한 번에 적용한다는 말이다.\n\n\n\n\n4.5 Ternary Weight Networks(TWN)\nTernary는 Binary Quantization과 단계는 모두 같지만, 가질 수 있는 값으로 0 을 추가한다. 아래 그림은 Scale을 이용해서 Quantization Error를 줄이는 방법을 말하고 있다. \\[\nq = \\begin{dcases}\nr_t, &r &gt; \\Delta \\\\\n0, &\\lvert r\\lvert \\leq \\Delta \\\\\n-r_t, &r &lt; -\\Delta\n\\end{dcases} \\\\\nwhere\\ \\Delta = 0.7\\times \\mathbb{E}(\\lvert r \\lvert), r_t = \\mathbb{E}_{\\lvert r \\lvert &gt; \\Delta}(\\lvert r \\lvert )\n\\]  ### 4.6 Trained Ternary Quantization(TTQ)\nTenary Quantization에서 또 한가지 다르게 설명하는 것은 1과 -1로만 정해져 있던 Binary Quantization과 다르게 Tenary는 1, 0, -1로 Quantization을 한 후, 추가적인 훈련을 통해 \\(w_t\\)와 \\(-w_t\\)로 fine-tuning을 하는 방법도 제안한다(해당 논문에서는 이러한 기법을 이용해서 한 결과를 CIFAR-10 이미지 데이터를 가지고 ResNets, AlexNet, ImageNet에서 보여준다). \\[\nq = \\begin{dcases}\nw_t, &r &gt; \\Delta \\\\\n0, &\\lvert r\\lvert \\leq \\Delta \\\\\n-w_t, &r &lt; -\\Delta\n\\end{dcases}\n\\] \n\n\n4.7 Accuracy Degradation\nBinary, Ternary Quantization을 사용한 결과를 보여준다(Resnet-18 경우에는 Ternary 가 오히려 Binary보다 성능이 더 떨어진다!)\n\nBinarization\n\n\n\nReference. Binarized Neural Networks: Training Deep Neural Networks with Weights and Activations Constrained to +1 or −1. [Courbariaux et al., Arxiv 2016], XNOR-Net: ImageNet Classification using Binary Convolutional Neural Networks [Rastegari et al., ECCV 2016]\n\n\nTernary Weight Networks (TWN)\n\n\n\nReference. Ternary Weight Networks [Li et al., Arxiv 2016]\n\n\nTrained Ternary Quantization (TTQ)\n\n\n\nReference. Trained Ternary Quantization [Zhu et al., ICLR 2017]"
  },
  {
    "objectID": "posts/lecs/lec05.html#low-bit-width-quantization",
    "href": "posts/lecs/lec05.html#low-bit-width-quantization",
    "title": "🧑‍🏫 Lecture 5-6",
    "section": "5. Low Bit-Width Quantization",
    "text": "5. Low Bit-Width Quantization\n남은 부분들은 여러가지 실험 / 연구들을 소개하고 있다.\n\nBinary Quantization은 Quantization Aware Training을 할 수 있을까?\n2,3 bit과 8bit 그 중간으로는 Quantization을 할 수 없을까?\n레이어에서 Quantization을 하지 않는 레이어, 예를 들어 결과에 영향을 예민하게 미치는 첫 번째 레이어가 같은 경우 Quantization을 하지 않으면 어떻게 될까?\nActivation 함수를 바꾸면 어떨까?\n예를 들어 첫번째 레이어의 N배 넓게 하는 것과 같이 모델 구조를 바꾸면 어떻게 될까?\n조금씩 Quantization을 할 수 없을까? (20% → 40% → … → 100%)\n\n강의에서는 크게 언급하지 않고 간 내용들이라 설명을 하지는 않겠다. 해당 내용들은 자세한 내용을 알고싶으면 각 파트에 언급된 논문을 참조하길!\n\n5.1 Train Binarized Neural Networks From Scratch\n\nStraight-Through Estimator(STE)\n\n\n\n\nReference. MIT-TinyML-lecture06-Quantization-2\n\n\n\nGradient pass straight to floating-point weights\nFloating-point weight with in [-1, 1]\nReference. Binarized Neural Networks: Training Deep Neural Networks with Weights and Activations Constrained to +1 or −1. [Courbariaux et al., Arxiv 2016]\n\n\n\n5.2 Quantization-Aware Training: DoReFa-Net With Low Bit-Width Gradients\n\n\n\nReference. MIT-TinyML-lecture06-Quantization-2\n\n\n\nGradient Quantization\n\\[\n  Q(g) = 2 \\cdot max(\\lvert G \\lvert) \\cdot \\Large[ \\small quantize_k \\Large( \\small \\dfrac{g}{2\\cdot max(\\lvert G \\lvert)} + \\dfrac{1}{2} + N(k) \\Large ) \\small -\\dfrac{1}{2} \\Large]\\small\n  \\] \\[\n  where\\ N(k)=\\dfrac{\\sigma}{2^k-1} and\\ \\sigma \\thicksim Uniform(-0.5, 0.5)\n  \\]\n\nNoise function \\(N(k)\\) is added to compensate the potential bias introduced by gradient quantization.\n\nResult\n\n\n\nReference. DoReFa-Net: Training Low Bitwidth Convolutional Neural Networks with Low Bitwidth Gradients [Zhou et al., arXiv 2016]\n\n\n\n\n\n5.3 Replace the Activation Function: Parameterized Clipping Activation Function\n\nThe most common activation function ReLU is unbounded. The dynamic range of inputs becomes problematic for low bit-width quantization due to very limited range and resolution.\nReLU is replaced with hard-coded bounded activation functions: ReLU6, ReLU1, etc\nThe clipping value per layer can be learned as well: PACT(Parametrized Clipping Activation Function)\n \\[\n  y=PACT(x;\\alpha) = 0.5(\\lvert x \\lvert - \\lvert x -\\alpha \\lvert + \\alpha ) = \\begin{dcases}\n  0, & x \\in [-\\infty, 0) \\\\\n  x, & x \\in [0, \\alpha) \\\\\n  \\alpha, & x \\in [\\alpha, +\\infty)\n  \\end{dcases}\n  \\]\nThe upper clipping value of the activation function is a trainable. With STE, the gradient is computed as\n\\[\n  \\dfrac{\\partial Q(y)}{\\partial \\alpha} = \\dfrac{\\partial Q(y)}{\\partial y} \\cdot \\dfrac{\\partial y}{\\partial \\alpha} = \\begin{dcases}\n  0 & x \\in (-\\infty, \\alpha)\\\\\n  1 & x \\in [\\alpha, +\\infty)\\\\\n  \\end{dcases}\n  \\]\n\\[\n  \\rightarrow\n  \\dfrac{\\partial L}{\\partial \\alpha} = \\dfrac{\\partial L}{\\partial Q(y)} \\cdot \\dfrac{\\partial Q(y)}{\\partial \\alpha} = \\begin{dcases}\n  0 & x \\in (-\\infty, \\alpha)\\\\\n  \\frac{\\partial L}{\\partial Q(y)} & x \\in [\\alpha, +\\infty)\\\\\n  \\end{dcases}\n  \\]\nThe larger \\(\\alpha\\), the more the parameterized clipping function resembles a ReLU function\n\nTo avoid large quantization errors due to a wide dynamic range \\([0, \\alpha]\\), L2-regularizer for \\(\\alpha\\) is included in the training loss function.\n\nResult\n\n\n\nReference. PACT: Parameterized Clipping Activation for Quantized Neural Networks [Choi et al., arXiv 2018]\n\n\n\n\n\n5.4 Modify the Neural Network Architecture\n\nWiden the neural network to compensate for the loss of information due to quantization\nex. Double the channels, reduce the quantization precision\n\n\n\nReference. WRPN: Wide Reduced-Precision Networks [Mishra et al., ICLR 2018]\n\n\nReplace a single floating-point convolution with multiple binary convolutions.\n\nTowards Accurate Binary Convolutional Neural Network [Lin et al., NeurIPS 2017]\nQuantization [Neural Network Distiller]\n\n\n\n\n5.5 No Quantization on First and Last Layer\n\nBecause it is more sensitive to quantization and small portion of the overall computation\nQuantizing these layers to 8-bit integer does not reduce accuracy\n\n\n\n5.6 Iterative Quantization: Incremental Network Quantization\n\nReference. Incremental Network Quantization: Towards Lossless CNNs with Low-precision Weights [Zhou et al., ICLR 2017]\n\n\n\n\nReference. Incremental Network Quantization: Towards Lossless CNNs with Low-precision Weights [Zhou et al., ICLR 2017]\n\n\n\nSetting\n\nWeight quantization only\nQuantize weights to \\(2^n\\) for faster computation (bit shift instead of multiply)\n\nAlgorithm\n\nStart from a pre-trained fp32 model\nFor the remaining fp32 weights\n\nPartition into two disjoint groups(e.g., according to magnitude)\nQuantize the first group (higher magnitude), and re-train the other group to recover accuracy\n\nRepeat until all the weights are quantized (a popular stride is {50%, 75%, 87.5%, 100%})\n\n\n\n\nReference. MIT-TinyML-lecture06-Quantization-2"
  },
  {
    "objectID": "posts/lecs/lec05.html#mixed-precision-quantization",
    "href": "posts/lecs/lec05.html#mixed-precision-quantization",
    "title": "🧑‍🏫 Lecture 5-6",
    "section": "6. Mixed-precision quantization",
    "text": "6. Mixed-precision quantization\n마지막으로 레이어마다 Quantization bit를 다르게 가져가면 어떨지에 대해서 이야기한다. 하지만 경우의 수가 8bit 보다 작거나 같게 Quantization을 할 시, weight와 activation로 경우의 수를 고려를 한다면 N개 레이어에 대해서 \\((8 \\times 8)^N\\)라는 어마어마한 경우의 수가 나온다. 그리고 이에 대해서는 다음 파트에 나갈 Neural Architecture Search(NAS) 에서 다룰 듯 싶다.\n\n6.1 Uniform Quantization\n\n\n\n6.2 Mixed-precision Quantization\n\n\n\n6.3 Huge Design Space and Solution: Design Automation\n\n\nDesign Space: Each of Choices(8x8=64) → \\(64^n\\)\n\n\n\nReference. HAQ: Hardware-Aware Automated Quantization with Mixed Precision [Wang et al., CVPR 2019]\n\n\nResult in Mixed-Precision Quantized MobileNetV1\n\n\n\nReference. HAQ: Hardware-Aware Automated Quantization with Mixed Precision [Wang et al., CVPR 2019]\n\n\n\nThis paper compares with Model size, Latency and Energy\n\n\n가장 마지막에 언급하는 Edge와 클라우드에서는 Convolution 레이어의 종류 중 더하고 덜 Quantization하는 레이어가 각각 depthwise와 pointwise로 다르다고 이야기한다. 이 내용에 대해서 더 자세히 이해하기 위해서는 아마도 NAS로 넘어가봐야 알 수 있지 않을까 싶다.\n\nQuantization Policy for Edge and Cloud\n\n\n\nReference. HAQ: Hardware-Aware Automated Quantization with Mixed Precision [Wang et al., CVPR 2019]"
  },
  {
    "objectID": "posts/lecs/lec05.html#reference",
    "href": "posts/lecs/lec05.html#reference",
    "title": "🧑‍🏫 Lecture 5-6",
    "section": "7. Reference",
    "text": "7. Reference\n\nTinyML and Efficient Deep Learning Computing on MIT HAN LAB\nYoutube for TinyML and Efficient Deep Learning Computing on MIT HAN LAB\nDeep Compression [Han et al., ICLR 2016]\nQuantization and Training of Neural Networks for Efficient Integer-Arithmetic-Only Inference [Jacob et al., CVPR 2018]\nWith Shared Microexponents, A Little Shifting Goes a Long Way [Bita Rouhani et al.]\nData-Free Quantization Through Weight Equalization and Bias Correction [Markus et al., ICCV 2019]\nEstimating or Propagating Gradients Through Stochastic Neurons for Conditional Computation [Bengio, arXiv 2013]\nBinarized Neural Networks: Training Deep Neural Networks with Weights and Activations Constrained to +1 or −1. [Courbariaux et al., Arxiv 2016]\nXNOR-Net: ImageNet Classification using Binary Convolutional Neural Networks [Rastegari et al., ECCV 2016]\nTernary Weight Networks [Li et al., Arxiv 2016]\nTrained Ternary Quantization [Zhu et al., ICLR 2017]\nDoReFa-Net: Training Low Bitwidth Convolutional Neural Networks with Low Bitwidth Gradients [Zhou et al., arXiv 2016]\nWRPN: Wide Reduced-Precision Networks [Mishra et al., ICLR 2018]\nPACT: Parameterized Clipping Activation for Quantized Neural Networks [Choi et al., arXiv 2018]\nHAQ: Hardware-Aware Automated Quantization with Mixed Precision [Wang et al., CVPR 2019]"
  },
  {
    "objectID": "posts/lecs/lec14.html",
    "href": "posts/lecs/lec14.html",
    "title": "🧑‍🏫 Lecture 14",
    "section": "",
    "text": "이번 시간은 Transformer 모델에서도 Vision에 주로 어떻게 사용하는지 알아보려고 합니다. 그리고 이를 경량화하거나 가속화하는 기법, 그리고 제한된 리소스에서 어떻게 잘 활용할 수 있을지 알아봅시다.\n\n\nVision Transformer는 뭘까요? LLM으로 많이 사용하는 Language 모델의 경우, 입력으로 토큰이 들어와 Transformer 모델 구조로 Encoder, Decoder 구조에 따라 BERT(Encoder), GPT(Decoder) 그리고 BART, T5(Encoder - Decoder) 구조로 사용하죠. 그럼 Vision에서는 이 구조를 어떻게 사용할까요?\n\n\n\nReference. MIT-TinyML lecture14 Vision Transformer in https://efficientml.ai\n\n\n생각보다 간단해요. 이미지가 만약 96x96이 있다면 이름 32x32 이미지 9개로 나눕니다. 나눈 이미지를 Patch라고 부를게요. 그럼 이 Patch를 Linear Projection을 통해서 토큰처럼 768개의 Vision Transformer(ViT)의 입력으로 들어갑니다. 실제로 아이디어를 구현할 때는 32x32 Convolution 레이어에 stride 32, padding 0, 입력 채널 3, 출력 채널 768로 연산합니다. 그 다음은, 입력이 동일해 졌으니 모델 구조는 흔히 보는 아래 그림의 구조를 사용하죠.\n\n\n\nReference. MIT-TinyML lecture14 Vision Transformer in https://efficientml.ai\n\n\n여기서 Patch 의 크기 또한 파라미터로 할 수 있습니다. 앞으로 ViT를 말할 때는 Patch도 유심히 보셔야할 겁니다.\n\n\n\nReference. MIT-TinyML lecture14 Vision Transformer in https://efficientml.ai\n\n\n그런데 왜 Vision Transformer를 쓸까요? 기존에 CNN 구조에 ResNet이나 MobileNet의 구조도 충분히 성능이 괜찮지 않나요? CNN과 Transformer를 Vision task에서 비교해보면 훈련 데이터수가 적을 때는 확실히 CNN이 강세를 보이죠.\n\n\n\nReference. MIT-TinyML lecture14 Vision Transformer in https://efficientml.ai\n\n\n하.지.만. 데이터수가 300M인 경우를 살펴보시죠. ViT는 데이터수가 많으면 많을 수록 CNN에 비해서 훨씬 강세를 보이는 것을 확인할 수 있죠? 이 때문에, 저희는 ViT에 매료될 수 밖에 없었습니다.\n\n\n\nReference. MIT-TinyML lecture14 Vision Transformer in https://efficientml.ai\n\n\nVision에 대한 Application으로 Medical Image Segmentation, Super Resolution, Autonomous Driving, Segmentation로써 주로 사용해요.\n\n\n\nReference. MIT-TinyML lecture14 Vision Transformer in https://efficientml.ai\n\n\n문제는 이 어플리케이션들은 모두 High resolution에 prediction을 요구하지만, ViT는 Input resolution이 높아질 때마다 연산량이 어마어마해집니다. 선형적이라기 보단 지수적으로 증가하는 것 처럼 보이네요. 바로 이 문제 때문에, 우리가 “Efficient and Acceleration”에 대해서 고민할 수 밖에 없게 됩니다.\n*mIoU: mean Intersection of Union, GMAC: Giga Multiply and Add Computation\n\n\n\nReference. MIT-TinyML lecture14 Vision Transformer in https://efficientml.ai\n\n\n\n\n\n\n\n처음으로 소개할 기술은 Window attention 입니다. 기존에 attention은 레이어마다 patch의 크기가 동일하게 들어가겠죠. 하지만 Window attention은 레이어마다 patch의 크기를 다르게 하는 방법입니다. 그.리.고. 여기서 중요한 건 Window attention은 그 Patch안에 다시 Patch를 하는 방법입니다. 그리고 이를 통해 병렬연산을 가능하게 만드는 거죠.\n\n\n\nReference. MIT-TinyML lecture14 Vision Transformer in https://efficientml.ai\n\n\n하지만 저렇게 연산하면 문제점이 Window간 정보교환이 없다는 점인데요. 이는 레이어별로 “Shift Window”를 통해 해결합니다.\n\n\n\nReference. MIT-TinyML lecture14 Vision Transformer in https://efficientml.ai\n\n\n\n\n\n두 번째 소개할 기술은 Linear attention 입니다. 기존에 Attention 연산중에 Softmax가 있었죠? exponential 연산은 직접 구현해보면 연산량이 쪼금 힘듭니다. 그래서 이를 Linear 레이어인 Relu로 대체를 하는데요. 여기서 알고리즘 복잡도가 O(\\(n^2\\))인 부분까지 행렬의 곱셈에서 결합법칙이 가능하다는 부분을 이용해 O(\\(n\\))으로 줄여버립니다. 마지막 복잡도가 줄어드는 것이 이해가 안가신 다면, Scale이후에 나오는 레이어가 \\(n \\times d\\) 인 점과 n-차원과 d-차원에 대해서 비교해보시면 빠르게 납득이 가실겁니다!\n\n\n\nReference. MIT-TinyML lecture14 Vision Transformer in https://efficientml.ai\n\n\n하지만 여기서도 문제가 있습니다. attention에서 성능을 보는 방법중에 attention map을 통해 보면 Linear Attention이 사진의 특징을 잘 못잡아 냅니다.\n\n\n\nReference. MIT-TinyML lecture14 Vision Transformer in https://efficientml.ai\n\n\n당연히 Softmax 보다 distribution이 날카롭지 않기 때문에 특징점에서도 두드러지지 않는게 문제죠. 성능도, 나오지 않을 것이구요. “Multi-scale” learning을 하기가 어려울 겁니다.\n\n\n\nReference. MIT-TinyML lecture14 Vision Transformer in https://efficientml.ai\n\n\n해결해야겠죠? 방법은 레이어를 하나 더 넣으면 됩니다. 성능도 오히려 전보다 훨씬 좋아졌네요.\n\n\n\nReference. MIT-TinyML lecture14 Vision Transformer in https://efficientml.ai\n\n\n\n\n\n세 번째 기술을 소개드리기 전에, Vision Application을 하다 보면 아래와 같은 상황이 많습니다. 이미지의 해상도를 줄이거나, Pruning을 통해 이미지가 특정부분만 들어오게되는 경우가 있죠.\n\n\n\nReference. MIT-TinyML lecture14 Vision Transformer in https://efficientml.ai\n\n\n그래서, Sparse attention이라는 기술을 사용합니다. 이 기술을 Pruning 과 비슷하게 Patch마다 Importance를 계산해 줄을 세웁니다.\n\n\n\nReference. MIT-TinyML lecture14 Vision Transformer in https://efficientml.ai\n\n\n그리고 줄을 세운 Patch에서 N번의 반복하는 fine-tuning을 통해 모델을 재학습시키죠.\n\n\n\nReference. MIT-TinyML lecture14 Vision Transformer in https://efficientml.ai\n\n\n여기서 Constraint에 만족하는 조합을 찾기 위해 Evolutionary Search를 이용합니다(Evolutionary Search는 Lab 3를 참고해주세요).\n\n\n\nReference. MIT-TinyML lecture14 Vision Transformer in https://efficientml.ai\n\n\n성능이 궁금하다면 이 논문을 참고해주세요 :)\n\n\n\n\nEfficient + Acceleration에 대한 기술은 여기까지입니다. 그럼 다시 처음으로 돌아가서, 혹시 Vision Transformer 첫 성능 그래프 기억하시나요? 데이터가 어마어마하게 많아야 했던 부분이요(아래에 가져와 봤습니다). 그런데, 이렇게 데이터를 많이 구하는 건 현실적으로 많이 어려울 수 밖에 없습니다. 대표적인 예시로 의료데이터가 그렇죠. 그럼 이 데이터가 부족한 건 어떻게 해결할 수 있을까요?\n\n\n\nReference. MIT-TinyML lecture14 Vision Transformer in https://efficientml.ai\n\n\n\n\n첫번째 방법은 Contrastive learning 입니다. 이 방법은 TinyML에서 뿐 아니라 많이 쓰이는 방법인데, Positive Sample과 Negative Sample을 가지고 embedding vector를 멀게 하는 방식입니다. 예를 들어, 고양이 사진을 구분하는 테스크에서 Positive Sample은 고양이 사진이 될 것이고, Negative Sample은 여기서 강아지 사진이 될 겁니다.\n\n\n\nReference. MIT-TinyML lecture14 Vision Transformer in https://efficientml.ai\n\n\n실험결과를 보면 Supervised 방법으로는 CIFAR-100, Oxford Flowers-102, Oxford-IIIT Pets 데이터 셋에서는 모델 성능이 나오지 않지만, 위 방법을 적용한 모델은 성능이 어느정도 나오는 것을 볼 수 있습니다.\n\n\n\nReference. An Empirical Study of Training Self-Supervised Vision Transformers [Chen et al., 2021]\n\n\nContrastive Learning으로 Multi-Modal을 사용할 수도 있습니다. 아래 논문은 텍스트와 이미지를 모두 받는 형태로 디자인돼 있습니다.\n\n\n\nReference. Learning Transferable Visual Models From Natural Language Supervision [Radford et al., 2021]\n\n\n\n\n\n두번째 방법은 Mask 입니다. 아래 모델 구조를 보시죠.\n\n\n\nReference. MIT-TinyML lecture14 Vision Transformer in https://efficientml.ai\n\n\nBERT 모델입니다. Mask방법은 입력 토큰에 마스크를 씌워 출력에서 이를 맞추는 테스크로 모델을 훈련시키는 방법입니다. 그럼 Vision은 어떻게 할까요?\n\n\n\nReference. Masked Autoencoders Are Scalable Vision Learners [He et al., 2022]\n\n\nLLM과 훈련하는 방식은 Mask를 씌우고 이를 예측하도록 하는 훈련인 것은 비슷합니다. 다만 ViT의 경우 Encoder와 Decoder가 같이 있으면서 Encoder보다는 Decoder의 모델크기가 더 작다는 걸 강조합니다. 흥미로운 부분은 Masking ratio가 BERT의 경우 15%인 반면, 아래 실험결과에서 ViT의 경우는 무려 75%나 됩니다. 강의노트에서는 “이미지가 언어보다 더 information density가 낮아서 그렇다.”라고 말합니다.\n\n\n\nReference. Masked Autoencoders Are Scalable Vision Learners [He et al., 2022]\n\n\n\n\n\nLecture_14-Vision Transformer (9).jpg\n\n\n\n\n\n\n\nCross attention (Flamingo)\nVisual token (PaLM-E)\n\n마지막으로 Multi-modal LLM에 대해서 언급을 하는데, 자세한 내용은 다루지 않아 궁금하신 분들을 위해 논문은 링크로 달아두고 설명은 넘어가도록 하겠습니다. 여기까지 Vision Transformer 였습니다. LLM과 비슷하면서도 모델크기가 똑같다면 연산량 높은 것과 데이터가 부족한 것을 어떻게 해결하는가가 중점인 강의였습니다. 다음 시간에는 GAN, Video, and Point Cloud로 돌아오겠습니다 :D"
  },
  {
    "objectID": "posts/lecs/lec14.html#basics-of-vision-transformer-vit",
    "href": "posts/lecs/lec14.html#basics-of-vision-transformer-vit",
    "title": "🧑‍🏫 Lecture 14",
    "section": "",
    "text": "Vision Transformer는 뭘까요? LLM으로 많이 사용하는 Language 모델의 경우, 입력으로 토큰이 들어와 Transformer 모델 구조로 Encoder, Decoder 구조에 따라 BERT(Encoder), GPT(Decoder) 그리고 BART, T5(Encoder - Decoder) 구조로 사용하죠. 그럼 Vision에서는 이 구조를 어떻게 사용할까요?\n\n\n\nReference. MIT-TinyML lecture14 Vision Transformer in https://efficientml.ai\n\n\n생각보다 간단해요. 이미지가 만약 96x96이 있다면 이름 32x32 이미지 9개로 나눕니다. 나눈 이미지를 Patch라고 부를게요. 그럼 이 Patch를 Linear Projection을 통해서 토큰처럼 768개의 Vision Transformer(ViT)의 입력으로 들어갑니다. 실제로 아이디어를 구현할 때는 32x32 Convolution 레이어에 stride 32, padding 0, 입력 채널 3, 출력 채널 768로 연산합니다. 그 다음은, 입력이 동일해 졌으니 모델 구조는 흔히 보는 아래 그림의 구조를 사용하죠.\n\n\n\nReference. MIT-TinyML lecture14 Vision Transformer in https://efficientml.ai\n\n\n여기서 Patch 의 크기 또한 파라미터로 할 수 있습니다. 앞으로 ViT를 말할 때는 Patch도 유심히 보셔야할 겁니다.\n\n\n\nReference. MIT-TinyML lecture14 Vision Transformer in https://efficientml.ai\n\n\n그런데 왜 Vision Transformer를 쓸까요? 기존에 CNN 구조에 ResNet이나 MobileNet의 구조도 충분히 성능이 괜찮지 않나요? CNN과 Transformer를 Vision task에서 비교해보면 훈련 데이터수가 적을 때는 확실히 CNN이 강세를 보이죠.\n\n\n\nReference. MIT-TinyML lecture14 Vision Transformer in https://efficientml.ai\n\n\n하.지.만. 데이터수가 300M인 경우를 살펴보시죠. ViT는 데이터수가 많으면 많을 수록 CNN에 비해서 훨씬 강세를 보이는 것을 확인할 수 있죠? 이 때문에, 저희는 ViT에 매료될 수 밖에 없었습니다.\n\n\n\nReference. MIT-TinyML lecture14 Vision Transformer in https://efficientml.ai\n\n\nVision에 대한 Application으로 Medical Image Segmentation, Super Resolution, Autonomous Driving, Segmentation로써 주로 사용해요.\n\n\n\nReference. MIT-TinyML lecture14 Vision Transformer in https://efficientml.ai\n\n\n문제는 이 어플리케이션들은 모두 High resolution에 prediction을 요구하지만, ViT는 Input resolution이 높아질 때마다 연산량이 어마어마해집니다. 선형적이라기 보단 지수적으로 증가하는 것 처럼 보이네요. 바로 이 문제 때문에, 우리가 “Efficient and Acceleration”에 대해서 고민할 수 밖에 없게 됩니다.\n*mIoU: mean Intersection of Union, GMAC: Giga Multiply and Add Computation\n\n\n\nReference. MIT-TinyML lecture14 Vision Transformer in https://efficientml.ai"
  },
  {
    "objectID": "posts/lecs/lec14.html#efficient-vit-acceleration-techniques",
    "href": "posts/lecs/lec14.html#efficient-vit-acceleration-techniques",
    "title": "🧑‍🏫 Lecture 14",
    "section": "",
    "text": "처음으로 소개할 기술은 Window attention 입니다. 기존에 attention은 레이어마다 patch의 크기가 동일하게 들어가겠죠. 하지만 Window attention은 레이어마다 patch의 크기를 다르게 하는 방법입니다. 그.리.고. 여기서 중요한 건 Window attention은 그 Patch안에 다시 Patch를 하는 방법입니다. 그리고 이를 통해 병렬연산을 가능하게 만드는 거죠.\n\n\n\nReference. MIT-TinyML lecture14 Vision Transformer in https://efficientml.ai\n\n\n하지만 저렇게 연산하면 문제점이 Window간 정보교환이 없다는 점인데요. 이는 레이어별로 “Shift Window”를 통해 해결합니다.\n\n\n\nReference. MIT-TinyML lecture14 Vision Transformer in https://efficientml.ai\n\n\n\n\n\n두 번째 소개할 기술은 Linear attention 입니다. 기존에 Attention 연산중에 Softmax가 있었죠? exponential 연산은 직접 구현해보면 연산량이 쪼금 힘듭니다. 그래서 이를 Linear 레이어인 Relu로 대체를 하는데요. 여기서 알고리즘 복잡도가 O(\\(n^2\\))인 부분까지 행렬의 곱셈에서 결합법칙이 가능하다는 부분을 이용해 O(\\(n\\))으로 줄여버립니다. 마지막 복잡도가 줄어드는 것이 이해가 안가신 다면, Scale이후에 나오는 레이어가 \\(n \\times d\\) 인 점과 n-차원과 d-차원에 대해서 비교해보시면 빠르게 납득이 가실겁니다!\n\n\n\nReference. MIT-TinyML lecture14 Vision Transformer in https://efficientml.ai\n\n\n하지만 여기서도 문제가 있습니다. attention에서 성능을 보는 방법중에 attention map을 통해 보면 Linear Attention이 사진의 특징을 잘 못잡아 냅니다.\n\n\n\nReference. MIT-TinyML lecture14 Vision Transformer in https://efficientml.ai\n\n\n당연히 Softmax 보다 distribution이 날카롭지 않기 때문에 특징점에서도 두드러지지 않는게 문제죠. 성능도, 나오지 않을 것이구요. “Multi-scale” learning을 하기가 어려울 겁니다.\n\n\n\nReference. MIT-TinyML lecture14 Vision Transformer in https://efficientml.ai\n\n\n해결해야겠죠? 방법은 레이어를 하나 더 넣으면 됩니다. 성능도 오히려 전보다 훨씬 좋아졌네요.\n\n\n\nReference. MIT-TinyML lecture14 Vision Transformer in https://efficientml.ai\n\n\n\n\n\n세 번째 기술을 소개드리기 전에, Vision Application을 하다 보면 아래와 같은 상황이 많습니다. 이미지의 해상도를 줄이거나, Pruning을 통해 이미지가 특정부분만 들어오게되는 경우가 있죠.\n\n\n\nReference. MIT-TinyML lecture14 Vision Transformer in https://efficientml.ai\n\n\n그래서, Sparse attention이라는 기술을 사용합니다. 이 기술을 Pruning 과 비슷하게 Patch마다 Importance를 계산해 줄을 세웁니다.\n\n\n\nReference. MIT-TinyML lecture14 Vision Transformer in https://efficientml.ai\n\n\n그리고 줄을 세운 Patch에서 N번의 반복하는 fine-tuning을 통해 모델을 재학습시키죠.\n\n\n\nReference. MIT-TinyML lecture14 Vision Transformer in https://efficientml.ai\n\n\n여기서 Constraint에 만족하는 조합을 찾기 위해 Evolutionary Search를 이용합니다(Evolutionary Search는 Lab 3를 참고해주세요).\n\n\n\nReference. MIT-TinyML lecture14 Vision Transformer in https://efficientml.ai\n\n\n성능이 궁금하다면 이 논문을 참고해주세요 :)"
  },
  {
    "objectID": "posts/lecs/lec14.html#self-supervised-learning-for-vit",
    "href": "posts/lecs/lec14.html#self-supervised-learning-for-vit",
    "title": "🧑‍🏫 Lecture 14",
    "section": "",
    "text": "Efficient + Acceleration에 대한 기술은 여기까지입니다. 그럼 다시 처음으로 돌아가서, 혹시 Vision Transformer 첫 성능 그래프 기억하시나요? 데이터가 어마어마하게 많아야 했던 부분이요(아래에 가져와 봤습니다). 그런데, 이렇게 데이터를 많이 구하는 건 현실적으로 많이 어려울 수 밖에 없습니다. 대표적인 예시로 의료데이터가 그렇죠. 그럼 이 데이터가 부족한 건 어떻게 해결할 수 있을까요?\n\n\n\nReference. MIT-TinyML lecture14 Vision Transformer in https://efficientml.ai\n\n\n\n\n첫번째 방법은 Contrastive learning 입니다. 이 방법은 TinyML에서 뿐 아니라 많이 쓰이는 방법인데, Positive Sample과 Negative Sample을 가지고 embedding vector를 멀게 하는 방식입니다. 예를 들어, 고양이 사진을 구분하는 테스크에서 Positive Sample은 고양이 사진이 될 것이고, Negative Sample은 여기서 강아지 사진이 될 겁니다.\n\n\n\nReference. MIT-TinyML lecture14 Vision Transformer in https://efficientml.ai\n\n\n실험결과를 보면 Supervised 방법으로는 CIFAR-100, Oxford Flowers-102, Oxford-IIIT Pets 데이터 셋에서는 모델 성능이 나오지 않지만, 위 방법을 적용한 모델은 성능이 어느정도 나오는 것을 볼 수 있습니다.\n\n\n\nReference. An Empirical Study of Training Self-Supervised Vision Transformers [Chen et al., 2021]\n\n\nContrastive Learning으로 Multi-Modal을 사용할 수도 있습니다. 아래 논문은 텍스트와 이미지를 모두 받는 형태로 디자인돼 있습니다.\n\n\n\nReference. Learning Transferable Visual Models From Natural Language Supervision [Radford et al., 2021]\n\n\n\n\n\n두번째 방법은 Mask 입니다. 아래 모델 구조를 보시죠.\n\n\n\nReference. MIT-TinyML lecture14 Vision Transformer in https://efficientml.ai\n\n\nBERT 모델입니다. Mask방법은 입력 토큰에 마스크를 씌워 출력에서 이를 맞추는 테스크로 모델을 훈련시키는 방법입니다. 그럼 Vision은 어떻게 할까요?\n\n\n\nReference. Masked Autoencoders Are Scalable Vision Learners [He et al., 2022]\n\n\nLLM과 훈련하는 방식은 Mask를 씌우고 이를 예측하도록 하는 훈련인 것은 비슷합니다. 다만 ViT의 경우 Encoder와 Decoder가 같이 있으면서 Encoder보다는 Decoder의 모델크기가 더 작다는 걸 강조합니다. 흥미로운 부분은 Masking ratio가 BERT의 경우 15%인 반면, 아래 실험결과에서 ViT의 경우는 무려 75%나 됩니다. 강의노트에서는 “이미지가 언어보다 더 information density가 낮아서 그렇다.”라고 말합니다.\n\n\n\nReference. Masked Autoencoders Are Scalable Vision Learners [He et al., 2022]\n\n\n\n\n\nLecture_14-Vision Transformer (9).jpg"
  },
  {
    "objectID": "posts/lecs/lec14.html#multi-modal-llm",
    "href": "posts/lecs/lec14.html#multi-modal-llm",
    "title": "🧑‍🏫 Lecture 14",
    "section": "",
    "text": "Cross attention (Flamingo)\nVisual token (PaLM-E)\n\n마지막으로 Multi-modal LLM에 대해서 언급을 하는데, 자세한 내용은 다루지 않아 궁금하신 분들을 위해 논문은 링크로 달아두고 설명은 넘어가도록 하겠습니다. 여기까지 Vision Transformer 였습니다. LLM과 비슷하면서도 모델크기가 똑같다면 연산량 높은 것과 데이터가 부족한 것을 어떻게 해결하는가가 중점인 강의였습니다. 다음 시간에는 GAN, Video, and Point Cloud로 돌아오겠습니다 :D"
  },
  {
    "objectID": "posts/lecs/lec17.html",
    "href": "posts/lecs/lec17.html",
    "title": "🧑‍🏫 Lecture 17-18",
    "section": "",
    "text": "이번 강의는 분산 학습(distributed training)에 대한 내용을 다룬다. LLM등 딥러닝 모델이 점점 복잡해지면서, 대규모 모델의 훈련은 단일 GPU로는 더 이상 충분하지 않고, 여러개의 GPU를 사용해야 가능하다. 이번 글에서는 딥러닝 모델의 훈련을 가속화하기 위한 다양한 분산 훈련 기법들을 다루고자 한다. 데이터 병렬화, 파이프라인 병렬화, 텐서 병렬화 등의 기법들을 통해 효율적인 분산 훈련을 실현하는 방법에 대해 살펴볼 것이다. 또한, 분산 훈련 뿐만 아니라 GPU간 통신의 병목 현상에 대한 대처법으로 가중치 압축(gradient compression) 및 지연 가중치 업데이트(delayed gradient update) 또한 살펴볼 것이다."
  },
  {
    "objectID": "posts/lecs/lec17.html#gradient-pruning-sparse-communication-deep-gradient-compression",
    "href": "posts/lecs/lec17.html#gradient-pruning-sparse-communication-deep-gradient-compression",
    "title": "🧑‍🏫 Lecture 17-18",
    "section": "10.1. Gradient Pruning: Sparse Communication, Deep Gradient Compression",
    "text": "10.1. Gradient Pruning: Sparse Communication, Deep Gradient Compression\n\npruning 방식은 간단하게 gradient중에서 top-k개만 전송하는 방식이다. top-k의 기준은 단순히 magnitude를 사용한다. 전송되지 않은 gradient도 error feedback을 통해 local에 내비두어 사용한다.\n\n하지만 이런 방식은 꽤나 성능 저하를 일으킨다. gradient만 사용하므로, momentum이 없기 떄문이다.\n\n단순히 accumulate하면 위와 같이 momentum을 사용하는 방식과 꽤 달라지게 된다.\n\n따라서 gradient가 아니라 velocity를 accumulate하는 것이 더 좋다. 또한 여러가지 warm up 방식을 사용하는 것이 좋다.\n\nlearning rate는 물론이고, pruning sparsity도 warm up 방식을 사용하는 것이 도움이 된다. optimizer가 적응하는데 도움을 준다.\n\npruning 방식의 문제점은, 여러 node끼리 정보를 교환하는 all-reduce 과정을 거치면서 pruning 하는 의미가 없어진다는 것이다(더 dense해진다)\n\n이런 방식을 해결하기 위해 sparse하게 만드는 것이 아니라 low rank matrix를 사용하는 방식을 사용하기도 한다."
  },
  {
    "objectID": "posts/lecs/lec17.html#gradient-quantization-1-bit-sgd-terngrad",
    "href": "posts/lecs/lec17.html#gradient-quantization-1-bit-sgd-terngrad",
    "title": "🧑‍🏫 Lecture 17-18",
    "section": "10.2. Gradient Quantization: 1-Bit SGD, TernGrad",
    "text": "10.2. Gradient Quantization: 1-Bit SGD, TernGrad\n\nquantization 방법을 소개하면, 1bit-SGD 방식이 있다. 이는 0보다 크면 +, 아니면 -로 둔 뒤 scaling factor(u1~u4)를 colum마다 적용하는 방식이다. quantization error는 locally하게 집계되고, quantize된 gradient만 전송된다.\n\n비슷한 방식으로, threshold를 특정 값으로 두고 quantize하는 방식도 있다.\n\nTerngrad는 확률값에 따라 0, 1, -1 중 하나로 양자화 하는 방식이다."
  },
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "Study Archive",
    "section": "",
    "text": "스터디 자료와 관련해서 어떤 토의나 의견 모두 감사합니다! Github Discussion에 글을 남겨주셔도 좋고 각 포스팅 하단에 있는 Giscus 댓글창에 코멘트들을 남겨주시면 됩니다.\nWelcome to any comments or opinions on our content! You can leave messages using the direct Discussion or the Giscus window on each post.\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n🧑‍🏫 Lecture 17-18\n\n\n\n\n\n\nlecture\n\n\ndistributed training\n\n\n\nDistributed training\n\n\n\n\n\nMay 10, 2024\n\n\nGijeong Seong\n\n\n\n\n\n\n\n\n\n\n\n\n👩‍💻 Lab 4\n\n\n\n\n\n\nLLM\n\n\nAWQ\n\n\nquantization\n\n\nlab\n\n\n\nLLM Quantization with AWQ\n\n\n\n\n\nApr 28, 2024\n\n\nSeunghyun Oh\n\n\n\n\n\n\n\n\n\n\n\n\n🧑‍🏫 Lecture 14\n\n\n\n\n\n\nlecture\n\n\ntransformer\n\n\nvision transformer\n\n\n\nVision Transformer for TinyML\n\n\n\n\n\nApr 26, 2024\n\n\nSeunghyun Oh\n\n\n\n\n\n\n\n\n\n\n\n\n🧑‍🏫 Lecture 12-13\n\n\n\n\n\n\nlecture\n\n\nTransformer\n\n\nLLM\n\n\n\nTransformer and LLM\n\n\n\n\n\nApr 19, 2024\n\n\nGijeong Seong\n\n\n\n\n\n\n\n\n\n\n\n\n🧑‍🏫 Lecture 9\n\n\n\n\n\n\nlecture\n\n\nknowledge distillation\n\n\n\nKnowledge Distillation(KD)\n\n\n\n\n\nMar 19, 2024\n\n\nSeunghyun Oh\n\n\n\n\n\n\n\n\n\n\n\n\n👩‍💻 Lab 3\n\n\n\n\n\n\nNAS\n\n\nTinyML\n\n\nlab\n\n\n\nNeural Architecture Search(NAS) Experiment\n\n\n\n\n\nMar 16, 2024\n\n\nSeunghyun Oh\n\n\n\n\n\n\n\n\n\n\n\n\n🧑‍🏫 Lecture 7-8\n\n\n\n\n\n\nlecture\n\n\nNAS\n\n\n\nNeural Architecture Search\n\n\n\n\n\nMar 12, 2024\n\n\nGijeong Seong\n\n\n\n\n\n\n\n\n\n\n\n\n👩‍💻 Lab 2\n\n\n\n\n\n\nlab\n\n\nquantization\n\n\nlinear\n\n\nkmeans\n\n\n\nK-means & Linear Quantization\n\n\n\n\n\nMar 6, 2024\n\n\nJung Yeon Lee\n\n\n\n\n\n\n\n\n\n\n\n\n🧑‍🏫 Lecture 5-6\n\n\n\n\n\n\nlecture\n\n\nquantization\n\n\n\nQuantization\n\n\n\n\n\nMar 5, 2024\n\n\nSeunghyun Oh\n\n\n\n\n\n\n\n\n\n\n\n\n👩‍💻 Lab 1\n\n\n\n\n\n\nlab\n\n\npruning\n\n\nfine-grained\n\n\nchannel\n\n\n\nFine-grained & Channel Pruning\n\n\n\n\n\nMar 1, 2024\n\n\ncastleflag\n\n\n\n\n\n\n\n\n\n\n\n\n🧑‍🏫 Lecture 4\n\n\n\n\n\n\npruning\n\n\nlecture\n\n\n\nPruning and Sparsity (Part II)\n\n\n\n\n\nFeb 18, 2024\n\n\nJung Yeon Lee\n\n\n\n\n\n\n\n\n\n\n\n\n🧑‍🏫 Lecture 3\n\n\n\n\n\n\npruning\n\n\nlecture\n\n\n\nPruning and Sparsity (Part I)\n\n\n\n\n\nJan 28, 2024\n\n\nSeunghyun Oh\n\n\n\n\n\n\n\n\n\n\n\n\n👩‍💻 Lab 0\n\n\n\n\n\n\nlab\n\n\npytorch\n\n\nbasic\n\n\n\nPyTorch Tutorial\n\n\n\n\n\nJan 11, 2024\n\n\nJung Yeon Lee\n\n\n\n\n\n\nNo matching items"
  }
]