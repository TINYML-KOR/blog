[
  {
    "objectID": "about.html",
    "href": "about.html",
    "title": "Study Group",
    "section": "",
    "text": "@Gueltto9th\n\nGroup Members"
  },
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "Study Archive",
    "section": "",
    "text": "No matching items"
  },
  {
    "objectID": "labs/Lab1.html",
    "href": "labs/Lab1.html",
    "title": "TinyML KOR",
    "section": "",
    "text": "This colab notebook provides code and a framework for Lab 1 Pruning. You can work out your solutions here.\nPlease fill out this feedback form when you finished this lab. We would love to hear your thoughts or feedback on how we can improve this lab!\n\n\nIn this assignment, you will practice pruning a classical neural network model to reduce both model size and latency. The goals of this assignment are as follows:\n\nUnderstand the basic concept of pruning\nImplement and apply fine-grained pruning\nImplement and apply channel pruning\nGet a basic understanding of performance improvement (such as speedup) from pruning\nUnderstand the differences and tradeoffs between these pruning approaches\n\n\n\n\nThere are two main sections in this lab: Fine-grained Pruning and Channel Pruning.\nThere are 9 questions in total: - For Fine-grained Pruning, there are 5 questions (Question 1-5). - For Channel Pruning, there are 3 questions (Question 6-8). - Question 9 compares fine-grained pruning and channel pruning."
  },
  {
    "objectID": "labs/Lab1.html#question-1-10-pts",
    "href": "labs/Lab1.html#question-1-10-pts",
    "title": "TinyML KOR",
    "section": "Question 1 (10 pts)",
    "text": "Question 1 (10 pts)\nPlease answer the following questions using the information in the above histograms of weights.\n\nQuestion 1.1 (5 pts)\nWhat are the common characteristics of the weight distribution in the different layers?\nYour Answer:\n\n\nQuestion 1.2 (5 pts)\nHow do these characteristics help pruning?\nYour Answer:"
  },
  {
    "objectID": "labs/Lab1.html#magnitude-based-pruning",
    "href": "labs/Lab1.html#magnitude-based-pruning",
    "title": "TinyML KOR",
    "section": "Magnitude-based Pruning",
    "text": "Magnitude-based Pruning\nFor fine-grained pruning, a widely-used importance is the magnitude of weight value, i.e.,\n\\(Importance=|W|\\)\nThis is known as Magnitude-based Pruning (see Learning both Weights and Connections for Efficient Neural Networks).\n\n\n\npruning.png\n\n\n\nQuestion 2 (15 pts)\nPlease complete the following magnitude-based fine-grained pruning function.\nHint: * In step 1, we calculate the number of zeros (num_zeros) after pruning. Note that num_zeros should be an integer. You could use either round() or int() to convert a floating number into an integer. Here we use round(). * In step 2, we calculate the importance of weight tensor. Pytorch provides torch.abs(), torch.Tensor.abs(), torch.Tensor.abs_() APIs. * In step 3, we calculate the pruning threshold so that all synapses with importance smaller than threshold will be removed. Pytorch provides torch.kthvalue(), torch.Tensor.kthvalue(), torch.topk() APIs. * In step 4, we calculate the pruning mask based on the threshold. 1 in the mask indicates the synapse will be kept, and 0 in the mask indicates the synapse will be removed. mask = importance > threshold. Pytorch provides torch.gt() API.\n\ndef fine_grained_prune(tensor: torch.Tensor, sparsity : float) -> torch.Tensor:\n    \"\"\"\n    magnitude-based pruning for single tensor\n    :param tensor: torch.(cuda.)Tensor, weight of conv/fc layer\n    :param sparsity: float, pruning sparsity\n        sparsity = #zeros / #elements = 1 - #nonzeros / #elements\n    :return:\n        torch.(cuda.)Tensor, mask for zeros\n    \"\"\"\n    sparsity = min(max(0.0, sparsity), 1.0)\n    if sparsity == 1.0:\n        tensor.zero_()\n        return torch.zeros_like(tensor)\n    elif sparsity == 0.0:\n        return torch.ones_like(tensor)\n\n    num_elements = tensor.numel()\n\n    ##################### YOUR CODE STARTS HERE #####################\n    # Step 1: calculate the #zeros (please use round())\n    num_zeros = round(0)\n    # Step 2: calculate the importance of weight\n    importance = 0\n    # Step 3: calculate the pruning threshold\n    threshold = 0\n    # Step 4: get binary mask (1 for nonzeros, 0 for zeros)\n    mask = 0\n    ##################### YOUR CODE ENDS HERE #######################\n\n    # Step 5: apply mask to prune the tensor\n    tensor.mul_(mask)\n\n    return mask\n\nLet’s verify the functionality of defined fine-grained pruning by applying the function above on a dummy tensor.\n\ntest_fine_grained_prune()\n\n\n\nQuestion 3 (5 pts)\nThe last cell plots the tensor before and after pruning. Nonzeros are rendered in blue while zeros are rendered in gray. Please modify the value of target_sparsity in the following code cell so that there are only 10 nonzeros in the sparse tensor after pruning.\n\n##################### YOUR CODE STARTS HERE #####################\ntarget_sparsity = 0 # please modify the value of target_sparsity\n##################### YOUR CODE ENDS HERE #####################\ntest_fine_grained_prune(target_sparsity=target_sparsity, target_nonzeros=10)\n\nWe now wrap the fine-grained pruning function into a class for pruning the whole model. In class FineGrainedPruner, we have to keep a record of the pruning masks so that we could apply the masks whenever the model weights change to make sure the model keep sparse all the time.\n\nclass FineGrainedPruner:\n    def __init__(self, model, sparsity_dict):\n        self.masks = FineGrainedPruner.prune(model, sparsity_dict)\n\n    @torch.no_grad()\n    def apply(self, model):\n        for name, param in model.named_parameters():\n            if name in self.masks:\n                param *= self.masks[name]\n\n    @staticmethod\n    @torch.no_grad()\n    def prune(model, sparsity_dict):\n        masks = dict()\n        for name, param in model.named_parameters():\n            if param.dim() > 1: # we only prune conv and fc weights\n                masks[name] = fine_grained_prune(param, sparsity_dict[name])\n        return masks"
  },
  {
    "objectID": "labs/Lab1.html#sensitivity-scan",
    "href": "labs/Lab1.html#sensitivity-scan",
    "title": "TinyML KOR",
    "section": "Sensitivity Scan",
    "text": "Sensitivity Scan\nDifferent layers contribute differently to the model performance. It is challenging to decide the proper sparsity for each layer. A widely used approach is sensitivity scan.\nDuring the sensitivity scan, at each time, we will only prune one layer to see the accuracy degradation. By scanning different sparsities, we could draw the sensitivity curve (i.e., accuracy vs. sparsity) of the corresponding layer.\nHere is an example figure for sensitivity curves. The x-axis is the sparsity or the percentage of #parameters dropped (i.e., sparsity). The y-axis is the validation accuracy. (This is Figure 6 in Learning both Weights and Connections for Efficient Neural Networks)\n\n\n\nsensitivity curves.png\n\n\nThe following code cell defines the sensitivity scan function that returns the sparsities scanned, and a list of accuracies for each weight to be pruned.\n\n@torch.no_grad()\ndef sensitivity_scan(model, dataloader, scan_step=0.1, scan_start=0.4, scan_end=1.0, verbose=True):\n    sparsities = np.arange(start=scan_start, stop=scan_end, step=scan_step)\n    accuracies = []\n    named_conv_weights = [(name, param) for (name, param) \\\n                          in model.named_parameters() if param.dim() > 1]\n    for i_layer, (name, param) in enumerate(named_conv_weights):\n        param_clone = param.detach().clone()\n        accuracy = []\n        for sparsity in tqdm(sparsities, desc=f'scanning {i_layer}/{len(named_conv_weights)} weight - {name}'):\n            fine_grained_prune(param.detach(), sparsity=sparsity)\n            acc = evaluate(model, dataloader, verbose=False)\n            if verbose:\n                print(f'\\r    sparsity={sparsity:.2f}: accuracy={acc:.2f}%', end='')\n            # restore\n            param.copy_(param_clone)\n            accuracy.append(acc)\n        if verbose:\n            print(f'\\r    sparsity=[{\",\".join([\"{:.2f}\".format(x) for x in sparsities])}]: accuracy=[{\", \".join([\"{:.2f}%\".format(x) for x in accuracy])}]', end='')\n        accuracies.append(accuracy)\n    return sparsities, accuracies\n\nPlease run the following cells to plot the sensitivity curves. It should take around 2 minutes to finish.\n\nsparsities, accuracies = sensitivity_scan(\n    model, dataloader['test'], scan_step=0.1, scan_start=0.4, scan_end=1.0)\n\n\ndef plot_sensitivity_scan(sparsities, accuracies, dense_model_accuracy):\n    lower_bound_accuracy = 100 - (100 - dense_model_accuracy) * 1.5\n    fig, axes = plt.subplots(3, int(math.ceil(len(accuracies) / 3)),figsize=(15,8))\n    axes = axes.ravel()\n    plot_index = 0\n    for name, param in model.named_parameters():\n        if param.dim() > 1:\n            ax = axes[plot_index]\n            curve = ax.plot(sparsities, accuracies[plot_index])\n            line = ax.plot(sparsities, [lower_bound_accuracy] * len(sparsities))\n            ax.set_xticks(np.arange(start=0.4, stop=1.0, step=0.1))\n            ax.set_ylim(80, 95)\n            ax.set_title(name)\n            ax.set_xlabel('sparsity')\n            ax.set_ylabel('top-1 accuracy')\n            ax.legend([\n                'accuracy after pruning',\n                f'{lower_bound_accuracy / dense_model_accuracy * 100:.0f}% of dense model accuracy'\n            ])\n            ax.grid(axis='x')\n            plot_index += 1\n    fig.suptitle('Sensitivity Curves: Validation Accuracy vs. Pruning Sparsity')\n    fig.tight_layout()\n    fig.subplots_adjust(top=0.925)\n    plt.show()\n\nplot_sensitivity_scan(sparsities, accuracies, dense_model_accuracy)\n\n\nQuestion 4 (15 pts)\nPlease answer the following questions using the information in the above sensitivity curves.\n\nQuestion 4.1 (5 pts)\nWhat’s the relationship between pruning sparsity and model accuracy? (i.e., does accuracy increase or decrease when sparsity becomes higher?)\nYour Answer:\n\n\nQuestion 4.2 (5 pts)\nDo all the layers have the same sensitivity?\nYour Answer:\n\n\nQuestion 4.3 (5 pts)\nWhich layer is the most sensitive to the pruning sparsity?\nYour Answer:"
  },
  {
    "objectID": "labs/Lab1.html#parameters-of-each-layer",
    "href": "labs/Lab1.html#parameters-of-each-layer",
    "title": "TinyML KOR",
    "section": "#Parameters of each layer",
    "text": "#Parameters of each layer\nIn addition to accuracy, the number of each layer’s parameters also affects the decision on sparsity selection. Layers with more #parameters require larger sparsities.\nPlease run the following code cell to plot the distribution of #parameters in the whole model.\n\ndef plot_num_parameters_distribution(model):\n    num_parameters = dict()\n    for name, param in model.named_parameters():\n        if param.dim() > 1:\n            num_parameters[name] = param.numel()\n    fig = plt.figure(figsize=(8, 6))\n    plt.grid(axis='y')\n    plt.bar(list(num_parameters.keys()), list(num_parameters.values()))\n    plt.title('#Parameter Distribution')\n    plt.ylabel('Number of Parameters')\n    plt.xticks(rotation=60)\n    plt.tight_layout()\n    plt.show()\n\nplot_num_parameters_distribution(model)"
  },
  {
    "objectID": "labs/Lab1.html#select-sparsity-based-on-sensitivity-curves-and-parameters-distribution",
    "href": "labs/Lab1.html#select-sparsity-based-on-sensitivity-curves-and-parameters-distribution",
    "title": "TinyML KOR",
    "section": "Select Sparsity Based on Sensitivity Curves and #Parameters Distribution",
    "text": "Select Sparsity Based on Sensitivity Curves and #Parameters Distribution\n\nQuestion 5 (10 pts)\nBased on the sensitivity curves and the distribution of #parameters in the model, please select the sparsity for each layer.\nNote that the overall compression ratio of pruned model mostly depends on the layers with larger #parameters, and different layers have different sensitivity to pruning (see Question 4).\nPlease make sure that after pruning, the sparse model is 25% of the size of the dense model, and validation accuracy is higher than 92.5 after finetuning.\nHint: * The layer with more #parameters should have larger sparsity. (see Figure #Parameter Distribution) * The layer that is sensitive to the pruning sparsity (i.e., the accuracy will drop quickly as sparsity becomes higher) should have smaller sparsity. (see Figure Sensitivity Curves)\n\nrecover_model()\n\nsparsity_dict = {\n##################### YOUR CODE STARTS HERE #####################\n    # please modify the sparsity value of each layer\n    # please DO NOT modify the key of sparsity_dict\n    'backbone.conv0.weight': 0,\n    'backbone.conv1.weight': 0,\n    'backbone.conv2.weight': 0,\n    'backbone.conv3.weight': 0,\n    'backbone.conv4.weight': 0,\n    'backbone.conv5.weight': 0,\n    'backbone.conv6.weight': 0,\n    'backbone.conv7.weight': 0,\n    'classifier.weight': 0\n##################### YOUR CODE ENDS HERE #######################\n}\n\nPlease run the following cell to prune the model according to your defined sparsity_dict, and print the information of sparse model.\n\npruner = FineGrainedPruner(model, sparsity_dict)\nprint(f'After pruning with sparsity dictionary')\nfor name, sparsity in sparsity_dict.items():\n    print(f'  {name}: {sparsity:.2f}')\nprint(f'The sparsity of each layer becomes')\nfor name, param in model.named_parameters():\n    if name in sparsity_dict:\n        print(f'  {name}: {get_sparsity(param):.2f}')\n\nsparse_model_size = get_model_size(model, count_nonzero_only=True)\nprint(f\"Sparse model has size={sparse_model_size / MiB:.2f} MiB = {sparse_model_size / dense_model_size * 100:.2f}% of dense model size\")\nsparse_model_accuracy = evaluate(model, dataloader['test'])\nprint(f\"Sparse model has accuracy={sparse_model_accuracy:.2f}% before fintuning\")\n\nplot_weight_distribution(model, count_nonzero_only=True)"
  },
  {
    "objectID": "labs/Lab1.html#finetune-the-fine-grained-pruned-model",
    "href": "labs/Lab1.html#finetune-the-fine-grained-pruned-model",
    "title": "TinyML KOR",
    "section": "Finetune the fine-grained pruned model",
    "text": "Finetune the fine-grained pruned model\nAs we can see from the outputs of previous cell, even though fine-grained pruning reduces the most of model weights, the accuracy of model also dropped. Therefore, we have to finetune the sparse model to recover the accuracy.\nPlease run the following cell to finetune the sparse model. It should take around 3 minutes to finish.\n\nnum_finetune_epochs = 5\noptimizer = torch.optim.SGD(model.parameters(), lr=0.01, momentum=0.9, weight_decay=1e-4)\nscheduler = torch.optim.lr_scheduler.CosineAnnealingLR(optimizer, num_finetune_epochs)\ncriterion = nn.CrossEntropyLoss()\n\nbest_sparse_model_checkpoint = dict()\nbest_accuracy = 0\nprint(f'Finetuning Fine-grained Pruned Sparse Model')\nfor epoch in range(num_finetune_epochs):\n    # At the end of each train iteration, we have to apply the pruning mask\n    #    to keep the model sparse during the training\n    train(model, dataloader['train'], criterion, optimizer, scheduler,\n          callbacks=[lambda: pruner.apply(model)])\n    accuracy = evaluate(model, dataloader['test'])\n    is_best = accuracy > best_accuracy\n    if is_best:\n        best_sparse_model_checkpoint['state_dict'] = copy.deepcopy(model.state_dict())\n        best_accuracy = accuracy\n    print(f'    Epoch {epoch+1} Accuracy {accuracy:.2f}% / Best Accuracy: {best_accuracy:.2f}%')\n\nRun the following cell to see the information of best finetuned sparse model.\n\n# load the best sparse model checkpoint to evaluate the final performance\nmodel.load_state_dict(best_sparse_model_checkpoint['state_dict'])\nsparse_model_size = get_model_size(model, count_nonzero_only=True)\nprint(f\"Sparse model has size={sparse_model_size / MiB:.2f} MiB = {sparse_model_size / dense_model_size * 100:.2f}% of dense model size\")\nsparse_model_accuracy = evaluate(model, dataloader['test'])\nprint(f\"Sparse model has accuracy={sparse_model_accuracy:.2f}% after fintuning\")"
  },
  {
    "objectID": "labs/Lab1.html#remove-channel-weights",
    "href": "labs/Lab1.html#remove-channel-weights",
    "title": "TinyML KOR",
    "section": "Remove Channel Weights",
    "text": "Remove Channel Weights\nUnlike fine-grained pruning, we can remove the weights entirely from the tensor in channel pruning. That is to say, the number of output channels is reduced:\n\n\\(\\#\\mathrm{out\\_channels}_{\\mathrm{new}} = \\#\\mathrm{out\\_channels}_{\\mathrm{origin}} \\cdot (1 - \\mathrm{sparsity})\\)\n\nThe weight tensor \\(W\\) is still dense after channel pruning. Thus, we will refer to sparsity as prune ratio.\nLike fine-grained pruning, we can use different pruning rates for different layers. However, we use a uniform pruning rate for all the layers for now. We are targeting 2x computation reduction, which is roughly 30% uniform pruning rate (think about why).\nFeel free to try out different pruning ratios per layer at the end of this section. You can pass in a list of ratios to the channel_prune function.\n\nQuestion 6 (10 pts)\nPlease complete the following functions for channel pruning.\nHere we naively prune all output channels other than the first \\(\\#\\mathrm{out\\_channels}_{\\mathrm{new}}\\) channels.\n\ndef get_num_channels_to_keep(channels: int, prune_ratio: float) -> int:\n    \"\"\"A function to calculate the number of layers to PRESERVE after pruning\n    Note that preserve_rate = 1. - prune_ratio\n    \"\"\"\n    ##################### YOUR CODE STARTS HERE #####################\n    return int(round(0))\n    ##################### YOUR CODE ENDS HERE #####################\n\n@torch.no_grad()\ndef channel_prune(model: nn.Module,\n                  prune_ratio: Union[List, float]) -> nn.Module:\n    \"\"\"Apply channel pruning to each of the conv layer in the backbone\n    Note that for prune_ratio, we can either provide a floating-point number,\n    indicating that we use a uniform pruning rate for all layers, or a list of\n    numbers to indicate per-layer pruning rate.\n    \"\"\"\n    # sanity check of provided prune_ratio\n    assert isinstance(prune_ratio, (float, list))\n    n_conv = len([m for m in model.backbone if isinstance(m, nn.Conv2d)])\n    # note that for the ratios, it affects the previous conv output and next\n    # conv input, i.e., conv0 - ratio0 - conv1 - ratio1-...\n    if isinstance(prune_ratio, list):\n        assert len(prune_ratio) == n_conv - 1\n    else:  # convert float to list\n        prune_ratio = [prune_ratio] * (n_conv - 1)\n\n    # we prune the convs in the backbone with a uniform ratio\n    model = copy.deepcopy(model)  # prevent overwrite\n    # we only apply pruning to the backbone features\n    all_convs = [m for m in model.backbone if isinstance(m, nn.Conv2d)]\n    all_bns = [m for m in model.backbone if isinstance(m, nn.BatchNorm2d)]\n    # apply pruning. we naively keep the first k channels\n    assert len(all_convs) == len(all_bns)\n    for i_ratio, p_ratio in enumerate(prune_ratio):\n        prev_conv = all_convs[i_ratio]\n        prev_bn = all_bns[i_ratio]\n        next_conv = all_convs[i_ratio + 1]\n        original_channels = prev_conv.out_channels  # same as next_conv.in_channels\n        n_keep = get_num_channels_to_keep(original_channels, p_ratio)\n\n        # prune the output of the previous conv and bn\n        prev_conv.weight.set_(prev_conv.weight.detach()[:n_keep])\n        prev_bn.weight.set_(prev_bn.weight.detach()[:n_keep])\n        prev_bn.bias.set_(prev_bn.bias.detach()[:n_keep])\n        prev_bn.running_mean.set_(prev_bn.running_mean.detach()[:n_keep])\n        prev_bn.running_var.set_(prev_bn.running_var.detach()[:n_keep])\n\n        # prune the input of the next conv (hint: just one line of code)\n        ##################### YOUR CODE STARTS HERE #####################\n        next_conv.weight.set_(0)\n        ##################### YOUR CODE ENDS HERE #####################\n\n    return model\n\nRun the following cell to perform a sanity check to make sure the implementation is correct.\n\ndummy_input = torch.randn(1, 3, 32, 32).cuda()\npruned_model = channel_prune(model, prune_ratio=0.3)\npruned_macs = get_model_macs(pruned_model, dummy_input)\nassert pruned_macs == 305388064\nprint('* Check passed. Right MACs for the pruned model.')\n\nNow let’s evaluate the performance of the model after uniform channel pruning with 30% pruning rate.\nAs you may see, directly removing 30% of the channels leads to low accuracy.\n\npruned_model_accuracy = evaluate(pruned_model, dataloader['test'])\nprint(f\"pruned model has accuracy={pruned_model_accuracy:.2f}%\")"
  },
  {
    "objectID": "labs/Lab1.html#ranking-channels-by-importance",
    "href": "labs/Lab1.html#ranking-channels-by-importance",
    "title": "TinyML KOR",
    "section": "Ranking Channels by Importance",
    "text": "Ranking Channels by Importance\nAs you can see, removing the first 30% of channels in all layers leads to significant accuracy reduction. One potential method to remedy the issue is to find the less important channel weights to remove. A popular criterion for importance is to use the Frobenius norm of the weights corresponding to each input channel:\n\n\\(importance_{i} = \\|W_{i}\\|_2, \\;\\; i = 0, 1, 2,\\cdots, \\#\\mathrm{in\\_channels}-1\\)\n\nWe can sort the channel weights from more important to less important, and then keep the frst \\(k\\) channels for each layer.\n\nQuestion 7 (15 pts)\nPlease complete the following functions for sorting the weight tensor based on the Frobenius norm.\nHint: * To calculate Frobenius norm of a tensor, Pytorch provides torch.norm APIs.\n\n# function to sort the channels from important to non-important\ndef get_input_channel_importance(weight):\n    in_channels = weight.shape[1]\n    importances = []\n    # compute the importance for each input channel\n    for i_c in range(weight.shape[1]):\n        channel_weight = weight.detach()[:, i_c]\n        ##################### YOUR CODE STARTS HERE #####################\n        importance = 0\n        ##################### YOUR CODE ENDS HERE #####################\n        importances.append(importance.view(1))\n    return torch.cat(importances)\n\n@torch.no_grad()\ndef apply_channel_sorting(model):\n    model = copy.deepcopy(model)  # do not modify the original model\n    # fetch all the conv and bn layers from the backbone\n    all_convs = [m for m in model.backbone if isinstance(m, nn.Conv2d)]\n    all_bns = [m for m in model.backbone if isinstance(m, nn.BatchNorm2d)]\n    # iterate through conv layers\n    for i_conv in range(len(all_convs) - 1):\n        # each channel sorting index, we need to apply it to:\n        # - the output dimension of the previous conv\n        # - the previous BN layer\n        # - the input dimension of the next conv (we compute importance here)\n        prev_conv = all_convs[i_conv]\n        prev_bn = all_bns[i_conv]\n        next_conv = all_convs[i_conv + 1]\n        # note that we always compute the importance according to input channels\n        importance = get_input_channel_importance(next_conv.weight)\n        # sorting from large to small\n        sort_idx = torch.argsort(importance, descending=True)\n\n        # apply to previous conv and its following bn\n        prev_conv.weight.copy_(torch.index_select(\n            prev_conv.weight.detach(), 0, sort_idx))\n        for tensor_name in ['weight', 'bias', 'running_mean', 'running_var']:\n            tensor_to_apply = getattr(prev_bn, tensor_name)\n            tensor_to_apply.copy_(\n                torch.index_select(tensor_to_apply.detach(), 0, sort_idx)\n            )\n\n        # apply to the next conv input (hint: one line of code)\n        ##################### YOUR CODE STARTS HERE #####################\n        next_conv.weight.copy_(0)\n        ##################### YOUR CODE ENDS HERE #####################\n\n    return model\n\nNow run the following cell to sanity check if the results are correct.\n\nprint('Before sorting...')\ndense_model_accuracy = evaluate(model, dataloader['test'])\nprint(f\"dense model has accuracy={dense_model_accuracy:.2f}%\")\n\nprint('After sorting...')\nsorted_model = apply_channel_sorting(model)\nsorted_model_accuracy = evaluate(sorted_model, dataloader['test'])\nprint(f\"sorted model has accuracy={sorted_model_accuracy:.2f}%\")\n\n# make sure accuracy does not change after sorting, since it is\n# equivalent transform\nassert abs(sorted_model_accuracy - dense_model_accuracy) < 0.1\nprint('* Check passed.')\n\nFinally, we compare the pruned models’ accuracy with and without sorting.\n\nchannel_pruning_ratio = 0.3  # pruned-out ratio\n\nprint(\" * Without sorting...\")\npruned_model = channel_prune(model, channel_pruning_ratio)\npruned_model_accuracy = evaluate(pruned_model, dataloader['test'])\nprint(f\"pruned model has accuracy={pruned_model_accuracy:.2f}%\")\n\n\nprint(\" * With sorting...\")\nsorted_model = apply_channel_sorting(model)\npruned_model = channel_prune(sorted_model, channel_pruning_ratio)\npruned_model_accuracy = evaluate(pruned_model, dataloader['test'])\nprint(f\"pruned model has accuracy={pruned_model_accuracy:.2f}%\")\n\nAs you can see, the channel sorting can slightly improve the pruned model’s accuracy, but there is still a huge degrade, which is quite common for channel pruning. But luckily, we can perform fine-tuning to recover the accuracy.\n\nnum_finetune_epochs = 5\noptimizer = torch.optim.SGD(pruned_model.parameters(), lr=0.01, momentum=0.9, weight_decay=1e-4)\nscheduler = torch.optim.lr_scheduler.CosineAnnealingLR(optimizer, num_finetune_epochs)\ncriterion = nn.CrossEntropyLoss()\n\nbest_accuracy = 0\nfor epoch in range(num_finetune_epochs):\n    train(pruned_model, dataloader['train'], criterion, optimizer, scheduler)\n    accuracy = evaluate(pruned_model, dataloader['test'])\n    is_best = accuracy > best_accuracy\n    if is_best:\n        best_accuracy = accuracy\n    print(f'Epoch {epoch+1} Accuracy {accuracy:.2f}% / Best Accuracy: {best_accuracy:.2f}%')"
  },
  {
    "objectID": "labs/Lab1.html#measure-acceleration-from-pruning",
    "href": "labs/Lab1.html#measure-acceleration-from-pruning",
    "title": "TinyML KOR",
    "section": "Measure acceleration from pruning",
    "text": "Measure acceleration from pruning\nAfter fine-tuning, the model almost recovers the accuracy. You may have already learned that channel pruning is usually more difficult to recover accuracy compared to fine-grained pruning. However, it directly leads to a smaller model size and smaller computation without specialized model format. It can also run faster on GPUs. Now we compare the model size, computation, and latency of the pruned model.\n\n# helper functions to measure latency of a regular PyTorch models.\n#   Unlike fine-grained pruning, channel pruning\n#   can directly leads to model size reduction and speed up.\n@torch.no_grad()\ndef measure_latency(model, dummy_input, n_warmup=20, n_test=100):\n    model.eval()\n    # warmup\n    for _ in range(n_warmup):\n        _ = model(dummy_input)\n    # real test\n    t1 = time.time()\n    for _ in range(n_test):\n        _ = model(dummy_input)\n    t2 = time.time()\n    return (t2 - t1) / n_test  # average latency\n\ntable_template = \"{:<15} {:<15} {:<15} {:<15}\"\nprint (table_template.format('', 'Original','Pruned','Reduction Ratio'))\n\n# 1. measure the latency of the original model and the pruned model on CPU\n#   which simulates inference on an edge device\ndummy_input = torch.randn(1, 3, 32, 32).to('cpu')\npruned_model = pruned_model.to('cpu')\nmodel = model.to('cpu')\n\npruned_latency = measure_latency(pruned_model, dummy_input)\noriginal_latency = measure_latency(model, dummy_input)\nprint(table_template.format('Latency (ms)',\n                            round(original_latency * 1000, 1),\n                            round(pruned_latency * 1000, 1),\n                            round(original_latency / pruned_latency, 1)))\n\n# 2. measure the computation (MACs)\noriginal_macs = get_model_macs(model, dummy_input)\npruned_macs = get_model_macs(pruned_model, dummy_input)\nprint(table_template.format('MACs (M)',\n                            round(original_macs / 1e6),\n                            round(pruned_macs / 1e6),\n                            round(original_macs / pruned_macs, 1)))\n\n# 3. measure the model size (params)\noriginal_param = get_num_parameters(model)\npruned_param = get_num_parameters(pruned_model)\nprint(table_template.format('Param (M)',\n                            round(original_param / 1e6, 2),\n                            round(pruned_param / 1e6, 2),\n                            round(original_param / pruned_param, 1)))\n\n# put model back to cuda\npruned_model = pruned_model.to('cuda')\nmodel = model.to('cuda')\n\n\nQuestion 8 (10 pts)\nPlease answer the following questions using the information in the previous code cell.\n\nQuestion 8.1 (5 pts)\nExplain why removing 30% of channels roughly leads to 50% computation reduction.\nYour Answer:\n\n\nQuestion 8.2 (5 pts)\nExplain why the latency reduction ratio is slightly smaller than computation reduction.\nYour Answer:"
  },
  {
    "objectID": "labs/Lab1.html#question-9-10-pts",
    "href": "labs/Lab1.html#question-9-10-pts",
    "title": "TinyML KOR",
    "section": "Question 9 (10 pts)",
    "text": "Question 9 (10 pts)\nAfter all experiments in this lab, you may have become familiar with both fine-grained pruning and channel pruning.\nPlease answer the following questions using what you have learned from the lectures and this lab.\n\nQuestion 9.1 (5 pts)\nWhat are the advantages and disadvantages of fine-grained pruning and channel pruning? You can discuss from the perspective of compression ratio, accuracy, latency, hardware support (i.e., requiring specialized hardware accelerator), etc.\nYour Answer:\n\n\nQuestion 9.2 (5 pts)\nIf you want to make your model run faster on a smartphone, which pruning method will you use? Why?\nYour Answer:"
  },
  {
    "objectID": "labs/lab01.html",
    "href": "labs/lab01.html",
    "title": "Lab 1",
    "section": "",
    "text": "TinyML\n\nmembers\n\n이정연\n오승현\n성기정\n\n\n\nTinyML and Efficient Deep Learning Computing\n강의 사이트: https://hanlab.mit.edu/courses/2023-fall-65940\n\n목표: 강의 영상 리뷰 및 Lab 실습 완료\n\nGithub 정리 자료 Archiving (참고: GNN Study )\n아카이빙을 하고 싶은 이유\n참고 링크에 남겨둔 스터디가 이전 글또 7기에서 같이 했던 그래프 스터디인데 남겨놓으니 나중에 잊어버렸을때 참고하기도 좋고, 각자 정리하면서 기억에도 더 잘 남는 것 같더라구요!\n\n21Lec + 4Lab (Lec1/2와 Lab0은 제외)\n\n강의를 듣고\n돌아가면서 로테이션 강의 복습 recap 발표 - 1명\n\n다른 사람들 질문/디스커션 - 외 3명 다같이\n\n\n주 1회 (약 16주 - 4개월 이내 완료 목표)\n\n\n\n[Chapter I: Efficient Inference]\n\nLecture 3: Pruning and Sparsity (Part I) [P1]\nLecture 4: Pruning and Sparsity (Part II) [P2]\n— Lab 1 → 2주 분량 [P3]\nLecture 5: Quantization (Part I) [P4]\nLecture 6: Quantization (Part II) [P1]\n— Lab 2 [P1]\nLecture 7: Neural Architecture Search (Part I) [P2]\nLecture 8: Neural Architecture Search (Part II) [P2]\n— Lab 3\nLecture 9: Knowledge Distillation [P1]\nLecture 10: MCUNet: TinyML on Microcontrollers\nLecture 11: TinyEngine and Parallel Processing [P2]\n\n\n\n[Chapter II: Domain-Specific Optimization]\n\nLecture 12: Transformer and LLM (Part I)\nLecture 13: Transformer and LLM (Part II)\nLecture 14: Vision Transformer\n— Lab 4\nLecture 15: GAN, Video, and Point Cloud\nLecture 16: Diffusion Model\nLecture 17: Distributed Training (Part I)\nLecture 18: Distributed Training (Part II)\n— Lab 5\nLecture 19: On-Device Training and Transfer Learning\nLecture 20: Efficient Fine-tuning and Prompt Engineering\n\n\n\n[Chapter IV: Advanced Topics]\n\nLecture 21: Basics of Quantum Computing\nLecture 22: Quantum Machine Learning\nLecture 23: Noise Robust Quantum ML"
  },
  {
    "objectID": "lecs/lec01.html",
    "href": "lecs/lec01.html",
    "title": "Lecture 1",
    "section": "",
    "text": "1.1 - Why Graphs\n그래프란?\n⇒ a general language for describing and analyzing entities with relations/interactions\n⇒ 서로 관계/상호작용하는 entity들을 설명하고 분석하기 위한 언어라고 할 수 있음 (여기서 entity란 정보의 세계에서 의미있는 하나의 정보 단위)\n\n그래프 예시\n\nevent graphs\ncomputer networks\ndisease pathways\nfood webs\nparticle networks\nunderground networks\nsocial networks\neconomic networks\ncommunication networks\ncitation networks\ninternet\nnetworks of neurons\nknowledge graphs\nregulatory networks\nscene graphs\ncode graphs\nmolecules\n3D shapes\n\n\n⇒ 그래프로 세상에 일어나는 모든 현상과 구조들을 설명할 수 있다(broadly applicable).\n(관심있는 분야의 현상을 그래프로 표현하여 딥러닝 모델 구조로 변환할 수 있는 능력이 있다면…)\ne.g) 분자 구조, 3D 이미지 모형(voxel), 먹이사슬, 소셜 네트워크 등\n(graph와 network의 차이?)\n그래프에서 얻을 수 있는 정보 유형\n\n데이터 포인트 간의 구성(organization)과 연결\n유사한 데이터 포인트 간의 밀접성(similarity)\n데이터 포인트 간의 연결들이 이루는 그래프 구조\n\n그래프가 갖는 구조를 어떻게 활용해 나은 예측을 할 수 있을까?\n⇒ 현상을 명시적(explicitly)으로 잘 반영한 그래프 모델링이 중요하다!\n그래프 ML이 더 어려운 이유?\n⇒ arbitrary size and complex topology\n⇒ spatial locality(공간 지역성)가 없다\n⇒ 이미지나 텍스트 인풋의 경우 어느 한 데이터포인트로부터 다른 데이터 포인트 간 상대적 위치가 정해져있다(e.g 상하좌우). 하지만, 그래프의 경우 축이 되는 데이터 포인트가 존재하지 않는다.\n그래프를 사용한 딥러닝\n⇒ 인풋으로는 그래프를 받고, 아웃풋으로는 아래와 같은 형식(ground truth도 동일한 형식)이 가능하다. >\n\nnode-level\nedge-level\ngraph/subgraph generation\ngraph/subgraph classification\n\n그래프 딥러닝 모델에서 우리가 바라는 플로우\n\n\n위 플로우를 거친 좋은 성능의 모델을 만들기 위해서는, 인풋이 현상을 잘 반영한 embedding vector로 변환될 수 있도록 학습하는 것이 중요하다.(Representation Learning)\n\n코스 과정동안 배울 그래프 방법들\n\ntraditional methods : graphlets, graph kernels\nnode embeddings : DeepWalk, Node2Vec\nGraph Neural Networks : GCN, GraphSAGE, GAT, Theory of GNNs\nKnowledge graphs and reasoning : TransE, BetaE\nDeep Generative Models for graphs\nApplications\n\n\n\n\n1.2 - Applications of Graph ML\n그래프 ML은 다양한 태스크 커버가 가능하다\n\nNode Level(node classification)\n\n\nexample1) protein folding (구글의 알파폴드)\n\n배경 : 단백질은 아미노산으로 이루어져있는데, 복잡한 3D 입체 구조의 아미노산 연결 때문에 단백질 구조를 파악하는 태스크는 많게는 1-2년까지 걸린다고 함.\nkey idea : spatial graph\n\ngraph : 단백질\nnodes : 아미노산\nedges : 사슬구조\n\n\n\nEdge Level\n\n\nexample1) Recommender system (PinSage)\n\nnodes : users and items\nedges : user-item interactions\n\n\nexample2) drugs and side effects\n\nnodes : drugs & proteins\nedges : interaction\n\nusing 2 heterogeneous graphs(drugs & proteins)\n\n\n\n스크린샷 2022-07-05 오후 11.15.49.png\n\n\n⇒ drug A와 B를 함께 썼을 때, 생길 수 있는 interaction(edge)는 무엇인가?\n\nCommunity(subgraph) level\n\n\nexample1) Traffic Prediction\n\nnodes : Road Segments(도로 구간)\nedges : 도로 구간 교차점\n\n\n⇒ 아웃풋 : 도착 예정 시간\n\nGraph-level prediction\n\n\n4-1) graph classification\nexample1) drug discovery\n\nnodes : atoms\nedges : chemical bonds\ngraph : molecules\n\n⇒ 노드와 에지 정보를 통해 그래프(분자) 예측\n4-2) Graph-level generation\nexample1) drug generation\n\nnodes : atoms\nedges : chemical bonds\ngraph : moelcules\n\n⇒ 새로운 그래프(분자) 생성\nexample2) physics simulation (graph evolution)\n\nnodes : particles\nedges : interaction between particles\n\n⇒ t시점 전의 정보로 t시점 이후의 그래프 생성\n\n\n\n1.3 - Choice of Graph\n그래프 구성요소\n\nobjects : nodes, vertices\nInteractions : links, edges\nsystem : network, graph\n**object와 interactions로 이루어진 데이터 구조 ⇒ graph\n그래프로 세상에 일어나는 모든 현상과 구조들을 설명할 수 있다(broadly applicable).\n\n\n표현법에 따라 그래프의 활용방법은 무궁무진하다. 중요한 것은 적절한 표현을 선택하는 것.\n설명하고자하는 현상을 그래프로 정의하고 싶다면, 먼저 아래 두가지 질문을 하자.\n\nwhat are nodes?\nwhat are edges?\n\n\nDirected vs Undirected Graphs\n\n’페이스북의 친구와 인스타그램의 팔로우’는 directed와 undirected graph를 이해하기에 좋은 예이다.\nNode Degrees (차수)\n\nundirected graph\n\nundirected graph의 경우, node degree는 해당 노드에 연결된 에지의 총 갯수\naverage degree는 연결된 에지의 총 갯수 곱하기 2(쌍방향 연결이기 때문)를 그래프를 이루는 전체 노드 수로 나눈 값\n\ndirected graph\n\ndirected graph의 경우, 해당 노드로 향하는 in-degree와 해당 노드로부터 뻗어나가는 out-degree로 나눌 수 있다. node degree는 이 in-degree와 out-degree의 합.\n\n\nBipartite Graph\n\n\n자주 등장하는 또다른 그래프의 종류는 bipartite graph(이분 그래프)이다.\n이분 그래프는 2개의 집합 U와 V의 interaction을 나타낸 그래프이다. U와 V는 서로 독립적인 집합이며, 같은 집합의 원소끼리는 연관되지 않는다. e.g) A와 B 간 연결X\n이분 그래프의 예로는 구매자-구매 아이템 관계 등이 있다.\n\nFolded/Projected Bipartite Graph\n\nBipartite 그래프에서 집합 간의 요소들 간의 상관관계가 명시되어있다면 Folded 또는 Projected Bipartite Graph라고 부른다.\n\n(정보에 depth가 생긴다는 의미에서 folded라고 붙인듯하다.)\n\nAdjacency Matrix\n\n\n그래프의 노드 간 연결관계(edge)를 나타낸 매트릭스이다.\n각 행과 열은 노드의 번수를 의미하고, 0은 연결되지 않음, 1은 연결됨을 의미한다. 만약 3번째 행에 4번째 열이 1이라면, 3번 노드와 4번 노드는 연결되어있음을 의미한다.\nundirect graph라면, 주대각선을 기준으로 adj matrix는 대칭이고, directed graph라면, 대칭이 아닐 수도 있다.\nadjacency matrix는 컴퓨터가 그래프를 이해할 수 있는 형태이지만, 문제는 노드의 수가 수백개에서 수십만개로 늘어나고, 많은 노드들의 연결이 몇 개 없을 때, 메모리 사이즈에 비해 0인 값이 너무 많게되는 문제(sparse)가 발생한다.\n\nEdge List\n\n\nEdge List는 그래프를 엣지들의 리스트로 나타낸 값이다.\n서로 연결된 노드를 짝지어 리스트에 배열한다.\n그래프가 크고 sparse할 때 유용하다.\n\nNode and Edge Attributes\n⇒ 노드와 엣지로 나타낼 수 있는 값들은 어떤 것들이 있을까?\n⇒ 그래프에서 어떤 정보들을 얻을 수 있을까?\n\nweight (e.g., frequency of communication)\n\n엣지가 0과 1 이외의 값을 갖는다면?\n\n\nranking (best friend, second best friend, …)\ntype (friend, relative, co-worker)\nsign (+ / - )\nproperties depending on the structure of the rest of the graph : Number of common friends\n\nMore Types of Graphs\n\nConnected(undirected) graph\n\n\n**연결이 부분적으로만 되어 있어도, 그래프는 adjacency matrix에 표현 가능하다.\n\nConnectivity of Driected Graphs\n\nStrongly connected directed graph\n\n모든 노드들이 다른 모든 노드들로 방향 상관없이 다다르는 path가 항상 있다면, strongly connected directed graph이다.\n\nWeakly connected directed graph\n\n에지 방향을 무시했을 때, 노드 간 전부 연결되어있다면, weakly connected directed graph이다.\n\nStrongly connected components(SCC)\n\n\n그래프에 속한 다른 노드들 전부는 아니지만, 해당 그룹 간의 연결이 한 노드에서 다른 노드로 항상 도달할 수 있다면(strong connection)한다면, 그 그룹을 strongly connected components라고 지칭한다."
  },
  {
    "objectID": "lecs/lec03.html",
    "href": "lecs/lec03.html",
    "title": "Lecture 1",
    "section": "",
    "text": "강의 3"
  }
]