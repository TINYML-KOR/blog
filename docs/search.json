[
  {
    "objectID": "about.html",
    "href": "about.html",
    "title": "Study Group",
    "section": "",
    "text": "@Gueltto9th\n\nGroup Members"
  },
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "Study Archive",
    "section": "",
    "text": "No matching items"
  },
  {
    "objectID": "labs/lab01.html",
    "href": "labs/lab01.html",
    "title": "Lab 1",
    "section": "",
    "text": "TinyML\n\nmembers\n\n이정연\n오승현\n성기정\n\n\n\nTinyML and Efficient Deep Learning Computing\n강의 사이트: https://hanlab.mit.edu/courses/2023-fall-65940\n\n목표: 강의 영상 리뷰 및 Lab 실습 완료\n\nGithub 정리 자료 Archiving (참고: GNN Study )\n아카이빙을 하고 싶은 이유\n참고 링크에 남겨둔 스터디가 이전 글또 7기에서 같이 했던 그래프 스터디인데 남겨놓으니 나중에 잊어버렸을때 참고하기도 좋고, 각자 정리하면서 기억에도 더 잘 남는 것 같더라구요!\n\n21Lec + 4Lab (Lec1/2와 Lab0은 제외)\n\n강의를 듣고\n돌아가면서 로테이션 강의 복습 recap 발표 - 1명\n\n다른 사람들 질문/디스커션 - 외 3명 다같이\n\n\n주 1회 (약 16주 - 4개월 이내 완료 목표)\n\n\n\n[Chapter I: Efficient Inference]\n\nLecture 3: Pruning and Sparsity (Part I) [P1]\nLecture 4: Pruning and Sparsity (Part II) [P2]\n— Lab 1 → 2주 분량 [P3]\nLecture 5: Quantization (Part I) [P4]\nLecture 6: Quantization (Part II) [P1]\n— Lab 2 [P1]\nLecture 7: Neural Architecture Search (Part I) [P2]\nLecture 8: Neural Architecture Search (Part II) [P2]\n— Lab 3\nLecture 9: Knowledge Distillation [P1]\nLecture 10: MCUNet: TinyML on Microcontrollers\nLecture 11: TinyEngine and Parallel Processing [P2]\n\n\n\n[Chapter II: Domain-Specific Optimization]\n\nLecture 12: Transformer and LLM (Part I)\nLecture 13: Transformer and LLM (Part II)\nLecture 14: Vision Transformer\n— Lab 4\nLecture 15: GAN, Video, and Point Cloud\nLecture 16: Diffusion Model\nLecture 17: Distributed Training (Part I)\nLecture 18: Distributed Training (Part II)\n— Lab 5\nLecture 19: On-Device Training and Transfer Learning\nLecture 20: Efficient Fine-tuning and Prompt Engineering\n\n\n\n[Chapter IV: Advanced Topics]\n\nLecture 21: Basics of Quantum Computing\nLecture 22: Quantum Machine Learning\nLecture 23: Noise Robust Quantum ML"
  },
  {
    "objectID": "lecs/lec01.html",
    "href": "lecs/lec01.html",
    "title": "Lecture 1",
    "section": "",
    "text": "1.1 - Why Graphs\n그래프란?\n⇒ a general language for describing and analyzing entities with relations/interactions\n⇒ 서로 관계/상호작용하는 entity들을 설명하고 분석하기 위한 언어라고 할 수 있음 (여기서 entity란 정보의 세계에서 의미있는 하나의 정보 단위)\n\n그래프 예시\n\nevent graphs\ncomputer networks\ndisease pathways\nfood webs\nparticle networks\nunderground networks\nsocial networks\neconomic networks\ncommunication networks\ncitation networks\ninternet\nnetworks of neurons\nknowledge graphs\nregulatory networks\nscene graphs\ncode graphs\nmolecules\n3D shapes\n\n\n⇒ 그래프로 세상에 일어나는 모든 현상과 구조들을 설명할 수 있다(broadly applicable).\n(관심있는 분야의 현상을 그래프로 표현하여 딥러닝 모델 구조로 변환할 수 있는 능력이 있다면…)\ne.g) 분자 구조, 3D 이미지 모형(voxel), 먹이사슬, 소셜 네트워크 등\n(graph와 network의 차이?)\n그래프에서 얻을 수 있는 정보 유형\n\n데이터 포인트 간의 구성(organization)과 연결\n유사한 데이터 포인트 간의 밀접성(similarity)\n데이터 포인트 간의 연결들이 이루는 그래프 구조\n\n그래프가 갖는 구조를 어떻게 활용해 나은 예측을 할 수 있을까?\n⇒ 현상을 명시적(explicitly)으로 잘 반영한 그래프 모델링이 중요하다!\n그래프 ML이 더 어려운 이유?\n⇒ arbitrary size and complex topology\n⇒ spatial locality(공간 지역성)가 없다\n⇒ 이미지나 텍스트 인풋의 경우 어느 한 데이터포인트로부터 다른 데이터 포인트 간 상대적 위치가 정해져있다(e.g 상하좌우). 하지만, 그래프의 경우 축이 되는 데이터 포인트가 존재하지 않는다.\n그래프를 사용한 딥러닝\n⇒ 인풋으로는 그래프를 받고, 아웃풋으로는 아래와 같은 형식(ground truth도 동일한 형식)이 가능하다. >\n\nnode-level\nedge-level\ngraph/subgraph generation\ngraph/subgraph classification\n\n그래프 딥러닝 모델에서 우리가 바라는 플로우\n\n\n위 플로우를 거친 좋은 성능의 모델을 만들기 위해서는, 인풋이 현상을 잘 반영한 embedding vector로 변환될 수 있도록 학습하는 것이 중요하다.(Representation Learning)\n\n코스 과정동안 배울 그래프 방법들\n\ntraditional methods : graphlets, graph kernels\nnode embeddings : DeepWalk, Node2Vec\nGraph Neural Networks : GCN, GraphSAGE, GAT, Theory of GNNs\nKnowledge graphs and reasoning : TransE, BetaE\nDeep Generative Models for graphs\nApplications\n\n\n\n\n1.2 - Applications of Graph ML\n그래프 ML은 다양한 태스크 커버가 가능하다\n\nNode Level(node classification)\n\n\nexample1) protein folding (구글의 알파폴드)\n\n배경 : 단백질은 아미노산으로 이루어져있는데, 복잡한 3D 입체 구조의 아미노산 연결 때문에 단백질 구조를 파악하는 태스크는 많게는 1-2년까지 걸린다고 함.\nkey idea : spatial graph\n\ngraph : 단백질\nnodes : 아미노산\nedges : 사슬구조\n\n\n\nEdge Level\n\n\nexample1) Recommender system (PinSage)\n\nnodes : users and items\nedges : user-item interactions\n\n\nexample2) drugs and side effects\n\nnodes : drugs & proteins\nedges : interaction\n\nusing 2 heterogeneous graphs(drugs & proteins)\n\n\n\n스크린샷 2022-07-05 오후 11.15.49.png\n\n\n⇒ drug A와 B를 함께 썼을 때, 생길 수 있는 interaction(edge)는 무엇인가?\n\nCommunity(subgraph) level\n\n\nexample1) Traffic Prediction\n\nnodes : Road Segments(도로 구간)\nedges : 도로 구간 교차점\n\n\n⇒ 아웃풋 : 도착 예정 시간\n\nGraph-level prediction\n\n\n4-1) graph classification\nexample1) drug discovery\n\nnodes : atoms\nedges : chemical bonds\ngraph : molecules\n\n⇒ 노드와 에지 정보를 통해 그래프(분자) 예측\n4-2) Graph-level generation\nexample1) drug generation\n\nnodes : atoms\nedges : chemical bonds\ngraph : moelcules\n\n⇒ 새로운 그래프(분자) 생성\nexample2) physics simulation (graph evolution)\n\nnodes : particles\nedges : interaction between particles\n\n⇒ t시점 전의 정보로 t시점 이후의 그래프 생성\n\n\n\n1.3 - Choice of Graph\n그래프 구성요소\n\nobjects : nodes, vertices\nInteractions : links, edges\nsystem : network, graph\n**object와 interactions로 이루어진 데이터 구조 ⇒ graph\n그래프로 세상에 일어나는 모든 현상과 구조들을 설명할 수 있다(broadly applicable).\n\n\n표현법에 따라 그래프의 활용방법은 무궁무진하다. 중요한 것은 적절한 표현을 선택하는 것.\n설명하고자하는 현상을 그래프로 정의하고 싶다면, 먼저 아래 두가지 질문을 하자.\n\nwhat are nodes?\nwhat are edges?\n\n\nDirected vs Undirected Graphs\n\n’페이스북의 친구와 인스타그램의 팔로우’는 directed와 undirected graph를 이해하기에 좋은 예이다.\nNode Degrees (차수)\n\nundirected graph\n\nundirected graph의 경우, node degree는 해당 노드에 연결된 에지의 총 갯수\naverage degree는 연결된 에지의 총 갯수 곱하기 2(쌍방향 연결이기 때문)를 그래프를 이루는 전체 노드 수로 나눈 값\n\ndirected graph\n\ndirected graph의 경우, 해당 노드로 향하는 in-degree와 해당 노드로부터 뻗어나가는 out-degree로 나눌 수 있다. node degree는 이 in-degree와 out-degree의 합.\n\n\nBipartite Graph\n\n\n자주 등장하는 또다른 그래프의 종류는 bipartite graph(이분 그래프)이다.\n이분 그래프는 2개의 집합 U와 V의 interaction을 나타낸 그래프이다. U와 V는 서로 독립적인 집합이며, 같은 집합의 원소끼리는 연관되지 않는다. e.g) A와 B 간 연결X\n이분 그래프의 예로는 구매자-구매 아이템 관계 등이 있다.\n\nFolded/Projected Bipartite Graph\n\nBipartite 그래프에서 집합 간의 요소들 간의 상관관계가 명시되어있다면 Folded 또는 Projected Bipartite Graph라고 부른다.\n\n(정보에 depth가 생긴다는 의미에서 folded라고 붙인듯하다.)\n\nAdjacency Matrix\n\n\n그래프의 노드 간 연결관계(edge)를 나타낸 매트릭스이다.\n각 행과 열은 노드의 번수를 의미하고, 0은 연결되지 않음, 1은 연결됨을 의미한다. 만약 3번째 행에 4번째 열이 1이라면, 3번 노드와 4번 노드는 연결되어있음을 의미한다.\nundirect graph라면, 주대각선을 기준으로 adj matrix는 대칭이고, directed graph라면, 대칭이 아닐 수도 있다.\nadjacency matrix는 컴퓨터가 그래프를 이해할 수 있는 형태이지만, 문제는 노드의 수가 수백개에서 수십만개로 늘어나고, 많은 노드들의 연결이 몇 개 없을 때, 메모리 사이즈에 비해 0인 값이 너무 많게되는 문제(sparse)가 발생한다.\n\nEdge List\n\n\nEdge List는 그래프를 엣지들의 리스트로 나타낸 값이다.\n서로 연결된 노드를 짝지어 리스트에 배열한다.\n그래프가 크고 sparse할 때 유용하다.\n\nNode and Edge Attributes\n⇒ 노드와 엣지로 나타낼 수 있는 값들은 어떤 것들이 있을까?\n⇒ 그래프에서 어떤 정보들을 얻을 수 있을까?\n\nweight (e.g., frequency of communication)\n\n엣지가 0과 1 이외의 값을 갖는다면?\n\n\nranking (best friend, second best friend, …)\ntype (friend, relative, co-worker)\nsign (+ / - )\nproperties depending on the structure of the rest of the graph : Number of common friends\n\nMore Types of Graphs\n\nConnected(undirected) graph\n\n\n**연결이 부분적으로만 되어 있어도, 그래프는 adjacency matrix에 표현 가능하다.\n\nConnectivity of Driected Graphs\n\nStrongly connected directed graph\n\n모든 노드들이 다른 모든 노드들로 방향 상관없이 다다르는 path가 항상 있다면, strongly connected directed graph이다.\n\nWeakly connected directed graph\n\n에지 방향을 무시했을 때, 노드 간 전부 연결되어있다면, weakly connected directed graph이다.\n\nStrongly connected components(SCC)\n\n\n그래프에 속한 다른 노드들 전부는 아니지만, 해당 그룹 간의 연결이 한 노드에서 다른 노드로 항상 도달할 수 있다면(strong connection)한다면, 그 그룹을 strongly connected components라고 지칭한다."
  },
  {
    "objectID": "lecs/lec03.html",
    "href": "lecs/lec03.html",
    "title": "Lecture 1",
    "section": "",
    "text": "강의 3"
  }
]