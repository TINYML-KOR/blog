[
  {
    "objectID": "about.html",
    "href": "about.html",
    "title": "TinyML Study Group",
    "section": "",
    "text": "@Gueltto9th"
  },
  {
    "objectID": "about.html#lec-4lab",
    "href": "about.html#lec-4lab",
    "title": "TinyML Study Group",
    "section": "21Lec + 4Lab",
    "text": "21Lec + 4Lab\n\nLec1/2와 Lab0은 제외\n강의를 듣고 1명씩 돌아가면서 강의 복습 recap 발표\n다른 사람들은 질문/디스커션 토픽 가져오기\n주 1회 (약 16주 - 4개월 이내 완료 목표)\n\n\n\n[Chapter I: Efficient Inference]\n\nLecture 3: Pruning and Sparsity (Part I) @ooshyun\nLecture 4: Pruning and Sparsity (Part II) @curieuxjy\nLab 1 @CastleFlag\nLecture 5: Quantization (Part I) @ooshyun\nLecture 6: Quantization (Part II) @ooshyun\nLab 2 @curieuxjy\nLecture 7: Neural Architecture Search (Part I) @CastleFlag\nLecture 8: Neural Architecture Search (Part II) @CastleFlag\nLab 3 @ooshyun\nLecture 9: Knowledge Distillation @curieuxjy\nLecture 10: MCUNet: TinyML on Microcontrollers @curieuxjy\nLecture 11: TinyEngine and Parallel Processing @CastleFlag\n\n\n\n[Chapter II: Domain-Specific Optimization]\n\nLecture 12: Transformer and LLM (Part I) @ooshyun\nLecture 13: Transformer and LLM (Part II) @curieuxjy\nLecture 14: Vision Transformer @CastleFlag\nLab 4 @ooshyun\nLecture 15: GAN, Video, and Point Cloud @curieuxjy\nLecture 16: Diffusion Model @CastleFlag\nLecture 17: Distributed Training (Part I) @ooshyun\nLecture 18: Distributed Training (Part II) @curieuxjy\nLab 5 @CastleFlag\nLecture 19: On-Device Training and Transfer Learning @ooshyun\nLecture 20: Efficient Fine-tuning and Prompt Engineering @curieuxjy\n\n\n\n[Chapter IV: Advanced Topics]\n\nLecture 21: Basics of Quantum Computing @CastleFlag\nLecture 22: Quantum Machine Learning @ooshyun\nLecture 23: Noise Robust Quantum ML @curieuxjy"
  },
  {
    "objectID": "posts/lecs/lec03.html",
    "href": "posts/lecs/lec03.html",
    "title": "🧑‍🏫 Lecture 3",
    "section": "",
    "text": "앞으로 총 5장에 걸쳐서 딥러닝 모델 경량화 기법들에 대해서 소개하려고 한다. 경량화 기법으로는 Pruning, Quantization, Neural Network Architecture Search, Knowledge Distillation, 그리고 Tiny Engine에서 돌리기 위한 방법을 진행할 예정인데 본 내용은 MIT에서 Song Han 교수님이 Fall 2022에 한 강의 TinyML and Efficient Deep Learning Computing 6.S965를 바탕으로 재정리한 내용이다. 강의 자료와 영상은 이 링크를 참조하자!\n첫 번째 내용으로 “가지치기”라는 의미를 가진 Pruning에 대해서 이야기, 시작!"
  },
  {
    "objectID": "posts/lecs/lec03.html#introduction-to-pruning",
    "href": "posts/lecs/lec03.html#introduction-to-pruning",
    "title": "🧑‍🏫 Lecture 3",
    "section": "1. Introduction to Pruning",
    "text": "1. Introduction to Pruning\nPruning이란 의미처럼 Neural Network에서 매개변수(노드)를 제거하는 방법입니다. 이는 Dropout하고 비슷한 의미로 볼 수 있는데, Dropout의 경우 모델 훈련 도중 랜덤적으로 특정 노드를 제외시키고 훈련시켜 모델의 Robustness를 높이는 방법으로 훈련을 하고나서도 모델의 노드는 그대로 유지가 된다. 반면 Pruning의 경우 훈련을 마친 후에, 특정 Threshold 이하의 매개변수(노드)의 경우 시 Neural Network에서 제외시켜 모델의 크기를 줄이면서 동시에 추론 속도 또한 높일 수 있다.\n\\[\n\\underset{W_p}{argmin}\\ L(x;W_p), \\text{ subject to } \\lvert\\lvert W_p\\lvert\\lvert_0\\ &lt; N\n\\]\n\nL represents the objective function for neural network training\n\\(x\\) is input, \\(W\\) is original weights, \\(W_p\\) is pruned weights\n\\(\\lvert\\lvert W_p\\lvert\\lvert_0\\) calcuates the #nonzeros in \\(W_p\\) and \\(N\\) is the target #nonzeros\n\n이는 위와 같은 식으로 표현할 수 있다. 특정 W 의 경우 0 으로 만들어 노드를 없애는 경우라고 볼 수 있겠습니다. 그렇게 Pruning한 Neural Network는 아래 그림 처럼 된다.\n\n\n\nReference. MIT-TinyML-lecture03-Pruning-1\n\n\n그럼 왜 Pruning을 하는 걸까? 강의에서 Pruning을 사용하면 Latency, Memeory와 같은 리소스를 확보할 수 있다고 관련된 아래같은 연구결과를 같이 보여준다.\n\n\n\nReference. MIT-TinyML-lecture03-Pruning-1\n\n\nSong Han 교수님은 Vision 딥러닝 모델 경량화 연구를 주로하셔서, CNN을 기반으로 한 모델을 예시로 보여주신다. 모두 Pruning이후에 모델 사이즈의 경우 최대 12배 줄어 들며 연산의 경우 6.3배까지 줄어 든 것을 볼 수 다.\n그렇다면 저렇게 “크기가 줄어든 모델이 성능을 유지할 수 있을까?“\n\n\n\nReference. MIT-TinyML-lecture03-Pruning-1\n\n\n그래프에서 모델의 Weight 분포도를 위 그림에서 보면, Pruning을 하고 난 이후에 Weight 분포도의 중심에 파라미터가 잘려나간 게 보인다. 이후 Fine Tuning을 하고 난 다음의 분포가 나와 있는데, 어느 정도 정확도는 떨어지지만 성능이 유지되는 걸 관찰할 수 있다.\n\n\n\nReference. MIT-TinyML-lecture03-Pruning-1\n\n\n그런 Fine tuning을 반복적으로 하게 된다면(Iterative Pruning and Fine tuning) 그래프에서는 최대 90프로 이상의 파라미터를 덜어낼 수 있다고 한다.\n물론 특정 모델에서, 특정 Task를 대상으로 한 것이라 일반화할 수는 없지만 리소스를 고려하는 상황이라면 충분히 시도해볼 만한 가치가 있어 보인다. 그럼 이렇게 성능을 유지하면서 Pruning을 하기 위해서 어떤 요소를 고려해야 할지 더 자세히 이야기해보자!\n소개하는 고려요소는 아래와 같다. Pruning 패턴부터 차례대로 시작!\n\nPruning Granularity → Pruning 패턴\nPruning Criterion → 얼마만큼에 파라미터를 Pruning 할 건가?\nPruning Ratio → 전체 파라미터에서 Pruning을 얼마만큼의 비율로?\nFine Turning → Pruning 이후에 어떻게 Fine-Tuning 할 건가?\nADMM → Pruning 이후, 어떻게 Convex가 된다고 할 수 있지?\nLottery Ticket Hypothesis → Training부터 Pruning까지 모델을 만들어 보자!\nSystem Support → 하드웨어나 소프트웨어적으로 Pruning을 지원하는 경우는?"
  },
  {
    "objectID": "posts/lecs/lec03.html#determine-the-pruning-granularity",
    "href": "posts/lecs/lec03.html#determine-the-pruning-granularity",
    "title": "🧑‍🏫 Lecture 3",
    "section": "2. Determine the Pruning Granularity",
    "text": "2. Determine the Pruning Granularity\n\n\n\nThe case of convolutional layers, red box is preserved and white one is pruned referred from MIT-TinyML-lecture03-Pruning-1\n\n\n여기서 고려요소는 “얼마만큼 뉴런을 그룹화하여 고려할 것인가?” 입니다. Regular한 정로도 분류하면서 Irregular한 경우와 Regular한 경우의 특징을 아래처럼 말합니다.\n\nFine-grained/Unstructured\n\nMore flexible pruning index choice\nHard to accelerate (irregular data expression)\nCan deliver speed up on some custom hardware\n\nCoarse-grained/Structured\n\nLess flexible pruning index choice (a subset of the fine-grained case)\nEasy to accelerate\n\n\nPruning을 한다고 모델 출력이 나오는 시간이 짧아지는 것이 아님도 언급합니다. Hardware Acceleration의 가능도가 있는데, 이 특징을 보면 알 수 있듯, Pruning의 자유도와 Hardware Acceleration이 trade-off, 즉 경량화 정도와 Latency사이에 trade-off 가 있을 것이 예측됩니다. 하나씩, 자료를 보면서 살펴 보겠습니다.\n\n2.1 Pattern-based Pruning\nIrregular에서도 Pattern-based Pruning은 연속적인 뉴런 M개 중 N개를 Pruning 하는 방법이다. 일반적으로는 N:M = 2:4 으로 한다고 소개한다.\n\n\n\nReference. Accelerating Inference with Sparsity Using the NVIDIA Ampere Architecture and NVIDIA TensorRT\n\n\nReference. Accelerating Inference with Sparsity Using the NVIDIA Ampere Architecture and NVIDIA TensorRT\n예시를 들어 보면, 위와 같은 Matrix에서 행을 보시면 8개의 Weight중 4개가 Non-zero인 것을 볼 수 있습니다. 여기서 Zero인 부분을 없애고 2bit index로 하여 Matrix 연산을 하면 Nvidia’s Ampere GPU에서 속도를 2배까지 높일 수 있다고 한다. 여기서 Sparsity는 “얼마만큼 경량화 됐는지?” 이라고 생각하면 된다.\n\nN:M sparsity means that in each contiguous M elements, N of them is pruned\nA classic case is 2:4 sparsity (50% sparsity)\nIt is supported by Nvidia’s Ampere GPU Architecture, which delivers up to 2x speed up and usually maintains accuracy.\n\n\n\n\nReference. https://developer.nvidia.com/blog/accelerating-inference-with-sparsity-using-ampere-and-tensorrt/\n\n\n\n\n2.2 Channel-level Pruning\n반대로 패턴이 상대적으로 regular 한 쪽인 Channel-level Pruning은 추론시간을 줄일 수 있는 반면에 경량화 비율이 적다고 말한다. 아래 그림을 보시면 Layer마다 Sparsity가 다른 걸 보실 수 있다.\n\nPro: Direct speed up!\nCon: smaller compression ratio\n\n\n\n\nReference AMC: Automl for Model Compression and Acceleration on Mobile Devices [He et al., ECCV 2018]\n\n\n아래에 자료에서는 Channel 별로 한 Pruning의 경우 전체 뉴련을 가지고 한 Pruning보다 추론 시간을 더 줄일 수 있다고 말한다.\n\n\n\nReference AMC: Automl for Model Compression and Acceleration on Mobile Devices [He et al., ECCV 2018]\n\n\n자료를 보면 Sparsity에서는 패턴화 돼 있으면 가속화가 용이해 Latency, 추론 시간을 줄일 수 있지만 그 만큼 Pruning하는 뉴런의 수가 적어 경량화 비율이 줄 것으로 보인다. 하지만 비교적 불규칙한 쪽에 속하는 Pattern-based Pruning의 경우가 하드웨어에서 지원해주는 경우, 모델 크기와 Latency를 둘 다 최적으로 잡을 수 있을 것으로 보인다."
  },
  {
    "objectID": "posts/lecs/lec03.html#determine-the-pruning-criterion",
    "href": "posts/lecs/lec03.html#determine-the-pruning-criterion",
    "title": "🧑‍🏫 Lecture 3",
    "section": "3. Determine the Pruning Criterion",
    "text": "3. Determine the Pruning Criterion\n그렇다면 어떤 파라미터를 가지는 뉴런을 우리는 잘라내야 할까요? Synapse와 Neuron으로 나눠서 살펴보자.\n\nWhich synapses? Which neurons? Which one is less important?\nHow to Select Synapses and Select Neurons to Prune\n\n\n3.1 Select of Synapses\n크게 세 가지로 분류하는데, 각 뉴런의 크기, 각 채널에 전체 뉴런에 대한 크기, 그리고 테일러 급수를 이용하여 gradient와 weight를 모두 고려한 크기를 소개한다. Song han 교수님이 방법들을 소개하기에 앞서서 유수의 기업들도 지난 5년 동안 주로 Magnitude-based Pruning만을 사용해왔다고 하는데, 2023년이 돼서 On-device AI가 각광받기 시작해서 점차적으로 관심을 받기 시작한 건가 싶기도 하다.\n3.1.1 Magnitude-based Pruning\n크기를 기준으로 하는 경우, “얼마만큼 뉴런 그룹에서 고려할 것인가?”와 “그룹내에서 어떤 정규화를 사용할 것인가?를 고려한다.\n\nHeuristic pruning criterion, Element-wise Pruning\n\\[\nImportance = \\lvert W \\lvert\n\\]\n\n\n\nReference. MIT-TinyML-lecture03-Pruning-1\n\n\nHeuristic pruning criterion, Row-wise Pruning, L1-norm magnitude\n\\[\nImportance = \\sum_{i\\in S}\\lvert w_i \\lvert, \\\\where\\ W^{(S)}\\ is\\ the\\ structural\\ set\\ S\\ of\\ parameters\\ W\n\\]\n\n\n\nReference. MIT-TinyML-lecture03-Pruning-1\n\n\nHeuristic pruning criterion, Row-wise Pruning, L2-norm magnitude\n\\[\nImportance = \\sum_{i\\in S}\\lvert w_i \\lvert, \\\\where\\ W^{(S)}\\ is\\ the\\ structural\\ set\\ S\\ of\\ parameters\\ W\n\\]\n\n\n\nReference. MIT-TinyML-lecture03-Pruning-1\n\n\nHeuristic pruning criterion, \\(L_p\\)- norm\n\\[\n\\lvert\\lvert W^{(S)}\\lvert\\lvert=\\huge( \\large\n   \\sum_{i\\in S} \\lvert w_i \\lvert^p\n\\huge) \\large^{\\frac{1}{p}}\n\\]\n\n3.1.2 Scaling-based Pruning\n두 번째로 Scaling을 하는 경우 채널마다 Scaling Factor를 둬서 Pruning을 한다. 그럼 Scaling Factor를 어떻게 둬야 할까? 강의에서 소개하는 이 논문에서는 Scaling factor \\(\\gamma\\) 파라미터를 trainable 파라미터로 두면서 batch normalization layer에 사용한다.\n\nScale factor is associated with each filter(i.e. output channel) in convolution layers.\nThe filters or output channels with small scaling factor magnitude will be pruned\nThe scaling factors can be reused from batch normalization layer\n\\[\n  z_o = \\gamma\\dfrac{z_i-\\mu_{B}}{\\sqrt{\\sigma_B^2+\\epsilon}}+\\beta\n  \\]\n\n\n\n\nReference. MIT-TinyML-lecture03-Pruning-1\n\n\n3.1.3 Talyor Expansion Analysis on Pruning Error\n세 번째 방법은 테일러 급수를 이용하여 Objective function을 최소화 하는 지점을 찾는 방법입니다. Talyor Series에 대한 자세한 내용은 여기서!\n\nEvaluate pruning error induced by pruning synapses.\nMinimize the objective function L(x; W)\nA Taylor series can approximate the induced error.\n\n\\[\n\\delta L = L(x;W)-L(x;W_p=W-\\delta W) \\\\ = \\sum_i g_i\\delta w_i + \\frac{1}{2} \\sum_i h_{ii}\\delta w_i^2 + \\frac{1}{2}\\sum_{i\\not=j}h_{ij}\\delta w_i \\delta w_j + O(\\lvert\\lvert \\delta W \\lvert\\lvert^3)\n\\] \\[\nwhere\\ g_i=\\dfrac{\\delta L}{\\delta w_i}, h_{i, j} = \\dfrac{\\delta^2 L}{\\delta w_i \\delta w_j}\n\\]\n\nSecond-Order-based Pruning\n\n\n\nReference. MIT-TinyML-lecture03-Pruning-1\n\n\n\n\n\nReference. MIT-TinyML-lecture03-Pruning-1\n\n\nOptimal Brain Damage[LeCun et al., NeurIPS 1989] 논문에서는 이 방법을 이용하기 위해 세 가지를 가정한다.\n\nObjective function L이 quadratic 이기 때문에 마지막 항이 무시된다(이는 Talyor Series의 Error 항을 알면 이해가 더 쉽다!)\n만약 신경망이 수렴하게되면, 첫 번째항도 무시된다.\n각 파라미터가 독립적이라면 Cross-term도 무시된다.\n\n그러면 식을 아래처럼 정리할 수 있는데, 중요한 부분은 Hessian Matrix H에 사용하는 Computation이 어렵다는 점!\n\\[\n\\delta L_i = L(x;W)-L(x;W_p\\lvert w_i=0)\\approx \\dfrac{1}{2} h_{ii}w_i^2,\\ where\\ h_{ii}=\\dfrac{\\partial^2 L}{\\partial w_i \\partial w_j}\n\\]\n\\[\nimportance_{w_i} = \\lvert \\delta L_i\\lvert = \\frac{1}{2}h_{ii}w_i^2\n\\] \\[\n*\\ h_{ii} \\text{ is non-negative}\n\\]\nFirst-Order-based Pruning\n\n참고로 이 방법은 2023년에는 소개하지 않는다.\n\n\n\n\nReference. MIT-TinyML-lecture03-Pruning-1\n\n\n\nIf only first-order expansion is considered under an i.i.d(Independent and identically distributed) assumption,\n\n\\[\n\\delta L_i = L(x;W) - L(x; W_P\\lvert w_i=0) \\approx g_iw_i,\\ where\\ g_i=\\dfrac{\\partial L}{\\partial w_i}\n\\] \\[\nimportance_{w_i} = \\lvert \\delta L_i \\lvert = \\lvert g_i w_i \\lvert \\ or \\ importance_{w_i} = \\lvert \\delta L_i \\lvert^2 = (g_i w_i)^2\n\\]\n\nFor coarse-grained pruning, we have,\n\\[\n  importance_{\\ W^{(S)}} = \\sum_{i \\in S}\\lvert \\delta L_i \\lvert^2 = \\sum_{i \\in S} (g_i w_i)^2,\\ where \\ W^{(S)}is\\ the\\ structural\\ set\\ of\\ parameters\n  \\]\n\n\n\n\n3.2 Select of Neurons\n어떤 Neuron을 없앨 지를 고려(Less useful → Remove) 한 이 방법은 Neuron의 경우도 있지만 아래 그림처럼 Channel로 고려할 수도 있다. 확실히 전에 소개했던 방법들보다 “Coarse-grained pruning”인 방법이다.\n\n\nPercentage-of-Zero-based Pruning\n첫번째는 Channel마다 0의 비율을 봐서 비율이 높은 Channel 을 없내는 방법이다. ReLU activation을 사용하면 Output이 0이 나오는데, 여기서 0의 비율, Average Percentage of Zero activations(APoZ)라고 부르는 것을 보고 가지치기할 Channel을 제거한다.\n\nReLU activation will generate zeros in the output activation\nSimilar to magnitude of weights, the Average Percentage of Zero activations(APoZ) can be exploited to measure the importance the neuron has\n\n\n\n\nReference. MIT-TinyML-lecture03-Pruning-1\n\n\nFirst-Order-based Pruning\n\n참고로 이 방법은 2023년에는 소개하지 않는 방법이다.\nMinimize the error on loss function introduced by pruning neurons\nSimilar to previous Taylor expansion on weights, the induced error of the objective function L(x; W) can be approximated by a Taylor series expanded on activations.\n\\[\n  \\delta L_i = L(x; W) - L(x\\lvert x_i = 0; W) \\approx \\dfrac{\\partial L}{\\partial x_i}x_i\n  \\]\nFor a structural set of neurons \\(x^{(S)}\\) (e.g., a channel plane),\n\\[\n  \\lvert \\delta L_{x^{(S)}} \\lvert\\ = \\Large\\lvert \\small\\sum_{i\\in S}\\dfrac{\\partial L}{\\partial x_i}x_i\\Large\\lvert\n  \\]\n\nRegression-based Pruning\n이 방법은 Quantized한 레이어의 output \\(\\hat Z\\)(construction error of the corresponding layer’s outputs)와 \\(Z\\)를 Training을 통해 차이를 줄이는 방법이다. 참고로 문제를 푸는 자세한 과정은 2022년 강의에만 나와 있다.\n\n\\[\nZ=XW^T=\\sum_{c=0}^{c_i-1}X_cW_c^T\n\\]\n\n\n\nReference. MIT-TinyML-lecture03-Pruning-1\n\n\n문제를 식으로 정의해보면 아래와 같은데,\n\n\\(\\beta\\) is the coefficient vector of length \\(c_i\\) for channel selection.\n\\(\\beta_c = 0\\) means channel \\(c\\) is pruned.\n\\(N_c\\) is the number of none zero channel\n\n우선 문제를 푸는 단계는 두 단계로 나눈다. Channel의 Scale \\(\\beta\\)를 우선 계산한 후에 \\(W\\)를 Quantized한 레이어의 output \\(\\hat Z\\)(construction error of the corresponding layer’s outputs)와 \\(Z\\)의 차이가 최소화되는 지점까지 Training시킨다.\nSolve the problem in two folds:\n\nFix W, solve \\(\\beta\\) for channel selection → NP(Nondeterministic polynomial)-hard\nFix \\(\\beta\\), solve W to minimize reconstruction error(Weight Reconstruction)\n\n각 문제를 푸는 과정을 조금 더 자세히 살펴봐보자. 본 내용은 2022년 강의에 있으니 참고!\nNP(Nondeterministic polynomial)-hard는 아래와 같이 식으로 정리할 수 있다.\n\\[\n\\underset{\\beta}{argmin} \\lvert\\lvert Z- \\sum_{c=0}^{c_i-1} \\beta_cX_cW_c^T \\lvert\\lvert_F^2 = \\lvert\\lvert \\sum_{c=0}^{c_i-1}X_cW_c^T - \\sum_{c=0}^{c_i-1} \\beta_cX_cW_c^T \\lvert\\lvert_F^2\n\\] \\[\n= \\lvert\\lvert\\sum_{c=0}^{c_i-1} (1-\\beta_c)X_cW_c^T \\lvert\\lvert_F^2, \\ s.t.\\ \\lvert\\lvert\\beta\\lvert\\lvert_0 \\ \\leq N_c\n\\]\n강의에서 소개하는 ThiNet이라는 논문에서는 greedy solution을 이용해서 채널 하나하나씩 Pruning 해보며 objective function의 l2-norm 최솟값을 구한다.\n1: S = []\n2: while len(S) &lt; N:\n3:   min_norm, min_c = +inf, 0\n4:   for c in range(c_i):\n5:     tmpS=S+[c]\n6:     Z = X[:,tmpS] * W[:,tmpS].t()\n7:     norm = Z.norm(2)\n8:     if norm &lt; min_norm:\n9:       min_norm, min_c = norm, c\n10:   S.append(min_c)\n11:   c_i.pop(min_c)\n여기서 더해서 \\(\\beta\\) 를 구하는 과정에서 일반화를 위해 LASSO 방식을 사용한다(LASSO에 대한 자세한 내용은 여기서). Relax the \\(l_0\\) to \\(l_1\\) regularization (LASSO):\n\\[\n\\underset{\\beta}{argmin}\\ \\lvert\\lvert Z- \\sum_{c=0}^{c_i-1}\\beta_cX_cW_c^T\\lvert\\lvert^2_F+\\lambda\\lvert\\lvert \\beta \\lvert\\lvert_1\n\\]\n\n\\(\\lambda\\) is a penalty coefficient. By increasing \\(\\lambda\\), there will be more zeros in \\(\\beta\\).\nGradually increase \\(\\lambda\\) and solve the LASSO regression for \\(\\beta\\), until \\(\\lvert\\lvert \\beta \\lvert\\lvert_0==N_c\\) is met.\nWhy \\(\\lvert\\lvert \\beta \\lvert\\lvert_0==N_c\\)?\n여기에 대해서는 따로 언급되지 않았지만, 의미상 scale 전체 N개 중에서 최적값을 찾아야한다면 전체를 N으로 유지하면서 최적값을 찾기 위해서가 아닐까?\n\n두 번째는 구한 \\(\\beta\\)를 고정한 상태로 Weight를 Quantized 전후의 차이를 최소화 하게 “Weight Reconstruction” 한다. 구하는 과정은 least square approach를 이용한 unique closed-form solution 이므로 아래를 참조하자.\n\\[\n\\underset{\\beta}{argmin}\\ \\lvert\\lvert Z- \\sum_{c=0}^{c_i-1}\\beta_cX_cW_c^T\\lvert\\lvert^2_F\n\\]\n\n\\(\\beta\\) is a coefficient vector from the previous step\nThis is a classic linear regression problem, which has a unique closed-form solution using the least square approach.\n\\[\n  \\underset{W}{argmin} \\lvert\\lvert Z-\\hat{Z} \\lvert\\lvert^2_F = \\lvert\\lvert Z-UW^T \\lvert\\lvert_F^2\n  \\]\nwhere\n\\[\n  U= \\Large[ \\small\\beta_0X_0\\ \\beta_1X_1 \\ \\cdots \\beta_cX_c \\cdots \\beta_{c_i-1}X_{c_i-1} \\Large]\n  \\]\nand thus,\n\\[\n  W^T = (U^TU)^{-1}U^T Z\n  \\]\n\nQ. How \\((U^TU)^{-1}\\) exists?\nLeast Square method, 임의의 벡터 \\(v = (v_0, v_1, \\dots, v_n)\\) 가 있을 때 \\(v^Tv\\) 의 역행렬은 항상 있을까? 가정에서 “a unique closed-form solution”라고 했으므로 이는 즉 linearly independen로 고려할 있고 역행렬이 있다(\\(v^Tv\\) is invertible)는 이야기이다."
  },
  {
    "objectID": "posts/lecs/lec03.html#discussion",
    "href": "posts/lecs/lec03.html#discussion",
    "title": "🧑‍🏫 Lecture 3",
    "section": "4. Discussion",
    "text": "4. Discussion\n\nPruning을 Dropout이랑 비교해서 어떤 차이점이 있는가?\n두 가지 방법은 분명히 Neuron과 Synapse를 없댄다는 측면에서는 비슷하다. 하지만 두 가지 측면에서 차이점이 있는데, 한 가지는 목적하는 바이고, 두 번째는 시점이다. Dropout은 목적하는 바가 훈련중에 overfitting을 방지하기 위함이 있고 Pruning의 경우는 훈련을 마친 모델의 크기를 줄이는 것에 있다. 그리고 두 번째 시점의 경우 Dropout은 훈련중에 이뤄지는 반면 Pruning은 훈련을 마치고, 그 크기를 줄인 후에 성능이 떨어지면 그에 맞게 Fine-tuning을 한다.\n스터디에서는 “왜 dropout을 통해 사이즈를 줄이지 않았는가? 그리고 구지 훈련을 마친 다음에 할 필요가 있나?” 라고 질문이 나왔었다. 물론 훈련 중에 모델의 사이즈를 작게 만들 수 있으면, 가능한 그렇게 하면 될 것이다. 하지만, 이 또한 두가지 측면을 고려할 필요가 있다. 하나는 “과연 모델의 사이즈를 훈련 중 혹은 전에 줄여나가면서 충분히 성능을 낼 수 있는가?”이고 다른 하나는 Pruning이나 모델 경량화는 최적화에 초점을 맞춘다고 생각한다. 그렇기 때문에 훈련 중간에 Channel pruning과 같은 기법을 사용할 수 있을 지는 미지수이고, 설령 Fine-grained Pruning과 같은 기법을 사용한다 하더라도 이는 모델의 사이즈만 줄어 들 뿐, 나머지 메모리(e.g. RAM)이나 Latency같은 성능은 좋게 가져갈 수 있을지도 미지수라고 생각한다.\n필자는 위와 같은 최적화를 통한 성능 개선을 이 글에서처럼 2022년 TinyML 강의에서 제공하는 실습을 통해 경험했었다. 앞선 예시는 OS를 가진 디바이스가 아닌 Bare-metal firmware로 환경이 조금 특수하기도 하고, 실제로 Torch나 Tensorflowlite에서 제공하는 모델 경량화를 직접적으로 분석해봐야 실질적인 예시를 알 수 있겠지만, 혹여 이해해 참고가 될까 덧붙여 놓는다."
  },
  {
    "objectID": "posts/lecs/lec03.html#reference",
    "href": "posts/lecs/lec03.html#reference",
    "title": "🧑‍🏫 Lecture 3",
    "section": "5. Reference",
    "text": "5. Reference\n\nMIT-TinyML-lecture03-Pruning-1\nAMC: Automl for Model Compression and Acceleration on Mobile Devices, 2018\nLearning Efficient Convolutional Networks through Network Slimming, 2017\nThiNet: A Filter Level Pruning Method for Deep Neural Network Compression, 2017\nChannel Pruning for Accelerating Very Deep Neural Networks"
  },
  {
    "objectID": "posts/lecs/lec05.html",
    "href": "posts/lecs/lec05.html",
    "title": "🧑‍🏫 Lecture 5-6",
    "section": "",
    "text": "이번 글에서는 MIT HAN LAB에서 강의하는 TinyML and Efficient Deep Learning Computing에 나오는 Quantization 방법을 소개하려 한다. Quantization(양자화) 신호와 이미지에서 아날로그를 디지털로 변환하는 과정에서 사용하는 개념이다. 아래 그림과 같이 연속적인 센서로 부터 들어오는 아날로그 데이터 나 이미지를 표현하기 위해 단위 시간에 대해서 데이터를 샘플링하여 데이터를 수집한다.\n디지털로 데이터를 변환하기 위해 데이터 타입을 정하면서 이를 하나씩 양자화한다. 양수와 음수를 표현하기 위해 Unsigned Integer 에서 Signed Integer, Signed에서도 Sign-Magnitude 방식과 Two’s Complement방식으로, 그리고 더 많은 소숫점 자리를 표현하기 위해 Fixed-point에서 Floating point로 데이터 타입에서 수의 범주를 확장시킨다. 참고로 Device의 Computationality와 ML 모델의 성능지표중 하나인 FLOP이 바로 floating point operations per second이다.\n이 글에서 floating point를 이해하면, fixed point를 사용하는 것이 매모리에서, 그리고 연산에서 더 효율적일 것이라고 예상해볼 있 수 있다. ML모델을 클라우드 서버에서 돌릴 때는 크게 문제되지 않았지만 아래 두 가지 표를 보면 에너지소모, 즉 배터리 효율에서 크게 차이가 보인다. 그렇기 때문에 모델에서 Floating point를 fixed point로 더 많이 바꾸려고 하는데 이 방법으로 나온 것이 바로 Quatization이다.\n이번 글에서는 Quntization 중에서 Quantization 방법과 그 중 Linear한 방법에 대해 더 자세하게, 그리고 Post-training Quantization까지 다루고, 다음 글에서는 Quantization-Aware Training, Binary/Tenary Quantization, Mixed Precision Quantization까지 다루려고 한다."
  },
  {
    "objectID": "posts/lecs/lec05.html#common-network-quantization",
    "href": "posts/lecs/lec05.html#common-network-quantization",
    "title": "🧑‍🏫 Lecture 5-6",
    "section": "1. Common Network Quantization",
    "text": "1. Common Network Quantization\n앞서서 소개한 것처럼 Neural Netowork를 위한 Quantization은 다음과 같이 나눌 수 있다. Quantization 방법을 하나씩 알아보자.\n\n\n\nReference. MIT-TinyML-lecture5-Quantization-1\n\n\n\n\n\n Reference. MIT-TinyML-lecture5-Quantization-1 in https://efficientml.ai \n\n\n\n1.1 K-Means-based Quantization\n그 중 첫 번째로 K-means-based Quantization이 있다. Deep Compression [Han et al., ICLR 2016] 논문에 소개했다는 이 방법은 중심값을 기준으로 clustering을 하는 방법이다. 예제를 봐보자.\n\n\n\nReference. MIT-TinyML-lecture5-Quantization-1\n\n\n위 예제는 weight를 codebook에서 -1, 0, 1.5, 2로 나눠 각각에 맞는 인덱스로 표기한다. 이렇게 연산을 하면 기존에 64bytes를 사용했던 weight가 20bytes로 줄어든다. codebook으로 예제는 2bit로 나눴지만, 이를 N-bit만큼 줄인다면 우리는 총 32/N배의 메모리를 줄일 수 있다. 하지만 이 과정에서 quantizatio error, 즉 quantization을 하기 전과 한 후에 오차가 생기는 것을 위 예제에서 볼 수 있다. 메모리 사용량을 줄이는 것도 좋지만, 이 때문에 성능에 오차가 생기지 않게 하기위해 이 오차를 줄이는 것 또한 중요하다.\n\n\n\nReference. MIT-TinyML-lecture5-Quantization-1\n\n\n이를 보완하기 위해 Quantized한 Weight를 위에 그림처럼 Fine-tuning하기도 한다. centroid를 fine-tuning한다고 생각하면 되는데, 각 centroid에서 생기는 오차를 평균내 tuning하는 방법이다. 이 방법을 제안한 논문 에서는 Convolution 레이어에서는 4bit까지 centroid를 가졌을 때, Full-Connected layer에서는 2 bit까지 centroid를 가졌을 때 성능에 하락이 없다고 말하고 있었다.\n\n\n\nReference. Deep Compression [Han et al., ICLR 2016]\n\n\n이렇게 Quantization 된 Weight는 위처럼 연속적인 값에서 아래처럼 Discrete한 값으로 바뀐다.\n\n\n\nReference. Deep Compression [Han et al., ICLR 2016]\n\n\n논문은 이렇게 Quantization한 weight를 한 번 더 Huffman coding를 이용해 최적화시킨다. 짧게 설명하자면, 빈도수가 높은 문자는 짧은 이진코드를, 빈도 수가 낮은 문자에는 긴 이진코드를 쓰는 방법이다. 압축 결과로 General한 모델과 압축 비율이 꽤 큰 SqueezeNet을 예로 든다. 자세한 내용은 논문을 참고하는 걸로.\n\n\n\nReference. Deep Compression [Han et al., ICLR 2016]\n\n\n\n\n\nReference. Deep Compression [Han et al., ICLR 2016]\n\n\ninference를 위해 weight를 Decoding하는 과정은 inference과정에서 저장한 cluster의 인덱스를 이용해 codebook에서 해당하는 값을 찾아내는 것이다. 이 방법은 저장 공간을 줄일 수는 있지만, floating point Computation이나 메모리 접근하는 방식으로 centroid를 쓰는 한계가 있을 수 밖에 없다.\n\n\n\nReference. MIT-TinyML-lecture5-Quantization-1\n\n\n\n\n\n Reference. Deep Compression [Han et al., ICLR 2016] \n\n\n\n\n1.2 Linear Quantization\n두 번째 방법은 Linear Quatization이다. floating-point인 weight를 N-bit의 정수로 affine mapping을 시키는 방법이다. 간단하게 식으로 보는 게 더 이해가 쉽다.\n\n\n\nReference. MIT-TinyML-lecture5-Quantization-1\n\n\n여기서 S(Scale of Linear Quantization)와 Z(Zero point of Linear Quantization)가 있는데 이 둘이 quantization parameter 로써 tuning을 할 수 있는 값인 것이다.\n\n\n\nReference. MIT-TinyML-lecture5-Quantization-1\n\n\n\n\n1.3 Scale and Zero point\n\n\n\nReference. MIT-TinyML-lecture5-Quantization-1\n\n\n이 Scale과 Zero point 두 파라미터를 이용해서 affine mapping은 위 그림과 같다. Bit 수(Bit Width)가 낮아지면 낮아질 수록, floating point에서 표현할 있는 수 또한 줄어들 것이다. 그렇다면 Scale와 Zero point는 각각 어떻게 계산할까?\n우선 floating-point 인 숫자의 범위 중 최대값과 최솟값에 맞게 두 식을 세우고 이를 연립방정식으로 Scale과 Zero point을 구할 수 있다.\n\nScale point \\[\n  r_{max} = S(q_{max}-Z)\n  \\] \\[\n  r_{min} = S(q_{min}-Z)\n  \\]\n\\[\n  r_{max} - r_{min} = S(q_{max} - q_{min})\n  \\]\n\\[\n  S = \\dfrac{r_{max}-r_{min}}{q_{max}-q_{min}}\n  \\]\nZero point \\[\n  r_{min} = S(q_{min}-Z)\n  \\]\n\\[\n  Z=q_{min}-\\dfrac{r_{min}}{S}\n  \\]\n\\[\n  Z = round\\Big(q_{min}-\\dfrac{r_{min}}{S}\\Big)\n  \\]\n\n예를 들어, 아래와 같은 예제에서 \\(r_{max}\\) 는\\(2.12\\) 이고 \\(r_{min}\\) 은 \\(-1.08\\) 로 Scale을 계산하면 아래 그림처럼 된다. Zero point는 \\(-1\\) 로 계산할 수 있다.\n\n\n\n\nReference. MIT-TinyML-lecture5-Quantization-1\n\n\n그럼 Symmetric하게 r의 범위를 제한하는 것과 같은 다른 Linear Quantization은 없을까? 이를 앞서, Quatized된 값들이 Matrix Multiplication을 하면서 미리 계산될 수 있는 수 (Quantized Weight, Scale, Zero point)가 있으니 inference시 연산량을 줄이기 위해 미리 계산할 수 있는 파라미터는 없을까?\n\n\n1.4 Quantized Matrix Multiplication\n입력 X, Weight W, 결과 Y가 Matrix Multiplication을 했다고 할 때 식을 계산해보자.\n\\[\nY=WX\n\\]\n\\[\nS_Y(q_Y-Z_Y) = S_W(q_W-Z_W) \\cdot S_X(q_X-Z_X\n\\]\n\\[\n\\vdots\n\\]\n\n\n\nReference. MIT-TinyML-lecture5-Quantization-1\n\n\n여기서 마지막 정리한 식을 살펴보면,\n\\(Z_x\\) 와 \\(q_w, Z_w, Z_X\\) 의 경우는 미리 연산이 가능하다. 또 \\(S_wS_X/S_Y\\) 의 경우 항상 수의 범위가 \\((0, 1)\\) 로 \\(2^{-n}M_0\\) , \\(M_0 \\in [0.5, 1)\\) 로 변형하면 N-bit Integer로 Fixed-point 형태로 표현 가능하다. 여기에 \\(Z_w\\)가 0이면 어떨까? 또 미리 계산할 수 있는 항이 보인다.\n\n\n1.5 Symmetric Linear Quantization\n\n\n\nReference. MIT-TinyML-lecture5-Quantization-1\n\n\n\\(Z_w = 0\\) 이라고 함은 바로 위와 같은 Weight 분포인데, 바로 Symmetric한 Linear Quantization으로 \\(Z_w\\)를 0으로 만들어 \\(Z_w q_x\\)항을 0으로 둘 수 있어 연산을 또 줄일 수 있을 것이다.\nSymmetric Linear Quantization은 주어진 데이터에서 Full range mode와 Restrict range mode로 나뉜다.\n첫 번째 Full range mode 는 Scale을 real number(데이터, weight)에서 범위가 넓은 쪽에 맞추는 것이다. 예를 들어 아래의 경우, r_min이 r_max보다 절댓값이 더 크기 때문에 r_min에 맞춰 q_min을 가지고 Scale을 구한다. 이 방법은 Pytorch native quantization과 ONNX에서 사용된다고 강의에서 소개한다.\n\n\n\nReference. MIT-TinyML-lecture5-Quantization-1\n\n\n두 번째 Restrict range mode는 Scale을 real number(데이터, weight)에서 범위가 좁은 쪽에 맞추는 것이다. 예를 들어 아래의 경우, r_min가 r_max보다 절댓값이 더 크기 때문에 r_min에 맞추면서 q_max에 맞도록 Scale을 구한다. 이 방법은 TensorFlow, NVIDIA TensorRT, Intel DNNL에서 사용된다고 강의에서 소개한다.\n\n\n\nReference. MIT-TinyML-lecture5-Quantization-1\n\n\n그렇다면 왜 Symmetric 써야할까? Asymmetric 방법과 Symmetric 방법의 차이는 뭘까? (feat. Neural Network Distiller) 아래 그림을 참고하면 되지만, 가장 큰 차이로 보이는 것은 Computation vs Compactful quantized range로 이해간다.\n\n\n\nReference. MIT-TinyML-lecture5-Quantization-1\n\n\n\n\n1.6 Linear Quantization examples\n그럼 Quatization 방법에 대해 알아봤으니 이를 Full-Connected Layer, Convolution Layer에 적용해보고 어떤 효과가 있는지 알아보자.\n\n1.6.1 Full-Connected Layer\n아래처럼 식을 전개해보면 미리 연산할 계산할 수 있는 항과 N-bit integer로 표현할 있는 항으로 나눌 수 있다(전개하는 이유는 아마 미리 계산할 수 있는 항을 알아보기 위함이 아닐까 싶다).\n\\[\nY=WX+b\n\\]\n\\[\n\\downarrow\n\\]\n\\[\nS_Y(q_Y - Z_Y) = S_W(q_W - Z_W) \\cdot S_X(q_X - Z_X) + S_b(q_b - Z_b)\n\\]\n\\[\n\\downarrow \\ Z_w=0\n\\]\n\\[\nS_Y(q_Y - Z_Y) = S_WS_X(q_Wq_X - Z_xq_W) + S_b(q_b - Z_b)\n\\]\n\\[\n\\downarrow \\ Z_b=0, S_b=S_WS_X\n\\]\n\\[\nS_Y(q_Y - Z_Y) = S_WS_X(q_Wq_X - Z_xq_W+q_b)\n\\]\n\\[\n\\downarrow\n\\]\n\\[\nq_Y = \\dfrac{S_WS_X}{S_Y}(q_Wq_X+q_b - Z_Xq_W) + Z_Y\n\\]\n\\[\n\\downarrow \\ q_{bias}=q_b-Z_xq_W\\\\\n\\]\n\\[\nq_Y = \\dfrac{S_WS_X}{S_Y}(q_Wq_X+ q_{bias}) + Z_Y\\\\\n\\]\n간단히 표기하기 위해 \\(Z_W=0, Z_b=0, S_b = S_W S_X\\) 이라고 가정한다.\n\n\n\nReference. MIT-TinyML-lecture5-Quantization-1\n\n\n\n\n1.6.2 Convolutional Layer\nConvolution Layer의 경우는 Weight와 X의 곱의 경우를 Convolution으로 바꿔서 생각해보면 된다. 그도 그럴 것이 Convolution은 Kernel과 Input의 곱의 합으로 이루어져 있기 때문에 Full-Connected와 거의 유사하게 전개될 수 있을 것이다.\n\n\n\nReference. MIT-TinyML-lecture5-Quantization-1"
  },
  {
    "objectID": "posts/lecs/lec05.html#post-training-quantization-ptq",
    "href": "posts/lecs/lec05.html#post-training-quantization-ptq",
    "title": "🧑‍🏫 Lecture 5-6",
    "section": "2. Post-training Quantization (PTQ)",
    "text": "2. Post-training Quantization (PTQ)\n그럼 앞서서 Quantizaed한 Layer를 Fine tuning할 없을까? “How should we get the optimal linear quantization parameters (S, Z)?” 이 질문에 대해서 Weight, Activation, Bias 세 가지와 그에 대하여 논문에서 보여주는 결과까지 알아보자.\n\n2.1 Weight quantization\nTL;DR. 이 강의에서 소개하는 Weight quantization은 Grandularity에 따라 Whole(Per-Tensor), Channel, 그리고 Layer로 들어간다.\n\n2.1.1 Granularity\nWeight quantization에서 Granularity에 따라서 Per-Tensor, Per-Channel, Group, 그리고 Generalized 하는 방법으로 확장시켜 Shared Micro-exponent(MX) data type을 차례로 보여준다. Scale을 몇 개나 둘 것이냐, 그 Scale을 적용하는 범위를 어떻게 둘 것이냐, 그리고 Scale을 얼마나 디테일하게(e.g. floating-point)할 것이냐에 초점을 둔다.\n\n\n\nReference. MIT-TinyML-lecture5-Quantization-2\n\n\n첫 번째는 Per-Tensor Quantization 특별하게 설명할 것 없이 이전까지 설명했던 하나의 Scale을 사용하는 Linear Quantization이라고 생각하면 되겠다. 특징으로는 Large model에 대해서는 성능이 괜찮지만 작은 모델로 떨어지면 성능이 급격하게 떨어진다고 설명한다. Channel별로 weight 범주가 넓은 경우나 outlier weight가 있는 경우 quantization 이후에 성능이 하락했다고 말한다.\n\n\n\nReference. MIT-TinyML-lecture5-Quantization-2\n\n\n그래서 그 해결방안으로 나오는 것이 두 번째 방법인 Per-Channel Quantization이다. 위 예제에서 보면 Channel 마다 최대값과 각각에 맞는 Scale을 따로 가지는 것을 볼 수 있다. 그리고 적용한 결과인 아래 그림을 보면 Per-Channel과 Per-Tensor를 비교해보면 Per-Channel이 기존에 floating point weight와의 차이가 더 적다. 하지만, 만약 하드웨어에서 Per-Channel Quantization을 지원하지 않는다면 불필요한 연산을 추가로 해야하기 때문에 이는 적합한 방법이 될 수 없다는 점도 고려해야할 것이다(이는 이전 Tiny Engine에 대한 글에서 Channel내에 캐싱을 이용한 최적화와 연관이 있다). 그럼 또 다른 방법은 없을까?\n\n\n\nReference. MIT-TinyML-lecture5-Quantization-2\n\n\n세 번째 방법은 Group Quantization으로 소개하는 Per-vector Scaled Quantization와 Shared Micro-exponent(MX) data type 이다. Per-vector Scaled Quantization은 2023년도 강의부터 소개하는데, 이 방법은 Scale factor를 그룹별로 하나, Per-Tensor로 하나로 두개를 두는 방법이다. 아래의 그림을 보면,\n\n\n\nReference. MIT-TinyML-lecture5-Quantization-2\n\n\n\\[\nr=S(q-Z) \\rightarrow r=\\gamma \\cdot S_{q}(q-Z)\n\\]\n\\(S_q\\) 로 vector별 스케일링을 하나, \\(\\gamma\\) 로 Tensor에 스케일링을 하며 감마는 floating point로 하는 것을 볼 수있다. 아무래도 vector단위로 스케일링을 하게되면 channel과 비교해서 하드웨어 플랫폼에 맞게 accuracy의 trade-off를 조절하기 더 수월할 것으로 보인다.\n여기서 강의는 지표인 Memory Overhead로 “Effective Bit Width”를 소개한다. 이는 Microsoft에서 제공하는 Quantization Approach MX4, MX6, MX9과 연결돼 있는데, 이 데이터타입은 조금 이후에 더 자세히 설명할 것이다. Effective Bit Width? 예시 하나를 들어 이해해보자. 만약 4-bit Quatization을 4-bit per-vector scale을 16 elements(4개의 weight가 각각 4bit를 가진다고 생각하면 16 element로 계산된다 유추할 있다) 라면, Effective Bit Width는 4(Scale bit) + 4(Vector Scale bit) / 16(Vector Size) = 4.25가 된다. Element당 Scale bit라고 간단하게 생각할 수도 있을 듯 싶다.\n마지막 Per-vector Scaled Quantization을 이해하다보면 이전에 Per-Tensor, Per-Channel도 그룹으로 얼마만큼 묶는 차이가 있고, 이는 이들을 일반화할 수 있어 보인다. 강의에서 바로 다음에 소개하는 방법이 바로 Multi-level scaling scheme이다. Per-Channel Quantization와 Per-Vector Quantization(VSQ, Vector-Scale Quantization)부터 봐보자.\n\n\n\nReference. With Shared Microexponents, A Little Shifting Goes a Long Way [Bita Rouhani et al.]\n\n\nPer-Channel Quantization는 Scale factor가 하나로 Effective Bit Width는 4가 된다. 그리고 VSQ는 이전에 계산했 듯 4.25가 될 것이다(참고로 Per Channel로 적용되는 Scale의 경우 element의 수가 많아서 그런지 따로 Effective Bit Width로 계산하지는 않는다). VSQ까지 보면서 Effective Bit Width는,\nEffective Bit Width = Scale bit + Group 0 Scale bit / Group 0 Size +...\ne.g. VSQ Data type int4 = Scale bit (4) + Group 0 Scale bit(4) / Group 0 Size(16) = 4.25\n이렇게 계산할 수 있다. 그리고, MX4, MX6, MX9가 나온다. 참고로 S는 Sign bit, M은 Mantissa bit, E는 Exponent bit를 의미한다(Mantissa나 Exponent에 대한 자세한 내용은 floating point vs fixed point 글을 참고하자). 아래는 Microsoft에서 제공하는 Quantization Approach MX4, MX6, MX9에 대한 표이다.\n\n\n\nReference. MIT-TinyML-lecture5-Quantization-1\n\n\n\n\n2.1.2 Weight Equalization\n여기까지 Weight Quatization에서 그룹으로 얼마만큼 묶는지에 따라(강의에서는 Granularity) Quatization을 하는 여러 방법을 소개했다. 다음으로 소개 할 방법은 Weight Equalization이다. 2022년에 소개해준 내용인데, 이는 i번째 layer의 output channel를 scaling down 하면서 i+1번째 layer의 input channel을 scaling up 해서 Scale로 인해 Quantization 전후로 생기는 Layer간 차이를 줄이는 방법이다.\n\n\n\nReference. Data-Free Quantization Through Weight Equalization and Bias Correction [Markus et al., ICCV 2019]\n\n\n예를 들어 위에 그림처럼 Layer i의 output channel과 Layer i+1의 input channel이 있다. 여기서 식을 전개하면 아래와 같은데,\n\\[\n\\begin{aligned}\ny^{(i+1)}&=f(W^{(i+1)}x^{(i+1)}+b^{(i+1)}) \\\\\n         &=f(W^{(i+1)} \\cdot f(W^{(i)}x^{(i)}+b^{(i)}) +b^{(i+1)}) \\\\\n         &=f(W^{(i+1)}S \\cdot f(S^{-1}(W^{(i)}x^{(i)}+S^{-1}b^{(i)})) +b^{(i+1)})\n\\end{aligned}\n\\]\nwhere \\(S = diag(s)\\) , \\(s_j\\) is the weight equalization scale factor of output channel \\(j\\)\n여기서 Scale(S)가 i+1번째 layer의 weight에, i번째 weight에 1/S 로 Scale될 떄 기존에 Scale 하지 않은 식과 유사하게 유지할 있는 것을 볼 수 있다. 즉,\n\\[\nr^{(i)}_{oc_j} / s = r^{(i+1)}_{ic_j} \\cdot s\n\\]\n\\[\ns_j = \\dfrac{1}{r^{(i+1)}_{ic=j}}\\sqrt{r_{oc=j}^{(i)}\\cdot r_{ic=j}^{(i+1)}}\n\\]\n\\[\nr^{(i)}_{oc_j} = r^{(i)}_{oc_j} / s = \\sqrt{r_{oc=j}^{(i)}\\cdot r_{ic=j}^{(i+1)}}\n\\]\n\\[\nr^{(i)}_{ic_j} =r^{(i)}_{ic_j} \\cdot s = \\sqrt{r_{oc=j}^{(i)}\\cdot r_{ic=j}^{(i+1)}}\n\\]\n이렇게 하면 i번째 layer의 output channel과 i+1번째 layer의 input channel의 Scale을 각각 \\(S\\) 와 \\(1/S\\) 로하며 weight간의 격차를 줄일 수 있다.\n\n\n2.1.3 Adaptive rounding\n\n\n\nReference. MIT-TinyML-lecture5-Quantization-1\n\n\n마지막 소개하는 방법은 Adaptive rounding 이다. 반올림은 Round-to-nearest으로 불리는 일반적인 반올림을 생각할 수 있고, 하나의 기준을 가지고 반올림을 하는 Adaptive Round를 생각할 할 수 있다. 강의에서는 Round-to-nearest가 최적의 방법이 되지 않는다고 말하며, Adaptive round로 weight에 0부터 1 사이의 값을 더해 수식처럼 \\(\\tilde{w} = \\lfloor\\lfloor  w\\rfloor + \\delta\\rceil, \\delta \\in [0, 1]\\) 최적의 Optimal한 반올림 값을 구한다. $$\n\\[\\begin{aligned}\n&argmin_V\\lvert\\lvert Wx-\\tilde Wx\\lvert\\lvert ^2_F + \\lambda f_{reg}(V) \\\\\n\n\\rightarrow & argmin_V\\lvert\\lvert Wx-\\lfloor\\lfloor W \\rfloor + h(V) \\rceil x\\lvert\\lvert ^2_F + \\lambda f_{reg}(V)\n\\end{aligned}\\]\n$$ ### 2.2 Activation quantization 두 번째로 Activation quantization이 있다. 모델결과로 나오는 결과를 직접적으로 결정하는 Activation Quatization에서는 두 가지를 고려한 방법을 소개한다. 하나는 Activation 레이어에서 결과값을 Smoothing한 분포를 가지게 하기 위해 Exponential Moving Average(EMA)를 사용하는 방법이고, 다른 하나는 다양한 입력값을 고려해 batch samples을 FP32 모델과 calibration하는 방법이다.\nExponential Moving Average (EMA)은 아래 식에서 \\(\\alpha\\) 를 구하는 방법이다. \\[\n\\hat r^{(t)}_{max, min} = \\alpha r^{(t)}_{max, min} + (1-\\alpha) \\hat r^{(t)}_{max, min}  \n\\] Calibration의 컨셉은 많은 input의 min/max 평균을 이용하자는 것이다. 그래서 trained FP32 model과 sample batch를 가지고 quantized한 모델의 결과와 calibration을 돌리면서 그 차이를 최소화 시키는데, 여기에 이용하는 지표는 loss of information와 Newton-Raphson method를 사용한 Mean Square Error(MSE)가 있다. \\[\nMSE = \\underset{\\lvert r \\lvert_{max}}{min}\\ \\mathbb{E}[(X-Q(X))^2]\n\\] \\[\nKL\\ divergence=D_{KL}(P\\lvert\\lvert Q) = \\sum_i^N P(x_i)log\\dfrac{P(x_i)}{Q(x_i)}\n\\] ### 2.3 Quanization Bias Correction\n마지막으로 Quatization으로 biased error를 잡는다는 것을 소개한다. \\(\\epsilon = Q(W)-W\\) 이라고 두고 아래처럼 식이 전개시키면 마지막 항에서 보이는 \\(-\\epsilon\\mathbb{E}[x]\\) 부분이 bias를 quatization을 할 때 제거 된다고 한다(이 부분은 2023년에는 소개하진 않는데, 당연한 것이어서 안하는지, 혹은 영향이 크지 않아서 그런지는 모르겠다. Bias Quatization이후에 MobileNetV2에서 한 레이어의 output을 보면 어느정도 제거되는 것처럼 보인다). \\[\n\\begin{aligned}\n\\mathbb{E}[y] &= \\mathbb{E}[Wx] + \\mathbb{E}[\\epsilon x] - \\mathbb{E}[\\epsilon x],\\ \\mathbb{E}[Q(W)x] = \\mathbb{E}[Wx] + \\mathbb{E}[\\epsilon x] \\\\\n\\mathbb{E}[y] &= \\mathbb{E}[Q(W)x] - \\epsilon\\mathbb{E}[x]\n\\end{aligned}\n\\]\n\n\n\nReference. MIT-TinyML-lecture5-Quantization-2\n\n\n\n\n\n2.4 Post-Training INT8 Linear Quantization Result\n앞선 Post-Training Quantization을 적용한 결과를 보여준다. 이미지계열 모델을 모두 사용했으며, 성능하락폭은 지표로 보여준다. 비교적 큰 모델들의 경우 준수한 성능을 보여주지만 MobileNetV1, V2와 같은 작은 모델은 생각보다 Quantization으로 떨어지는 성능폭(-11.8%, -2.1%) 이 큰 것을 볼 수 있다. 그럼 작은 크기의 모델들은 어떻게 Training 해야할까?\n\n\n\nReference. MIT-TinyML-lecture5-Quantization-2"
  },
  {
    "objectID": "posts/lecs/lec05.html#quantization-aware-trainingqat",
    "href": "posts/lecs/lec05.html#quantization-aware-trainingqat",
    "title": "🧑‍🏫 Lecture 5-6",
    "section": "3. Quantization-Aware Training(QAT)",
    "text": "3. Quantization-Aware Training(QAT)\n\n3.1 Quantization-Aware Training\n\n\n\nReference. MIT-TinyML-lecture06-Quantization-2\n\n\n\nUsually, fine-tuning a pre-trained floating point model provides better accuracy than training from scratch.\n\n이전에 K-mean Quantization에서 Fine-tuning때 Centroid에 gradient를 반영했었다. Quantization-Aware Training은 이와 유사하게 Quantization - Reconstruction을 통해 만들어진 Weight로 Training을 하는 방법을 말한다. 예시를 들어서 자세히 살펴보자.\n\n\n\nReference. MIT-TinyML-lecture06-Quantization-2\n\n\n\nA full precision copy of the weights W is maintained throughout the training.\nThe small gradients are accumulated without loss of precision\nOnce the model is trained, only the quantized weights are used for inference\n\n위 그림에서 Layer N이 보인다. 이 Layer N은 weights를 파라미터로 가지지만, 실제로 Training 과정에서 쓰이는 weight는 “weight quantization”을 통해 Quantization - Reconstruction을 통해 만들어진 Weight를 가지고 훈련을 할 것이다.\n\n\n3.2 Straight-Through Estimator(STE)\n\n\n\nReference. MIT-TinyML-lecture06-Quantization-2\n\n\n그럼 훈련에서 gradient는 어떻게 전달할 수 있을까? Quantization의 개념상, weight quantization에서 weight로 넘어가는 gradient는 없을 수 밖에 없다. 그렇게 되면 사실상 weight로 back propagation이 될 수 없게 되고, 그래서 소개하는 개념이 Straight-Through Estimator(STE) 입니다. 말이 거창해서 그렇지, Q(W)에서 받은 gradient를 그대로 weights 로 넘겨주는 방식이다.\n\nQuantization is discrete-valued, and thus the derivative is 0 almost everywhere → NN will learn nothing!\nStraight-Through Estimator(STE) simply passes the gradients through the quantization as if it had been the identity function.\n\\[\n  g_W = \\dfrac{\\partial L}{\\partial  W} = \\dfrac{\\partial L}{\\partial  Q(W)}\n  \\]\n\n\n\n\nReference. MIT-TinyML-lecture06-Quantization-2\n\n\n\nReference\n\nNeural Networks for Machine Learning [Hinton et al., Coursera Video Lecture, 2012]\nEstimating or Propagating Gradients Through Stochastic Neurons for Conditional Computation [Bengio, arXiv 2013]\n\n\n이 훈련의 결과가 궁금하시다면 이 논문을 참고하자. 참고로 논문에서는 MobileNetV1, V2 그리고 NASNet-Mobile을 이용해 Post-Training Quantization과 Quantization-Aware Training을 비교하고 있다."
  },
  {
    "objectID": "posts/lecs/lec05.html#binary-and-ternary-quantization",
    "href": "posts/lecs/lec05.html#binary-and-ternary-quantization",
    "title": "🧑‍🏫 Lecture 5-6",
    "section": "4. Binary and Ternary Quantization",
    "text": "4. Binary and Ternary Quantization\n자, 그럼 Quantization을 궁극적으로 2bit로 할 수는 없을까? 바로 Binary(1, -1)과 Tenary(1, 0, -1) 이다.\n\nCan we push the quantization precision to 1 bit?\n\n\n\nReference. MIT-TinyML-lecture06-Quantization-2\n\n\nReference\n\nBinaryConnect: Training Deep Neural Networks with Binary Weights during Propagations [Courbariaux et al., NeurIPS 2015]\nXNOR-Net: ImageNet Classification using Binary Convolutional Neural Networks [Rastegari et al., ECCV 2016]\n\n\n먼저 Weight를 2bit로 Quantization을 하게 되면, 메모리에서는 32bit를 1bit로 줄이니 32배나 줄일 수 있고, Computation도 (8x5)+(-3x2)+(5x0)+(-1x1)에서 5-2+0-1 로 절반을 줄일 수 있다.\n\n4.1 Binarization: Deterministic Binarization\n그럼 Binarization에서 +1과 -1을 어떤 기준으로 해야할까? 가장 쉬운 방법은 threhold를 기준으로 +-1로 나누는 것이다.\nDirectly computes the bit value base on a threshold, usually 0 resulting in a sign function.\n\\[\nq = sign(r) = \\begin{dcases}\n+1, &r \\geq 0 \\\\\n-1, &r &lt; 0\n\\end{dcases}\n\\]\n\n\n4.2 Binarization: Stochastic Binarization\n다른 방법으로는 output에서 hard-sigmoid function을 거쳐서 나온 값만큼 확률적으로 +-1이 나오도록 하는 것이다. 하지만 이 방법은 무작위로 비트를 생성하는 하드웨어를 하는 것이 어렵기 때문에 사용하진 않는다고 언급한다.\n\nUse global statistics or the value of input data to determine the probability of being -1 or +1\nIn Binary Connect(BC), probability is determined by hard sigmoid function \\(\\sigma(r)\\)\n\\[\n  q=\\begin{dcases}\n  +1, &\\text{with probability } p=\\sigma(r)\\\\\n  -1, & 1-p\n  \\end{dcases}\n  \\\\\n  where\\ \\sigma(r)=min(max(\\dfrac{r+1}{2}, 0), 1)\n  \\]\n\n\n\nReference. MIT-TinyML-lecture06-Quantization-2\n\n\nHarder to implement as it requires the hardware to generate random bits when quantizing.\n\n\n\n4.3 Binarization: Use Scale\n앞선 방법을 이용해서 ImageNet Top-1 을 평가해보면 Quantization이후 -21.2%나 성능이 하락하는 걸 볼 수 있다. “어떻게 보완할 수 있을까?” 한 것이 linear qunatization에서 사용했던 Scale 개념이다.\n\nUsing Scale, Minimizing Quantization Error in Binarization\n\n\n\nReference. MIT-TinyML-lecture06-Quantization-2\n\n\n\n여기서 Scale은 \\(\\dfrac{1}{n}\\lvert\\lvert W \\lvert\\lvert_1\\) 로 계산할 수 있고, 성능은 하락이 거의 없는 것도 확인할 수 있다. 왜 \\(\\dfrac{1}{n}\\lvert\\lvert W \\lvert\\lvert_1\\)인지는 아래 증명과정을 참고하자!\n\nWhy \\(\\alpha\\) is \\(\\dfrac{1}{n}\\lvert\\lvert W \\lvert\\lvert_1\\)?\n\\[\n  \\begin{aligned}\n  &J(B, \\alpha)=\\lvert\\lvert W-\\alpha B\\lvert\\lvert^2 \\\\\n  &\\alpha^*, B^*= \\underset{\\alpha, B}{argmin}\\ J(B, \\alpha) \\\\\n  &J(B,\\alpha) = \\alpha^2B^TB-2\\alpha W^T B + W^TW\\ since\\ B \\in \\{+1, -1\\}^n \\\\\n  &B^TB=n(constant), W^TW= constant(a \\ known\\ variable) \\\\\n  &J(B,\\alpha) = \\alpha^2n-2\\alpha W^T B + C \\\\\n  &B^* = \\underset{B}{argmax} \\{W^T B\\}\\ s.t.\\ B\\in \\{+1,-1 \\}^n \\\\\n  &\\alpha^*=\\dfrac{W^TB^*}{n} \\\\\n  &\\alpha^*=\\dfrac{W^Tsign(W)}{n} = \\dfrac{\\lvert W_i \\lvert}{n} = \\dfrac{1}{n}\\lvert\\lvert W\\lvert\\lvert_{l1}\n  \\end{aligned}\n  \\]\n\nReference. XNOR-Net: ImageNet Classification using Binary Convolutional Neural Networks [Rastegari et al., ECCV 2016]\nB*는 J(B,\\(\\alpha\\))에서 최솟값을 구해야하므로 \\(W^T\\)B 가 최대여야하고 그러기 위해서는 W가 양수일때는 B도 양수, W가 음수일 때는 B도 음수여야 \\(W^TB=\\sum\\lvert W \\lvert\\) 이 되면서 최댓값이 될 수 있다.\n\n\n\n\n4.4 Binarization: Activation\n그럼 Activation까지 Quantization을 해봅시다.\n4.4.1 Activation\n\n\n\nUntitled\n\n\n여기서 조금 더 연산을 최적화 할 수 있어보이는 것이 Matrix Muliplication이 XOR 연산과 비슷하게 보인다.\n4.4.2 XNOR bit count\n\n\n\nReference. MIT-TinyML-lecture06-Quantization-2\n\n\n\n\\(y_i=-n+ popcount(W_i\\ xnor\\ x) &lt;&lt; 1\\) → popcount returns the number of 1\n\n그래서 popcount과 XNOR을 이용해서 Computation에서 좀 더 최적화를 진행합니다. 이렇게 최적화를 진행하게 되면, 메모리는 32배, Computation은 58배가량 줄어들 수 있다고 말한다.\n\n\n\nReference. MIT-TinyML-lecture06-Quantization-2\n\n\n이렇게 Weight, Scale factor, Activation, 그리고 XNOR-Bitcout 까지. 총 네 가지 단계로 Binary Quantization을 나눈다. 다음으로는 Ternary Quantization은 알아보자.\n\n\n\nReference. XNOR-Net: ImageNet Classification using Binary Convolutional Neural Networks [Rastegari et al., ECCV 2016]\n\n\n\n\nBinarizing Input 의 경우는 average를 모든 channel에 같이 적용할 것이기 때문에 그 c만큼을 average filter로 한 번에 적용한다는 말이다.\n\n\n\n\n4.5 Ternary Weight Networks(TWN)\nTernary는 Binary Quantization과 단계는 모두 같지만, 가질 수 있는 값으로 0 을 추가한다. 아래 그림은 Scale을 이용해서 Quantization Error를 줄이는 방법을 말하고 있다. \\[\nq = \\begin{dcases}\nr_t, &r &gt; \\Delta \\\\\n0, &\\lvert r\\lvert \\leq \\Delta \\\\\n-r_t, &r &lt; -\\Delta\n\\end{dcases} \\\\\nwhere\\ \\Delta = 0.7\\times \\mathbb{E}(\\lvert r \\lvert), r_t = \\mathbb{E}_{\\lvert r \\lvert &gt; \\Delta}(\\lvert r \\lvert )\n\\]  ### 4.6 Trained Ternary Quantization(TTQ)\nTenary Quantization에서 또 한가지 다르게 설명하는 것은 1과 -1로만 정해져 있던 Binary Quantization과 다르게 Tenary는 1, 0, -1로 Quantization을 한 후, 추가적인 훈련을 통해 \\(w_t\\)와 \\(-w_t\\)로 fine-tuning을 하는 방법도 제안한다(해당 논문에서는 이러한 기법을 이용해서 한 결과를 CIFAR-10 이미지 데이터를 가지고 ResNets, AlexNet, ImageNet에서 보여준다). \\[\nq = \\begin{dcases}\nw_t, &r &gt; \\Delta \\\\\n0, &\\lvert r\\lvert \\leq \\Delta \\\\\n-w_t, &r &lt; -\\Delta\n\\end{dcases}\n\\] \n\n\n4.7 Accuracy Degradation\nBinary, Ternary Quantization을 사용한 결과를 보여준다(Resnet-18 경우에는 Ternary 가 오히려 Binary보다 성능이 더 떨어진다!)\n\nBinarization\n\n\n\nReference. Binarized Neural Networks: Training Deep Neural Networks with Weights and Activations Constrained to +1 or −1. [Courbariaux et al., Arxiv 2016], XNOR-Net: ImageNet Classification using Binary Convolutional Neural Networks [Rastegari et al., ECCV 2016]\n\n\nTernary Weight Networks (TWN)\n\n\n\nReference. Ternary Weight Networks [Li et al., Arxiv 2016]\n\n\nTrained Ternary Quantization (TTQ)\n\n\n\nReference. Trained Ternary Quantization [Zhu et al., ICLR 2017]"
  },
  {
    "objectID": "posts/lecs/lec05.html#low-bit-width-quantization",
    "href": "posts/lecs/lec05.html#low-bit-width-quantization",
    "title": "🧑‍🏫 Lecture 5-6",
    "section": "5. Low Bit-Width Quantization",
    "text": "5. Low Bit-Width Quantization\n남은 부분들은 여러가지 실험 / 연구들을 소개하고 있다.\n\nBinary Quantization은 Quantization Aware Training을 할 수 있을까?\n2,3 bit과 8bit 그 중간으로는 Quantization을 할 수 없을까?\n레이어에서 Quantization을 하지 않는 레이어, 예를 들어 결과에 영향을 예민하게 미치는 첫 번째 레이어가 같은 경우 Quantization을 하지 않으면 어떻게 될까?\nActivation 함수를 바꾸면 어떨까?\n예를 들어 첫번째 레이어의 N배 넓게 하는 것과 같이 모델 구조를 바꾸면 어떻게 될까?\n조금씩 Quantization을 할 수 없을까? (20% → 40% → … → 100%)\n\n강의에서는 크게 언급하지 않고 간 내용들이라 설명을 하지는 않겠다. 해당 내용들은 자세한 내용을 알고싶으면 각 파트에 언급된 논문을 참조하길!\n\n5.1 Train Binarized Neural Networks From Scratch\n\nStraight-Through Estimator(STE)\n\n\n\n\nReference. MIT-TinyML-lecture06-Quantization-2\n\n\n\nGradient pass straight to floating-point weights\nFloating-point weight with in [-1, 1]\nReference. Binarized Neural Networks: Training Deep Neural Networks with Weights and Activations Constrained to +1 or −1. [Courbariaux et al., Arxiv 2016]\n\n\n\n5.2 Quantization-Aware Training: DoReFa-Net With Low Bit-Width Gradients\n\n\n\nReference. MIT-TinyML-lecture06-Quantization-2\n\n\n\nGradient Quantization\n\\[\n  Q(g) = 2 \\cdot max(\\lvert G \\lvert) \\cdot \\Large[ \\small quantize_k \\Large( \\small \\dfrac{g}{2\\cdot max(\\lvert G \\lvert)} + \\dfrac{1}{2} + N(k) \\Large ) \\small -\\dfrac{1}{2} \\Large]\\small\n  \\] \\[\n  where\\ N(k)=\\dfrac{\\sigma}{2^k-1} and\\ \\sigma \\thicksim Uniform(-0.5, 0.5)\n  \\]\n\nNoise function \\(N(k)\\) is added to compensate the potential bias introduced by gradient quantization.\n\nResult\n\n\n\nReference. DoReFa-Net: Training Low Bitwidth Convolutional Neural Networks with Low Bitwidth Gradients [Zhou et al., arXiv 2016]\n\n\n\n\n\n5.3 Replace the Activation Function: Parameterized Clipping Activation Function\n\nThe most common activation function ReLU is unbounded. The dynamic range of inputs becomes problematic for low bit-width quantization due to very limited range and resolution.\nReLU is replaced with hard-coded bounded activation functions: ReLU6, ReLU1, etc\nThe clipping value per layer can be learned as well: PACT(Parametrized Clipping Activation Function)\n \\[\n  y=PACT(x;\\alpha) = 0.5(\\lvert x \\lvert - \\lvert x -\\alpha \\lvert + \\alpha ) = \\begin{dcases}\n  0, & x \\in [-\\infty, 0) \\\\\n  x, & x \\in [0, \\alpha) \\\\\n  \\alpha, & x \\in [\\alpha, +\\infty)\n  \\end{dcases}\n  \\]\nThe upper clipping value of the activation function is a trainable. With STE, the gradient is computed as\n\\[\n  \\dfrac{\\partial Q(y)}{\\partial \\alpha} = \\dfrac{\\partial Q(y)}{\\partial y} \\cdot \\dfrac{\\partial y}{\\partial \\alpha} = \\begin{dcases}\n  0 & x \\in (-\\infty, \\alpha)\\\\\n  1 & x \\in [\\alpha, +\\infty)\\\\\n  \\end{dcases}\n  \\]\n\\[\n  \\rightarrow\n  \\dfrac{\\partial L}{\\partial \\alpha} = \\dfrac{\\partial L}{\\partial Q(y)} \\cdot \\dfrac{\\partial Q(y)}{\\partial \\alpha} = \\begin{dcases}\n  0 & x \\in (-\\infty, \\alpha)\\\\\n  \\frac{\\partial L}{\\partial Q(y)} & x \\in [\\alpha, +\\infty)\\\\\n  \\end{dcases}\n  \\]\nThe larger \\(\\alpha\\), the more the parameterized clipping function resembles a ReLU function\n\nTo avoid large quantization errors due to a wide dynamic range \\([0, \\alpha]\\), L2-regularizer for \\(\\alpha\\) is included in the training loss function.\n\nResult\n\n\n\nReference. PACT: Parameterized Clipping Activation for Quantized Neural Networks [Choi et al., arXiv 2018]\n\n\n\n\n\n5.4 Modify the Neural Network Architecture\n\nWiden the neural network to compensate for the loss of information due to quantization\nex. Double the channels, reduce the quantization precision\n\n\n\nReference. WRPN: Wide Reduced-Precision Networks [Mishra et al., ICLR 2018]\n\n\nReplace a single floating-point convolution with multiple binary convolutions.\n\nTowards Accurate Binary Convolutional Neural Network [Lin et al., NeurIPS 2017]\nQuantization [Neural Network Distiller]\n\n\n\n\n5.5 No Quantization on First and Last Layer\n\nBecause it is more sensitive to quantization and small portion of the overall computation\nQuantizing these layers to 8-bit integer does not reduce accuracy\n\n\n\n5.6 Iterative Quantization: Incremental Network Quantization\n\nReference. Incremental Network Quantization: Towards Lossless CNNs with Low-precision Weights [Zhou et al., ICLR 2017]\n\n\n\n\nReference. Incremental Network Quantization: Towards Lossless CNNs with Low-precision Weights [Zhou et al., ICLR 2017]\n\n\n\nSetting\n\nWeight quantization only\nQuantize weights to \\(2^n\\) for faster computation (bit shift instead of multiply)\n\nAlgorithm\n\nStart from a pre-trained fp32 model\nFor the remaining fp32 weights\n\nPartition into two disjoint groups(e.g., according to magnitude)\nQuantize the first group (higher magnitude), and re-train the other group to recover accuracy\n\nRepeat until all the weights are quantized (a popular stride is {50%, 75%, 87.5%, 100%})\n\n\n\n\nReference. MIT-TinyML-lecture06-Quantization-2"
  },
  {
    "objectID": "posts/lecs/lec05.html#mixed-precision-quantization",
    "href": "posts/lecs/lec05.html#mixed-precision-quantization",
    "title": "🧑‍🏫 Lecture 5-6",
    "section": "6. Mixed-precision quantization",
    "text": "6. Mixed-precision quantization\n마지막으로 레이어마다 Quantization bit를 다르게 가져가면 어떨지에 대해서 이야기한다. 하지만 경우의 수가 8bit 보다 작거나 같게 Quantization을 할 시, weight와 activation로 경우의 수를 고려를 한다면 N개 레이어에 대해서 \\((8 \\times 8)^N\\)라는 어마어마한 경우의 수가 나온다. 그리고 이에 대해서는 다음 파트에 나갈 Neural Architecture Search(NAS) 에서 다룰 듯 싶다.\n\n6.1 Uniform Quantization\n\n\n\n6.2 Mixed-precision Quantization\n\n\n\n6.3 Huge Design Space and Solution: Design Automation\n\n\nDesign Space: Each of Choices(8x8=64) → \\(64^n\\)\n\n\n\nReference. HAQ: Hardware-Aware Automated Quantization with Mixed Precision [Wang et al., CVPR 2019]\n\n\nResult in Mixed-Precision Quantized MobileNetV1\n\n\n\nReference. HAQ: Hardware-Aware Automated Quantization with Mixed Precision [Wang et al., CVPR 2019]\n\n\n\nThis paper compares with Model size, Latency and Energy\n\n\n가장 마지막에 언급하는 Edge와 클라우드에서는 Convolution 레이어의 종류 중 더하고 덜 Quantization하는 레이어가 각각 depthwise와 pointwise로 다르다고 이야기한다. 이 내용에 대해서 더 자세히 이해하기 위해서는 아마도 NAS로 넘어가봐야 알 수 있지 않을까 싶다.\n\nQuantization Policy for Edge and Cloud\n\n\n\nReference. HAQ: Hardware-Aware Automated Quantization with Mixed Precision [Wang et al., CVPR 2019]"
  },
  {
    "objectID": "posts/labs/lab00.html",
    "href": "posts/labs/lab00.html",
    "title": "👩‍💻 Lab 0",
    "section": "",
    "text": "Lab 0 PyTorch Tutorial\nIn this tutorial, we will explore how to train a neural network with PyTorch.\n\nSetup\nWe will first install a few packages that will be used in this tutorial:\n\n!pip install torchprofile 1&gt;/dev/null\n\nWe will then import a few libraries:\n\nimport random\nfrom collections import OrderedDict, defaultdict\n\nimport numpy as np\nimport torch\nfrom matplotlib import pyplot as plt\nfrom torch import nn\nfrom torch.optim import *\nfrom torch.optim.lr_scheduler import *\nfrom torch.utils.data import DataLoader\nfrom torchprofile import profile_macs\nfrom torchvision.datasets import *\nfrom torchvision.transforms import *\nfrom tqdm.auto import tqdm\n\nTo ensure the reproducibility, we will control the seed of random generators:\n\nrandom.seed(0)\nnp.random.seed(0)\ntorch.manual_seed(0)\n\n\n\nData\nIn this tutorial, we will use CIFAR-10 as our target dataset. This dataset contains images from 10 classes, where each image is of size 3x32x32, i.e. 3-channel color images of 32x32 pixels in size.\n\ntransforms = {\n  \"train\": Compose([\n    RandomCrop(32, padding=4),\n    RandomHorizontalFlip(),\n    ToTensor(),\n  ]),\n  \"test\": ToTensor(),\n}\n\ndataset = {}\nfor split in [\"train\", \"test\"]:\n  dataset[split] = CIFAR10(\n    root=\"data/cifar10\",\n    train=(split == \"train\"),\n    download=True,\n    transform=transforms[split],\n  )\n\nWe can visualize a few images in the dataset and their corresponding class labels:\n\nsamples = [[] for _ in range(10)]\nfor image, label in dataset[\"test\"]:\n  if len(samples[label]) &lt; 4:\n    samples[label].append(image)\n\nplt.figure(figsize=(20, 9))\nfor index in range(40):\n  label = index % 10\n  image = samples[label][index // 10]\n\n  # Convert from CHW to HWC for visualization\n  image = image.permute(1, 2, 0)\n\n  # Convert from class index to class name\n  label = dataset[\"test\"].classes[label]\n\n  # Visualize the image\n  plt.subplot(4, 10, index + 1)\n  plt.imshow(image)\n  plt.title(label)\n  plt.axis(\"off\")\nplt.show()\n\nTo train a neural network, we will need to feed data in batches. We create data loaders with batch size of 512:\n\ndataflow = {}\nfor split in ['train', 'test']:\n  dataflow[split] = DataLoader(\n    dataset[split],\n    batch_size=512,\n    shuffle=(split == 'train'),\n    num_workers=0,\n    pin_memory=True,\n  )\n\nWe can print the data type and shape from the training data loader:\n\nfor inputs, targets in dataflow[\"train\"]:\n  print(\"[inputs] dtype: {}, shape: {}\".format(inputs.dtype, inputs.shape))\n  print(\"[targets] dtype: {}, shape: {}\".format(targets.dtype, targets.shape))\n  break\n\n\n\nModel\nIn this tutorial, we will use a variant of VGG-11 (with fewer downsamples and a smaller classifier) as our model.\n\nclass VGG(nn.Module):\n  ARCH = [64, 128, 'M', 256, 256, 'M', 512, 512, 'M', 512, 512, 'M']\n\n  def __init__(self) -&gt; None:\n    super().__init__()\n\n    layers = []\n    counts = defaultdict(int)\n\n    def add(name: str, layer: nn.Module) -&gt; None:\n      layers.append((f\"{name}{counts[name]}\", layer))\n      counts[name] += 1\n\n    in_channels = 3\n    for x in self.ARCH:\n      if x != 'M':\n        # conv-bn-relu\n        add(\"conv\", nn.Conv2d(in_channels, x, 3, padding=1, bias=False))\n        add(\"bn\", nn.BatchNorm2d(x))\n        add(\"relu\", nn.ReLU(True))\n        in_channels = x\n      else:\n        # maxpool\n        add(\"pool\", nn.MaxPool2d(2))\n\n    self.backbone = nn.Sequential(OrderedDict(layers))\n    self.classifier = nn.Linear(512, 10)\n\n  def forward(self, x: torch.Tensor) -&gt; torch.Tensor:\n    # backbone: [N, 3, 32, 32] =&gt; [N, 512, 2, 2]\n    x = self.backbone(x)\n\n    # avgpool: [N, 512, 2, 2] =&gt; [N, 512]\n    x = x.mean([2, 3])\n\n    # classifier: [N, 512] =&gt; [N, 10]\n    x = self.classifier(x)\n    return x\n\nmodel = VGG().cuda()\n\nIts backbone is composed of eight conv-bn-relu blocks interleaved with four maxpool’s to downsample the feature map by 2^4 = 16 times:\n\nprint(model.backbone)\n\nAfter the feature map is pooled, its classifier predicts the final output with a linear layer:\n\nprint(model.classifier)\n\nAs this course focuses on efficiency, we will then inspect its model size and (theoretical) computation cost.\n\nThe model size can be estimated by the number of trainable parameters:\n\n\nnum_params = 0\nfor param in model.parameters():\n  if param.requires_grad:\n    num_params += param.numel()\nprint(\"#Params:\", num_params)\n\n\nThe computation cost can be estimated by the number of multiply–accumulate operations (MACs) using TorchProfile:\n\n\nnum_macs = profile_macs(model, torch.zeros(1, 3, 32, 32).cuda())\nprint(\"#MACs:\", num_macs)\n\nThis model has 9.2M parameters and requires 606M MACs for inference. We will work together in the next few labs to improve its efficiency.\n\n\nOptimization\nAs we are working on a classification problem, we will apply cross entropy as our loss function to optimize the model:\n\ncriterion = nn.CrossEntropyLoss()\n\nOptimization will be carried out using stochastic gradient descent (SGD) with momentum:\n\noptimizer = SGD(\n  model.parameters(),\n  lr=0.4,\n  momentum=0.9,\n  weight_decay=5e-4,\n)\n\nThe learning rate will be modulated using the following scheduler (which is adapted from this blog series):\n\nnum_epochs = 20\nsteps_per_epoch = len(dataflow[\"train\"])\n\n# Define the piecewise linear scheduler\nlr_lambda = lambda step: np.interp(\n  [step / steps_per_epoch],\n  [0, num_epochs * 0.3, num_epochs],\n  [0, 1, 0]\n)[0]\n\n# Visualize the learning rate schedule\nsteps = np.arange(steps_per_epoch * num_epochs)\nplt.plot(steps, [lr_lambda(step) * 0.4 for step in steps])\nplt.xlabel(\"Number of Steps\")\nplt.ylabel(\"Learning Rate\")\nplt.grid(\"on\")\nplt.show()\n\nscheduler = LambdaLR(optimizer, lr_lambda)\n\n\n\nTraining\nWe first define the training function that optimizes the model for one epoch (i.e., a pass over the training set):\n\ndef train(\n  model: nn.Module,\n  dataflow: DataLoader,\n  criterion: nn.Module,\n  optimizer: Optimizer,\n  scheduler: LambdaLR,\n) -&gt; None:\n  model.train()\n\n  for inputs, targets in tqdm(dataflow, desc='train', leave=False):\n    # Move the data from CPU to GPU\n    inputs = inputs.cuda()\n    targets = targets.cuda()\n\n    # Reset the gradients (from the last iteration)\n    optimizer.zero_grad()\n\n    # Forward inference\n    outputs = model(inputs)\n    loss = criterion(outputs, targets)\n\n    # Backward propagation\n    loss.backward()\n\n    # Update optimizer and LR scheduler\n    optimizer.step()\n    scheduler.step()\n\nWe then define the evaluation function that calculates the metric (i.e., accuracy in our case) on the test set:\n\n@torch.inference_mode()\ndef evaluate(\n  model: nn.Module,\n  dataflow: DataLoader\n) -&gt; float:\n  model.eval()\n\n  num_samples = 0\n  num_correct = 0\n\n  for inputs, targets in tqdm(dataflow, desc=\"eval\", leave=False):\n    # Move the data from CPU to GPU\n    inputs = inputs.cuda()\n    targets = targets.cuda()\n\n    # Inference\n    outputs = model(inputs)\n\n    # Convert logits to class indices\n    outputs = outputs.argmax(dim=1)\n\n    # Update metrics\n    num_samples += targets.size(0)\n    num_correct += (outputs == targets).sum()\n\n  return (num_correct / num_samples * 100).item()\n\nWith training and evaluation functions, we can finally start training the model! This will take around 10 minutes.\n\nfor epoch_num in tqdm(range(1, num_epochs + 1)):\n  train(model, dataflow[\"train\"], criterion, optimizer, scheduler)\n  metric = evaluate(model, dataflow[\"test\"])\n  print(f\"epoch {epoch_num}:\", metric)\n\nIf everything goes well, your trained model should be able to achieve &gt;92.5% of accuracy!\n\n\nVisualization\nWe can visualize the model’s prediction to see how the model truly performs:\n\nplt.figure(figsize=(20, 10))\nfor index in range(40):\n  image, label = dataset[\"test\"][index]\n\n  # Model inference\n  model.eval()\n  with torch.inference_mode():\n    pred = model(image.unsqueeze(dim=0).cuda())\n    pred = pred.argmax(dim=1)\n\n  # Convert from CHW to HWC for visualization\n  image = image.permute(1, 2, 0)\n\n  # Convert from class indices to class names\n  pred = dataset[\"test\"].classes[pred]\n  label = dataset[\"test\"].classes[label]\n\n  # Visualize the image\n  plt.subplot(4, 10, index + 1)\n  plt.imshow(image)\n  plt.title(f\"pred: {pred}\" + \"\\n\" + f\"label: {label}\")\n  plt.axis(\"off\")\nplt.show()"
  },
  {
    "objectID": "posts/lecs/lec04.html",
    "href": "posts/lecs/lec04.html",
    "title": "🧑‍🏫 Lecture 4",
    "section": "",
    "text": "이전 포스팅에서 Pruning에 대해서 배웠었다. 이번에는 Pruning에 대한 남은 이야기인 Pruning Ratio를 정하는 방법, Fine-tuning 과정에 대해 알아보고, 마지막으로 Sparsity를 위한 System Support에 대해 알아보고자 한다."
  },
  {
    "objectID": "posts/lecs/lec04.html#pruning-ratio",
    "href": "posts/lecs/lec04.html#pruning-ratio",
    "title": "🧑‍🏫 Lecture 4",
    "section": "1. Pruning Ratio",
    "text": "1. Pruning Ratio\nPruning을 하기 위해서 어느 정도 Pruning을 해야 할지 어떻게 정해야 할까?\n즉, 다시 말해서 몇 % 정도 그리고 어떻게 Pruning을 해야 좋을까?\n\n\n\nPruning 방식 비교\n\n\n우선 Channel 별 Pruning을 할 때, Channel 구분 없이 동일한 Pruning 비율(Uniform)을 적용하면 성능이 좋지 않다. 오른쪽 그래프에서 지향해야 하는 방향은 Latency는 적게, Accuracy는 높게이므로 왼쪽 상단의 영역이 되도록 Pruning을 진행해야 한다. 그렇다면 결론은 Channel 별 구분을 해서 어떤 Channel은 Pruning 비율을 높게, 어떤 Channel은 Pruning 비율을 낮게 해야 한다는 이야기가 된다.\n\n1.1 Sensitiviy Analysis\nChannel 별 구분을 해서 Pruning을 한다는 기본 아이디어는 아래와 같다.\n\nAccuracy에 영향을 많이 주는 Layer는 Pruning을 적게 해야 한다.\nAccuracy에 영향을 적게 주는 Layer는 Pruning을 많이 해야 한다.\n\nAccuracy를 되도록이면 원래의 모델보다 덜 떨어지게 만들면서 Pruning을 해서 모델을 가볍게 만드는 것이 목표이기 때문에 당연한 아이디어일 것이다. Accuracy에 영향을 많이 준다는 말은 Sensitive한 Layer이다라는 표현으로 다르게 말할 수 있다. 따라서 각 Layer의 Senstivity를 측정해서 Sensitive한 Layer는 Pruning Ratio를 낮게 설계하면 된다.\nLayer의 Sensitivity를 측정하기 위해 Sensitivity Analysis를 진행해보자. 당연히 특정 Layer의 Pruning Ratio가 높을 수록 weight가 많이 가지치기 된 것이므로 Accuracy는 떨어지게 된다.\n\n\n\nL0 Pruning Rate 그래프\n\n\n\n\n\n\n\n\nPruning을 하는 Weight는 어떻게 결정하나요?\n\n\n\n\n\nPruning Ratio에 의해 Pruned 되는 weight는 이전 강의에서 배운 “Importance(weight의 절댓값 크기)”에 따라 선택된다.\n\n\n\n위의 그림에서 처럼 Layer 0(L0)만을 가지고 Pruning Ratio를 높여가면서 관찰해보면, 약 70% 이후부터는 Accuracy가 급격하게 떨어지는 것을 볼 수 있다. L0에서 Ratio를 높여가며 Accuracy의 변화를 관찰한 것처럼 다른 Layer들도 관찰해보자.\n\n\n\nLayer별 Sensitivity 비교\n\n\nL1은 다른 Layer들에 비해 상대적으로 Pruning Ratio를 높여가도 Accuracy의 떨어지는 정도가 약한 반면, L0는 다른 Layer들에 비해 상대적으로 Pruning Ratio를 높여가면 Accuracy의 떨어지는 정도가 심한 것을 확인할 수 있다. 따라서 L1은 Sensitivity가 높다고 볼 수 있으며 Pruning을 적게해야 하고, L0은 Sensitivity가 낮다고 볼 수 있으며 Pruning을 많게해야 함을 알 수 있다.\n여기서 Sensitivity Analysis에서 고려해야할 몇가지 사항들에 대해서 짚고 넘어가자.\n\nSensitivity Analysis에서 모든 Layer들이 독립적으로 작동한다는 것을 전제로 한다. 즉, L0의 Pruning이 L1의 효과에 영향을 주지 않는 독립성을 가진다는 것을 전제로 한다.\nLayer의 Pruning Ratio가 동일하다고 해서 Pruned Weight수가 같음을 의미하지 않는다.\n\n100개의 weight가 있는 layer의 10% Pruning Ratio 적용은 10개의 weight가 pruned 되었음을 의미하고, 500개의 weight가 있는 layer의 10% Pruning Ratio 적용은 50개의 weight가 pruned 되었음을 의미한다.\nLayer의 전체 크기에 따라 Pruning Ratio의 적용 효과는 다를 수 있다.\n\n\nSensitivity Analysis까지 진행한 후에는 보통 사람이 Accuracy가 떨어지는 정도, threshold를 정해서 Pruning Ratio를 정한다.\n\n\n\nThreshold 정하기\n\n\n위 그래프에서는 Accuracy가 약 75%수준으로 유지되는 threhsold \\(T\\) 수평선을 기준으로 L0는 약 74%, L4는 약 80%, L3는 약 82%, L2는 90%까지 Pruning을 진행해야 겠다고 정한 예시를 보여준다. 민감한 layer인 L0는 상대적으로 Pruning을 적게, 덜 민감한 layer인 L2는 Pruning을 많게 하는 것을 확인할 수 있다.\n물론 사람이 정하는 threshold는 개선의 여지가 물론 있다. Pruning Ratio를 좀 더 Automatic하게 찾는 방법에 대해 알아보자.\n\n\n1.2 AMC\nAMC는 AutoML for Model Compression의 약자로, 강화학습(Reinforcement Learning) 방법으로 최적의 Pruning Ratio를 찾도록 하는 방법이다.\n\n\n\nAMC 전체 구조\n\n\nAMC의 구조는 위 그림과 같다. 강화학습 알고리즘 계열 중, Actor-Critic 계열의 알고리즘인 Deep Deterministic Policy Gradient(DDPG)을 활용하여 Pruning Ratio를 정하는 Action을 선택하도록 학습한다. 자세한 MDP(Markov Decision Process) 설계는 아래와 같다.\n\n\n\nAMC의 MDP\n\n\n강화학습 Agent의 학습 방향을 결정하는 중요한 Reward Function은 모델의 Accuracy를 고려해서 Error를 줄이도록 유도할 뿐만 아니라 Latency를 간접적으로 고려할 수 있도록 모델의 연산량을 나타내는 FLOP를 적게 하도록 유도하도록 설계한다. 오른쪽에 모델들의 연산량 별(Operations) Top-1 Accuracy 그래프를 보면 연산량이 많을수록 로그함수처럼 Accuracy가 증가하는 것을 보고 이를 보고 반영한 부분이라고 볼 수 있다.\n\n\n\nAMC의 Reward Function\n\n\n이렇게 AMC로 Pruning을 진행했을 때, Human Expert가 Pruning 한 것과 비교해보자. 아래 모델 섹션별 Density 히스토그램 그래프에서 Total을 보면, 동일 Accuracy가 나오도록 Pruning을 진행했을 때 AMC로 Pruning을 진행한 것(주황색)이 Human Expert Pruning 모델(파란색)보다 Density가 낮은 것을 확인할 수 있다. 즉, AMC로 Pruning 진행했을 때 더 많은 weight를 Pruning 더 가벼운 모델을 가지고도 Accuracy를 유지했다고 볼 수 있다.\n\n\n\nAMC의 Density Graph\n\n\n두번째 꺾은 선 그래프에서 AMC를 가지고 Pruning과 Fine-tuning을 번갈아 가며 여러 스텝으로 진행해가면서 관찰한 것을 조금 더 자세히 살펴보자. 각 Iteration(Pruning+Fine-tuning)을 stage1, 2, 3, 4로 나타내어 plot한 것을 보면, 1x1 conv보다 3x3 conv에서 Density가 더 낮은 것을 확인할 수 있다. 즉, 3x3 conv에서 1x1 conv보다 Pruning을 많이 한 것을 볼 수 있다. 이를 해석해보자면, AMC가 3x3 conv을 Pruning하면 9개의 weight를 pruning하고 이는 1x1 conv pruning해서 1개의 weight를 없애는 것보다 한번에 더 많은 weight 수를 줄일 수 있기 때문에 3x3 conv pruning을 적극 활용했을 것으로 볼 수 있다.\n\n\n\nAMC Result\n\n\n이 AMC 실험 결과표에서 보면, FLOP와 Time 각각 50%로 줄인 AMC 모델 둘다 Top-1 Accuracy가 기존의 1.0 MobileNet의 Accuracy보다 약 0.1~0.4% 정도만 줄고 Latency나 SpeedUp이 효율적으로 조정된 것을 확인할 수 있다.\n\n\n\n\n\n\nAMC 실험 결과표에서 0.75 MobileNet의 SpeedUp이 왜 1.7x 인가요?\n\n\n\n\n\n결과표에서 0.75 MobileNet은 25%의 weight를 감소시킨 것이기 때문에 SpeedUp이 \\(\\frac{4}{3} \\simeq 1.3\\)x이어야 한다고 생각할 수 있다. 하지만 연산량은 quadratic하게 감소하게 되기 때문에 \\(\\frac{4}{3} \\cdot \\frac{4}{3} \\simeq 1.7\\)x로 SpeedUp이 된다.\n\n\n\n\n\n1.3 NetAdapt\n또 다른 Pruning Ratio를 정하는 기법으로 NetAdapt이 있다. Latency Constraint를 가지고 layer마다 pruning을 적용해본다. 예를 들어, 줄일 목표 latency 량을 lms로 정하면, 10ms → 9ms로 줄 때까지 layer의 pruning ratio를 높여가는 방법이다.\n\n\n\nNetAdapt\n\n\nNetAdapt의 전체적인 과정은 아래와 같이 진행된다. 기존 모델에서 각 layer를 Latency Constraint에 도달하도록 Pruning하면서 Accuracy(\\(Acc_A\\)등)을 반복적으로 측정한다.\n\n각 layer의 pruning ratio를 조절한다.\nShort-term fine tuning을 진행한다.\nLatency Constraint에 도달했는지 확인한다.\nLatency Constraint 도달하면 해당 layer의 최적의 Pruning ratio로 판단한다.\n각 layer의 최적 Pruning ratio가 정해졌다면 마지막으로 Long-term fine tuning을 진행한다.\n\n\n\n\nNetAdapt 과정\n\n\n이와 같이 NetAdapt의 과정을 진행하면 아래와 같은 실험 결과를 볼 수 있다. Uniform하게 Pruning을 진행한 Multipilers보다 NetAdapt가 1.7x 더 빠르고 오히려 Accuracy는 약 0.3% 정도 높은 것을 알 수 있다.\n\n\n\nNetAdapt의 Latency / Top-1 Accuracy 그래프"
  },
  {
    "objectID": "posts/lecs/lec04.html#fine-tuningtrain",
    "href": "posts/lecs/lec04.html#fine-tuningtrain",
    "title": "🧑‍🏫 Lecture 4",
    "section": "2. Fine-tuning/Train",
    "text": "2. Fine-tuning/Train\nPrunned 모델의 퍼포먼스를 향상하기 위해서는 Pruning를 진행하고 나서 Fine-tuning 과정이 필요하다.\n\n2.1 Iterative Pruning\n보통 Pruned 모델의 Fine-tuning 과정에서는 기존에 학습했던 learning rate보다 작은 rate를 사용한다. 예를들어 기존의 모델을 학습할 때 사용한 learning rate의 \\(1/100\\) 또는 \\(1/10\\)을 사용한다. 또한 Pruning 과정과 Fine-tuning 과정은 1번만 진행하기보다 점차적으로 pruning ratio를 늘려가며 Pruning, Fine-tuning을 번갈아가며 여러번 진행하는게 더 좋다.\n\n\n\nIterative Pruning + Fine-tuning 비교 그래프\n\n\n\n\n2.2 Regularization\nTinyML의 목표는 가능한 많은 weight들을 0으로 만드는 것으로 생각할 수 있다. 그래야 모델을 가볍게 만들 수 있기 때문이다. 그래서 Regularization 기법을 이용해서 모델의 weight들을 0으로, 혹은 0과 가깝게 작은 값을 가지도록 만든다. 작은 값의 weight가 되도록 하는 이유는 0과 가까운 작은 값들은 다음 layer들로 넘어가면서 0이 될 가능성이 높아지기 때문이다. 기존의 딥러닝 모델들의 과적합(Overfitting)을 막기 위한 Regularization 기법들과 다르지 않으나 의도와 목적은 다른 것을 짚어볼 수 있다.\n\n\n\nPruning을 위한 Regularization\n\n\n\n\n2.3 The Lottery Ticket Hypothesis\n2019년 ICLR에서 발표된 논문에서 Jonathan Frankle과 Michael Carbin이 소개한 The Lottery Ticket Hypothesis(LTH)은 심층 신경망(DNN) 훈련에 대한 흥미로운 아이디어를 제안한다. 무작위로 초기화된 대규모 신경망 내에 더 작은 하위 네트워크(Winning Ticket)가 존재한다는 것을 말한다. 이 하위 네트워크는 처음부터 별도로 훈련할 때 원래 네트워크의 성능에 도달하거나 능가할 수 있다. 이 가설은 이러한 Winning Ticket이 학습하는 데 적합한 초기 가중치를 갖는다고 가정한다.\n\n\n\nLTH 설명 그림"
  },
  {
    "objectID": "posts/lecs/lec04.html#system-support-for-sparsity",
    "href": "posts/lecs/lec04.html#system-support-for-sparsity",
    "title": "🧑‍🏫 Lecture 4",
    "section": "3. System Support for Sparsity",
    "text": "3. System Support for Sparsity\nDNN을 가속화 시키는 방법은 크게 3가지, Sparse Weight, Sparse Activation, Weight Sharing이 있다. Sparse Weight, Sparse Activation은 Pruning이고 Weight Sharing은 Quantization의 방법이다.\n\n\n\nDNN을 가속화 시키는 방법\n\n\n\nSparse Weight: Weight를 Pruning하여 Computation은 Pruning Ratio에 대응하여 빨라진다. 하지만 Memory는 Pruning된 weight의 위치를 기억하기 위한 memory 용량이 필요하므로 Pruning Ratio에 비례하여 줄진 않는다.\nSparse Activation: Weight를 Pruning하는 것과 다르게 Activation은 Test Input에 따라 dynamic 하므로 Weight를 Pruning하는 것보다 Computation이 덜 줄어든다.\nWeight Sharing: Quantization 방법으로 32-bit data를 4-bit data로 변경함으로써 8배의 memory 절약을 할 수 있다.\n\n\n3.1 EIE\nEfficient Inference Engine은 기계 학습 모델을 실시간으로 실행하기 위해 최적화된 소프트웨어 라이브러리나 프레임워크를 말한다. Processing Elements(PE)의 구조\n\n\n\nPE 연산 Logically / Physically 분석\n\n\n아래 그림에서 Input별(\\(\\vec{a}\\)) 연산은 아래와 같이 Input이 0일 때는 skip되고 0이 아닐 때는 prunning 되지 않은 weight와 연산이 진행된다.\n\n\n\nInput별 연산 과정\n\n\nEIE 실험은 가장 loss가 적은 data 자료형인 16 bit Int형을 사용했다.(0.5% loss) AlexNet이나 VGG와 같이 ReLU Activation이 많이 사용되는 모델들은 경량화가 많이 된 반면, RNN와 LSTM이 사용된 NeuralTalk 모델들 같은 경우에는 ReLU를 사용하지 않아 경량화될 수 있는 부분이 없어 Activation Density가 100%인 것을 확인할 수 있다.\n\n\n\nEIE 실험 결과\n\n\n\n\n3.2 M:N Weight Sparsity\n이 방법은 Nvidia 하드웨어의 지원이 필요한 방법으로 보통 2:4 Weight Sparsity를 사용한다. 왼쪽의 Sparse Matrix를 재배치해서 Non-zero data matrix와 인덱스를 저장하는 Index matrix를 따로 만들어서 저장한다.\n\n\n\n2:4 Weight Sparsity\n\n\nM:N Weight Sparsity 적용하지 않은 Dense GEMM과 적용한 Sparse GEMM을 계산할 때는 아래의 그림과 같은 과정으로 연산이 진행된다.\n\n\n\nDense VS. Sparse GEMM\n\n\n\n\n3.3 Sparse Convolution\nSubmanifold Sparse Convolutional Networks (SSCN)은 고차원 데이터에서 효율적인 계산을 가능하게 하는 신경망 아키텍처의 한 형태이다. 이 기술은 특히 3D 포인트 클라우드 또는 고해상도 이미지와 같이 대규모 및 고차원 데이터를 처리할 때 중요하다. SSCN의 핵심 아이디어는 데이터의 Sparcity을 활용하여 계산과 메모리 사용량을 크게 줄이는 것이다.\n\n\n\n출처: Submanifold Sparse Convolutional Networks\n\n\n이러한 Sparse Convolution은 기본 Convolution과 비교했을 때 아래 그림과 같이 나타내볼 수 있다.\n\n\n\nConventional VS. Sparse Convolution\n\n\n연산 과정을 비교해보기 위해 Input Point Cloud(\\(P\\)), Feature Map(\\(W\\)), Ouput Point Cloud(\\(Q\\))를 아래와 같이 있다고 하자. 기존의 Convolution과 Sparse Convolution을 비교해보면 연산량이 9:2로 매우 적은 연산만 필요한 것을 알 수 있다.\n\n\n\nConventional VS. Sparse 연산량 비교\n\n\nFeature Map(\\(W\\))을 기준으로 각 weight 마다 필요한 Input data의 크기가 다르다. 예를 들어 \\(W_{-1, 0}\\)은 \\(P1\\)과 만의 연산이 진행되므로 \\(P1\\)만 연산시 불러내게 된다.\n\n\n\nSparse Convolution 계산 과정\n\n\n따라서 Feature Map의 \\(W\\)에 따라 필요한 Input data를 표현하고 따로 computation을 진행하면 아래와 같이 고르지 못한 연산량 분배가 진행되는데(왼쪽 그림) 이는 computation에 overhead는 없지만 regularity가 좋지 않다. 또는 가장 computation이 많은 것을 기준으로 Batch 단위로 계산하게 된다면(가운데 그림) 적은 computation weight에서의 비효율적인 계산 대기시간이 생기므로 overhead가 생긴다. 따라서 적절히 비슷한 연산량을 가지는 grouping을 진행한 뒤 batch로 묶으면 적절히 computation을 진행할 수 있다.(오른쪽 그림)\n\n\n\nGrouping Computation\n\n\n이런 Grouping을 적용한 후 Sparse Convolution을 진행하면 Adaptive Grouping이 적용되어 아래와 같이 진행된다.\n\n\n\nSparse Convolution 예시\n\n\n여기까지가 2023년도 강의에서 마지막 Sparse Convolution에 대해 설명한 부분을 정리한 부분이다. 하지만 강의에서 설명이 많이 생략되어 있으므로 좀 더 자세한 내용은 Youtube 발표 영상이나 2022년도 강의를 참고하는 것을 권장한다."
  },
  {
    "objectID": "posts/lecs/lec04.html#reference",
    "href": "posts/lecs/lec04.html#reference",
    "title": "🧑‍🏫 Lecture 4",
    "section": "4. Reference",
    "text": "4. Reference\n\nMIT-TinyML-lecture04-Pruning-2\nAMC: Automl for Model Compression and Acceleration on Mobile Devices, 2018\nContinuous control with deep reinforcement learning\nFLOPs란? 딥러닝 연산량에 대해서\nNetAdapt: Platform-Aware Neural Network Adaptation for Mobile Applications\nThe Lottery Ticket Hypothesis: Finding Sparse, Trainable Neural Networks\nThe Lottery Ticket Hypothesis Finding Sparse, Trainable Neural Networks 논문 리뷰\nLLM Inference - HW/SW Optimizations\nAccelerating Sparse Deep Neural Networks\nSubmanifold Sparse Convolutional Networks\nTorchSparse: Efficient Point Cloud Inference Engine\nTorchSparse++: Efficient Training and Inference Framework for Sparse Convolution on GPUs\nmit-han-lab/torchsparse\nMLSys’22 TorchSparse: Efficient Point Cloud Inference Engine"
  },
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "Study Archive",
    "section": "",
    "text": "스터디 자료와 관련해서 어떤 토의나 의견 모두 감사합니다! Github Discussion에 글을 남겨주셔도 좋고 각 포스팅 하단에 있는 Giscus 댓글창에 코멘트들을 남겨주시면 됩니다.\nWelcome to any comments or opinions on our content! You can leave messages using the direct Discussion or the Giscus window on each post.\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n🧑‍🏫 Lecture 5-6\n\n\n\n\n\n\nTinyML\n\n\nEdgeAI\n\n\n\nQuantization\n\n\n\n\n\nMar 5, 2024\n\n\nSeunghyun Oh(ooshyun)\n\n\n\n\n\n\n\n\n\n\n\n\n🧑‍🏫 Lecture 4\n\n\n\n\n\n\npruning\n\n\nlecture\n\n\n\nPruning and Sparsity (Part II)\n\n\n\n\n\nFeb 18, 2024\n\n\nJung Yeon Lee\n\n\n\n\n\n\n\n\n\n\n\n\n🧑‍🏫 Lecture 3\n\n\n\n\n\n\npruning\n\n\nlecture\n\n\n\nPruning and Sparsity (Part I)\n\n\n\n\n\nJan 28, 2024\n\n\nSeunghyun Oh\n\n\n\n\n\n\n\n\n\n\n\n\n👩‍💻 Lab 0\n\n\n\n\n\n\nlab\n\n\npytorch\n\n\nbasic\n\n\n\nPyTorch Tutorial\n\n\n\n\n\nJan 11, 2024\n\n\nJung Yeon Lee\n\n\n\n\n\n\nNo matching items"
  },
  {
    "objectID": "posts/lecs/lec05.html#reference",
    "href": "posts/lecs/lec05.html#reference",
    "title": "🧑‍🏫 Lecture 5-6",
    "section": "7. Reference",
    "text": "7. Reference\n\nTinyML and Efficient Deep Learning Computing on MIT HAN LAB\nYoutube for TinyML and Efficient Deep Learning Computing on MIT HAN LAB\nDeep Compression [Han et al., ICLR 2016]\nQuantization and Training of Neural Networks for Efficient Integer-Arithmetic-Only Inference [Jacob et al., CVPR 2018]\nWith Shared Microexponents, A Little Shifting Goes a Long Way [Bita Rouhani et al.]\nData-Free Quantization Through Weight Equalization and Bias Correction [Markus et al., ICCV 2019]\nEstimating or Propagating Gradients Through Stochastic Neurons for Conditional Computation [Bengio, arXiv 2013]\nBinarized Neural Networks: Training Deep Neural Networks with Weights and Activations Constrained to +1 or −1. [Courbariaux et al., Arxiv 2016]\nXNOR-Net: ImageNet Classification using Binary Convolutional Neural Networks [Rastegari et al., ECCV 2016]\nTernary Weight Networks [Li et al., Arxiv 2016]\nTrained Ternary Quantization [Zhu et al., ICLR 2017]\nDoReFa-Net: Training Low Bitwidth Convolutional Neural Networks with Low Bitwidth Gradients [Zhou et al., arXiv 2016]\nWRPN: Wide Reduced-Precision Networks [Mishra et al., ICLR 2018]\nPACT: Parameterized Clipping Activation for Quantized Neural Networks [Choi et al., arXiv 2018]\nHAQ: Hardware-Aware Automated Quantization with Mixed Precision [Wang et al., CVPR 2019]"
  }
]