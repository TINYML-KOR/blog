[
  {
    "objectID": "posts/lecs/lec05.html",
    "href": "posts/lecs/lec05.html",
    "title": "ğŸ§‘â€ğŸ« Lecture 5-6",
    "section": "",
    "text": "ì´ë²ˆ ê¸€ì—ì„œëŠ” MIT HAN LABì—ì„œ ê°•ì˜í•˜ëŠ” TinyML and Efficient Deep Learning Computingì— ë‚˜ì˜¤ëŠ” Quantization ë°©ë²•ì„ ì†Œê°œí•˜ë ¤ í•œë‹¤. Quantization(ì–‘ìí™”) ì‹ í˜¸ì™€ ì´ë¯¸ì§€ì—ì„œ ì•„ë‚ ë¡œê·¸ë¥¼ ë””ì§€í„¸ë¡œ ë³€í™˜í•˜ëŠ” ê³¼ì •ì—ì„œ ì‚¬ìš©í•˜ëŠ” ê°œë…ì´ë‹¤. ì•„ë˜ ê·¸ë¦¼ê³¼ ê°™ì´ ì—°ì†ì ì¸ ì„¼ì„œë¡œ ë¶€í„° ë“¤ì–´ì˜¤ëŠ” ì•„ë‚ ë¡œê·¸ ë°ì´í„° ë‚˜ ì´ë¯¸ì§€ë¥¼ í‘œí˜„í•˜ê¸° ìœ„í•´ ë‹¨ìœ„ ì‹œê°„ì— ëŒ€í•´ì„œ ë°ì´í„°ë¥¼ ìƒ˜í”Œë§í•˜ì—¬ ë°ì´í„°ë¥¼ ìˆ˜ì§‘í•œë‹¤.\në””ì§€í„¸ë¡œ ë°ì´í„°ë¥¼ ë³€í™˜í•˜ê¸° ìœ„í•´ ë°ì´í„° íƒ€ì…ì„ ì •í•˜ë©´ì„œ ì´ë¥¼ í•˜ë‚˜ì”© ì–‘ìí™”í•œë‹¤. ì–‘ìˆ˜ì™€ ìŒìˆ˜ë¥¼ í‘œí˜„í•˜ê¸° ìœ„í•´ Unsigned Integer ì—ì„œ Signed Integer, Signedì—ì„œë„ Sign-Magnitude ë°©ì‹ê³¼ Twoâ€™s Complementë°©ì‹ìœ¼ë¡œ, ê·¸ë¦¬ê³  ë” ë§ì€ ì†Œìˆ«ì  ìë¦¬ë¥¼ í‘œí˜„í•˜ê¸° ìœ„í•´ Fixed-pointì—ì„œ Floating pointë¡œ ë°ì´í„° íƒ€ì…ì—ì„œ ìˆ˜ì˜ ë²”ì£¼ë¥¼ í™•ì¥ì‹œí‚¨ë‹¤. ì°¸ê³ ë¡œ Deviceì˜ Computationalityì™€ ML ëª¨ë¸ì˜ ì„±ëŠ¥ì§€í‘œì¤‘ í•˜ë‚˜ì¸ FLOPì´ ë°”ë¡œ floating point operations per secondì´ë‹¤.\nì´ ê¸€ì—ì„œ floating pointë¥¼ ì´í•´í•˜ë©´, fixed pointë¥¼ ì‚¬ìš©í•˜ëŠ” ê²ƒì´ ë§¤ëª¨ë¦¬ì—ì„œ, ê·¸ë¦¬ê³  ì—°ì‚°ì—ì„œ ë” íš¨ìœ¨ì ì¼ ê²ƒì´ë¼ê³  ì˜ˆìƒí•´ë³¼ ìˆ ìˆ˜ ìˆë‹¤. MLëª¨ë¸ì„ í´ë¼ìš°ë“œ ì„œë²„ì—ì„œ ëŒë¦´ ë•ŒëŠ” í¬ê²Œ ë¬¸ì œë˜ì§€ ì•Šì•˜ì§€ë§Œ ì•„ë˜ ë‘ ê°€ì§€ í‘œë¥¼ ë³´ë©´ ì—ë„ˆì§€ì†Œëª¨, ì¦‰ ë°°í„°ë¦¬ íš¨ìœ¨ì—ì„œ í¬ê²Œ ì°¨ì´ê°€ ë³´ì¸ë‹¤. ê·¸ë ‡ê¸° ë•Œë¬¸ì— ëª¨ë¸ì—ì„œ Floating pointë¥¼ fixed pointë¡œ ë” ë§ì´ ë°”ê¾¸ë ¤ê³  í•˜ëŠ”ë° ì´ ë°©ë²•ìœ¼ë¡œ ë‚˜ì˜¨ ê²ƒì´ ë°”ë¡œ Quatizationì´ë‹¤.\nì´ë²ˆ ê¸€ì—ì„œëŠ” Quntization ì¤‘ì—ì„œ Quantization ë°©ë²•ê³¼ ê·¸ ì¤‘ Linearí•œ ë°©ë²•ì— ëŒ€í•´ ë” ìì„¸í•˜ê²Œ, ê·¸ë¦¬ê³  Post-training Quantizationê¹Œì§€ ë‹¤ë£¨ê³ , ë‹¤ìŒ ê¸€ì—ì„œëŠ” Quantization-Aware Training, Binary/Tenary Quantization, Mixed Precision Quantizationê¹Œì§€ ë‹¤ë£¨ë ¤ê³  í•œë‹¤."
  },
  {
    "objectID": "posts/lecs/lec05.html#common-network-quantization",
    "href": "posts/lecs/lec05.html#common-network-quantization",
    "title": "ğŸ§‘â€ğŸ« Lecture 5-6",
    "section": "1. Common Network Quantization",
    "text": "1. Common Network Quantization\nì•ì„œì„œ ì†Œê°œí•œ ê²ƒì²˜ëŸ¼ Neural Netoworkë¥¼ ìœ„í•œ Quantizationì€ ë‹¤ìŒê³¼ ê°™ì´ ë‚˜ëˆŒ ìˆ˜ ìˆë‹¤. Quantization ë°©ë²•ì„ í•˜ë‚˜ì”© ì•Œì•„ë³´ì.\n\n\n\nReference. MIT-TinyML-lecture5-Quantization-1\n\n\n\n\n\n Reference. MIT-TinyML-lecture5-Quantization-1 in https://efficientml.ai \n\n\n\n1.1 K-Means-based Quantization\nê·¸ ì¤‘ ì²« ë²ˆì§¸ë¡œ K-means-based Quantizationì´ ìˆë‹¤. Deep Compression [HanÂ et al., ICLR 2016] ë…¼ë¬¸ì— ì†Œê°œí–ˆë‹¤ëŠ” ì´ ë°©ë²•ì€ ì¤‘ì‹¬ê°’ì„ ê¸°ì¤€ìœ¼ë¡œ clusteringì„ í•˜ëŠ” ë°©ë²•ì´ë‹¤. ì˜ˆì œë¥¼ ë´ë³´ì.\n\n\n\nReference. MIT-TinyML-lecture5-Quantization-1\n\n\nìœ„ ì˜ˆì œëŠ” weightë¥¼ codebookì—ì„œ -1, 0, 1.5, 2ë¡œ ë‚˜ëˆ  ê°ê°ì— ë§ëŠ” ì¸ë±ìŠ¤ë¡œ í‘œê¸°í•œë‹¤. ì´ë ‡ê²Œ ì—°ì‚°ì„ í•˜ë©´ ê¸°ì¡´ì— 64bytesë¥¼ ì‚¬ìš©í–ˆë˜ weightê°€ 20bytesë¡œ ì¤„ì–´ë“ ë‹¤. codebookìœ¼ë¡œ ì˜ˆì œëŠ” 2bitë¡œ ë‚˜ëˆ´ì§€ë§Œ, ì´ë¥¼ N-bitë§Œí¼ ì¤„ì¸ë‹¤ë©´ ìš°ë¦¬ëŠ” ì´ 32/Në°°ì˜ ë©”ëª¨ë¦¬ë¥¼ ì¤„ì¼ ìˆ˜ ìˆë‹¤. í•˜ì§€ë§Œ ì´ ê³¼ì •ì—ì„œ quantizatio error, ì¦‰ quantizationì„ í•˜ê¸° ì „ê³¼ í•œ í›„ì— ì˜¤ì°¨ê°€ ìƒê¸°ëŠ” ê²ƒì„ ìœ„ ì˜ˆì œì—ì„œ ë³¼ ìˆ˜ ìˆë‹¤. ë©”ëª¨ë¦¬ ì‚¬ìš©ëŸ‰ì„ ì¤„ì´ëŠ” ê²ƒë„ ì¢‹ì§€ë§Œ, ì´ ë•Œë¬¸ì— ì„±ëŠ¥ì— ì˜¤ì°¨ê°€ ìƒê¸°ì§€ ì•Šê²Œ í•˜ê¸°ìœ„í•´ ì´ ì˜¤ì°¨ë¥¼ ì¤„ì´ëŠ” ê²ƒ ë˜í•œ ì¤‘ìš”í•˜ë‹¤.\n\n\n\nReference. MIT-TinyML-lecture5-Quantization-1\n\n\nì´ë¥¼ ë³´ì™„í•˜ê¸° ìœ„í•´ Quantizedí•œ Weightë¥¼ ìœ„ì— ê·¸ë¦¼ì²˜ëŸ¼ Fine-tuningí•˜ê¸°ë„ í•œë‹¤. centroidë¥¼ fine-tuningí•œë‹¤ê³  ìƒê°í•˜ë©´ ë˜ëŠ”ë°, ê° centroidì—ì„œ ìƒê¸°ëŠ” ì˜¤ì°¨ë¥¼ í‰ê· ë‚´ tuningí•˜ëŠ” ë°©ë²•ì´ë‹¤. ì´ ë°©ë²•ì„ ì œì•ˆí•œ ë…¼ë¬¸ ì—ì„œëŠ” Convolution ë ˆì´ì–´ì—ì„œëŠ” 4bitê¹Œì§€ centroidë¥¼ ê°€ì¡Œì„ ë•Œ, Full-Connected layerì—ì„œëŠ” 2 bitê¹Œì§€ centroidë¥¼ ê°€ì¡Œì„ ë•Œ ì„±ëŠ¥ì— í•˜ë½ì´ ì—†ë‹¤ê³  ë§í•˜ê³  ìˆì—ˆë‹¤.\n\n\n\nReference. Deep Compression [Han et al., ICLR 2016]\n\n\nì´ë ‡ê²Œ Quantization ëœ WeightëŠ” ìœ„ì²˜ëŸ¼ ì—°ì†ì ì¸ ê°’ì—ì„œ ì•„ë˜ì²˜ëŸ¼ Discreteí•œ ê°’ìœ¼ë¡œ ë°”ë€ë‹¤.\n\n\n\nReference. Deep Compression [Han et al., ICLR 2016]\n\n\në…¼ë¬¸ì€ ì´ë ‡ê²Œ Quantizationí•œ weightë¥¼ í•œ ë²ˆ ë” Huffman codingë¥¼ ì´ìš©í•´ ìµœì í™”ì‹œí‚¨ë‹¤. ì§§ê²Œ ì„¤ëª…í•˜ìë©´, ë¹ˆë„ìˆ˜ê°€ ë†’ì€ ë¬¸ìëŠ” ì§§ì€ ì´ì§„ì½”ë“œë¥¼, ë¹ˆë„ ìˆ˜ê°€ ë‚®ì€ ë¬¸ìì—ëŠ” ê¸´ ì´ì§„ì½”ë“œë¥¼ ì“°ëŠ” ë°©ë²•ì´ë‹¤. ì••ì¶• ê²°ê³¼ë¡œ Generalí•œ ëª¨ë¸ê³¼ ì••ì¶• ë¹„ìœ¨ì´ ê½¤ í° SqueezeNetì„ ì˜ˆë¡œ ë“ ë‹¤. ìì„¸í•œ ë‚´ìš©ì€ ë…¼ë¬¸ì„ ì°¸ê³ í•˜ëŠ” ê±¸ë¡œ.\n\n\n\nReference. Deep Compression [Han et al., ICLR 2016]\n\n\n\n\n\nReference. Deep Compression [Han et al., ICLR 2016]\n\n\ninferenceë¥¼ ìœ„í•´ weightë¥¼ Decodingí•˜ëŠ” ê³¼ì •ì€ inferenceê³¼ì •ì—ì„œ ì €ì¥í•œ clusterì˜ ì¸ë±ìŠ¤ë¥¼ ì´ìš©í•´ codebookì—ì„œ í•´ë‹¹í•˜ëŠ” ê°’ì„ ì°¾ì•„ë‚´ëŠ” ê²ƒì´ë‹¤. ì´ ë°©ë²•ì€ ì €ì¥ ê³µê°„ì„ ì¤„ì¼ ìˆ˜ëŠ” ìˆì§€ë§Œ, floating point Computationì´ë‚˜ ë©”ëª¨ë¦¬ ì ‘ê·¼í•˜ëŠ” ë°©ì‹ìœ¼ë¡œ centroidë¥¼ ì“°ëŠ” í•œê³„ê°€ ìˆì„ ìˆ˜ ë°–ì— ì—†ë‹¤.\n\n\n\nReference. MIT-TinyML-lecture5-Quantization-1\n\n\n\n\n\n Reference. Deep Compression [Han et al., ICLR 2016] \n\n\n\n\n1.2 Linear Quantization\në‘ ë²ˆì§¸ ë°©ë²•ì€ Linear Quatizationì´ë‹¤. floating-pointì¸ weightë¥¼ N-bitì˜ ì •ìˆ˜ë¡œ affine mappingì„ ì‹œí‚¤ëŠ” ë°©ë²•ì´ë‹¤. ê°„ë‹¨í•˜ê²Œ ì‹ìœ¼ë¡œ ë³´ëŠ” ê²Œ ë” ì´í•´ê°€ ì‰½ë‹¤.\n\n\n\nReference. MIT-TinyML-lecture5-Quantization-1\n\n\nì—¬ê¸°ì„œ S(Scale of Linear Quantization)ì™€ Z(Zero point of Linear Quantization)ê°€ ìˆëŠ”ë° ì´ ë‘˜ì´ quantization parameter ë¡œì¨ tuningì„ í•  ìˆ˜ ìˆëŠ” ê°’ì¸ ê²ƒì´ë‹¤.\n\n\n\nReference. MIT-TinyML-lecture5-Quantization-1\n\n\n\n\n1.3 Scale and Zero point\n\n\n\nReference. MIT-TinyML-lecture5-Quantization-1\n\n\nì´ Scaleê³¼ Zero point ë‘ íŒŒë¼ë¯¸í„°ë¥¼ ì´ìš©í•´ì„œ affine mappingì€ ìœ„ ê·¸ë¦¼ê³¼ ê°™ë‹¤. Bit ìˆ˜(Bit Width)ê°€ ë‚®ì•„ì§€ë©´ ë‚®ì•„ì§ˆ ìˆ˜ë¡, floating pointì—ì„œ í‘œí˜„í•  ìˆëŠ” ìˆ˜ ë˜í•œ ì¤„ì–´ë“¤ ê²ƒì´ë‹¤. ê·¸ë ‡ë‹¤ë©´ Scaleì™€ Zero pointëŠ” ê°ê° ì–´ë–»ê²Œ ê³„ì‚°í• ê¹Œ?\nìš°ì„  floating-point ì¸ ìˆ«ìì˜ ë²”ìœ„ ì¤‘ ìµœëŒ€ê°’ê³¼ ìµœì†Ÿê°’ì— ë§ê²Œ ë‘ ì‹ì„ ì„¸ìš°ê³  ì´ë¥¼ ì—°ë¦½ë°©ì •ì‹ìœ¼ë¡œ Scaleê³¼ Zero pointì„ êµ¬í•  ìˆ˜ ìˆë‹¤.\n\nScale point \\[\n  r_{max} = S(q_{max}-Z)\n  \\] \\[\n  r_{min} = S(q_{min}-Z)\n  \\]\n\\[\n  r_{max} - r_{min} = S(q_{max} - q_{min})\n  \\]\n\\[\n  S = \\dfrac{r_{max}-r_{min}}{q_{max}-q_{min}}\n  \\]\nZero point \\[\n  r_{min} = S(q_{min}-Z)\n  \\]\n\\[\n  Z=q_{min}-\\dfrac{r_{min}}{S}\n  \\]\n\\[\n  Z = round\\Big(q_{min}-\\dfrac{r_{min}}{S}\\Big)\n  \\]\n\nì˜ˆë¥¼ ë“¤ì–´, ì•„ë˜ì™€ ê°™ì€ ì˜ˆì œì—ì„œ \\(r_{max}\\) ëŠ”\\(2.12\\) ì´ê³  \\(r_{min}\\) ì€ \\(-1.08\\) ë¡œ Scaleì„ ê³„ì‚°í•˜ë©´ ì•„ë˜ ê·¸ë¦¼ì²˜ëŸ¼ ëœë‹¤. Zero pointëŠ” \\(-1\\) ë¡œ ê³„ì‚°í•  ìˆ˜ ìˆë‹¤.\n\n\n\n\nReference. MIT-TinyML-lecture5-Quantization-1\n\n\nê·¸ëŸ¼ Symmetricí•˜ê²Œ rì˜ ë²”ìœ„ë¥¼ ì œí•œí•˜ëŠ” ê²ƒê³¼ ê°™ì€ ë‹¤ë¥¸ Linear Quantizationì€ ì—†ì„ê¹Œ? ì´ë¥¼ ì•ì„œ, Quatizedëœ ê°’ë“¤ì´ Matrix Multiplicationì„ í•˜ë©´ì„œ ë¯¸ë¦¬ ê³„ì‚°ë  ìˆ˜ ìˆëŠ” ìˆ˜ (Quantized Weight, Scale, Zero point)ê°€ ìˆìœ¼ë‹ˆ inferenceì‹œ ì—°ì‚°ëŸ‰ì„ ì¤„ì´ê¸° ìœ„í•´ ë¯¸ë¦¬ ê³„ì‚°í•  ìˆ˜ ìˆëŠ” íŒŒë¼ë¯¸í„°ëŠ” ì—†ì„ê¹Œ?\n\n\n1.4 Quantized Matrix Multiplication\nì…ë ¥ X, Weight W, ê²°ê³¼ Yê°€ Matrix Multiplicationì„ í–ˆë‹¤ê³  í•  ë•Œ ì‹ì„ ê³„ì‚°í•´ë³´ì.\n\\[\nY=WX\n\\]\n\\[\nS_Y(q_Y-Z_Y) = S_W(q_W-Z_W) \\cdot S_X(q_X-Z_X\n\\]\n\\[\n\\vdots\n\\]\n\n\n\nReference. MIT-TinyML-lecture5-Quantization-1\n\n\nì—¬ê¸°ì„œ ë§ˆì§€ë§‰ ì •ë¦¬í•œ ì‹ì„ ì‚´í´ë³´ë©´,\n\\(Z_x\\) ì™€ \\(q_w, Z_w, Z_X\\) ì˜ ê²½ìš°ëŠ” ë¯¸ë¦¬ ì—°ì‚°ì´ ê°€ëŠ¥í•˜ë‹¤. ë˜ \\(S_wS_X/S_Y\\) ì˜ ê²½ìš° í•­ìƒ ìˆ˜ì˜ ë²”ìœ„ê°€ \\((0, 1)\\) ë¡œ \\(2^{-n}M_0\\) , \\(M_0 \\in [0.5, 1)\\) ë¡œ ë³€í˜•í•˜ë©´ N-bit Integerë¡œ Fixed-point í˜•íƒœë¡œ í‘œí˜„ ê°€ëŠ¥í•˜ë‹¤. ì—¬ê¸°ì— \\(Z_w\\)ê°€ 0ì´ë©´ ì–´ë–¨ê¹Œ? ë˜ ë¯¸ë¦¬ ê³„ì‚°í•  ìˆ˜ ìˆëŠ” í•­ì´ ë³´ì¸ë‹¤.\n\n\n1.5 Symmetric Linear Quantization\n\n\n\nReference. MIT-TinyML-lecture5-Quantization-1\n\n\n\\(Z_w = 0\\) ì´ë¼ê³  í•¨ì€ ë°”ë¡œ ìœ„ì™€ ê°™ì€ Weight ë¶„í¬ì¸ë°, ë°”ë¡œ Symmetricí•œ Linear Quantizationìœ¼ë¡œ \\(Z_w\\)ë¥¼ 0ìœ¼ë¡œ ë§Œë“¤ì–´ \\(Z_w q_x\\)í•­ì„ 0ìœ¼ë¡œ ë‘˜ ìˆ˜ ìˆì–´ ì—°ì‚°ì„ ë˜ ì¤„ì¼ ìˆ˜ ìˆì„ ê²ƒì´ë‹¤.\nSymmetric Linear Quantizationì€ ì£¼ì–´ì§„ ë°ì´í„°ì—ì„œ Full range modeì™€ Restrict range modeë¡œ ë‚˜ë‰œë‹¤.\nì²« ë²ˆì§¸ Full range mode ëŠ” Scaleì„ real number(ë°ì´í„°, weight)ì—ì„œ ë²”ìœ„ê°€ ë„“ì€ ìª½ì— ë§ì¶”ëŠ” ê²ƒì´ë‹¤. ì˜ˆë¥¼ ë“¤ì–´ ì•„ë˜ì˜ ê²½ìš°, r_minì´ r_maxë³´ë‹¤ ì ˆëŒ“ê°’ì´ ë” í¬ê¸° ë•Œë¬¸ì— r_minì— ë§ì¶° q_minì„ ê°€ì§€ê³  Scaleì„ êµ¬í•œë‹¤. ì´ ë°©ë²•ì€ Pytorch native quantizationê³¼ ONNXì—ì„œ ì‚¬ìš©ëœë‹¤ê³  ê°•ì˜ì—ì„œ ì†Œê°œí•œë‹¤.\n\n\n\nReference. MIT-TinyML-lecture5-Quantization-1\n\n\në‘ ë²ˆì§¸ Restrict range modeëŠ” Scaleì„ real number(ë°ì´í„°, weight)ì—ì„œ ë²”ìœ„ê°€ ì¢ì€ ìª½ì— ë§ì¶”ëŠ” ê²ƒì´ë‹¤. ì˜ˆë¥¼ ë“¤ì–´ ì•„ë˜ì˜ ê²½ìš°, r_minê°€ r_maxë³´ë‹¤ ì ˆëŒ“ê°’ì´ ë” í¬ê¸° ë•Œë¬¸ì— r_minì— ë§ì¶”ë©´ì„œ q_maxì— ë§ë„ë¡ Scaleì„ êµ¬í•œë‹¤. ì´ ë°©ë²•ì€ TensorFlow, NVIDIA TensorRT, Intel DNNLì—ì„œ ì‚¬ìš©ëœë‹¤ê³  ê°•ì˜ì—ì„œ ì†Œê°œí•œë‹¤.\n\n\n\nReference. MIT-TinyML-lecture5-Quantization-1\n\n\nê·¸ë ‡ë‹¤ë©´ ì™œ Symmetric ì¨ì•¼í• ê¹Œ? Asymmetric ë°©ë²•ê³¼ Symmetric ë°©ë²•ì˜ ì°¨ì´ëŠ” ë­˜ê¹Œ? (feat. Neural Network Distiller) ì•„ë˜ ê·¸ë¦¼ì„ ì°¸ê³ í•˜ë©´ ë˜ì§€ë§Œ, ê°€ì¥ í° ì°¨ì´ë¡œ ë³´ì´ëŠ” ê²ƒì€ Computation vs Compactful quantized rangeë¡œ ì´í•´ê°„ë‹¤.\n\n\n\nReference. MIT-TinyML-lecture5-Quantization-1\n\n\n\n\n1.6 Linear Quantization examples\nê·¸ëŸ¼ Quatization ë°©ë²•ì— ëŒ€í•´ ì•Œì•„ë´¤ìœ¼ë‹ˆ ì´ë¥¼ Full-Connected Layer, Convolution Layerì— ì ìš©í•´ë³´ê³  ì–´ë–¤ íš¨ê³¼ê°€ ìˆëŠ”ì§€ ì•Œì•„ë³´ì.\n\n1.6.1 Full-Connected Layer\nì•„ë˜ì²˜ëŸ¼ ì‹ì„ ì „ê°œí•´ë³´ë©´ ë¯¸ë¦¬ ì—°ì‚°í•  ê³„ì‚°í•  ìˆ˜ ìˆëŠ” í•­ê³¼ N-bit integerë¡œ í‘œí˜„í•  ìˆëŠ” í•­ìœ¼ë¡œ ë‚˜ëˆŒ ìˆ˜ ìˆë‹¤(ì „ê°œí•˜ëŠ” ì´ìœ ëŠ” ì•„ë§ˆ ë¯¸ë¦¬ ê³„ì‚°í•  ìˆ˜ ìˆëŠ” í•­ì„ ì•Œì•„ë³´ê¸° ìœ„í•¨ì´ ì•„ë‹ê¹Œ ì‹¶ë‹¤).\n\\[\nY=WX+b\n\\]\n\\[\n\\downarrow\n\\]\n\\[\nS_Y(q_Y - Z_Y) = S_W(q_W - Z_W) \\cdot S_X(q_X - Z_X) + S_b(q_b - Z_b)\n\\]\n\\[\n\\downarrow \\ Z_w=0\n\\]\n\\[\nS_Y(q_Y - Z_Y) = S_WS_X(q_Wq_X - Z_xq_W) + S_b(q_b - Z_b)\n\\]\n\\[\n\\downarrow \\ Z_b=0, S_b=S_WS_X\n\\]\n\\[\nS_Y(q_Y - Z_Y) = S_WS_X(q_Wq_X - Z_xq_W+q_b)\n\\]\n\\[\n\\downarrow\n\\]\n\\[\nq_Y = \\dfrac{S_WS_X}{S_Y}(q_Wq_X+q_b - Z_Xq_W) + Z_Y\n\\]\n\\[\n\\downarrow \\ q_{bias}=q_b-Z_xq_W\\\\\n\\]\n\\[\nq_Y = \\dfrac{S_WS_X}{S_Y}(q_Wq_X+ q_{bias}) + Z_Y\\\\\n\\]\nê°„ë‹¨íˆ í‘œê¸°í•˜ê¸° ìœ„í•´ \\(Z_W=0, Z_b=0, S_b = S_W S_X\\) ì´ë¼ê³  ê°€ì •í•œë‹¤.\n\n\n\nReference. MIT-TinyML-lecture5-Quantization-1\n\n\n\n\n1.6.2 Convolutional Layer\nConvolution Layerì˜ ê²½ìš°ëŠ” Weightì™€ Xì˜ ê³±ì˜ ê²½ìš°ë¥¼ Convolutionìœ¼ë¡œ ë°”ê¿”ì„œ ìƒê°í•´ë³´ë©´ ëœë‹¤. ê·¸ë„ ê·¸ëŸ´ ê²ƒì´ Convolutionì€ Kernelê³¼ Inputì˜ ê³±ì˜ í•©ìœ¼ë¡œ ì´ë£¨ì–´ì ¸ ìˆê¸° ë•Œë¬¸ì— Full-Connectedì™€ ê±°ì˜ ìœ ì‚¬í•˜ê²Œ ì „ê°œë  ìˆ˜ ìˆì„ ê²ƒì´ë‹¤.\n\n\n\nReference. MIT-TinyML-lecture5-Quantization-1"
  },
  {
    "objectID": "posts/lecs/lec05.html#post-training-quantization-ptq",
    "href": "posts/lecs/lec05.html#post-training-quantization-ptq",
    "title": "ğŸ§‘â€ğŸ« Lecture 5-6",
    "section": "2. Post-training Quantization (PTQ)",
    "text": "2. Post-training Quantization (PTQ)\nê·¸ëŸ¼ ì•ì„œì„œ Quantizaedí•œ Layerë¥¼ Fine tuningí•  ì—†ì„ê¹Œ? â€œHow should we get the optimal linear quantization parameters (S, Z)?â€ ì´ ì§ˆë¬¸ì— ëŒ€í•´ì„œ Weight, Activation, Bias ì„¸ ê°€ì§€ì™€ ê·¸ì— ëŒ€í•˜ì—¬ ë…¼ë¬¸ì—ì„œ ë³´ì—¬ì£¼ëŠ” ê²°ê³¼ê¹Œì§€ ì•Œì•„ë³´ì.\n\n2.1 Weight quantization\nTL;DR. ì´ ê°•ì˜ì—ì„œ ì†Œê°œí•˜ëŠ” Weight quantizationì€ Grandularityì— ë”°ë¼ Whole(Per-Tensor), Channel, ê·¸ë¦¬ê³  Layerë¡œ ë“¤ì–´ê°„ë‹¤.\n\n2.1.1 Granularity\nWeight quantizationì—ì„œ Granularityì— ë”°ë¼ì„œ Per-Tensor, Per-Channel, Group, ê·¸ë¦¬ê³  Generalized í•˜ëŠ” ë°©ë²•ìœ¼ë¡œ í™•ì¥ì‹œì¼œ Shared Micro-exponent(MX) data typeì„ ì°¨ë¡€ë¡œ ë³´ì—¬ì¤€ë‹¤. Scaleì„ ëª‡ ê°œë‚˜ ë‘˜ ê²ƒì´ëƒ, ê·¸ Scaleì„ ì ìš©í•˜ëŠ” ë²”ìœ„ë¥¼ ì–´ë–»ê²Œ ë‘˜ ê²ƒì´ëƒ, ê·¸ë¦¬ê³  Scaleì„ ì–¼ë§ˆë‚˜ ë””í…Œì¼í•˜ê²Œ(e.g.Â floating-point)í•  ê²ƒì´ëƒì— ì´ˆì ì„ ë‘”ë‹¤.\n\n\n\nReference. MIT-TinyML-lecture5-Quantization-2\n\n\nì²« ë²ˆì§¸ëŠ” Per-Tensor Quantization íŠ¹ë³„í•˜ê²Œ ì„¤ëª…í•  ê²ƒ ì—†ì´ ì´ì „ê¹Œì§€ ì„¤ëª…í–ˆë˜ í•˜ë‚˜ì˜ Scaleì„ ì‚¬ìš©í•˜ëŠ” Linear Quantizationì´ë¼ê³  ìƒê°í•˜ë©´ ë˜ê² ë‹¤. íŠ¹ì§•ìœ¼ë¡œëŠ” Large modelì— ëŒ€í•´ì„œëŠ” ì„±ëŠ¥ì´ ê´œì°®ì§€ë§Œ ì‘ì€ ëª¨ë¸ë¡œ ë–¨ì–´ì§€ë©´ ì„±ëŠ¥ì´ ê¸‰ê²©í•˜ê²Œ ë–¨ì–´ì§„ë‹¤ê³  ì„¤ëª…í•œë‹¤. Channelë³„ë¡œ weight ë²”ì£¼ê°€ ë„“ì€ ê²½ìš°ë‚˜ outlier weightê°€ ìˆëŠ” ê²½ìš° quantization ì´í›„ì— ì„±ëŠ¥ì´ í•˜ë½í–ˆë‹¤ê³  ë§í•œë‹¤.\n\n\n\nReference. MIT-TinyML-lecture5-Quantization-2\n\n\nê·¸ë˜ì„œ ê·¸ í•´ê²°ë°©ì•ˆìœ¼ë¡œ ë‚˜ì˜¤ëŠ” ê²ƒì´ ë‘ ë²ˆì§¸ ë°©ë²•ì¸ Per-Channel Quantizationì´ë‹¤. ìœ„ ì˜ˆì œì—ì„œ ë³´ë©´ Channel ë§ˆë‹¤ ìµœëŒ€ê°’ê³¼ ê°ê°ì— ë§ëŠ” Scaleì„ ë”°ë¡œ ê°€ì§€ëŠ” ê²ƒì„ ë³¼ ìˆ˜ ìˆë‹¤. ê·¸ë¦¬ê³  ì ìš©í•œ ê²°ê³¼ì¸ ì•„ë˜ ê·¸ë¦¼ì„ ë³´ë©´ Per-Channelê³¼ Per-Tensorë¥¼ ë¹„êµí•´ë³´ë©´ Per-Channelì´ ê¸°ì¡´ì— floating point weightì™€ì˜ ì°¨ì´ê°€ ë” ì ë‹¤. í•˜ì§€ë§Œ, ë§Œì•½ í•˜ë“œì›¨ì–´ì—ì„œ Per-Channel Quantizationì„ ì§€ì›í•˜ì§€ ì•ŠëŠ”ë‹¤ë©´ ë¶ˆí•„ìš”í•œ ì—°ì‚°ì„ ì¶”ê°€ë¡œ í•´ì•¼í•˜ê¸° ë•Œë¬¸ì— ì´ëŠ” ì í•©í•œ ë°©ë²•ì´ ë  ìˆ˜ ì—†ë‹¤ëŠ” ì ë„ ê³ ë ¤í•´ì•¼í•  ê²ƒì´ë‹¤(ì´ëŠ” ì´ì „ Tiny Engineì— ëŒ€í•œ ê¸€ì—ì„œ Channelë‚´ì— ìºì‹±ì„ ì´ìš©í•œ ìµœì í™”ì™€ ì—°ê´€ì´ ìˆë‹¤). ê·¸ëŸ¼ ë˜ ë‹¤ë¥¸ ë°©ë²•ì€ ì—†ì„ê¹Œ?\n\n\n\nReference. MIT-TinyML-lecture5-Quantization-2\n\n\nì„¸ ë²ˆì§¸ ë°©ë²•ì€ Group Quantizationìœ¼ë¡œ ì†Œê°œí•˜ëŠ” Per-vector Scaled Quantizationì™€ Shared Micro-exponent(MX) data type ì´ë‹¤. Per-vector Scaled Quantizationì€ 2023ë…„ë„ ê°•ì˜ë¶€í„° ì†Œê°œí•˜ëŠ”ë°, ì´ ë°©ë²•ì€ Scale factorë¥¼ ê·¸ë£¹ë³„ë¡œ í•˜ë‚˜, Per-Tensorë¡œ í•˜ë‚˜ë¡œ ë‘ê°œë¥¼ ë‘ëŠ” ë°©ë²•ì´ë‹¤. ì•„ë˜ì˜ ê·¸ë¦¼ì„ ë³´ë©´,\n\n\n\nReference. MIT-TinyML-lecture5-Quantization-2\n\n\n\\[\nr=S(q-Z) \\rightarrow r=\\gamma \\cdot S_{q}(q-Z)\n\\]\n\\(S_q\\) ë¡œ vectorë³„ ìŠ¤ì¼€ì¼ë§ì„ í•˜ë‚˜, \\(\\gamma\\) ë¡œ Tensorì— ìŠ¤ì¼€ì¼ë§ì„ í•˜ë©° ê°ë§ˆëŠ” floating pointë¡œ í•˜ëŠ” ê²ƒì„ ë³¼ ìˆ˜ìˆë‹¤. ì•„ë¬´ë˜ë„ vectorë‹¨ìœ„ë¡œ ìŠ¤ì¼€ì¼ë§ì„ í•˜ê²Œë˜ë©´ channelê³¼ ë¹„êµí•´ì„œ í•˜ë“œì›¨ì–´ í”Œë«í¼ì— ë§ê²Œ accuracyì˜ trade-offë¥¼ ì¡°ì ˆí•˜ê¸° ë” ìˆ˜ì›”í•  ê²ƒìœ¼ë¡œ ë³´ì¸ë‹¤.\nì—¬ê¸°ì„œ ê°•ì˜ëŠ” ì§€í‘œì¸ Memory Overheadë¡œ â€œEffective Bit Widthâ€ë¥¼ ì†Œê°œí•œë‹¤. ì´ëŠ” Microsoftì—ì„œ ì œê³µí•˜ëŠ” Quantization Approach MX4, MX6, MX9ê³¼ ì—°ê²°ë¼ ìˆëŠ”ë°, ì´ ë°ì´í„°íƒ€ì…ì€ ì¡°ê¸ˆ ì´í›„ì— ë” ìì„¸íˆ ì„¤ëª…í•  ê²ƒì´ë‹¤. Effective Bit Width? ì˜ˆì‹œ í•˜ë‚˜ë¥¼ ë“¤ì–´ ì´í•´í•´ë³´ì. ë§Œì•½ 4-bit Quatizationì„ 4-bit per-vector scaleì„ 16 elements(4ê°œì˜ weightê°€ ê°ê° 4bitë¥¼ ê°€ì§„ë‹¤ê³  ìƒê°í•˜ë©´ 16 elementë¡œ ê³„ì‚°ëœë‹¤ ìœ ì¶”í•  ìˆë‹¤) ë¼ë©´, Effective Bit WidthëŠ” 4(Scale bit) + 4(Vector Scale bit) / 16(Vector Size) = 4.25ê°€ ëœë‹¤. Elementë‹¹ Scale bitë¼ê³  ê°„ë‹¨í•˜ê²Œ ìƒê°í•  ìˆ˜ë„ ìˆì„ ë“¯ ì‹¶ë‹¤.\në§ˆì§€ë§‰ Per-vector Scaled Quantizationì„ ì´í•´í•˜ë‹¤ë³´ë©´ ì´ì „ì— Per-Tensor, Per-Channelë„ ê·¸ë£¹ìœ¼ë¡œ ì–¼ë§ˆë§Œí¼ ë¬¶ëŠ” ì°¨ì´ê°€ ìˆê³ , ì´ëŠ” ì´ë“¤ì„ ì¼ë°˜í™”í•  ìˆ˜ ìˆì–´ ë³´ì¸ë‹¤. ê°•ì˜ì—ì„œ ë°”ë¡œ ë‹¤ìŒì— ì†Œê°œí•˜ëŠ” ë°©ë²•ì´ ë°”ë¡œ Multi-level scaling schemeì´ë‹¤. Per-Channel Quantizationì™€ Per-Vector Quantization(VSQ, Vector-Scale Quantization)ë¶€í„° ë´ë³´ì.\n\n\n\nReference. With Shared Microexponents, A Little Shifting Goes a Long Way [Bita Rouhani et al.]\n\n\nPer-Channel QuantizationëŠ” Scale factorê°€ í•˜ë‚˜ë¡œ Effective Bit WidthëŠ” 4ê°€ ëœë‹¤. ê·¸ë¦¬ê³  VSQëŠ” ì´ì „ì— ê³„ì‚°í–ˆ ë“¯ 4.25ê°€ ë  ê²ƒì´ë‹¤(ì°¸ê³ ë¡œ Per Channelë¡œ ì ìš©ë˜ëŠ” Scaleì˜ ê²½ìš° elementì˜ ìˆ˜ê°€ ë§ì•„ì„œ ê·¸ëŸ°ì§€ ë”°ë¡œ Effective Bit Widthë¡œ ê³„ì‚°í•˜ì§€ëŠ” ì•ŠëŠ”ë‹¤). VSQê¹Œì§€ ë³´ë©´ì„œ Effective Bit WidthëŠ”,\nEffective Bit Width = Scale bit + Group 0 Scale bit / Group 0 Size +...\ne.g. VSQ Data type int4 = Scale bit (4) + Group 0 Scale bit(4) / Group 0 Size(16) = 4.25\nì´ë ‡ê²Œ ê³„ì‚°í•  ìˆ˜ ìˆë‹¤. ê·¸ë¦¬ê³ , MX4, MX6, MX9ê°€ ë‚˜ì˜¨ë‹¤. ì°¸ê³ ë¡œ SëŠ” Sign bit, Mì€ Mantissa bit, EëŠ” Exponent bitë¥¼ ì˜ë¯¸í•œë‹¤(Mantissaë‚˜ Exponentì— ëŒ€í•œ ìì„¸í•œ ë‚´ìš©ì€ floating point vs fixed point ê¸€ì„ ì°¸ê³ í•˜ì). ì•„ë˜ëŠ” Microsoftì—ì„œ ì œê³µí•˜ëŠ” Quantization Approach MX4, MX6, MX9ì— ëŒ€í•œ í‘œì´ë‹¤.\n\n\n\nReference. MIT-TinyML-lecture5-Quantization-1\n\n\n\n\n2.1.2 Weight Equalization\nì—¬ê¸°ê¹Œì§€ Weight Quatizationì—ì„œ ê·¸ë£¹ìœ¼ë¡œ ì–¼ë§ˆë§Œí¼ ë¬¶ëŠ”ì§€ì— ë”°ë¼(ê°•ì˜ì—ì„œëŠ” Granularity) Quatizationì„ í•˜ëŠ” ì—¬ëŸ¬ ë°©ë²•ì„ ì†Œê°œí–ˆë‹¤. ë‹¤ìŒìœ¼ë¡œ ì†Œê°œ í•  ë°©ë²•ì€ Weight Equalizationì´ë‹¤. 2022ë…„ì— ì†Œê°œí•´ì¤€ ë‚´ìš©ì¸ë°, ì´ëŠ” ië²ˆì§¸ layerì˜ output channelë¥¼ scaling down í•˜ë©´ì„œ i+1ë²ˆì§¸ layerì˜ input channelì„ scaling up í•´ì„œ Scaleë¡œ ì¸í•´ Quantization ì „í›„ë¡œ ìƒê¸°ëŠ” Layerê°„ ì°¨ì´ë¥¼ ì¤„ì´ëŠ” ë°©ë²•ì´ë‹¤.\n\n\n\nReference. Data-Free Quantization Through Weight Equalization and Bias Correction [Markus et al., ICCV 2019]\n\n\nì˜ˆë¥¼ ë“¤ì–´ ìœ„ì— ê·¸ë¦¼ì²˜ëŸ¼ Layer iì˜ output channelê³¼ Layer i+1ì˜ input channelì´ ìˆë‹¤. ì—¬ê¸°ì„œ ì‹ì„ ì „ê°œí•˜ë©´ ì•„ë˜ì™€ ê°™ì€ë°,\n\\[\n\\begin{aligned}\ny^{(i+1)}&=f(W^{(i+1)}x^{(i+1)}+b^{(i+1)}) \\\\\n         &=f(W^{(i+1)} \\cdot f(W^{(i)}x^{(i)}+b^{(i)}) +b^{(i+1)}) \\\\\n         &=f(W^{(i+1)}S \\cdot f(S^{-1}(W^{(i)}x^{(i)}+S^{-1}b^{(i)})) +b^{(i+1)})\n\\end{aligned}\n\\]\nwhere \\(S = diag(s)\\) , \\(s_j\\) is the weight equalization scale factor of output channel \\(j\\)\nì—¬ê¸°ì„œ Scale(S)ê°€ i+1ë²ˆì§¸ layerì˜ weightì—, ië²ˆì§¸ weightì— 1/S ë¡œ Scaleë  ë–„ ê¸°ì¡´ì— Scale í•˜ì§€ ì•Šì€ ì‹ê³¼ ìœ ì‚¬í•˜ê²Œ ìœ ì§€í•  ìˆëŠ” ê²ƒì„ ë³¼ ìˆ˜ ìˆë‹¤. ì¦‰,\n\\[\nr^{(i)}_{oc_j} / s = r^{(i+1)}_{ic_j} \\cdot s\n\\]\n\\[\ns_j = \\dfrac{1}{r^{(i+1)}_{ic=j}}\\sqrt{r_{oc=j}^{(i)}\\cdot r_{ic=j}^{(i+1)}}\n\\]\n\\[\nr^{(i)}_{oc_j} = r^{(i)}_{oc_j} / s = \\sqrt{r_{oc=j}^{(i)}\\cdot r_{ic=j}^{(i+1)}}\n\\]\n\\[\nr^{(i)}_{ic_j} =r^{(i)}_{ic_j} \\cdot s = \\sqrt{r_{oc=j}^{(i)}\\cdot r_{ic=j}^{(i+1)}}\n\\]\nì´ë ‡ê²Œ í•˜ë©´ ië²ˆì§¸ layerì˜ output channelê³¼ i+1ë²ˆì§¸ layerì˜ input channelì˜ Scaleì„ ê°ê° \\(S\\) ì™€ \\(1/S\\) ë¡œí•˜ë©° weightê°„ì˜ ê²©ì°¨ë¥¼ ì¤„ì¼ ìˆ˜ ìˆë‹¤.\n\n\n2.1.3 Adaptive rounding\n\n\n\nReference. MIT-TinyML-lecture5-Quantization-1\n\n\në§ˆì§€ë§‰ ì†Œê°œí•˜ëŠ” ë°©ë²•ì€ Adaptive rounding ì´ë‹¤. ë°˜ì˜¬ë¦¼ì€ Round-to-nearestìœ¼ë¡œ ë¶ˆë¦¬ëŠ” ì¼ë°˜ì ì¸ ë°˜ì˜¬ë¦¼ì„ ìƒê°í•  ìˆ˜ ìˆê³ , í•˜ë‚˜ì˜ ê¸°ì¤€ì„ ê°€ì§€ê³  ë°˜ì˜¬ë¦¼ì„ í•˜ëŠ” Adaptive Roundë¥¼ ìƒê°í•  í•  ìˆ˜ ìˆë‹¤. ê°•ì˜ì—ì„œëŠ” Round-to-nearestê°€ ìµœì ì˜ ë°©ë²•ì´ ë˜ì§€ ì•ŠëŠ”ë‹¤ê³  ë§í•˜ë©°, Adaptive roundë¡œ weightì— 0ë¶€í„° 1 ì‚¬ì´ì˜ ê°’ì„ ë”í•´ ìˆ˜ì‹ì²˜ëŸ¼ \\(\\tilde{w} = \\lfloor\\lfloor  w\\rfloor + \\delta\\rceil, \\delta \\in [0, 1]\\) ìµœì ì˜ Optimalí•œ ë°˜ì˜¬ë¦¼ ê°’ì„ êµ¬í•œë‹¤. $$\n\\[\\begin{aligned}\n&argmin_V\\lvert\\lvert Wx-\\tilde Wx\\lvert\\lvert ^2_F + \\lambda f_{reg}(V) \\\\\n\n\\rightarrow & argmin_V\\lvert\\lvert Wx-\\lfloor\\lfloor W \\rfloor + h(V) \\rceil x\\lvert\\lvert ^2_F + \\lambda f_{reg}(V)\n\\end{aligned}\\]\n$$ ### 2.2 Activation quantization ë‘ ë²ˆì§¸ë¡œ Activation quantizationì´ ìˆë‹¤. ëª¨ë¸ê²°ê³¼ë¡œ ë‚˜ì˜¤ëŠ” ê²°ê³¼ë¥¼ ì§ì ‘ì ìœ¼ë¡œ ê²°ì •í•˜ëŠ” Activation Quatizationì—ì„œëŠ” ë‘ ê°€ì§€ë¥¼ ê³ ë ¤í•œ ë°©ë²•ì„ ì†Œê°œí•œë‹¤. í•˜ë‚˜ëŠ” Activation ë ˆì´ì–´ì—ì„œ ê²°ê³¼ê°’ì„ Smoothingí•œ ë¶„í¬ë¥¼ ê°€ì§€ê²Œ í•˜ê¸° ìœ„í•´ Exponential Moving Average(EMA)ë¥¼ ì‚¬ìš©í•˜ëŠ” ë°©ë²•ì´ê³ , ë‹¤ë¥¸ í•˜ë‚˜ëŠ” ë‹¤ì–‘í•œ ì…ë ¥ê°’ì„ ê³ ë ¤í•´ batch samplesì„ FP32 ëª¨ë¸ê³¼ calibrationí•˜ëŠ” ë°©ë²•ì´ë‹¤.\nExponential Moving Average (EMA)ì€ ì•„ë˜ ì‹ì—ì„œ \\(\\alpha\\) ë¥¼ êµ¬í•˜ëŠ” ë°©ë²•ì´ë‹¤. \\[\n\\hat r^{(t)}_{max, min} = \\alpha r^{(t)}_{max, min} + (1-\\alpha) \\hat r^{(t)}_{max, min}  \n\\] Calibrationì˜ ì»¨ì…‰ì€ ë§ì€ inputì˜ min/max í‰ê· ì„ ì´ìš©í•˜ìëŠ” ê²ƒì´ë‹¤. ê·¸ë˜ì„œ trained FP32 modelê³¼ sample batchë¥¼ ê°€ì§€ê³  quantizedí•œ ëª¨ë¸ì˜ ê²°ê³¼ì™€ calibrationì„ ëŒë¦¬ë©´ì„œ ê·¸ ì°¨ì´ë¥¼ ìµœì†Œí™” ì‹œí‚¤ëŠ”ë°, ì—¬ê¸°ì— ì´ìš©í•˜ëŠ” ì§€í‘œëŠ” loss of informationì™€ Newton-Raphson methodë¥¼ ì‚¬ìš©í•œ Mean Square Error(MSE)ê°€ ìˆë‹¤. \\[\nMSE = \\underset{\\lvert r \\lvert_{max}}{min}\\ \\mathbb{E}[(X-Q(X))^2]\n\\] \\[\nKL\\ divergence=D_{KL}(P\\lvert\\lvert Q) = \\sum_i^N P(x_i)log\\dfrac{P(x_i)}{Q(x_i)}\n\\] ### 2.3 Quanization Bias Correction\në§ˆì§€ë§‰ìœ¼ë¡œ Quatizationìœ¼ë¡œ biased errorë¥¼ ì¡ëŠ”ë‹¤ëŠ” ê²ƒì„ ì†Œê°œí•œë‹¤. \\(\\epsilon = Q(W)-W\\) ì´ë¼ê³  ë‘ê³  ì•„ë˜ì²˜ëŸ¼ ì‹ì´ ì „ê°œì‹œí‚¤ë©´ ë§ˆì§€ë§‰ í•­ì—ì„œ ë³´ì´ëŠ” \\(-\\epsilon\\mathbb{E}[x]\\) ë¶€ë¶„ì´ biasë¥¼ quatizationì„ í•  ë•Œ ì œê±° ëœë‹¤ê³  í•œë‹¤(ì´ ë¶€ë¶„ì€ 2023ë…„ì—ëŠ” ì†Œê°œí•˜ì§„ ì•ŠëŠ”ë°, ë‹¹ì—°í•œ ê²ƒì´ì–´ì„œ ì•ˆí•˜ëŠ”ì§€, í˜¹ì€ ì˜í–¥ì´ í¬ì§€ ì•Šì•„ì„œ ê·¸ëŸ°ì§€ëŠ” ëª¨ë¥´ê² ë‹¤. Bias Quatizationì´í›„ì— MobileNetV2ì—ì„œ í•œ ë ˆì´ì–´ì˜ outputì„ ë³´ë©´ ì–´ëŠì •ë„ ì œê±°ë˜ëŠ” ê²ƒì²˜ëŸ¼ ë³´ì¸ë‹¤). \\[\n\\begin{aligned}\n\\mathbb{E}[y] &= \\mathbb{E}[Wx] + \\mathbb{E}[\\epsilon x] - \\mathbb{E}[\\epsilon x],\\ \\mathbb{E}[Q(W)x] = \\mathbb{E}[Wx] + \\mathbb{E}[\\epsilon x] \\\\\n\\mathbb{E}[y] &= \\mathbb{E}[Q(W)x] - \\epsilon\\mathbb{E}[x]\n\\end{aligned}\n\\]\n\n\n\nReference. MIT-TinyML-lecture5-Quantization-2\n\n\n\n\n\n2.4 Post-Training INT8 Linear Quantization Result\nì•ì„  Post-Training Quantizationì„ ì ìš©í•œ ê²°ê³¼ë¥¼ ë³´ì—¬ì¤€ë‹¤. ì´ë¯¸ì§€ê³„ì—´ ëª¨ë¸ì„ ëª¨ë‘ ì‚¬ìš©í–ˆìœ¼ë©°, ì„±ëŠ¥í•˜ë½í­ì€ ì§€í‘œë¡œ ë³´ì—¬ì¤€ë‹¤. ë¹„êµì  í° ëª¨ë¸ë“¤ì˜ ê²½ìš° ì¤€ìˆ˜í•œ ì„±ëŠ¥ì„ ë³´ì—¬ì£¼ì§€ë§Œ MobileNetV1, V2ì™€ ê°™ì€ ì‘ì€ ëª¨ë¸ì€ ìƒê°ë³´ë‹¤ Quantizationìœ¼ë¡œ ë–¨ì–´ì§€ëŠ” ì„±ëŠ¥í­(-11.8%, -2.1%) ì´ í° ê²ƒì„ ë³¼ ìˆ˜ ìˆë‹¤. ê·¸ëŸ¼ ì‘ì€ í¬ê¸°ì˜ ëª¨ë¸ë“¤ì€ ì–´ë–»ê²Œ Training í•´ì•¼í• ê¹Œ?\n\n\n\nReference. MIT-TinyML-lecture5-Quantization-2"
  },
  {
    "objectID": "posts/lecs/lec05.html#quantization-aware-trainingqat",
    "href": "posts/lecs/lec05.html#quantization-aware-trainingqat",
    "title": "ğŸ§‘â€ğŸ« Lecture 5-6",
    "section": "3. Quantization-Aware Training(QAT)",
    "text": "3. Quantization-Aware Training(QAT)\n\n3.1 Quantization-Aware Training\n\n\n\nReference. MIT-TinyML-lecture06-Quantization-2\n\n\n\nUsually, fine-tuning a pre-trained floating point model provides better accuracy than training from scratch.\n\nì´ì „ì— K-mean Quantizationì—ì„œ Fine-tuningë•Œ Centroidì— gradientë¥¼ ë°˜ì˜í–ˆì—ˆë‹¤. Quantization-Aware Trainingì€ ì´ì™€ ìœ ì‚¬í•˜ê²Œ Quantization - Reconstructionì„ í†µí•´ ë§Œë“¤ì–´ì§„ Weightë¡œ Trainingì„ í•˜ëŠ” ë°©ë²•ì„ ë§í•œë‹¤. ì˜ˆì‹œë¥¼ ë“¤ì–´ì„œ ìì„¸íˆ ì‚´í´ë³´ì.\n\n\n\nReference. MIT-TinyML-lecture06-Quantization-2\n\n\n\nA full precision copy of the weights W is maintained throughout the training.\nThe small gradients are accumulated without loss of precision\nOnce the model is trained, only the quantized weights are used for inference\n\nìœ„ ê·¸ë¦¼ì—ì„œ Layer Nì´ ë³´ì¸ë‹¤. ì´ Layer Nì€ weightsë¥¼ íŒŒë¼ë¯¸í„°ë¡œ ê°€ì§€ì§€ë§Œ, ì‹¤ì œë¡œ Training ê³¼ì •ì—ì„œ ì“°ì´ëŠ” weightëŠ” â€œweight quantizationâ€ì„ í†µí•´ Quantization - Reconstructionì„ í†µí•´ ë§Œë“¤ì–´ì§„ Weightë¥¼ ê°€ì§€ê³  í›ˆë ¨ì„ í•  ê²ƒì´ë‹¤.\n\n\n3.2 Straight-Through Estimator(STE)\n\n\n\nReference. MIT-TinyML-lecture06-Quantization-2\n\n\nê·¸ëŸ¼ í›ˆë ¨ì—ì„œ gradientëŠ” ì–´ë–»ê²Œ ì „ë‹¬í•  ìˆ˜ ìˆì„ê¹Œ? Quantizationì˜ ê°œë…ìƒ, weight quantizationì—ì„œ weightë¡œ ë„˜ì–´ê°€ëŠ” gradientëŠ” ì—†ì„ ìˆ˜ ë°–ì— ì—†ë‹¤. ê·¸ë ‡ê²Œ ë˜ë©´ ì‚¬ì‹¤ìƒ weightë¡œ back propagationì´ ë  ìˆ˜ ì—†ê²Œ ë˜ê³ , ê·¸ë˜ì„œ ì†Œê°œí•˜ëŠ” ê°œë…ì´ Straight-Through Estimator(STE) ì…ë‹ˆë‹¤. ë§ì´ ê±°ì°½í•´ì„œ ê·¸ë ‡ì§€, Q(W)ì—ì„œ ë°›ì€ gradientë¥¼ ê·¸ëŒ€ë¡œ weights ë¡œ ë„˜ê²¨ì£¼ëŠ” ë°©ì‹ì´ë‹¤.\n\nQuantization is discrete-valued, and thus the derivative is 0 almost everywhere â†’ NN will learn nothing!\nStraight-Through Estimator(STE) simply passes the gradients through the quantization as if it had been the identity function.\n\\[\n  g_W = \\dfrac{\\partial L}{\\partial  W} = \\dfrac{\\partial L}{\\partial  Q(W)}\n  \\]\n\n\n\n\nReference. MIT-TinyML-lecture06-Quantization-2\n\n\n\nReference\n\nNeural Networks for Machine Learning [HintonÂ et al., Coursera Video Lecture, 2012]\nEstimating or Propagating Gradients Through Stochastic Neurons for Conditional Computation [Bengio, arXiv 2013]\n\n\nì´ í›ˆë ¨ì˜ ê²°ê³¼ê°€ ê¶ê¸ˆí•˜ì‹œë‹¤ë©´ ì´ ë…¼ë¬¸ì„ ì°¸ê³ í•˜ì. ì°¸ê³ ë¡œ ë…¼ë¬¸ì—ì„œëŠ” MobileNetV1, V2 ê·¸ë¦¬ê³  NASNet-Mobileì„ ì´ìš©í•´ Post-Training Quantizationê³¼ Quantization-Aware Trainingì„ ë¹„êµí•˜ê³  ìˆë‹¤."
  },
  {
    "objectID": "posts/lecs/lec05.html#binary-and-ternary-quantization",
    "href": "posts/lecs/lec05.html#binary-and-ternary-quantization",
    "title": "ğŸ§‘â€ğŸ« Lecture 5-6",
    "section": "4. Binary and Ternary Quantization",
    "text": "4. Binary and Ternary Quantization\nì, ê·¸ëŸ¼ Quantizationì„ ê¶ê·¹ì ìœ¼ë¡œ 2bitë¡œ í•  ìˆ˜ëŠ” ì—†ì„ê¹Œ? ë°”ë¡œ Binary(1, -1)ê³¼ Tenary(1, 0, -1) ì´ë‹¤.\n\nCan we push the quantization precision to 1 bit?\n\n\n\nReference. MIT-TinyML-lecture06-Quantization-2\n\n\nReference\n\nBinaryConnect: Training Deep Neural Networks with Binary Weights during Propagations [CourbariauxÂ et al., NeurIPS 2015]\nXNOR-Net: ImageNet Classification using Binary Convolutional Neural Networks [RastegariÂ et al., ECCV 2016]\n\n\në¨¼ì € Weightë¥¼ 2bitë¡œ Quantizationì„ í•˜ê²Œ ë˜ë©´, ë©”ëª¨ë¦¬ì—ì„œëŠ” 32bitë¥¼ 1bitë¡œ ì¤„ì´ë‹ˆ 32ë°°ë‚˜ ì¤„ì¼ ìˆ˜ ìˆê³ , Computationë„ (8x5)+(-3x2)+(5x0)+(-1x1)ì—ì„œ 5-2+0-1 ë¡œ ì ˆë°˜ì„ ì¤„ì¼ ìˆ˜ ìˆë‹¤.\n\n4.1 Binarization: Deterministic Binarization\nê·¸ëŸ¼ Binarizationì—ì„œ +1ê³¼ -1ì„ ì–´ë–¤ ê¸°ì¤€ìœ¼ë¡œ í•´ì•¼í• ê¹Œ? ê°€ì¥ ì‰¬ìš´ ë°©ë²•ì€ threholdë¥¼ ê¸°ì¤€ìœ¼ë¡œ +-1ë¡œ ë‚˜ëˆ„ëŠ” ê²ƒì´ë‹¤.\nDirectly computes the bit value base on a threshold, usually 0 resulting in a sign function.\n\\[\nq = sign(r) = \\begin{dcases}\n+1, &r \\geq 0 \\\\\n-1, &r &lt; 0\n\\end{dcases}\n\\]\n\n\n4.2 Binarization: Stochastic Binarization\në‹¤ë¥¸ ë°©ë²•ìœ¼ë¡œëŠ” outputì—ì„œ hard-sigmoid functionì„ ê±°ì³ì„œ ë‚˜ì˜¨ ê°’ë§Œí¼ í™•ë¥ ì ìœ¼ë¡œ +-1ì´ ë‚˜ì˜¤ë„ë¡ í•˜ëŠ” ê²ƒì´ë‹¤. í•˜ì§€ë§Œ ì´ ë°©ë²•ì€ ë¬´ì‘ìœ„ë¡œ ë¹„íŠ¸ë¥¼ ìƒì„±í•˜ëŠ” í•˜ë“œì›¨ì–´ë¥¼ í•˜ëŠ” ê²ƒì´ ì–´ë µê¸° ë•Œë¬¸ì— ì‚¬ìš©í•˜ì§„ ì•ŠëŠ”ë‹¤ê³  ì–¸ê¸‰í•œë‹¤.\n\nUse global statistics or the value of input data to determine the probability of being -1 or +1\nIn Binary Connect(BC), probability is determined by hard sigmoid function \\(\\sigma(r)\\)\n\\[\n  q=\\begin{dcases}\n  +1, &\\text{with probability } p=\\sigma(r)\\\\\n  -1, & 1-p\n  \\end{dcases}\n  \\\\\n  where\\ \\sigma(r)=min(max(\\dfrac{r+1}{2}, 0), 1)\n  \\]\n\n\n\nReference. MIT-TinyML-lecture06-Quantization-2\n\n\nHarder to implement as it requires the hardware to generate random bits when quantizing.\n\n\n\n4.3 Binarization: Use Scale\nì•ì„  ë°©ë²•ì„ ì´ìš©í•´ì„œ ImageNet Top-1 ì„ í‰ê°€í•´ë³´ë©´ Quantizationì´í›„ -21.2%ë‚˜ ì„±ëŠ¥ì´ í•˜ë½í•˜ëŠ” ê±¸ ë³¼ ìˆ˜ ìˆë‹¤. â€œì–´ë–»ê²Œ ë³´ì™„í•  ìˆ˜ ìˆì„ê¹Œ?â€ í•œ ê²ƒì´ linear qunatizationì—ì„œ ì‚¬ìš©í–ˆë˜ Scale ê°œë…ì´ë‹¤.\n\nUsing Scale, Minimizing Quantization Error in Binarization\n\n\n\nReference. MIT-TinyML-lecture06-Quantization-2\n\n\n\nì—¬ê¸°ì„œ Scaleì€ \\(\\dfrac{1}{n}\\lvert\\lvert W \\lvert\\lvert_1\\) ë¡œ ê³„ì‚°í•  ìˆ˜ ìˆê³ , ì„±ëŠ¥ì€ í•˜ë½ì´ ê±°ì˜ ì—†ëŠ” ê²ƒë„ í™•ì¸í•  ìˆ˜ ìˆë‹¤. ì™œ \\(\\dfrac{1}{n}\\lvert\\lvert W \\lvert\\lvert_1\\)ì¸ì§€ëŠ” ì•„ë˜ ì¦ëª…ê³¼ì •ì„ ì°¸ê³ í•˜ì!\n\nWhy \\(\\alpha\\) is \\(\\dfrac{1}{n}\\lvert\\lvert W \\lvert\\lvert_1\\)?\n\\[\n  \\begin{aligned}\n  &J(B, \\alpha)=\\lvert\\lvert W-\\alpha B\\lvert\\lvert^2 \\\\\n  &\\alpha^*, B^*= \\underset{\\alpha, B}{argmin}\\ J(B, \\alpha) \\\\\n  &J(B,\\alpha) = \\alpha^2B^TB-2\\alpha W^T B + W^TW\\ since\\ B \\in \\{+1, -1\\}^n \\\\\n  &B^TB=n(constant), W^TW= constant(a \\ known\\ variable) \\\\\n  &J(B,\\alpha) = \\alpha^2n-2\\alpha W^T B + C \\\\\n  &B^* = \\underset{B}{argmax} \\{W^T B\\}\\ s.t.\\ B\\in \\{+1,-1 \\}^n \\\\\n  &\\alpha^*=\\dfrac{W^TB^*}{n} \\\\\n  &\\alpha^*=\\dfrac{W^Tsign(W)}{n} = \\dfrac{\\lvert W_i \\lvert}{n} = \\dfrac{1}{n}\\lvert\\lvert W\\lvert\\lvert_{l1}\n  \\end{aligned}\n  \\]\n\nReference. XNOR-Net: ImageNet Classification using Binary Convolutional Neural Networks [RastegariÂ et al., ECCV 2016]\nB*ëŠ” J(B,\\(\\alpha\\))ì—ì„œ ìµœì†Ÿê°’ì„ êµ¬í•´ì•¼í•˜ë¯€ë¡œ \\(W^T\\)B ê°€ ìµœëŒ€ì—¬ì•¼í•˜ê³  ê·¸ëŸ¬ê¸° ìœ„í•´ì„œëŠ” Wê°€ ì–‘ìˆ˜ì¼ë•ŒëŠ” Bë„ ì–‘ìˆ˜, Wê°€ ìŒìˆ˜ì¼ ë•ŒëŠ” Bë„ ìŒìˆ˜ì—¬ì•¼ \\(W^TB=\\sum\\lvert W \\lvert\\) ì´ ë˜ë©´ì„œ ìµœëŒ“ê°’ì´ ë  ìˆ˜ ìˆë‹¤.\n\n\n\n\n4.4 Binarization: Activation\nê·¸ëŸ¼ Activationê¹Œì§€ Quantizationì„ í•´ë´…ì‹œë‹¤.\n4.4.1 Activation\n\n\n\nUntitled\n\n\nì—¬ê¸°ì„œ ì¡°ê¸ˆ ë” ì—°ì‚°ì„ ìµœì í™” í•  ìˆ˜ ìˆì–´ë³´ì´ëŠ” ê²ƒì´ Matrix Muliplicationì´ XOR ì—°ì‚°ê³¼ ë¹„ìŠ·í•˜ê²Œ ë³´ì¸ë‹¤.\n4.4.2 XNOR bit count\n\n\n\nReference. MIT-TinyML-lecture06-Quantization-2\n\n\n\n\\(y_i=-n+ popcount(W_i\\ xnor\\ x) &lt;&lt; 1\\) â†’ popcount returns the number of 1\n\nê·¸ë˜ì„œ popcountê³¼ XNORì„ ì´ìš©í•´ì„œ Computationì—ì„œ ì¢€ ë” ìµœì í™”ë¥¼ ì§„í–‰í•©ë‹ˆë‹¤. ì´ë ‡ê²Œ ìµœì í™”ë¥¼ ì§„í–‰í•˜ê²Œ ë˜ë©´, ë©”ëª¨ë¦¬ëŠ” 32ë°°, Computationì€ 58ë°°ê°€ëŸ‰ ì¤„ì–´ë“¤ ìˆ˜ ìˆë‹¤ê³  ë§í•œë‹¤.\n\n\n\nReference. MIT-TinyML-lecture06-Quantization-2\n\n\nì´ë ‡ê²Œ Weight, Scale factor, Activation, ê·¸ë¦¬ê³  XNOR-Bitcout ê¹Œì§€. ì´ ë„¤ ê°€ì§€ ë‹¨ê³„ë¡œ Binary Quantizationì„ ë‚˜ëˆˆë‹¤. ë‹¤ìŒìœ¼ë¡œëŠ” Ternary Quantizationì€ ì•Œì•„ë³´ì.\n\n\n\nReference. XNOR-Net: ImageNet Classification using Binary Convolutional Neural Networks [RastegariÂ et al., ECCV 2016]\n\n\n\n\nBinarizing Input ì˜ ê²½ìš°ëŠ” averageë¥¼ ëª¨ë“  channelì— ê°™ì´ ì ìš©í•  ê²ƒì´ê¸° ë•Œë¬¸ì— ê·¸ cë§Œí¼ì„ average filterë¡œ í•œ ë²ˆì— ì ìš©í•œë‹¤ëŠ” ë§ì´ë‹¤.\n\n\n\n\n4.5 Ternary Weight Networks(TWN)\nTernaryëŠ” Binary Quantizationê³¼ ë‹¨ê³„ëŠ” ëª¨ë‘ ê°™ì§€ë§Œ, ê°€ì§ˆ ìˆ˜ ìˆëŠ” ê°’ìœ¼ë¡œ 0 ì„ ì¶”ê°€í•œë‹¤. ì•„ë˜ ê·¸ë¦¼ì€ Scaleì„ ì´ìš©í•´ì„œ Quantization Errorë¥¼ ì¤„ì´ëŠ” ë°©ë²•ì„ ë§í•˜ê³  ìˆë‹¤. \\[\nq = \\begin{dcases}\nr_t, &r &gt; \\Delta \\\\\n0, &\\lvert r\\lvert \\leq \\Delta \\\\\n-r_t, &r &lt; -\\Delta\n\\end{dcases} \\\\\nwhere\\ \\Delta = 0.7\\times \\mathbb{E}(\\lvert r \\lvert), r_t = \\mathbb{E}_{\\lvert r \\lvert &gt; \\Delta}(\\lvert r \\lvert )\n\\]  ### 4.6 Trained Ternary Quantization(TTQ)\nTenary Quantizationì—ì„œ ë˜ í•œê°€ì§€ ë‹¤ë¥´ê²Œ ì„¤ëª…í•˜ëŠ” ê²ƒì€ 1ê³¼ -1ë¡œë§Œ ì •í•´ì ¸ ìˆë˜ Binary Quantizationê³¼ ë‹¤ë¥´ê²Œ TenaryëŠ” 1, 0, -1ë¡œ Quantizationì„ í•œ í›„, ì¶”ê°€ì ì¸ í›ˆë ¨ì„ í†µí•´ \\(w_t\\)ì™€ \\(-w_t\\)ë¡œ fine-tuningì„ í•˜ëŠ” ë°©ë²•ë„ ì œì•ˆí•œë‹¤(í•´ë‹¹ ë…¼ë¬¸ì—ì„œëŠ” ì´ëŸ¬í•œ ê¸°ë²•ì„ ì´ìš©í•´ì„œ í•œ ê²°ê³¼ë¥¼ CIFAR-10 ì´ë¯¸ì§€ ë°ì´í„°ë¥¼ ê°€ì§€ê³  ResNets, AlexNet, ImageNetì—ì„œ ë³´ì—¬ì¤€ë‹¤). \\[\nq = \\begin{dcases}\nw_t, &r &gt; \\Delta \\\\\n0, &\\lvert r\\lvert \\leq \\Delta \\\\\n-w_t, &r &lt; -\\Delta\n\\end{dcases}\n\\] \n\n\n4.7 Accuracy Degradation\nBinary, Ternary Quantizationì„ ì‚¬ìš©í•œ ê²°ê³¼ë¥¼ ë³´ì—¬ì¤€ë‹¤(Resnet-18 ê²½ìš°ì—ëŠ” Ternary ê°€ ì˜¤íˆë ¤ Binaryë³´ë‹¤ ì„±ëŠ¥ì´ ë” ë–¨ì–´ì§„ë‹¤!)\n\nBinarization\n\n\n\nReference. Binarized Neural Networks: Training Deep Neural Networks with Weights and Activations Constrained to +1 orÂ âˆ’1. [CourbariauxÂ et al., Arxiv 2016], XNOR-Net: ImageNet Classification using Binary Convolutional Neural Networks [RastegariÂ et al., ECCV 2016]\n\n\nTernary Weight Networks (TWN)\n\n\n\nReference. Ternary Weight Networks [LiÂ et al., Arxiv 2016]\n\n\nTrained Ternary Quantization (TTQ)\n\n\n\nReference. Trained Ternary Quantization [ZhuÂ et al., ICLR 2017]"
  },
  {
    "objectID": "posts/lecs/lec05.html#low-bit-width-quantization",
    "href": "posts/lecs/lec05.html#low-bit-width-quantization",
    "title": "ğŸ§‘â€ğŸ« Lecture 5-6",
    "section": "5. Low Bit-Width Quantization",
    "text": "5. Low Bit-Width Quantization\në‚¨ì€ ë¶€ë¶„ë“¤ì€ ì—¬ëŸ¬ê°€ì§€ ì‹¤í—˜ / ì—°êµ¬ë“¤ì„ ì†Œê°œí•˜ê³  ìˆë‹¤.\n\nBinary Quantizationì€ Quantization Aware Trainingì„ í•  ìˆ˜ ìˆì„ê¹Œ?\n2,3 bitê³¼ 8bit ê·¸ ì¤‘ê°„ìœ¼ë¡œëŠ” Quantizationì„ í•  ìˆ˜ ì—†ì„ê¹Œ?\në ˆì´ì–´ì—ì„œ Quantizationì„ í•˜ì§€ ì•ŠëŠ” ë ˆì´ì–´, ì˜ˆë¥¼ ë“¤ì–´ ê²°ê³¼ì— ì˜í–¥ì„ ì˜ˆë¯¼í•˜ê²Œ ë¯¸ì¹˜ëŠ” ì²« ë²ˆì§¸ ë ˆì´ì–´ê°€ ê°™ì€ ê²½ìš° Quantizationì„ í•˜ì§€ ì•Šìœ¼ë©´ ì–´ë–»ê²Œ ë ê¹Œ?\nActivation í•¨ìˆ˜ë¥¼ ë°”ê¾¸ë©´ ì–´ë–¨ê¹Œ?\nì˜ˆë¥¼ ë“¤ì–´ ì²«ë²ˆì§¸ ë ˆì´ì–´ì˜ Në°° ë„“ê²Œ í•˜ëŠ” ê²ƒê³¼ ê°™ì´ ëª¨ë¸ êµ¬ì¡°ë¥¼ ë°”ê¾¸ë©´ ì–´ë–»ê²Œ ë ê¹Œ?\nì¡°ê¸ˆì”© Quantizationì„ í•  ìˆ˜ ì—†ì„ê¹Œ? (20% â†’ 40% â†’ â€¦ â†’ 100%)\n\nê°•ì˜ì—ì„œëŠ” í¬ê²Œ ì–¸ê¸‰í•˜ì§€ ì•Šê³  ê°„ ë‚´ìš©ë“¤ì´ë¼ ì„¤ëª…ì„ í•˜ì§€ëŠ” ì•Šê² ë‹¤. í•´ë‹¹ ë‚´ìš©ë“¤ì€ ìì„¸í•œ ë‚´ìš©ì„ ì•Œê³ ì‹¶ìœ¼ë©´ ê° íŒŒíŠ¸ì— ì–¸ê¸‰ëœ ë…¼ë¬¸ì„ ì°¸ì¡°í•˜ê¸¸!\n\n5.1 Train Binarized Neural Networks From Scratch\n\nStraight-Through Estimator(STE)\n\n\n\n\nReference. MIT-TinyML-lecture06-Quantization-2\n\n\n\nGradient pass straight to floating-point weights\nFloating-point weight with in [-1, 1]\nReference. Binarized Neural Networks: Training Deep Neural Networks with Weights and Activations Constrained to +1 orÂ âˆ’1. [Courbariaux et al., Arxiv 2016]\n\n\n\n5.2 Quantization-Aware Training: DoReFa-Net With Low Bit-Width Gradients\n\n\n\nReference. MIT-TinyML-lecture06-Quantization-2\n\n\n\nGradient Quantization\n\\[\n  Q(g) = 2 \\cdot max(\\lvert G \\lvert) \\cdot \\Large[ \\small quantize_k \\Large( \\small \\dfrac{g}{2\\cdot max(\\lvert G \\lvert)} + \\dfrac{1}{2} + N(k) \\Large ) \\small -\\dfrac{1}{2} \\Large]\\small\n  \\] \\[\n  where\\ N(k)=\\dfrac{\\sigma}{2^k-1} and\\ \\sigma \\thicksim Uniform(-0.5, 0.5)\n  \\]\n\nNoise function \\(N(k)\\) is added to compensate the potential bias introduced by gradient quantization.\n\nResult\n\n\n\nReference. DoReFa-Net: Training Low Bitwidth Convolutional Neural Networks with Low Bitwidth Gradients [ZhouÂ et al., arXiv 2016]\n\n\n\n\n\n5.3 Replace the Activation Function: Parameterized Clipping Activation Function\n\nThe most common activation function ReLU is unbounded. The dynamic range of inputs becomes problematic for low bit-width quantization due to very limited range and resolution.\nReLU is replaced with hard-coded bounded activation functions: ReLU6, ReLU1, etc\nThe clipping value per layer can be learned as well: PACT(Parametrized Clipping Activation Function)\n \\[\n  y=PACT(x;\\alpha) = 0.5(\\lvert x \\lvert - \\lvert x -\\alpha \\lvert + \\alpha ) = \\begin{dcases}\n  0, & x \\in [-\\infty, 0) \\\\\n  x, & x \\in [0, \\alpha) \\\\\n  \\alpha, & x \\in [\\alpha, +\\infty)\n  \\end{dcases}\n  \\]\nThe upper clipping value of the activation function is a trainable. With STE, the gradient is computed as\n\\[\n  \\dfrac{\\partial Q(y)}{\\partial \\alpha} = \\dfrac{\\partial Q(y)}{\\partial y} \\cdot \\dfrac{\\partial y}{\\partial \\alpha} = \\begin{dcases}\n  0 & x \\in (-\\infty, \\alpha)\\\\\n  1 & x \\in [\\alpha, +\\infty)\\\\\n  \\end{dcases}\n  \\]\n\\[\n  \\rightarrow\n  \\dfrac{\\partial L}{\\partial \\alpha} = \\dfrac{\\partial L}{\\partial Q(y)} \\cdot \\dfrac{\\partial Q(y)}{\\partial \\alpha} = \\begin{dcases}\n  0 & x \\in (-\\infty, \\alpha)\\\\\n  \\frac{\\partial L}{\\partial Q(y)} & x \\in [\\alpha, +\\infty)\\\\\n  \\end{dcases}\n  \\]\nThe larger \\(\\alpha\\), the more the parameterized clipping function resembles a ReLU function\n\nTo avoid large quantization errors due to a wide dynamic range \\([0, \\alpha]\\), L2-regularizer for \\(\\alpha\\) is included in the training loss function.\n\nResult\n\n\n\nReference. PACT: Parameterized Clipping Activation for Quantized Neural Networks [ChoiÂ et al., arXiv 2018]\n\n\n\n\n\n5.4 Modify the Neural Network Architecture\n\nWiden the neural network to compensate for the loss of information due to quantization\nex. Double the channels, reduce the quantization precision\n\n\n\nReference. WRPN: Wide Reduced-Precision Networks [MishraÂ et al., ICLR 2018]\n\n\nReplace a single floating-point convolution with multiple binary convolutions.\n\nTowards Accurate Binary Convolutional Neural Network [LinÂ et al., NeurIPS 2017]\nQuantization [Neural Network Distiller]\n\n\n\n\n5.5 No Quantization on First and Last Layer\n\nBecause it is more sensitive to quantization and small portion of the overall computation\nQuantizing these layers to 8-bit integer does not reduce accuracy\n\n\n\n5.6 Iterative Quantization: Incremental Network Quantization\n\nReference. Incremental Network Quantization: Towards Lossless CNNs with Low-precision Weights [ZhouÂ et al., ICLR 2017]\n\n\n\n\nReference. Incremental Network Quantization: Towards Lossless CNNs with Low-precision Weights [ZhouÂ et al., ICLR 2017]\n\n\n\nSetting\n\nWeight quantization only\nQuantize weights to \\(2^n\\) for faster computation (bit shift instead of multiply)\n\nAlgorithm\n\nStart from a pre-trained fp32 model\nFor the remaining fp32 weights\n\nPartition into two disjoint groups(e.g., according to magnitude)\nQuantize the first group (higher magnitude), and re-train the other group to recover accuracy\n\nRepeat until all the weights are quantized (a popular stride is {50%, 75%, 87.5%, 100%})\n\n\n\n\nReference. MIT-TinyML-lecture06-Quantization-2"
  },
  {
    "objectID": "posts/lecs/lec05.html#mixed-precision-quantization",
    "href": "posts/lecs/lec05.html#mixed-precision-quantization",
    "title": "ğŸ§‘â€ğŸ« Lecture 5-6",
    "section": "6. Mixed-precision quantization",
    "text": "6. Mixed-precision quantization\në§ˆì§€ë§‰ìœ¼ë¡œ ë ˆì´ì–´ë§ˆë‹¤ Quantization bitë¥¼ ë‹¤ë¥´ê²Œ ê°€ì ¸ê°€ë©´ ì–´ë–¨ì§€ì— ëŒ€í•´ì„œ ì´ì•¼ê¸°í•œë‹¤. í•˜ì§€ë§Œ ê²½ìš°ì˜ ìˆ˜ê°€ 8bit ë³´ë‹¤ ì‘ê±°ë‚˜ ê°™ê²Œ Quantizationì„ í•  ì‹œ, weightì™€ activationë¡œ ê²½ìš°ì˜ ìˆ˜ë¥¼ ê³ ë ¤ë¥¼ í•œë‹¤ë©´ Nê°œ ë ˆì´ì–´ì— ëŒ€í•´ì„œ \\((8 \\times 8)^N\\)ë¼ëŠ” ì–´ë§ˆì–´ë§ˆí•œ ê²½ìš°ì˜ ìˆ˜ê°€ ë‚˜ì˜¨ë‹¤. ê·¸ë¦¬ê³  ì´ì— ëŒ€í•´ì„œëŠ” ë‹¤ìŒ íŒŒíŠ¸ì— ë‚˜ê°ˆ Neural Architecture Search(NAS) ì—ì„œ ë‹¤ë£° ë“¯ ì‹¶ë‹¤.\n\n6.1 Uniform Quantization\n\n\n\n6.2 Mixed-precision Quantization\n\n\n\n6.3 Huge Design Space and Solution: Design Automation\n\n\nDesign Space: Each of Choices(8x8=64) â†’ \\(64^n\\)\n\n\n\nReference. HAQ: Hardware-Aware Automated Quantization with Mixed Precision [WangÂ et al., CVPR 2019]\n\n\nResult in Mixed-Precision Quantized MobileNetV1\n\n\n\nReference. HAQ: Hardware-Aware Automated Quantization with Mixed Precision [WangÂ et al., CVPR 2019]\n\n\n\nThis paper compares with Model size, Latency and Energy\n\n\nê°€ì¥ ë§ˆì§€ë§‰ì— ì–¸ê¸‰í•˜ëŠ” Edgeì™€ í´ë¼ìš°ë“œì—ì„œëŠ” Convolution ë ˆì´ì–´ì˜ ì¢…ë¥˜ ì¤‘ ë”í•˜ê³  ëœ Quantizationí•˜ëŠ” ë ˆì´ì–´ê°€ ê°ê° depthwiseì™€ pointwiseë¡œ ë‹¤ë¥´ë‹¤ê³  ì´ì•¼ê¸°í•œë‹¤. ì´ ë‚´ìš©ì— ëŒ€í•´ì„œ ë” ìì„¸íˆ ì´í•´í•˜ê¸° ìœ„í•´ì„œëŠ” ì•„ë§ˆë„ NASë¡œ ë„˜ì–´ê°€ë´ì•¼ ì•Œ ìˆ˜ ìˆì§€ ì•Šì„ê¹Œ ì‹¶ë‹¤.\n\nQuantization Policy for Edge and Cloud\n\n\n\nReference. HAQ: Hardware-Aware Automated Quantization with Mixed Precision [WangÂ et al., CVPR 2019]"
  },
  {
    "objectID": "posts/lecs/lec05.html#reference",
    "href": "posts/lecs/lec05.html#reference",
    "title": "ğŸ§‘â€ğŸ« Lecture 5-6",
    "section": "7. Reference",
    "text": "7. Reference\n\nTinyML and Efficient Deep Learning Computing on MIT HAN LAB\nYoutube for TinyML and Efficient Deep Learning Computing on MIT HAN LAB\nDeep Compression [HanÂ et al., ICLR 2016]\nQuantization and Training of Neural Networks for Efficient Integer-Arithmetic-Only Inference [JacobÂ et al., CVPR 2018]\nWith Shared Microexponents, A Little Shifting Goes a Long Way [Bita Rouhani et al.]\nData-Free Quantization Through Weight Equalization and Bias Correction [Markus et al., ICCV 2019]\nEstimating or Propagating Gradients Through Stochastic Neurons for Conditional Computation [Bengio, arXiv 2013]\nBinarized Neural Networks: Training Deep Neural Networks with Weights and Activations Constrained to +1 orÂ âˆ’1. [CourbariauxÂ et al., Arxiv 2016]\nXNOR-Net: ImageNet Classification using Binary Convolutional Neural Networks [RastegariÂ et al., ECCV 2016]\nTernary Weight Networks [LiÂ et al., Arxiv 2016]\nTrained Ternary Quantization [ZhuÂ et al., ICLR 2017]\nDoReFa-Net: Training Low Bitwidth Convolutional Neural Networks with Low Bitwidth Gradients [ZhouÂ et al., arXiv 2016]\nWRPN: Wide Reduced-Precision Networks [MishraÂ et al., ICLR 2018]\nPACT: Parameterized Clipping Activation for Quantized Neural Networks [ChoiÂ et al., arXiv 2018]\nHAQ: Hardware-Aware Automated Quantization with Mixed Precision [WangÂ et al., CVPR 2019]"
  },
  {
    "objectID": "posts/lecs/lec03.html",
    "href": "posts/lecs/lec03.html",
    "title": "ğŸ§‘â€ğŸ« Lecture 3",
    "section": "",
    "text": "ì•ìœ¼ë¡œ ì´ 5ì¥ì— ê±¸ì³ì„œ ë”¥ëŸ¬ë‹ ëª¨ë¸ ê²½ëŸ‰í™” ê¸°ë²•ë“¤ì— ëŒ€í•´ì„œ ì†Œê°œí•˜ë ¤ê³  í•œë‹¤. ê²½ëŸ‰í™” ê¸°ë²•ìœ¼ë¡œëŠ” Pruning, Quantization, Neural Network Architecture Search, Knowledge Distillation, ê·¸ë¦¬ê³  Tiny Engineì—ì„œ ëŒë¦¬ê¸° ìœ„í•œ ë°©ë²•ì„ ì§„í–‰í•  ì˜ˆì •ì¸ë° ë³¸ ë‚´ìš©ì€ MITì—ì„œ Song Han êµìˆ˜ë‹˜ì´ Fall 2022ì— í•œ ê°•ì˜ TinyML and Efficient Deep Learning Computing 6.S965ë¥¼ ë°”íƒ•ìœ¼ë¡œ ì¬ì •ë¦¬í•œ ë‚´ìš©ì´ë‹¤. ê°•ì˜ ìë£Œì™€ ì˜ìƒì€ ì´ ë§í¬ë¥¼ ì°¸ì¡°í•˜ì!\nì²« ë²ˆì§¸ ë‚´ìš©ìœ¼ë¡œ â€œê°€ì§€ì¹˜ê¸°â€ë¼ëŠ” ì˜ë¯¸ë¥¼ ê°€ì§„ Pruningì— ëŒ€í•´ì„œ ì´ì•¼ê¸°, ì‹œì‘!"
  },
  {
    "objectID": "posts/lecs/lec03.html#introduction-to-pruning",
    "href": "posts/lecs/lec03.html#introduction-to-pruning",
    "title": "ğŸ§‘â€ğŸ« Lecture 3",
    "section": "1. Introduction to Pruning",
    "text": "1. Introduction to Pruning\nPruningì´ë€ ì˜ë¯¸ì²˜ëŸ¼ Neural Networkì—ì„œ ë§¤ê°œë³€ìˆ˜(ë…¸ë“œ)ë¥¼ ì œê±°í•˜ëŠ” ë°©ë²•ì…ë‹ˆë‹¤. ì´ëŠ” Dropoutí•˜ê³  ë¹„ìŠ·í•œ ì˜ë¯¸ë¡œ ë³¼ ìˆ˜ ìˆëŠ”ë°, Dropoutì˜ ê²½ìš° ëª¨ë¸ í›ˆë ¨ ë„ì¤‘ ëœë¤ì ìœ¼ë¡œ íŠ¹ì • ë…¸ë“œë¥¼ ì œì™¸ì‹œí‚¤ê³  í›ˆë ¨ì‹œì¼œ ëª¨ë¸ì˜ Robustnessë¥¼ ë†’ì´ëŠ” ë°©ë²•ìœ¼ë¡œ í›ˆë ¨ì„ í•˜ê³ ë‚˜ì„œë„ ëª¨ë¸ì˜ ë…¸ë“œëŠ” ê·¸ëŒ€ë¡œ ìœ ì§€ê°€ ëœë‹¤. ë°˜ë©´ Pruningì˜ ê²½ìš° í›ˆë ¨ì„ ë§ˆì¹œ í›„ì—, íŠ¹ì • Threshold ì´í•˜ì˜ ë§¤ê°œë³€ìˆ˜(ë…¸ë“œ)ì˜ ê²½ìš° ì‹œ Neural Networkì—ì„œ ì œì™¸ì‹œì¼œ ëª¨ë¸ì˜ í¬ê¸°ë¥¼ ì¤„ì´ë©´ì„œ ë™ì‹œì— ì¶”ë¡  ì†ë„ ë˜í•œ ë†’ì¼ ìˆ˜ ìˆë‹¤.\n\\[\n\\underset{W_p}{argmin}\\ L(x;W_p), \\text{ subject to } \\lvert\\lvert W_p\\lvert\\lvert_0\\ &lt; N\n\\]\n\nL represents the objective function for neural network training\n\\(x\\) is input, \\(W\\) is original weights, \\(W_p\\) is pruned weights\n\\(\\lvert\\lvert W_p\\lvert\\lvert_0\\) calcuates the #nonzeros in \\(W_p\\) and \\(N\\) is the target #nonzeros\n\nì´ëŠ” ìœ„ì™€ ê°™ì€ ì‹ìœ¼ë¡œ í‘œí˜„í•  ìˆ˜ ìˆë‹¤. íŠ¹ì • W ì˜ ê²½ìš° 0 ìœ¼ë¡œ ë§Œë“¤ì–´ ë…¸ë“œë¥¼ ì—†ì• ëŠ” ê²½ìš°ë¼ê³  ë³¼ ìˆ˜ ìˆê² ìŠµë‹ˆë‹¤. ê·¸ë ‡ê²Œ Pruningí•œ Neural NetworkëŠ” ì•„ë˜ ê·¸ë¦¼ ì²˜ëŸ¼ ëœë‹¤.\n\n\n\nReference. MIT-TinyML-lecture03-Pruning-1\n\n\nê·¸ëŸ¼ ì™œ Pruningì„ í•˜ëŠ” ê±¸ê¹Œ? ê°•ì˜ì—ì„œ Pruningì„ ì‚¬ìš©í•˜ë©´ Latency, Memeoryì™€ ê°™ì€ ë¦¬ì†ŒìŠ¤ë¥¼ í™•ë³´í•  ìˆ˜ ìˆë‹¤ê³  ê´€ë ¨ëœ ì•„ë˜ê°™ì€ ì—°êµ¬ê²°ê³¼ë¥¼ ê°™ì´ ë³´ì—¬ì¤€ë‹¤.\n\n\n\nReference. MIT-TinyML-lecture03-Pruning-1\n\n\nSong Han êµìˆ˜ë‹˜ì€ Vision ë”¥ëŸ¬ë‹ ëª¨ë¸ ê²½ëŸ‰í™” ì—°êµ¬ë¥¼ ì£¼ë¡œí•˜ì…”ì„œ, CNNì„ ê¸°ë°˜ìœ¼ë¡œ í•œ ëª¨ë¸ì„ ì˜ˆì‹œë¡œ ë³´ì—¬ì£¼ì‹ ë‹¤. ëª¨ë‘ Pruningì´í›„ì— ëª¨ë¸ ì‚¬ì´ì¦ˆì˜ ê²½ìš° ìµœëŒ€ 12ë°° ì¤„ì–´ ë“¤ë©° ì—°ì‚°ì˜ ê²½ìš° 6.3ë°°ê¹Œì§€ ì¤„ì–´ ë“  ê²ƒì„ ë³¼ ìˆ˜ ë‹¤.\nê·¸ë ‡ë‹¤ë©´ ì €ë ‡ê²Œ â€œí¬ê¸°ê°€ ì¤„ì–´ë“  ëª¨ë¸ì´ ì„±ëŠ¥ì„ ìœ ì§€í•  ìˆ˜ ìˆì„ê¹Œ?â€œ\n\n\n\nReference. MIT-TinyML-lecture03-Pruning-1\n\n\nê·¸ë˜í”„ì—ì„œ ëª¨ë¸ì˜ Weight ë¶„í¬ë„ë¥¼ ìœ„ ê·¸ë¦¼ì—ì„œ ë³´ë©´, Pruningì„ í•˜ê³  ë‚œ ì´í›„ì— Weight ë¶„í¬ë„ì˜ ì¤‘ì‹¬ì— íŒŒë¼ë¯¸í„°ê°€ ì˜ë ¤ë‚˜ê°„ ê²Œ ë³´ì¸ë‹¤. ì´í›„ Fine Tuningì„ í•˜ê³  ë‚œ ë‹¤ìŒì˜ ë¶„í¬ê°€ ë‚˜ì™€ ìˆëŠ”ë°, ì–´ëŠ ì •ë„ ì •í™•ë„ëŠ” ë–¨ì–´ì§€ì§€ë§Œ ì„±ëŠ¥ì´ ìœ ì§€ë˜ëŠ” ê±¸ ê´€ì°°í•  ìˆ˜ ìˆë‹¤.\n\n\n\nReference. MIT-TinyML-lecture03-Pruning-1\n\n\nê·¸ëŸ° Fine tuningì„ ë°˜ë³µì ìœ¼ë¡œ í•˜ê²Œ ëœë‹¤ë©´(Iterative Pruning and Fine tuning) ê·¸ë˜í”„ì—ì„œëŠ” ìµœëŒ€ 90í”„ë¡œ ì´ìƒì˜ íŒŒë¼ë¯¸í„°ë¥¼ ëœì–´ë‚¼ ìˆ˜ ìˆë‹¤ê³  í•œë‹¤.\në¬¼ë¡  íŠ¹ì • ëª¨ë¸ì—ì„œ, íŠ¹ì • Taskë¥¼ ëŒ€ìƒìœ¼ë¡œ í•œ ê²ƒì´ë¼ ì¼ë°˜í™”í•  ìˆ˜ëŠ” ì—†ì§€ë§Œ ë¦¬ì†ŒìŠ¤ë¥¼ ê³ ë ¤í•˜ëŠ” ìƒí™©ì´ë¼ë©´ ì¶©ë¶„íˆ ì‹œë„í•´ë³¼ ë§Œí•œ ê°€ì¹˜ê°€ ìˆì–´ ë³´ì¸ë‹¤. ê·¸ëŸ¼ ì´ë ‡ê²Œ ì„±ëŠ¥ì„ ìœ ì§€í•˜ë©´ì„œ Pruningì„ í•˜ê¸° ìœ„í•´ì„œ ì–´ë–¤ ìš”ì†Œë¥¼ ê³ ë ¤í•´ì•¼ í• ì§€ ë” ìì„¸íˆ ì´ì•¼ê¸°í•´ë³´ì!\nì†Œê°œí•˜ëŠ” ê³ ë ¤ìš”ì†ŒëŠ” ì•„ë˜ì™€ ê°™ë‹¤. Pruning íŒ¨í„´ë¶€í„° ì°¨ë¡€ëŒ€ë¡œ ì‹œì‘!\n\nPruning Granularity â†’ Pruning íŒ¨í„´\nPruning Criterion â†’ ì–¼ë§ˆë§Œí¼ì— íŒŒë¼ë¯¸í„°ë¥¼ Pruning í•  ê±´ê°€?\nPruning Ratio â†’ ì „ì²´ íŒŒë¼ë¯¸í„°ì—ì„œ Pruningì„ ì–¼ë§ˆë§Œí¼ì˜ ë¹„ìœ¨ë¡œ?\nFine Turning â†’ Pruning ì´í›„ì— ì–´ë–»ê²Œ Fine-Tuning í•  ê±´ê°€?\nADMM â†’ Pruning ì´í›„, ì–´ë–»ê²Œ Convexê°€ ëœë‹¤ê³  í•  ìˆ˜ ìˆì§€?\nLottery Ticket Hypothesis â†’ Trainingë¶€í„° Pruningê¹Œì§€ ëª¨ë¸ì„ ë§Œë“¤ì–´ ë³´ì!\nSystem Support â†’ í•˜ë“œì›¨ì–´ë‚˜ ì†Œí”„íŠ¸ì›¨ì–´ì ìœ¼ë¡œ Pruningì„ ì§€ì›í•˜ëŠ” ê²½ìš°ëŠ”?"
  },
  {
    "objectID": "posts/lecs/lec03.html#determine-the-pruning-granularity",
    "href": "posts/lecs/lec03.html#determine-the-pruning-granularity",
    "title": "ğŸ§‘â€ğŸ« Lecture 3",
    "section": "2. Determine the Pruning Granularity",
    "text": "2. Determine the Pruning Granularity\n\n\n\nThe case of convolutional layers, red box is preserved and white one is pruned referred from MIT-TinyML-lecture03-Pruning-1\n\n\nì—¬ê¸°ì„œ ê³ ë ¤ìš”ì†ŒëŠ” â€œì–¼ë§ˆë§Œí¼ ë‰´ëŸ°ì„ ê·¸ë£¹í™”í•˜ì—¬ ê³ ë ¤í•  ê²ƒì¸ê°€?â€ ì…ë‹ˆë‹¤. Regularí•œ ì •ë¡œë„ ë¶„ë¥˜í•˜ë©´ì„œ Irregularí•œ ê²½ìš°ì™€ Regularí•œ ê²½ìš°ì˜ íŠ¹ì§•ì„ ì•„ë˜ì²˜ëŸ¼ ë§í•©ë‹ˆë‹¤.\n\nFine-grained/Unstructured\n\nMore flexible pruning index choice\nHard to accelerate (irregular data expression)\nCan deliver speed up on some custom hardware\n\nCoarse-grained/Structured\n\nLess flexible pruning index choice (a subset of the fine-grained case)\nEasy to accelerate\n\n\nPruningì„ í•œë‹¤ê³  ëª¨ë¸ ì¶œë ¥ì´ ë‚˜ì˜¤ëŠ” ì‹œê°„ì´ ì§§ì•„ì§€ëŠ” ê²ƒì´ ì•„ë‹˜ë„ ì–¸ê¸‰í•©ë‹ˆë‹¤. Hardware Accelerationì˜ ê°€ëŠ¥ë„ê°€ ìˆëŠ”ë°, ì´ íŠ¹ì§•ì„ ë³´ë©´ ì•Œ ìˆ˜ ìˆë“¯, Pruningì˜ ììœ ë„ì™€ Hardware Accelerationì´ trade-off, ì¦‰ ê²½ëŸ‰í™” ì •ë„ì™€ Latencyì‚¬ì´ì— trade-off ê°€ ìˆì„ ê²ƒì´ ì˜ˆì¸¡ë©ë‹ˆë‹¤. í•˜ë‚˜ì”©, ìë£Œë¥¼ ë³´ë©´ì„œ ì‚´í´ ë³´ê² ìŠµë‹ˆë‹¤.\n\n2.1 Pattern-based Pruning\nIrregularì—ì„œë„ Pattern-based Pruningì€ ì—°ì†ì ì¸ ë‰´ëŸ° Mê°œ ì¤‘ Nê°œë¥¼ Pruning í•˜ëŠ” ë°©ë²•ì´ë‹¤. ì¼ë°˜ì ìœ¼ë¡œëŠ” N:M = 2:4 ìœ¼ë¡œ í•œë‹¤ê³  ì†Œê°œí•œë‹¤.\n\n\n\nReference. Accelerating Inference with Sparsity Using the NVIDIA Ampere Architecture and NVIDIA TensorRT\n\n\nReference. Accelerating Inference with Sparsity Using the NVIDIA Ampere Architecture and NVIDIA TensorRT\nì˜ˆì‹œë¥¼ ë“¤ì–´ ë³´ë©´, ìœ„ì™€ ê°™ì€ Matrixì—ì„œ í–‰ì„ ë³´ì‹œë©´ 8ê°œì˜ Weightì¤‘ 4ê°œê°€ Non-zeroì¸ ê²ƒì„ ë³¼ ìˆ˜ ìˆìŠµë‹ˆë‹¤. ì—¬ê¸°ì„œ Zeroì¸ ë¶€ë¶„ì„ ì—†ì• ê³  2bit indexë¡œ í•˜ì—¬ Matrix ì—°ì‚°ì„ í•˜ë©´ Nvidiaâ€™s Ampere GPUì—ì„œ ì†ë„ë¥¼ 2ë°°ê¹Œì§€ ë†’ì¼ ìˆ˜ ìˆë‹¤ê³  í•œë‹¤. ì—¬ê¸°ì„œ SparsityëŠ” â€œì–¼ë§ˆë§Œí¼ ê²½ëŸ‰í™” ëëŠ”ì§€?â€ ì´ë¼ê³  ìƒê°í•˜ë©´ ëœë‹¤.\n\nN:M sparsity means that in each contiguous M elements, N of them is pruned\nA classic case is 2:4 sparsity (50% sparsity)\nIt is supported by Nvidiaâ€™s Ampere GPU Architecture, which delivers up to 2x speed up and usually maintains accuracy.\n\n\n\n\nReference. https://developer.nvidia.com/blog/accelerating-inference-with-sparsity-using-ampere-and-tensorrt/\n\n\n\n\n2.2 Channel-level Pruning\në°˜ëŒ€ë¡œ íŒ¨í„´ì´ ìƒëŒ€ì ìœ¼ë¡œ regular í•œ ìª½ì¸ Channel-level Pruningì€ ì¶”ë¡ ì‹œê°„ì„ ì¤„ì¼ ìˆ˜ ìˆëŠ” ë°˜ë©´ì— ê²½ëŸ‰í™” ë¹„ìœ¨ì´ ì ë‹¤ê³  ë§í•œë‹¤. ì•„ë˜ ê·¸ë¦¼ì„ ë³´ì‹œë©´ Layerë§ˆë‹¤ Sparsityê°€ ë‹¤ë¥¸ ê±¸ ë³´ì‹¤ ìˆ˜ ìˆë‹¤.\n\nPro: Direct speed up!\nCon: smaller compression ratio\n\n\n\n\nReference AMC: Automl for Model Compression and Acceleration on Mobile Devices [HeÂ et al., ECCV 2018]\n\n\nì•„ë˜ì— ìë£Œì—ì„œëŠ” Channel ë³„ë¡œ í•œ Pruningì˜ ê²½ìš° ì „ì²´ ë‰´ë ¨ì„ ê°€ì§€ê³  í•œ Pruningë³´ë‹¤ ì¶”ë¡  ì‹œê°„ì„ ë” ì¤„ì¼ ìˆ˜ ìˆë‹¤ê³  ë§í•œë‹¤.\n\n\n\nReference AMC: Automl for Model Compression and Acceleration on Mobile Devices [HeÂ et al., ECCV 2018]\n\n\nìë£Œë¥¼ ë³´ë©´ Sparsityì—ì„œëŠ” íŒ¨í„´í™” ë¼ ìˆìœ¼ë©´ ê°€ì†í™”ê°€ ìš©ì´í•´ Latency, ì¶”ë¡  ì‹œê°„ì„ ì¤„ì¼ ìˆ˜ ìˆì§€ë§Œ ê·¸ ë§Œí¼ Pruningí•˜ëŠ” ë‰´ëŸ°ì˜ ìˆ˜ê°€ ì ì–´ ê²½ëŸ‰í™” ë¹„ìœ¨ì´ ì¤„ ê²ƒìœ¼ë¡œ ë³´ì¸ë‹¤. í•˜ì§€ë§Œ ë¹„êµì  ë¶ˆê·œì¹™í•œ ìª½ì— ì†í•˜ëŠ” Pattern-based Pruningì˜ ê²½ìš°ê°€ í•˜ë“œì›¨ì–´ì—ì„œ ì§€ì›í•´ì£¼ëŠ” ê²½ìš°, ëª¨ë¸ í¬ê¸°ì™€ Latencyë¥¼ ë‘˜ ë‹¤ ìµœì ìœ¼ë¡œ ì¡ì„ ìˆ˜ ìˆì„ ê²ƒìœ¼ë¡œ ë³´ì¸ë‹¤."
  },
  {
    "objectID": "posts/lecs/lec03.html#determine-the-pruning-criterion",
    "href": "posts/lecs/lec03.html#determine-the-pruning-criterion",
    "title": "ğŸ§‘â€ğŸ« Lecture 3",
    "section": "3. Determine the Pruning Criterion",
    "text": "3. Determine the Pruning Criterion\nê·¸ë ‡ë‹¤ë©´ ì–´ë–¤ íŒŒë¼ë¯¸í„°ë¥¼ ê°€ì§€ëŠ” ë‰´ëŸ°ì„ ìš°ë¦¬ëŠ” ì˜ë¼ë‚´ì•¼ í• ê¹Œìš”? Synapseì™€ Neuronìœ¼ë¡œ ë‚˜ëˆ ì„œ ì‚´í´ë³´ì.\n\nWhich synapses? Which neurons? Which one is less important?\nHow to Select Synapses and Select Neurons to Prune\n\n\n3.1 Select of Synapses\ní¬ê²Œ ì„¸ ê°€ì§€ë¡œ ë¶„ë¥˜í•˜ëŠ”ë°, ê° ë‰´ëŸ°ì˜ í¬ê¸°, ê° ì±„ë„ì— ì „ì²´ ë‰´ëŸ°ì— ëŒ€í•œ í¬ê¸°, ê·¸ë¦¬ê³  í…Œì¼ëŸ¬ ê¸‰ìˆ˜ë¥¼ ì´ìš©í•˜ì—¬ gradientì™€ weightë¥¼ ëª¨ë‘ ê³ ë ¤í•œ í¬ê¸°ë¥¼ ì†Œê°œí•œë‹¤. Song han êµìˆ˜ë‹˜ì´ ë°©ë²•ë“¤ì„ ì†Œê°œí•˜ê¸°ì— ì•ì„œì„œ ìœ ìˆ˜ì˜ ê¸°ì—…ë“¤ë„ ì§€ë‚œ 5ë…„ ë™ì•ˆ ì£¼ë¡œ Magnitude-based Pruningë§Œì„ ì‚¬ìš©í•´ì™”ë‹¤ê³  í•˜ëŠ”ë°, 2023ë…„ì´ ë¼ì„œ On-device AIê°€ ê°ê´‘ë°›ê¸° ì‹œì‘í•´ì„œ ì ì°¨ì ìœ¼ë¡œ ê´€ì‹¬ì„ ë°›ê¸° ì‹œì‘í•œ ê±´ê°€ ì‹¶ê¸°ë„ í•˜ë‹¤.\n3.1.1 Magnitude-based Pruning\ní¬ê¸°ë¥¼ ê¸°ì¤€ìœ¼ë¡œ í•˜ëŠ” ê²½ìš°, â€œì–¼ë§ˆë§Œí¼ ë‰´ëŸ° ê·¸ë£¹ì—ì„œ ê³ ë ¤í•  ê²ƒì¸ê°€?â€ì™€ â€œê·¸ë£¹ë‚´ì—ì„œ ì–´ë–¤ ì •ê·œí™”ë¥¼ ì‚¬ìš©í•  ê²ƒì¸ê°€?ë¥¼ ê³ ë ¤í•œë‹¤.\n\nHeuristic pruning criterion, Element-wise Pruning\n\\[\nImportance = \\lvert W \\lvert\n\\]\n\n\n\nReference. MIT-TinyML-lecture03-Pruning-1\n\n\nHeuristic pruning criterion, Row-wise Pruning, L1-norm magnitude\n\\[\nImportance = \\sum_{i\\in S}\\lvert w_i \\lvert, \\\\where\\ W^{(S)}\\ is\\ the\\ structural\\ set\\ S\\ of\\ parameters\\ W\n\\]\n\n\n\nReference. MIT-TinyML-lecture03-Pruning-1\n\n\nHeuristic pruning criterion, Row-wise Pruning, L2-norm magnitude\n\\[\nImportance = \\sum_{i\\in S}\\lvert w_i \\lvert, \\\\where\\ W^{(S)}\\ is\\ the\\ structural\\ set\\ S\\ of\\ parameters\\ W\n\\]\n\n\n\nReference. MIT-TinyML-lecture03-Pruning-1\n\n\nHeuristic pruning criterion, \\(L_p\\)- norm\n\\[\n\\lvert\\lvert W^{(S)}\\lvert\\lvert=\\huge( \\large\n   \\sum_{i\\in S} \\lvert w_i \\lvert^p\n\\huge) \\large^{\\frac{1}{p}}\n\\]\n\n3.1.2 Scaling-based Pruning\në‘ ë²ˆì§¸ë¡œ Scalingì„ í•˜ëŠ” ê²½ìš° ì±„ë„ë§ˆë‹¤ Scaling Factorë¥¼ ë‘¬ì„œ Pruningì„ í•œë‹¤. ê·¸ëŸ¼ Scaling Factorë¥¼ ì–´ë–»ê²Œ ë‘¬ì•¼ í• ê¹Œ? ê°•ì˜ì—ì„œ ì†Œê°œí•˜ëŠ” ì´ ë…¼ë¬¸ì—ì„œëŠ” Scaling factor \\(\\gamma\\) íŒŒë¼ë¯¸í„°ë¥¼ trainable íŒŒë¼ë¯¸í„°ë¡œ ë‘ë©´ì„œ batch normalization layerì— ì‚¬ìš©í•œë‹¤.\n\nScale factor is associated with each filter(i.e.Â output channel) in convolution layers.\nThe filters or output channels with small scaling factor magnitude will be pruned\nThe scaling factors can be reused from batch normalization layer\n\\[\n  z_o = \\gamma\\dfrac{z_i-\\mu_{B}}{\\sqrt{\\sigma_B^2+\\epsilon}}+\\beta\n  \\]\n\n\n\n\nReference. MIT-TinyML-lecture03-Pruning-1\n\n\n3.1.3 Talyor Expansion Analysis on Pruning Error\nì„¸ ë²ˆì§¸ ë°©ë²•ì€ í…Œì¼ëŸ¬ ê¸‰ìˆ˜ë¥¼ ì´ìš©í•˜ì—¬ Objective functionì„ ìµœì†Œí™” í•˜ëŠ” ì§€ì ì„ ì°¾ëŠ” ë°©ë²•ì…ë‹ˆë‹¤. Talyor Seriesì— ëŒ€í•œ ìì„¸í•œ ë‚´ìš©ì€ ì—¬ê¸°ì„œ!\n\nEvaluate pruning error induced by pruning synapses.\nMinimize the objective function L(x; W)\nA Taylor series can approximate the induced error.\n\n\\[\n\\delta L = L(x;W)-L(x;W_p=W-\\delta W) \\\\ = \\sum_i g_i\\delta w_i + \\frac{1}{2} \\sum_i h_{ii}\\delta w_i^2 + \\frac{1}{2}\\sum_{i\\not=j}h_{ij}\\delta w_i \\delta w_j + O(\\lvert\\lvert \\delta W \\lvert\\lvert^3)\n\\] \\[\nwhere\\ g_i=\\dfrac{\\delta L}{\\delta w_i}, h_{i, j} = \\dfrac{\\delta^2 L}{\\delta w_i \\delta w_j}\n\\]\n\nSecond-Order-based Pruning\n\n\n\nReference. MIT-TinyML-lecture03-Pruning-1\n\n\n\n\n\nReference. MIT-TinyML-lecture03-Pruning-1\n\n\nOptimal Brain Damage[LeCunÂ et al.,Â NeurIPS 1989] ë…¼ë¬¸ì—ì„œëŠ” ì´ ë°©ë²•ì„ ì´ìš©í•˜ê¸° ìœ„í•´ ì„¸ ê°€ì§€ë¥¼ ê°€ì •í•œë‹¤.\n\nObjective function Lì´ quadratic ì´ê¸° ë•Œë¬¸ì— ë§ˆì§€ë§‰ í•­ì´ ë¬´ì‹œëœë‹¤(ì´ëŠ” Talyor Seriesì˜ Error í•­ì„ ì•Œë©´ ì´í•´ê°€ ë” ì‰½ë‹¤!)\në§Œì•½ ì‹ ê²½ë§ì´ ìˆ˜ë ´í•˜ê²Œë˜ë©´, ì²« ë²ˆì§¸í•­ë„ ë¬´ì‹œëœë‹¤.\nê° íŒŒë¼ë¯¸í„°ê°€ ë…ë¦½ì ì´ë¼ë©´ Cross-termë„ ë¬´ì‹œëœë‹¤.\n\nê·¸ëŸ¬ë©´ ì‹ì„ ì•„ë˜ì²˜ëŸ¼ ì •ë¦¬í•  ìˆ˜ ìˆëŠ”ë°, ì¤‘ìš”í•œ ë¶€ë¶„ì€ Hessian Matrix Hì— ì‚¬ìš©í•˜ëŠ” Computationì´ ì–´ë µë‹¤ëŠ” ì !\n\\[\n\\delta L_i = L(x;W)-L(x;W_p\\lvert w_i=0)\\approx \\dfrac{1}{2} h_{ii}w_i^2,\\ where\\ h_{ii}=\\dfrac{\\partial^2 L}{\\partial w_i \\partial w_j}\n\\]\n\\[\nimportance_{w_i} = \\lvert \\delta L_i\\lvert = \\frac{1}{2}h_{ii}w_i^2\n\\] \\[\n*\\ h_{ii} \\text{ is non-negative}\n\\]\nFirst-Order-based Pruning\n\nì°¸ê³ ë¡œ ì´ ë°©ë²•ì€ 2023ë…„ì—ëŠ” ì†Œê°œí•˜ì§€ ì•ŠëŠ”ë‹¤.\n\n\n\n\nReference. MIT-TinyML-lecture03-Pruning-1\n\n\n\nIf only first-order expansion is considered under an i.i.d(Independent and identically distributed) assumption,\n\n\\[\n\\delta L_i = L(x;W) - L(x; W_P\\lvert w_i=0) \\approx g_iw_i,\\ where\\ g_i=\\dfrac{\\partial L}{\\partial w_i}\n\\] \\[\nimportance_{w_i} = \\lvert \\delta L_i \\lvert = \\lvert g_i w_i \\lvert \\ or \\ importance_{w_i} = \\lvert \\delta L_i \\lvert^2 = (g_i w_i)^2\n\\]\n\nFor coarse-grained pruning, we have,\n\\[\n  importance_{\\ W^{(S)}} = \\sum_{i \\in S}\\lvert \\delta L_i \\lvert^2 = \\sum_{i \\in S} (g_i w_i)^2,\\ where \\ W^{(S)}is\\ the\\ structural\\ set\\ of\\ parameters\n  \\]\n\n\n\n\n3.2 Select of Neurons\nì–´ë–¤ Neuronì„ ì—†ì•¨ ì§€ë¥¼ ê³ ë ¤(Less useful â†’ Remove) í•œ ì´ ë°©ë²•ì€ Neuronì˜ ê²½ìš°ë„ ìˆì§€ë§Œ ì•„ë˜ ê·¸ë¦¼ì²˜ëŸ¼ Channelë¡œ ê³ ë ¤í•  ìˆ˜ë„ ìˆë‹¤. í™•ì‹¤íˆ ì „ì— ì†Œê°œí–ˆë˜ ë°©ë²•ë“¤ë³´ë‹¤ â€œCoarse-grained pruningâ€ì¸ ë°©ë²•ì´ë‹¤.\n\n\nPercentage-of-Zero-based Pruning\nì²«ë²ˆì§¸ëŠ” Channelë§ˆë‹¤ 0ì˜ ë¹„ìœ¨ì„ ë´ì„œ ë¹„ìœ¨ì´ ë†’ì€ Channel ì„ ì—†ë‚´ëŠ” ë°©ë²•ì´ë‹¤. ReLU activationì„ ì‚¬ìš©í•˜ë©´ Outputì´ 0ì´ ë‚˜ì˜¤ëŠ”ë°, ì—¬ê¸°ì„œ 0ì˜ ë¹„ìœ¨, Average Percentage of Zero activations(APoZ)ë¼ê³  ë¶€ë¥´ëŠ” ê²ƒì„ ë³´ê³  ê°€ì§€ì¹˜ê¸°í•  Channelì„ ì œê±°í•œë‹¤.\n\nReLU activation will generate zeros in the output activation\nSimilar to magnitude of weights, the Average Percentage of Zero activations(APoZ) can be exploited to measure the importance the neuron has\n\n\n\n\nReference. MIT-TinyML-lecture03-Pruning-1\n\n\nFirst-Order-based Pruning\n\nì°¸ê³ ë¡œ ì´ ë°©ë²•ì€ 2023ë…„ì—ëŠ” ì†Œê°œí•˜ì§€ ì•ŠëŠ” ë°©ë²•ì´ë‹¤.\nMinimize the error on loss function introduced by pruning neurons\nSimilar to previous Taylor expansion on weights, the induced error of the objective functionÂ L(x;Â W)Â can be approximated by a Taylor series expanded on activations.\n\\[\n  \\delta L_i = L(x; W) - L(x\\lvert x_i = 0; W) \\approx \\dfrac{\\partial L}{\\partial x_i}x_i\n  \\]\nFor a structural set of neuronsÂ \\(x^{(S)}\\)Â (e.g., a channel plane),\n\\[\n  \\lvert \\delta L_{x^{(S)}} \\lvert\\ = \\Large\\lvert \\small\\sum_{i\\in S}\\dfrac{\\partial L}{\\partial x_i}x_i\\Large\\lvert\n  \\]\n\nRegression-based Pruning\nì´ ë°©ë²•ì€ Quantizedí•œ ë ˆì´ì–´ì˜ output \\(\\hat Z\\)(construction error of the corresponding layerâ€™s outputs)ì™€ \\(Z\\)ë¥¼ Trainingì„ í†µí•´ ì°¨ì´ë¥¼ ì¤„ì´ëŠ” ë°©ë²•ì´ë‹¤. ì°¸ê³ ë¡œ ë¬¸ì œë¥¼ í‘¸ëŠ” ìì„¸í•œ ê³¼ì •ì€ 2022ë…„ ê°•ì˜ì—ë§Œ ë‚˜ì™€ ìˆë‹¤.\n\n\\[\nZ=XW^T=\\sum_{c=0}^{c_i-1}X_cW_c^T\n\\]\n\n\n\nReference. MIT-TinyML-lecture03-Pruning-1\n\n\në¬¸ì œë¥¼ ì‹ìœ¼ë¡œ ì •ì˜í•´ë³´ë©´ ì•„ë˜ì™€ ê°™ì€ë°,\n\n\\(\\beta\\) is the coefficient vector of length \\(c_i\\) for channel selection.\n\\(\\beta_c = 0\\) means channel \\(c\\) is pruned.\n\\(N_c\\) is the number of none zero channel\n\nìš°ì„  ë¬¸ì œë¥¼ í‘¸ëŠ” ë‹¨ê³„ëŠ” ë‘ ë‹¨ê³„ë¡œ ë‚˜ëˆˆë‹¤. Channelì˜ Scale \\(\\beta\\)ë¥¼ ìš°ì„  ê³„ì‚°í•œ í›„ì— \\(W\\)ë¥¼ Quantizedí•œ ë ˆì´ì–´ì˜ output \\(\\hat Z\\)(construction error of the corresponding layerâ€™s outputs)ì™€ \\(Z\\)ì˜ ì°¨ì´ê°€ ìµœì†Œí™”ë˜ëŠ” ì§€ì ê¹Œì§€ Trainingì‹œí‚¨ë‹¤.\nSolve the problem in two folds:\n\nFix W, solve \\(\\beta\\) for channel selection â†’ NP(Nondeterministic polynomial)-hard\nFix \\(\\beta\\), solve W to minimize reconstruction error(Weight Reconstruction)\n\nê° ë¬¸ì œë¥¼ í‘¸ëŠ” ê³¼ì •ì„ ì¡°ê¸ˆ ë” ìì„¸íˆ ì‚´í´ë´ë³´ì. ë³¸ ë‚´ìš©ì€ 2022ë…„ ê°•ì˜ì— ìˆìœ¼ë‹ˆ ì°¸ê³ !\nNP(Nondeterministic polynomial)-hardëŠ” ì•„ë˜ì™€ ê°™ì´ ì‹ìœ¼ë¡œ ì •ë¦¬í•  ìˆ˜ ìˆë‹¤.\n\\[\n\\underset{\\beta}{argmin} \\lvert\\lvert Z- \\sum_{c=0}^{c_i-1} \\beta_cX_cW_c^T \\lvert\\lvert_F^2 = \\lvert\\lvert \\sum_{c=0}^{c_i-1}X_cW_c^T - \\sum_{c=0}^{c_i-1} \\beta_cX_cW_c^T \\lvert\\lvert_F^2\n\\] \\[\n= \\lvert\\lvert\\sum_{c=0}^{c_i-1} (1-\\beta_c)X_cW_c^T \\lvert\\lvert_F^2, \\ s.t.\\ \\lvert\\lvert\\beta\\lvert\\lvert_0 \\ \\leq N_c\n\\]\nê°•ì˜ì—ì„œ ì†Œê°œí•˜ëŠ” ThiNetì´ë¼ëŠ” ë…¼ë¬¸ì—ì„œëŠ” greedy solutionì„ ì´ìš©í•´ì„œ ì±„ë„ í•˜ë‚˜í•˜ë‚˜ì”© Pruning í•´ë³´ë©° objective functionì˜ l2-norm ìµœì†Ÿê°’ì„ êµ¬í•œë‹¤.\n1: S = []\n2: while len(S) &lt; N:\n3:   min_norm, min_c = +inf, 0\n4:   for c in range(c_i):\n5:     tmpS=S+[c]\n6:     Z = X[:,tmpS] * W[:,tmpS].t()\n7:     norm = Z.norm(2)\n8:     if norm &lt; min_norm:\n9:       min_norm, min_c = norm, c\n10:   S.append(min_c)\n11:   c_i.pop(min_c)\nì—¬ê¸°ì„œ ë”í•´ì„œ \\(\\beta\\) ë¥¼ êµ¬í•˜ëŠ” ê³¼ì •ì—ì„œ ì¼ë°˜í™”ë¥¼ ìœ„í•´ LASSO ë°©ì‹ì„ ì‚¬ìš©í•œë‹¤(LASSOì— ëŒ€í•œ ìì„¸í•œ ë‚´ìš©ì€ ì—¬ê¸°ì„œ). Relax the \\(l_0\\) to \\(l_1\\) regularization (LASSO):\n\\[\n\\underset{\\beta}{argmin}\\ \\lvert\\lvert Z- \\sum_{c=0}^{c_i-1}\\beta_cX_cW_c^T\\lvert\\lvert^2_F+\\lambda\\lvert\\lvert \\beta \\lvert\\lvert_1\n\\]\n\n\\(\\lambda\\) is a penalty coefficient. By increasing \\(\\lambda\\), there will be more zeros in \\(\\beta\\).\nGradually increase \\(\\lambda\\) and solve the LASSO regression for \\(\\beta\\), until \\(\\lvert\\lvert \\beta \\lvert\\lvert_0==N_c\\) is met.\nWhy \\(\\lvert\\lvert \\beta \\lvert\\lvert_0==N_c\\)?\nì—¬ê¸°ì— ëŒ€í•´ì„œëŠ” ë”°ë¡œ ì–¸ê¸‰ë˜ì§€ ì•Šì•˜ì§€ë§Œ, ì˜ë¯¸ìƒ scale ì „ì²´ Nê°œ ì¤‘ì—ì„œ ìµœì ê°’ì„ ì°¾ì•„ì•¼í•œë‹¤ë©´ ì „ì²´ë¥¼ Nìœ¼ë¡œ ìœ ì§€í•˜ë©´ì„œ ìµœì ê°’ì„ ì°¾ê¸° ìœ„í•´ì„œê°€ ì•„ë‹ê¹Œ?\n\në‘ ë²ˆì§¸ëŠ” êµ¬í•œ \\(\\beta\\)ë¥¼ ê³ ì •í•œ ìƒíƒœë¡œ Weightë¥¼ Quantized ì „í›„ì˜ ì°¨ì´ë¥¼ ìµœì†Œí™” í•˜ê²Œ â€œWeight Reconstructionâ€ í•œë‹¤. êµ¬í•˜ëŠ” ê³¼ì •ì€ least square approachë¥¼ ì´ìš©í•œ unique closed-form solution ì´ë¯€ë¡œ ì•„ë˜ë¥¼ ì°¸ì¡°í•˜ì.\n\\[\n\\underset{\\beta}{argmin}\\ \\lvert\\lvert Z- \\sum_{c=0}^{c_i-1}\\beta_cX_cW_c^T\\lvert\\lvert^2_F\n\\]\n\n\\(\\beta\\) is a coefficient vector from the previous step\nThis is a classic linear regression problem, which has a unique closed-form solution using the least square approach.\n\\[\n  \\underset{W}{argmin} \\lvert\\lvert Z-\\hat{Z} \\lvert\\lvert^2_F = \\lvert\\lvert Z-UW^T \\lvert\\lvert_F^2\n  \\]\nwhere\n\\[\n  U= \\Large[ \\small\\beta_0X_0\\ \\beta_1X_1 \\ \\cdots \\beta_cX_c \\cdots \\beta_{c_i-1}X_{c_i-1} \\Large]\n  \\]\nand thus,\n\\[\n  W^T = (U^TU)^{-1}U^T Z\n  \\]\n\nQ. How \\((U^TU)^{-1}\\) exists?\nLeast Square method, ì„ì˜ì˜ ë²¡í„° \\(v = (v_0, v_1, \\dots, v_n)\\) ê°€ ìˆì„ ë•Œ \\(v^Tv\\) ì˜ ì—­í–‰ë ¬ì€ í•­ìƒ ìˆì„ê¹Œ? ê°€ì •ì—ì„œ â€œa unique closed-form solutionâ€ë¼ê³  í–ˆìœ¼ë¯€ë¡œ ì´ëŠ” ì¦‰ linearly independenë¡œ ê³ ë ¤í•  ìˆê³  ì—­í–‰ë ¬ì´ ìˆë‹¤(\\(v^Tv\\) is invertible)ëŠ” ì´ì•¼ê¸°ì´ë‹¤."
  },
  {
    "objectID": "posts/lecs/lec03.html#discussion",
    "href": "posts/lecs/lec03.html#discussion",
    "title": "ğŸ§‘â€ğŸ« Lecture 3",
    "section": "4. Discussion",
    "text": "4. Discussion\n\nPruningì„ Dropoutì´ë‘ ë¹„êµí•´ì„œ ì–´ë–¤ ì°¨ì´ì ì´ ìˆëŠ”ê°€?\në‘ ê°€ì§€ ë°©ë²•ì€ ë¶„ëª…íˆ Neuronê³¼ Synapseë¥¼ ì—†ëŒ„ë‹¤ëŠ” ì¸¡ë©´ì—ì„œëŠ” ë¹„ìŠ·í•˜ë‹¤. í•˜ì§€ë§Œ ë‘ ê°€ì§€ ì¸¡ë©´ì—ì„œ ì°¨ì´ì ì´ ìˆëŠ”ë°, í•œ ê°€ì§€ëŠ” ëª©ì í•˜ëŠ” ë°”ì´ê³ , ë‘ ë²ˆì§¸ëŠ” ì‹œì ì´ë‹¤. Dropoutì€ ëª©ì í•˜ëŠ” ë°”ê°€ í›ˆë ¨ì¤‘ì— overfittingì„ ë°©ì§€í•˜ê¸° ìœ„í•¨ì´ ìˆê³  Pruningì˜ ê²½ìš°ëŠ” í›ˆë ¨ì„ ë§ˆì¹œ ëª¨ë¸ì˜ í¬ê¸°ë¥¼ ì¤„ì´ëŠ” ê²ƒì— ìˆë‹¤. ê·¸ë¦¬ê³  ë‘ ë²ˆì§¸ ì‹œì ì˜ ê²½ìš° Dropoutì€ í›ˆë ¨ì¤‘ì— ì´ë¤„ì§€ëŠ” ë°˜ë©´ Pruningì€ í›ˆë ¨ì„ ë§ˆì¹˜ê³ , ê·¸ í¬ê¸°ë¥¼ ì¤„ì¸ í›„ì— ì„±ëŠ¥ì´ ë–¨ì–´ì§€ë©´ ê·¸ì— ë§ê²Œ Fine-tuningì„ í•œë‹¤.\nìŠ¤í„°ë””ì—ì„œëŠ” â€œì™œ dropoutì„ í†µí•´ ì‚¬ì´ì¦ˆë¥¼ ì¤„ì´ì§€ ì•Šì•˜ëŠ”ê°€? ê·¸ë¦¬ê³  êµ¬ì§€ í›ˆë ¨ì„ ë§ˆì¹œ ë‹¤ìŒì— í•  í•„ìš”ê°€ ìˆë‚˜?â€ ë¼ê³  ì§ˆë¬¸ì´ ë‚˜ì™”ì—ˆë‹¤. ë¬¼ë¡  í›ˆë ¨ ì¤‘ì— ëª¨ë¸ì˜ ì‚¬ì´ì¦ˆë¥¼ ì‘ê²Œ ë§Œë“¤ ìˆ˜ ìˆìœ¼ë©´, ê°€ëŠ¥í•œ ê·¸ë ‡ê²Œ í•˜ë©´ ë  ê²ƒì´ë‹¤. í•˜ì§€ë§Œ, ì´ ë˜í•œ ë‘ê°€ì§€ ì¸¡ë©´ì„ ê³ ë ¤í•  í•„ìš”ê°€ ìˆë‹¤. í•˜ë‚˜ëŠ” â€œê³¼ì—° ëª¨ë¸ì˜ ì‚¬ì´ì¦ˆë¥¼ í›ˆë ¨ ì¤‘ í˜¹ì€ ì „ì— ì¤„ì—¬ë‚˜ê°€ë©´ì„œ ì¶©ë¶„íˆ ì„±ëŠ¥ì„ ë‚¼ ìˆ˜ ìˆëŠ”ê°€?â€ì´ê³  ë‹¤ë¥¸ í•˜ë‚˜ëŠ” Pruningì´ë‚˜ ëª¨ë¸ ê²½ëŸ‰í™”ëŠ” ìµœì í™”ì— ì´ˆì ì„ ë§ì¶˜ë‹¤ê³  ìƒê°í•œë‹¤. ê·¸ë ‡ê¸° ë•Œë¬¸ì— í›ˆë ¨ ì¤‘ê°„ì— Channel pruningê³¼ ê°™ì€ ê¸°ë²•ì„ ì‚¬ìš©í•  ìˆ˜ ìˆì„ ì§€ëŠ” ë¯¸ì§€ìˆ˜ì´ê³ , ì„¤ë ¹ Fine-grained Pruningê³¼ ê°™ì€ ê¸°ë²•ì„ ì‚¬ìš©í•œë‹¤ í•˜ë”ë¼ë„ ì´ëŠ” ëª¨ë¸ì˜ ì‚¬ì´ì¦ˆë§Œ ì¤„ì–´ ë“¤ ë¿, ë‚˜ë¨¸ì§€ ë©”ëª¨ë¦¬(e.g.Â RAM)ì´ë‚˜ Latencyê°™ì€ ì„±ëŠ¥ì€ ì¢‹ê²Œ ê°€ì ¸ê°ˆ ìˆ˜ ìˆì„ì§€ë„ ë¯¸ì§€ìˆ˜ë¼ê³  ìƒê°í•œë‹¤.\ní•„ìëŠ” ìœ„ì™€ ê°™ì€ ìµœì í™”ë¥¼ í†µí•œ ì„±ëŠ¥ ê°œì„ ì„ ì´ ê¸€ì—ì„œì²˜ëŸ¼ 2022ë…„ TinyML ê°•ì˜ì—ì„œ ì œê³µí•˜ëŠ” ì‹¤ìŠµì„ í†µí•´ ê²½í—˜í–ˆì—ˆë‹¤. ì•ì„  ì˜ˆì‹œëŠ” OSë¥¼ ê°€ì§„ ë””ë°”ì´ìŠ¤ê°€ ì•„ë‹Œ Bare-metal firmwareë¡œ í™˜ê²½ì´ ì¡°ê¸ˆ íŠ¹ìˆ˜í•˜ê¸°ë„ í•˜ê³ , ì‹¤ì œë¡œ Torchë‚˜ Tensorflowliteì—ì„œ ì œê³µí•˜ëŠ” ëª¨ë¸ ê²½ëŸ‰í™”ë¥¼ ì§ì ‘ì ìœ¼ë¡œ ë¶„ì„í•´ë´ì•¼ ì‹¤ì§ˆì ì¸ ì˜ˆì‹œë¥¼ ì•Œ ìˆ˜ ìˆê² ì§€ë§Œ, í˜¹ì—¬ ì´í•´í•´ ì°¸ê³ ê°€ ë ê¹Œ ë§ë¶™ì—¬ ë†“ëŠ”ë‹¤."
  },
  {
    "objectID": "posts/lecs/lec03.html#reference",
    "href": "posts/lecs/lec03.html#reference",
    "title": "ğŸ§‘â€ğŸ« Lecture 3",
    "section": "5. Reference",
    "text": "5. Reference\n\nMIT-TinyML-lecture03-Pruning-1\nAMC: Automl for Model Compression and Acceleration on Mobile Devices, 2018\nLearning Efficient Convolutional Networks through Network Slimming, 2017\nThiNet: A Filter Level Pruning Method for Deep Neural Network Compression, 2017\nChannel Pruning for Accelerating Very Deep Neural Networks"
  },
  {
    "objectID": "posts/labs/lab02.html",
    "href": "posts/labs/lab02.html",
    "title": "ğŸ‘©â€ğŸ’» Lab 2",
    "section": "",
    "text": "Lecture 5ì™€ 6ì„ í†µí•´ ë°°ìš´ Quantization ë‚´ìš© ì¤‘ì— K-means Quantizationê³¼ Linear Quantizationì— ëŒ€í•´ ì‹¤ìŠµí•˜ë©° ë°°ì›Œë³´ëŠ” Lab2ì— ëŒ€í•œ í’€ì´ì™€ ì„¤ëª…ì— ëŒ€í•œ í¬ìŠ¤íŒ…ì´ë‹¤. ê¸°ì¡´ì˜ ì‹¤ìŠµ ë…¸íŠ¸ëŠ” Original ê°•ì˜ì˜ ë§í¬ë¥¼, í•œêµ­ì–´ ë²ˆì—­ê³¼ Solutionì€ ì´ ë§í¬ë¥¼ ì°¸ê³ í•˜ë©´ ë©ë‹ˆë‹¤. ì•„ë˜ Colaboratory ë²„íŠ¼ì„ ëˆ„ë¥´ë©´ ì‹¤ìŠµë…¸íŠ¸ë¥¼ ë°”ë¡œ ì‹¤í–‰ì‹œí‚¤ëŠ” Colab Notebookì„ ì‹¤í–‰ì‹œí‚¬ ìˆ˜ ìˆìŠµë‹ˆë‹¤."
  },
  {
    "objectID": "posts/labs/lab02.html#goals",
    "href": "posts/labs/lab02.html#goals",
    "title": "ğŸ‘©â€ğŸ’» Lab 2",
    "section": "Goals",
    "text": "Goals\nì´ë²ˆ ì‹¤ìŠµì—ì„œëŠ” ëª¨ë¸ í¬ê¸°ì™€ ì§€ì—° ì‹œê°„ì„ ì¤„ì´ê¸° ìœ„í•´ í´ë˜ì‹í•œ neural network modelì„ quantizingí•˜ëŠ” ì—°ìŠµì„ í•  ê²ƒì…ë‹ˆë‹¤. ì´ ì‹¤ìŠµì˜ ëª©í‘œëŠ” ë‹¤ìŒê³¼ ê°™ìŠµë‹ˆë‹¤:\n\nQuantizationì˜ ê¸°ë³¸ ê°œë…ì„ ì´í•´í•©ë‹ˆë‹¤.\nk-means quantizationì„ êµ¬í˜„í•˜ê³  ì ìš©í•©ë‹ˆë‹¤.\nk-means quantizationì— ëŒ€í•´ quantization-aware trainingì„ êµ¬í˜„í•˜ê³  ì ìš©í•©ë‹ˆë‹¤.\nlinear quantizationì„ êµ¬í˜„í•˜ê³  ì ìš©í•©ë‹ˆë‹¤.\nlinear quantizationì— ëŒ€í•´ integer-only inferenceë¥¼ êµ¬í˜„í•˜ê³  ì ìš©í•©ë‹ˆë‹¤.\nQuantizationì—ì„œì˜ ì„±ëŠ¥ ê°œì„ (ì˜ˆ: ì†ë„ í–¥ìƒ)ì— ëŒ€í•œ ê¸°ë³¸ì ì¸ ì´í•´ë¥¼ ì–»ìŠµë‹ˆë‹¤.\nì´ëŸ¬í•œ quantization ì ‘ê·¼ ë°©ì‹ ì‚¬ì´ì˜ ì°¨ì´ì ê³¼ íŠ¸ë ˆì´ë“œì˜¤í”„ë¥¼ ì´í•´í•©ë‹ˆë‹¤."
  },
  {
    "objectID": "posts/labs/lab02.html#contents",
    "href": "posts/labs/lab02.html#contents",
    "title": "ğŸ‘©â€ğŸ’» Lab 2",
    "section": "Contents",
    "text": "Contents\nì£¼ìš” ì„¹ì…˜ì€ K-Means Quantization ê³¼ Linear Quantization 2ê°€ì§€ë¡œ êµ¬ì„±ë˜ì–´ ìˆìŠµë‹ˆë‹¤.\nì´ë²ˆ ì‹¤ìŠµ ë…¸íŠ¸ì—ì„œ ì´ 10ê°œì˜ ì§ˆë¬¸ì„ í†µí•´ í•™ìŠµí•˜ê²Œ ë©ë‹ˆë‹¤.:\n\nK-Means Quantizationì— ëŒ€í•´ì„œëŠ” 3ê°œì˜ ì§ˆë¬¸ì´ ìˆìŠµë‹ˆë‹¤ (Question 1-3).\nLinear Quantizationì— ëŒ€í•´ì„œëŠ” 6ê°œì˜ ì§ˆë¬¸ì´ ìˆìŠµë‹ˆë‹¤ (Question 4-9).\nQuestion 10ì€ k-means quantizationê³¼ linear quantizationì„ ë¹„êµí•©ë‹ˆë‹¤.\n\n\nì‹¤ìŠµë…¸íŠ¸ì— ëŒ€í•œ ì„¤ì • ë¶€ë¶„(Setup)ì€ Colaboratory Noteë¥¼ ì—´ë©´ í™•ì¸í•˜ì‹¤ ìˆ˜ ìˆìŠµë‹ˆë‹¤. í¬ìŠ¤íŒ…ì—ì„œëŠ” ë³´ë‹¤ ì‹¤ìŠµë‚´ìš©ì— ì§‘ì¤‘í•  ìˆ˜ ìˆë„ë¡ ìƒëµë˜ì–´ ìˆìŠµë‹ˆë‹¤."
  },
  {
    "objectID": "posts/labs/lab02.html#question-1-10-pts",
    "href": "posts/labs/lab02.html#question-1-10-pts",
    "title": "ğŸ‘©â€ğŸ’» Lab 2",
    "section": "Question 1 (10 pts)",
    "text": "Question 1 (10 pts)\nì•„ë˜ì˜ K-Means quantization functionì„ ì™„ì„±í•˜ì„¸ìš”.\n\nfrom fast_pytorch_kmeans import KMeans\n\ndef k_means_quantize(fp32_tensor: torch.Tensor, bitwidth=4, codebook=None):\n    \"\"\"\n    quantize tensor using k-means clustering\n    :param fp32_tensor:\n    :param bitwidth: [int] quantization bit width, default=4\n    :param codebook: [Codebook] (the cluster centroids, the cluster label tensor)\n    :return:\n        [Codebook = (centroids, labels)]\n            centroids: [torch.(cuda.)FloatTensor] the cluster centroids\n            labels: [torch.(cuda.)LongTensor] cluster label tensor\n    \"\"\"\n    if codebook is None:\n        ############### YOUR CODE STARTS HERE ###############\n        # get number of clusters based on the quantization precision\n        n_clusters = 2 ** bitwidth  # Calculate number of clusters as 2^bitwidth\n        ############### YOUR CODE ENDS HERE #################\n        # use k-means to get the quantization centroids\n        kmeans = KMeans(n_clusters=n_clusters, mode='euclidean', verbose=0)\n        labels = kmeans.fit_predict(fp32_tensor.view(-1, 1)).to(torch.long)\n        centroids = kmeans.centroids.to(torch.float).view(-1)\n        codebook = Codebook(centroids, labels)\n\n    ############### YOUR CODE STARTS HERE ###############\n    # decode the codebook into k-means quantized tensor for inference\n    # hint: one line of code\n    quantized_tensor = codebook.centroids[codebook.labels].view_as(fp32_tensor)\n    ############### YOUR CODE ENDS HERE #################\n    fp32_tensor.set_(quantized_tensor.view_as(fp32_tensor))\n    return codebook\n\nìœ„ì—ì„œ ì‘ì„±í•œ k-means quantization functionì„ ë”ë¯¸ í…ì„œì— ì ìš©í•˜ì—¬ í™•ì¸í•´ë´…ì‹œë‹¤.\n\ntest_k_means_quantize()\n\ntensor([[-0.3747,  0.0874,  0.3200, -0.4868,  0.4404],\n        [-0.0402,  0.2322, -0.2024, -0.4986,  0.1814],\n        [ 0.3102, -0.3942, -0.2030,  0.0883, -0.4741],\n        [-0.1592, -0.0777, -0.3946, -0.2128,  0.2675],\n        [ 0.0611, -0.1933, -0.4350,  0.2928, -0.1087]])\n* Test k_means_quantize()\n    target bitwidth: 2 bits\n        num unique values before k-means quantization: 25\n        num unique values after  k-means quantization: 4\n* Test passed."
  },
  {
    "objectID": "posts/labs/lab02.html#question-2-10-pts",
    "href": "posts/labs/lab02.html#question-2-10-pts",
    "title": "ğŸ‘©â€ğŸ’» Lab 2",
    "section": "Question 2 (10 pts)",
    "text": "Question 2 (10 pts)\në§ˆì§€ë§‰ ì½”ë“œ ì…€ì€ 2ë¹„íŠ¸ k-means quantizationì„ ìˆ˜í–‰í•˜ê³  quantization ì „í›„ì˜ í…ì„œë¥¼ í”Œë¡¯í•©ë‹ˆë‹¤. ê° í´ëŸ¬ìŠ¤í„°ëŠ” ê³ ìœ í•œ ìƒ‰ìƒìœ¼ë¡œ ë Œë”ë§ë˜ë©°, quantized í…ì„œë“¤ì´ 4(\\(2^2\\))ê°€ì§€ ê³ ìœ í•œ ìƒ‰ìƒìœ¼ë¡œ í‘œì‹œë©ë‹ˆë‹¤.\nì´ëŸ¬í•œ í˜„ìƒì„ ê´€ì°°í•œ ê²ƒì„ ë°”íƒ•ìœ¼ë¡œ ì§ˆë¬¸ë“¤ì— ë‹µí•˜ì„¸ìš”.\n\nQuestion 2.1 (5 pts)\n4ë¹„íŠ¸ë¡œ k-means quantizationì´ ìˆ˜í–‰ë˜ë©´, quantized í…ì„œì—ëŠ” ëª‡ ê°œì˜ ê³ ìœ í•œ ìƒ‰ìƒì´ ë Œë”ë§ë ê¹Œìš”?\nYour Answer:\n4ë¹„íŠ¸ k-means quantizationì´ ìˆ˜í–‰ë˜ë©´, quantized í…ì„œì— \\((2^4 = 16)\\)ê°œì˜ ê³ ìœ í•œ ìƒ‰ìƒì´ ë Œë”ë§ë©ë‹ˆë‹¤. ì´ëŠ” 4ë¹„íŠ¸ë¡œ 0000ë¶€í„° 1111ê¹Œì§€ì˜ 16ê°€ì§€ ë‹¤ë¥¸ ìƒíƒœë‚˜ ì¡°í•©ì„ ë‚˜íƒ€ë‚¼ ìˆ˜ ìˆìœ¼ë©°, ì´ëŠ” í…ì„œ ê°’ì´ ê·¸ë£¹í™”ë  ìˆ˜ ìˆëŠ” 16ê°œì˜ ê³ ìœ í•œ í´ëŸ¬ìŠ¤í„°ì— í•´ë‹¹í•©ë‹ˆë‹¤.\n\n\nQuestion 2.2 (5 pts)\nn-ë¹„íŠ¸ k-means quantizationì´ ìˆ˜í–‰ë˜ë©´, quantized í…ì„œì— ëª‡ ê°œì˜ ê³ ìœ í•œ ìƒ‰ìƒì´ ë Œë”ë§ë ê¹Œìš”?\nYour Answer:\nn-ë¹„íŠ¸ k-means quantizationì´ ìˆ˜í–‰ë˜ë©´, quantized í…ì„œì—ëŠ” \\((2^n)\\)ê°œì˜ ê³ ìœ í•œ ìƒ‰ìƒì´ ë Œë”ë§ ë©ë‹ˆë‹¤. ì´ëŠ” në¹„íŠ¸ë¥¼ ì‚¬ìš©í•˜ì—¬ \\((2^n)\\)ê°œì˜ ë‹¤ë¥¸ ìƒíƒœë‚˜ ì¡°í•©ì„ ë‚˜íƒ€ë‚¼ ìˆ˜ ìˆìœ¼ë©°, ì´ëŠ” í…ì„œ ê°’ì´ ê·¸ë£¹í™”ë  ìˆ˜ ìˆëŠ” \\((2^n)\\)ê°œì˜ ê³ ìœ í•œ í´ëŸ¬ìŠ¤í„°ì— í•´ë‹¹í•©ë‹ˆë‹¤."
  },
  {
    "objectID": "posts/labs/lab02.html#k-means-quantization-on-whole-model",
    "href": "posts/labs/lab02.html#k-means-quantization-on-whole-model",
    "title": "ğŸ‘©â€ğŸ’» Lab 2",
    "section": "K-Means Quantization on Whole Model",
    "text": "K-Means Quantization on Whole Model\nlab 1ì—ì„œ í–ˆë˜ ê²ƒê³¼ ìœ ì‚¬í•˜ê²Œ, ì´ì œ ì „ì²´ ëª¨ë¸ì„ quantizingí•˜ê¸° ìœ„í•´ k-means quantization í•¨ìˆ˜ë¥¼ í´ë˜ìŠ¤ë¡œ ë˜í•‘í•©ë‹ˆë‹¤. KMeansQuantizer í´ë˜ìŠ¤ì—ì„œëŠ” ëª¨ë¸ ê°€ì¤‘ì¹˜ê°€ ë³€ê²½ë  ë•Œë§ˆë‹¤ codebooks(i.e., centroidsì™€ labels)ì„ ì ìš©í•˜ê±°ë‚˜ ì—…ë°ì´íŠ¸í•  ìˆ˜ ìˆë„ë¡ codebooksì˜ ë³€í™”ë¥¼ ê¸°ë¡í•´ì•¼ í•©ë‹ˆë‹¤.\n\nfrom torch.nn import parameter\nclass KMeansQuantizer:\n    def __init__(self, model : nn.Module, bitwidth=4):\n        self.codebook = KMeansQuantizer.quantize(model, bitwidth)\n\n    @torch.no_grad()\n    def apply(self, model, update_centroids):\n        for name, param in model.named_parameters():\n            if name in self.codebook:\n                if update_centroids:\n                    update_codebook(param, codebook=self.codebook[name])\n                self.codebook[name] = k_means_quantize(\n                    param, codebook=self.codebook[name])\n\n    @staticmethod\n    @torch.no_grad()\n    def quantize(model: nn.Module, bitwidth=4):\n        codebook = dict()\n        if isinstance(bitwidth, dict):\n            for name, param in model.named_parameters():\n                if name in bitwidth:\n                    codebook[name] = k_means_quantize(param, bitwidth=bitwidth[name])\n        else:\n            for name, param in model.named_parameters():\n                if param.dim() &gt; 1:\n                    codebook[name] = k_means_quantize(param, bitwidth=bitwidth)\n        return codebook\n\nì´ì œ K-Means Quantizationì„ ì‚¬ìš©í•˜ì—¬ ëª¨ë¸ì„ 8ë¹„íŠ¸, 4ë¹„íŠ¸, 2ë¹„íŠ¸ë¡œ quantizeí•´ë´…ì‹œë‹¤. ëª¨ë¸ í¬ê¸°ë¥¼ ê³„ì‚°í•  ë•Œ codebooksì˜ ì €ì¥ ê³µê°„ì€ ë¬´ì‹œí•œë‹¤ëŠ” ì ì„ ìœ ì˜í•˜ì„¸ìš”.\n\nprint('Note that the storage for codebooks is ignored when calculating the model size.')\nquantizers = dict()\nfor bitwidth in [8, 4, 2]:\n    recover_model()\n    print(f'k-means quantizing model into {bitwidth} bits')\n    quantizer = KMeansQuantizer(model, bitwidth)\n    quantized_model_size = get_model_size(model, bitwidth)\n    print(f\"    {bitwidth}-bit k-means quantized model has size={quantized_model_size/MiB:.2f} MiB\")\n    quantized_model_accuracy = evaluate(model, dataloader['test'])\n    print(f\"    {bitwidth}-bit k-means quantized model has accuracy={quantized_model_accuracy:.2f}%\")\n    quantizers[bitwidth] = quantizer\n\nNote that the storage for codebooks is ignored when calculating the model size.\nk-means quantizing model into 8 bits\n    8-bit k-means quantized model has size=8.80 MiB\n    8-bit k-means quantized model has accuracy=92.76%\nk-means quantizing model into 4 bits\n    4-bit k-means quantized model has size=4.40 MiB\n    4-bit k-means quantized model has accuracy=79.07%\nk-means quantizing model into 2 bits\n    2-bit k-means quantized model has size=2.20 MiB\n    2-bit k-means quantized model has accuracy=10.00%"
  },
  {
    "objectID": "posts/labs/lab02.html#trained-k-means-quantization",
    "href": "posts/labs/lab02.html#trained-k-means-quantization",
    "title": "ğŸ‘©â€ğŸ’» Lab 2",
    "section": "Trained K-Means Quantization",
    "text": "Trained K-Means Quantization\në§ˆì§€ë§‰ ì…€ì˜ ê²°ê³¼ì—ì„œ ë³¼ ìˆ˜ ìˆë“¯ì´, ëª¨ë¸ì„ ì ì€ ë¹„íŠ¸ë¡œ quantizeí•  ë•Œ ì •í™•ë„ê°€ í¬ê²Œ ë–¨ì–´ì§‘ë‹ˆë‹¤. ë”°ë¼ì„œ, ì •í™•ë„ë¥¼ íšŒë³µí•˜ê¸° ìœ„í•´ quantization-aware trainingì„ í•´ì•¼ í•©ë‹ˆë‹¤.\nk-means quantization-aware í›ˆë ¨ ë™ì•ˆ, centroidsë„ ì—…ë°ì´íŠ¸ë©ë‹ˆë‹¤. ì´ëŠ” Deep Compression: Compressing Deep Neural Networks With Pruning, Trained Quantization And Huffman Codingì—ì„œ ì œì•ˆë˜ì—ˆìŠµë‹ˆë‹¤.\ncentroidsì— ëŒ€í•œ ê·¸ë˜ë””ì–¸íŠ¸ëŠ” ë‹¤ìŒê³¼ ê°™ì´ ê³„ì‚°ë©ë‹ˆë‹¤,\n\n\\(\\frac{\\partial \\mathcal{L} }{\\partial C_k} = \\sum_{j} \\frac{\\partial \\mathcal{L} }{\\partial W_{j}} \\frac{\\partial W_{j} }{\\partial C_k} = \\sum_{j} \\frac{\\partial \\mathcal{L} }{\\partial W_{j}} \\mathbf{1}(I_{j}=k)\\)\n\nì—¬ê¸°ì„œ \\(\\mathcal{L}\\)ì€ ì†ì‹¤, \\(C_k\\)ëŠ” k-ë²ˆì§¸ centroid, \\(I_{j}\\)ëŠ” ê°€ì¤‘ì¹˜ \\(W_{j}\\)ì˜ ë¼ë²¨ì…ë‹ˆë‹¤.\n\\(\\mathbf{1}()\\)ì€ ì§€ì‹œ í•¨ìˆ˜ì´ë©°, \\(\\mathbf{1}(I_{j}=k)\\)ëŠ” \\(1\\;\\mathrm{if}\\;I_{j}=k\\;\\mathrm{else}\\;0\\), ì¦‰, \\(I_{j}==k\\)ë¥¼ ì˜ë¯¸í•©ë‹ˆë‹¤.\nlabì—ì„œëŠ” ê°„ë‹¨íˆ ìµœì‹  ê°€ì¤‘ì¹˜ì— ë”°ë¼ centroidsë¥¼ ì§ì ‘ ì—…ë°ì´íŠ¸í•©ë‹ˆë‹¤:\n\n\\(C_k = \\frac{\\sum_{j}W_{j}\\mathbf{1}(I_{j}=k)}{\\sum_{j}\\mathbf{1}(I_{j}=k)}\\)\n\n\nQuestion 3 (10 pts)\nì•„ë˜ì˜ codebook update functionì„ ì™„ì„±í•˜ì„¸ìš”.\nHint:\nìœ„ì˜ centroidsë¥¼ ì—…ë°ì´íŠ¸í•˜ëŠ” ë°©ì •ì‹ì€ ì‹¤ì œë¡œ ë™ì¼í•œ í´ëŸ¬ìŠ¤í„°ì— ìˆëŠ” ê°€ì¤‘ì¹˜ì˜ í‰ê· (mean)ì„ ì—…ë°ì´íŠ¸ëœ centroid ê°’ìœ¼ë¡œ ì‚¬ìš©í•˜ê³  ìˆìŠµë‹ˆë‹¤.\n\ndef update_codebook(fp32_tensor: torch.Tensor, codebook: Codebook):\n    \"\"\"\n    update the centroids in the codebook using updated fp32_tensor\n    :param fp32_tensor: [torch.(cuda.)Tensor]\n    :param codebook: [Codebook] (the cluster centroids, the cluster label tensor)\n    \"\"\"\n    n_clusters = codebook.centroids.numel()\n    fp32_tensor = fp32_tensor.view(-1)\n    for k in range(n_clusters):\n    ############### YOUR CODE STARTS HERE ###############\n        codebook.centroids[k] = fp32_tensor[codebook.labels == k].mean()\n    ############### YOUR CODE ENDS HERE #################\n\nì´ì œ ë‹¤ìŒ ì½”ë“œ ì…€ì„ ì‹¤í–‰í•˜ì—¬ k-means quantized ëª¨ë¸ì„ finetuningí•˜ì—¬ ì •í™•ë„ë¥¼ íšŒë³µí•´ë´…ì‹œë‹¤. ì •í™•ë„ í•˜ë½ì´ 0.5ë³´ë‹¤ ì‘ìœ¼ë©´ finetuningì„ ì¤‘ë‹¨í•©ë‹ˆë‹¤.\n\naccuracy_drop_threshold = 0.5\nquantizers_before_finetune = copy.deepcopy(quantizers)\nquantizers_after_finetune = quantizers\n\nfor bitwidth in [8, 4, 2]:\n    recover_model()\n    quantizer = quantizers[bitwidth]\n    print(f'k-means quantizing model into {bitwidth} bits')\n    quantizer.apply(model, update_centroids=False)\n    quantized_model_size = get_model_size(model, bitwidth)\n    print(f\"    {bitwidth}-bit k-means quantized model has size={quantized_model_size/MiB:.2f} MiB\")\n    quantized_model_accuracy = evaluate(model, dataloader['test'])\n    print(f\"    {bitwidth}-bit k-means quantized model has accuracy={quantized_model_accuracy:.2f}% before quantization-aware training \")\n    accuracy_drop = fp32_model_accuracy - quantized_model_accuracy\n    if accuracy_drop &gt; accuracy_drop_threshold:\n        print(f\"        Quantization-aware training due to accuracy drop={accuracy_drop:.2f}% is larger than threshold={accuracy_drop_threshold:.2f}%\")\n        num_finetune_epochs = 5\n        optimizer = torch.optim.SGD(model.parameters(), lr=0.01, momentum=0.9)\n        scheduler = torch.optim.lr_scheduler.CosineAnnealingLR(optimizer, num_finetune_epochs)\n        criterion = nn.CrossEntropyLoss()\n        best_accuracy = 0\n        epoch = num_finetune_epochs\n        while accuracy_drop &gt; accuracy_drop_threshold and epoch &gt; 0:\n            train(model, dataloader['train'], criterion, optimizer, scheduler,\n                  callbacks=[lambda: quantizer.apply(model, update_centroids=True)])\n            model_accuracy = evaluate(model, dataloader['test'])\n            is_best = model_accuracy &gt; best_accuracy\n            best_accuracy = max(model_accuracy, best_accuracy)\n            print(f'        Epoch {num_finetune_epochs-epoch} Accuracy {model_accuracy:.2f}% / Best Accuracy: {best_accuracy:.2f}%')\n            accuracy_drop = fp32_model_accuracy - best_accuracy\n            epoch -= 1\n    else:\n        print(f\"        No need for quantization-aware training since accuracy drop={accuracy_drop:.2f}% is smaller than threshold={accuracy_drop_threshold:.2f}%\")\n\nk-means quantizing model into 8 bits\n    8-bit k-means quantized model has size=8.80 MiB\n    8-bit k-means quantized model has accuracy=92.76% before quantization-aware training \n        No need for quantization-aware training since accuracy drop=0.19% is smaller than threshold=0.50%\nk-means quantizing model into 4 bits\n    4-bit k-means quantized model has size=4.40 MiB\n    4-bit k-means quantized model has accuracy=79.07% before quantization-aware training \n        Quantization-aware training due to accuracy drop=13.88% is larger than threshold=0.50%\n        Epoch 0 Accuracy 92.47% / Best Accuracy: 92.47%\nk-means quantizing model into 2 bits\n    2-bit k-means quantized model has size=2.20 MiB\n    2-bit k-means quantized model has accuracy=10.00% before quantization-aware training \n        Quantization-aware training due to accuracy drop=82.95% is larger than threshold=0.50%\n        Epoch 0 Accuracy 90.21% / Best Accuracy: 90.21%\n        Epoch 1 Accuracy 90.82% / Best Accuracy: 90.82%\n        Epoch 2 Accuracy 91.00% / Best Accuracy: 91.00%\n        Epoch 3 Accuracy 91.12% / Best Accuracy: 91.12%\n        Epoch 4 Accuracy 91.17% / Best Accuracy: 91.17%"
  },
  {
    "objectID": "posts/labs/lab02.html#n-bit-integer",
    "href": "posts/labs/lab02.html#n-bit-integer",
    "title": "ğŸ‘©â€ğŸ’» Lab 2",
    "section": "n-bit Integer",
    "text": "n-bit Integer\nn-ë¹„íŠ¸ signed integerëŠ” ë³´í†µ twoâ€™s complement í‘œê¸°ë²•ìœ¼ë¡œ í‘œí˜„ë©ë‹ˆë‹¤.\nn-ë¹„íŠ¸ signed integerëŠ” ë²”ìœ„ \\([-2^{n-1}, 2^{n-1}-1]\\) ë‚´ì˜ ì •ìˆ˜ë¥¼ ì¸ì½”ë”©í•  ìˆ˜ ìˆìŠµë‹ˆë‹¤. ì˜ˆë¥¼ ë“¤ì–´, 8ë¹„íŠ¸ ì •ìˆ˜ëŠ” [-128, 127] ë²”ìœ„ì— ì†í•©ë‹ˆë‹¤.\n\ndef get_quantized_range(bitwidth):\n    quantized_max = (1 &lt;&lt; (bitwidth - 1)) - 1\n    quantized_min = -(1 &lt;&lt; (bitwidth - 1))\n    return quantized_min, quantized_max"
  },
  {
    "objectID": "posts/labs/lab02.html#question-4-15-pts",
    "href": "posts/labs/lab02.html#question-4-15-pts",
    "title": "ğŸ‘©â€ğŸ’» Lab 2",
    "section": "Question 4 (15 pts)",
    "text": "Question 4 (15 pts)\nì•„ë˜ì˜ linear quantization functionì„ ì™„ì„±í•˜ì„¸ìš”.\nHint:\n\n\\(r=S(q-Z)\\)ì—ì„œ, \\(q = r/S + Z\\)ìœ¼ë¡œ ë°”ê¿”ì„œ ë³¼ ìˆ˜ ìˆìŠµë‹ˆë‹¤.\n\\(r\\)ê³¼ \\(S\\) ëª¨ë‘ ë¶€ë™ ì†Œìˆ˜ì  ìˆ«ì(floating number)ì´ë¯€ë¡œ, ì •ìˆ˜ \\(Z\\)ë¥¼ ì§ì ‘ \\(r/S\\)ì— ë”í•  ìˆ˜ ì—†ìŠµë‹ˆë‹¤. ë”°ë¼ì„œ \\(q = \\mathrm{int}(\\mathrm{round}(r/S)) + Z\\)ì…ë‹ˆë‹¤.\ntorch.FloatTensorë¥¼ torch.IntTensorë¡œ ë³€í™˜í•˜ê¸° ìœ„í•´ì„œ, torch.round(), torch.Tensor.round(), torch.Tensor.round_()ì„ ì‚¬ìš©í•˜ì—¬ ëª¨ë“  ê°’ì„ ë¶€ë™ ì†Œìˆ˜ì  ì •ìˆ˜ë¡œ ë¨¼ì € ë³€í™˜í•©ë‹ˆë‹¤.\nê·¸ ë‹¤ìŒ torch.Tensor.to(torch.int8)ë¥¼ ì‚¬ìš©í•˜ì—¬ ë°ì´í„° íƒ€ì…ì„ torch.floatì—ì„œ torch.int8ë¡œ ë³€í™˜í•  ìˆ˜ ìˆìŠµë‹ˆë‹¤.\n\n\ndef linear_quantize(fp_tensor, bitwidth, scale, zero_point, dtype=torch.int8) -&gt; torch.Tensor:\n    \"\"\"\n    linear quantization for single fp_tensor\n      from\n        fp_tensor = (quantized_tensor - zero_point) * scale\n      we have,\n        quantized_tensor = int(round(fp_tensor / scale)) + zero_point\n    :param tensor: [torch.(cuda.)FloatTensor] floating tensor to be quantized\n    :param bitwidth: [int] quantization bit width\n    :param scale: [torch.(cuda.)FloatTensor] scaling factor\n    :param zero_point: [torch.(cuda.)IntTensor] the desired centroid of tensor values\n    :return:\n        [torch.(cuda.)FloatTensor] quantized tensor whose values are integers\n    \"\"\"\n    assert(fp_tensor.dtype == torch.float)\n    assert(isinstance(scale, float) or\n           (scale.dtype == torch.float and scale.dim() == fp_tensor.dim()))\n    assert(isinstance(zero_point, int) or\n           (zero_point.dtype == dtype and zero_point.dim() == fp_tensor.dim()))\n\n    ############### YOUR CODE STARTS HERE ###############\n    # Step 1: scale the fp_tensor\n    scaled_tensor = fp_tensor / scale\n    # Step 2: round the floating value to integer value\n    rounded_tensor = torch.round(scaled_tensor)\n    ############### YOUR CODE ENDS HERE #################\n\n    rounded_tensor = rounded_tensor.to(dtype)\n\n    ############### YOUR CODE STARTS HERE ###############\n    # Step 3: shift the rounded_tensor to make zero_point 0\n    shifted_tensor = rounded_tensor + zero_point\n    ############### YOUR CODE ENDS HERE #################\n\n    # Step 4: clamp the shifted_tensor to lie in bitwidth-bit range\n    quantized_min, quantized_max = get_quantized_range(bitwidth)\n    quantized_tensor = shifted_tensor.clamp_(quantized_min, quantized_max)\n    return quantized_tensor\n\nìœ„ì—ì„œ ì‘ì„±í•œ linear quantization ê¸°ëŠ¥ì„ ë”ë¯¸ í…ì„œì— ì ìš©í•˜ì—¬ ê¸°ëŠ¥ì„ ê²€ì¦í•´ë´…ì‹œë‹¤.\n\ntest_linear_quantize()\n\n* Test linear_quantize()\n    target bitwidth: 2 bits\n        scale: 0.3333333333333333\n        zero point: -1\n* Test passed."
  },
  {
    "objectID": "posts/labs/lab02.html#question-5-10-pts",
    "href": "posts/labs/lab02.html#question-5-10-pts",
    "title": "ğŸ‘©â€ğŸ’» Lab 2",
    "section": "Question 5 (10 pts)",
    "text": "Question 5 (10 pts)\nì´ì œ linear quantizationì„ ìœ„í•œ ìŠ¤ì¼€ì¼ë§ ì¸ì \\(S\\)ì™€ ì œë¡œ í¬ì¸íŠ¸ \\(Z\\)ë¥¼ ê²°ì •í•´ì•¼ í•©ë‹ˆë‹¤.\nlinear quantizationì€ \\(r = S(q-Z)\\) ë¡œ í‘œí˜„ë  ìˆ˜ ìˆë‹¤ëŠ” ê²ƒì„ ê¸°ì–µí•˜ì„¸ìš”.\n\nScale\nLinear quantizationì€ ë¶€ë™ ì†Œìˆ˜ì  ë²”ìœ„ [fp_min, fp_max]ë¥¼ ì–‘ìí™”ëœ ë²”ìœ„ [quantized_min, quantized_max]ë¡œ íˆ¬ì˜(projection)í•©ë‹ˆë‹¤. ì¦‰,\n\n\\(r_{\\mathrm{max}} = S(q_{\\mathrm{max}}-Z)\\)\n\\(r_{\\mathrm{min}} = S(q_{\\mathrm{min}}-Z)\\)\n\nì´ ë‘ ë°©ì •ì‹ì„ ë¹¼ë©´, ìš°ë¦¬ëŠ” ë‹¤ìŒì„ ì–»ìŠµë‹ˆë‹¤,\n\nQuestion 5.1 (1 pts)\në‹¤ìŒ í…ìŠ¤íŠ¸ ì…€ì—ì„œ ì˜¬ë°”ë¥¸ ë‹µì„ ì„ íƒí•˜ê³  ì˜ëª»ëœ ë‹µì„ ì‚­ì œí•´ì£¼ì„¸ìš”.\n\n\\(S=r_{\\mathrm{max}} / q_{\\mathrm{max}}\\)\n\n\n\\(S=(r_{\\mathrm{max}} + r_{\\mathrm{min}}) / (q_{\\mathrm{max}} + q_{\\mathrm{min}})\\)\n\n\nâœ…\\(S=(r_{\\mathrm{max}} - r_{\\mathrm{min}}) / (q_{\\mathrm{max}} - q_{\\mathrm{min}})\\)\n\n\n\\(S=r_{\\mathrm{max}} / q_{\\mathrm{max}} - r_{\\mathrm{min}} / q_{\\mathrm{min}}\\)\n\nfp_tensorì˜ \\(r_{\\mathrm{min}}\\)ê³¼ \\(r_{\\mathrm{max}}\\)ë¥¼ ê²°ì •í•˜ëŠ” ë‹¤ì–‘í•œ ë°©ë²•ì´ ìˆìŠµë‹ˆë‹¤.\n\nê°€ì¥ í”í•œ ë°©ë²•ì€ fp_tensorì˜ ìµœì†Œê°’ê³¼ ìµœëŒ€ê°’ì„ ì‚¬ìš©í•˜ëŠ” ê²ƒì…ë‹ˆë‹¤.\në˜ ë‹¤ë¥¸ ë„ë¦¬ ì‚¬ìš©ë˜ëŠ” ë°©ë²•ì€ Kullback-Leibler-J ë°œì‚°ì„ ìµœì†Œí™”í•˜ì—¬ fp_maxë¥¼ ê²°ì •í•˜ëŠ” ê²ƒì…ë‹ˆë‹¤.\n\n\n\n\nzero point\nìŠ¤ì¼€ì¼ë§ ì¸ì \\(S\\)ë¥¼ ê²°ì •í•˜ë©´, \\(r_{\\mathrm{min}}\\)ê³¼ \\(q_{\\mathrm{min}}\\) ì‚¬ì´ì˜ ê´€ê³„ë¥¼ ì‚¬ìš©í•˜ì—¬ ì œë¡œ í¬ì¸íŠ¸ \\(Z\\)ë¥¼ ê³„ì‚°í•  ìˆ˜ ìˆìŠµë‹ˆë‹¤.\n\nQuestion 5.2 (1 pts)\në‹¤ìŒ í…ìŠ¤íŠ¸ ì…€ì—ì„œ ì˜¬ë°”ë¥¸ ë‹µì„ ì„ íƒí•˜ê³  ì˜ëª»ëœ ë‹µì„ ì‚­ì œí•´ì£¼ì„¸ìš”.\n\n\\(Z = \\mathrm{int}(\\mathrm{round}(r_{\\mathrm{min}} / S - q_{\\mathrm{min}})\\)\n\n\n\\(Z = \\mathrm{int}(\\mathrm{round}(q_{\\mathrm{min}} - r_{\\mathrm{min}} / S))\\)\n\n\nâœ…\\(Z = q_{\\mathrm{min}} - r_{\\mathrm{min}} / S\\)\n\n\n\\(Z = r_{\\mathrm{min}} / S - q_{\\mathrm{min}}\\)\n\n\n\n\nQuestion 5.3 (8 pts)\nfloating point tensor \\(r\\)ë¡œë¶€í„° scale \\(S\\)ì™€ zero point \\(Z\\)ë¥¼ ê³„ì‚°í•˜ëŠ” ì•„ë˜ì˜ í•¨ìˆ˜ë¥¼ ì™„ì„±í•˜ì„¸ìš”.\n\ndef get_quantization_scale_and_zero_point(fp_tensor, bitwidth):\n    \"\"\"\n    get quantization scale for single tensor\n    :param fp_tensor: [torch.(cuda.)Tensor] floating tensor to be quantized\n    :param bitwidth: [int] quantization bit width\n    :return:\n        [float] scale\n        [int] zero_point\n    \"\"\"\n    quantized_min, quantized_max = get_quantized_range(bitwidth)\n    fp_max = fp_tensor.max().item()\n    fp_min = fp_tensor.min().item()\n\n    ############### YOUR CODE STARTS HERE ###############\n    # Calculate scale\n    scale = (fp_max - fp_min) / (quantized_max - quantized_min)\n    # Calculate zero_point\n    zero_point = quantized_min - round(fp_min / scale)\n    ############### YOUR CODE ENDS HERE #################\n\n    # clip the zero_point to fall in [quantized_min, quantized_max]\n    if zero_point &lt; quantized_min:\n        zero_point = quantized_min\n    elif zero_point &gt; quantized_max:\n        zero_point = quantized_max\n    else: # convert from float to int using round()\n        zero_point = round(zero_point)\n    return scale, int(zero_point)\n\nì´ì œ Question 4ì˜ linear_quantize()ì™€ Question 5ì˜ get_quantization_scale_and_zero_point()ì„ í•˜ë‚˜ì˜ í•¨ìˆ˜ë¡œ ë˜í•‘í•©ë‹ˆë‹¤.\n\ndef linear_quantize_feature(fp_tensor, bitwidth):\n    \"\"\"\n    linear quantization for feature tensor\n    :param fp_tensor: [torch.(cuda.)Tensor] floating feature to be quantized\n    :param bitwidth: [int] quantization bit width\n    :return:\n        [torch.(cuda.)Tensor] quantized tensor\n        [float] scale tensor\n        [int] zero point\n    \"\"\"\n    scale, zero_point = get_quantization_scale_and_zero_point(fp_tensor, bitwidth)\n    quantized_tensor = linear_quantize(fp_tensor, bitwidth, scale, zero_point)\n    return quantized_tensor, scale, zero_point"
  },
  {
    "objectID": "posts/labs/lab02.html#special-case-linear-quantization-on-weight-tensor",
    "href": "posts/labs/lab02.html#special-case-linear-quantization-on-weight-tensor",
    "title": "ğŸ‘©â€ğŸ’» Lab 2",
    "section": "Special case: linear quantization on weight tensor",
    "text": "Special case: linear quantization on weight tensor\në¨¼ì € ê°€ì¤‘ì¹˜ ê°’ì˜ ë¶„í¬ë¥¼ ì‚´í´ë´…ì‹œë‹¤.\n\ndef plot_weight_distribution(model, bitwidth=32):\n    # bins = (1 &lt;&lt; bitwidth) if bitwidth &lt;= 8 else 256\n    if bitwidth &lt;= 8:\n        qmin, qmax = get_quantized_range(bitwidth)\n        bins = np.arange(qmin, qmax + 2)\n        align = 'left'\n    else:\n        bins = 256\n        align = 'mid'\n    fig, axes = plt.subplots(3,3, figsize=(10, 6))\n    axes = axes.ravel()\n    plot_index = 0\n    for name, param in model.named_parameters():\n        if param.dim() &gt; 1:\n            ax = axes[plot_index]\n            ax.hist(param.detach().view(-1).cpu(), bins=bins, density=True,\n                    align=align, color = 'blue', alpha = 0.5,\n                    edgecolor='black' if bitwidth &lt;= 4 else None)\n            if bitwidth &lt;= 4:\n                quantized_min, quantized_max = get_quantized_range(bitwidth)\n                ax.set_xticks(np.arange(start=quantized_min, stop=quantized_max+1))\n            ax.set_xlabel(name)\n            ax.set_ylabel('density')\n            plot_index += 1\n    fig.suptitle(f'Histogram of Weights (bitwidth={bitwidth} bits)')\n    fig.tight_layout()\n    fig.subplots_adjust(top=0.925)\n    plt.show()\n\nrecover_model()\nplot_weight_distribution(model)\n\n\n\n\n\n\n\n\nìœ„ì˜ íˆìŠ¤í† ê·¸ë¨ì—ì„œ ë³¼ ìˆ˜ ìˆë“¯ì´, ê°€ì¤‘ì¹˜ ê°’ì˜ ë¶„í¬ëŠ” (ì´ ê²½ìš°ì—ëŠ” classifierë¥¼ ì œì™¸í•˜ê³ ) ê±°ì˜ 0ì„ ì¤‘ì‹¬ìœ¼ë¡œ ëŒ€ì¹­ì ì…ë‹ˆë‹¤ . ë”°ë¼ì„œ ê°€ì¤‘ì¹˜ë¥¼ ì–‘ìí™”í•  ë•Œ ë³´í†µ ì œë¡œ í¬ì¸íŠ¸ \\(Z=0\\)ìœ¼ë¡œ ì„¤ì •í•©ë‹ˆë‹¤.\n\\(r = S(q-Z)\\)ì—ì„œ,\n\n\\(r_{\\mathrm{max}} = S \\cdot q_{\\mathrm{max}}\\)\n\n\n\\(S = r_{\\mathrm{max}} / q_{\\mathrm{max}}\\)\n\nê°€ì¤‘ì¹˜ ê°’ì˜ ìµœëŒ€ ì ˆëŒ“ê°’ì„ \\(r_{\\mathrm{max}}\\)ë¡œ ì´ìš©í•©ë‹ˆë‹¤.\n\ndef get_quantization_scale_for_weight(weight, bitwidth):\n    \"\"\"\n    get quantization scale for single tensor of weight\n    :param weight: [torch.(cuda.)Tensor] floating weight to be quantized\n    :param bitwidth: [integer] quantization bit width\n    :return:\n        [floating scalar] scale\n    \"\"\"\n    # we just assume values in weight are symmetric\n    # we also always make zero_point 0 for weight\n    fp_max = max(weight.abs().max().item(), 5e-7)\n    _, quantized_max = get_quantized_range(bitwidth)\n    return fp_max / quantized_max\n\n\nPer-channel Linear Quantization\n2D convolutionì˜ ê²½ìš°, ê°€ì¤‘ì¹˜ í…ì„œëŠ” (num_output_channels, num_input_channels, kernel_height, kernel_width) ëª¨ì–‘ì˜ 4ì°¨ì› í…ì„œì…ë‹ˆë‹¤.\në§ì€ ì‹¤í—˜ë“¤ì„ í†µí•´, ì„œë¡œ ë‹¤ë¥¸ ì¶œë ¥ ì±„ë„ì— ëŒ€í•´ ì„œë¡œ ë‹¤ë¥¸ ìŠ¤ì¼€ì¼ë§ ì¸ì \\(S\\)ì™€ ì œë¡œ í¬ì¸íŠ¸ \\(Z\\)ë¥¼ ì‚¬ìš©í•˜ëŠ” ê²ƒì´ ë” ë‚˜ì€ ì„±ëŠ¥ì„ ë°œíœ˜í•œë‹¤ëŠ” ê²ƒì„ ì•Œ ìˆ˜ ìˆì—ˆìŠµë‹ˆë‹¤. ë”°ë¼ì„œ ê° ì¶œë ¥ ì±„ë„ì˜ ì„œë¸Œí…ì„œì— ëŒ€í•œ ìŠ¤ì¼€ì¼ë§ ì¸ì \\(S\\)ì™€ ì œë¡œ í¬ì¸íŠ¸ \\(Z\\)ë¥¼ ë…ë¦½ì ìœ¼ë¡œ ì •í•´ì•¼ í•©ë‹ˆë‹¤.\n\ndef linear_quantize_weight_per_channel(tensor, bitwidth):\n    \"\"\"\n    linear quantization for weight tensor\n        using different scales and zero_points for different output channels\n    :param tensor: [torch.(cuda.)Tensor] floating weight to be quantized\n    :param bitwidth: [int] quantization bit width\n    :return:\n        [torch.(cuda.)Tensor] quantized tensor\n        [torch.(cuda.)Tensor] scale tensor\n        [int] zero point (which is always 0)\n    \"\"\"\n    dim_output_channels = 0\n    num_output_channels = tensor.shape[dim_output_channels]\n    scale = torch.zeros(num_output_channels, device=tensor.device)\n    for oc in range(num_output_channels):\n        _subtensor = tensor.select(dim_output_channels, oc)\n        _scale = get_quantization_scale_for_weight(_subtensor, bitwidth)\n        scale[oc] = _scale\n    scale_shape = [1] * tensor.dim()\n    scale_shape[dim_output_channels] = -1\n    scale = scale.view(scale_shape)\n    quantized_tensor = linear_quantize(tensor, bitwidth, scale, zero_point=0)\n    return quantized_tensor, scale, 0\n\n\n\nA Quick Peek at Linear Quantization on Weights\nì´ì œ ê°€ì¤‘ì¹˜ì— ëŒ€í•´ linear quantizationë¥¼ ì ìš©í•  ë•Œ ê°€ì¤‘ì¹˜ ë¶„í¬ì™€ ëª¨ë¸ í¬ê¸°ë¥¼ ì„œë¡œ ë‹¤ë¥¸ bitwidthsë¡œ ì‚´í´ë³´ê² ìŠµë‹ˆë‹¤.\n\n@torch.no_grad()\ndef peek_linear_quantization():\n    for bitwidth in [4, 2]:\n        for name, param in model.named_parameters():\n            if param.dim() &gt; 1:\n                quantized_param, scale, zero_point = \\\n                    linear_quantize_weight_per_channel(param, bitwidth)\n                param.copy_(quantized_param)\n        plot_weight_distribution(model, bitwidth)\n        recover_model()\n\npeek_linear_quantization()"
  },
  {
    "objectID": "posts/labs/lab02.html#quantized-inference",
    "href": "posts/labs/lab02.html#quantized-inference",
    "title": "ğŸ‘©â€ğŸ’» Lab 2",
    "section": "Quantized Inference",
    "text": "Quantized Inference\nì–‘ìí™” í›„, convolution ë° fully-connected layerì˜ ì¶”ë¡ ë„ ë³€ê²½ë©ë‹ˆë‹¤.\n\\(r = S(q-Z)\\)ë¥¼ ìƒê¸°í•´ ë³´ë©´, ë‹¤ìŒê³¼ ê°™ìŠµë‹ˆë‹¤.\n\n\\(r_{\\mathrm{input}} = S_{\\mathrm{input}}(q_{\\mathrm{input}}-Z_{\\mathrm{input}})\\)\n\\(r_{\\mathrm{weight}} = S_{\\mathrm{weight}}(q_{\\mathrm{weight}}-Z_{\\mathrm{weight}})\\)\n\\(r_{\\mathrm{bias}} = S_{\\mathrm{bias}}(q_{\\mathrm{bias}}-Z_{\\mathrm{bias}})\\)\n\n\\(Z_{\\mathrm{weight}}=0\\)ì´ë¯€ë¡œ, \\(r_{\\mathrm{weight}} = S_{\\mathrm{weight}}q_{\\mathrm{weight}}\\)ì…ë‹ˆë‹¤.\në¶€ë™ ì†Œìˆ˜ì  convolutionì€ ë‹¤ìŒê³¼ ê°™ì´ ì‘ì„±í•  ìˆ˜ ìˆìŠµë‹ˆë‹¤.\n\n\\(r_{\\mathrm{output}} = \\mathrm{CONV}[r_{\\mathrm{input}}, r_{\\mathrm{weight}}] + r_{\\mathrm{bias}}\\) \\(\\;\\;\\;\\;\\;\\;\\;\\;= \\mathrm{CONV}[S_{\\mathrm{input}}(q_{\\mathrm{input}}-Z_{\\mathrm{input}}), S_{\\mathrm{weight}}q_{\\mathrm{weight}}] + S_{\\mathrm{bias}}(q_{\\mathrm{bias}}-Z_{\\mathrm{bias}})\\) \\(\\;\\;\\;\\;\\;\\;\\;\\;= \\mathrm{CONV}[q_{\\mathrm{input}}-Z_{\\mathrm{input}}, q_{\\mathrm{weight}}]\\cdot (S_{\\mathrm{input}} \\cdot S_{\\mathrm{weight}}) + S_{\\mathrm{bias}}(q_{\\mathrm{bias}}-Z_{\\mathrm{bias}})\\)\n\nê³„ì‚°ì„ ë” ê°„ë‹¨í•˜ê²Œ í•˜ê¸° ìœ„í•´\n\n\\(Z_{\\mathrm{bias}} = 0\\)\n\\(S_{\\mathrm{bias}} = S_{\\mathrm{input}} \\cdot S_{\\mathrm{weight}}\\)\n\në¡œ ì„¤ì •í•˜ì—¬,\n\n\\(r_{\\mathrm{output}} = (\\mathrm{CONV}[q_{\\mathrm{input}}-Z_{\\mathrm{input}}, q_{\\mathrm{weight}}] + q_{\\mathrm{bias}})\\cdot (S_{\\mathrm{input}} \\cdot S_{\\mathrm{weight}})\\) \\(\\;\\;\\;\\;\\;\\;\\;\\;= (\\mathrm{CONV}[q_{\\mathrm{input}}, q_{\\mathrm{weight}}] - \\mathrm{CONV}[Z_{\\mathrm{input}}, q_{\\mathrm{weight}}] + q_{\\mathrm{bias}})\\cdot (S_{\\mathrm{input}}S_{\\mathrm{weight}})\\)\n\nì´ë©°,\n\n\\(r_{\\mathrm{output}} = S_{\\mathrm{output}}(q_{\\mathrm{output}}-Z_{\\mathrm{output}})\\)\n\nì´ë¯€ë¡œ\n\n\\(S_{\\mathrm{output}}(q_{\\mathrm{output}}-Z_{\\mathrm{output}}) = (\\mathrm{CONV}[q_{\\mathrm{input}}, q_{\\mathrm{weight}}] - \\mathrm{CONV}[Z_{\\mathrm{input}}, q_{\\mathrm{weight}}] + q_{\\mathrm{bias}})\\cdot (S_{\\mathrm{input}} S_{\\mathrm{weight}})\\)\n\në”°ë¼ì„œ\n\n\\(q_{\\mathrm{output}} = (\\mathrm{CONV}[q_{\\mathrm{input}}, q_{\\mathrm{weight}}] - \\mathrm{CONV}[Z_{\\mathrm{input}}, q_{\\mathrm{weight}}] + q_{\\mathrm{bias}})\\cdot (S_{\\mathrm{input}}S_{\\mathrm{weight}} / S_{\\mathrm{output}}) + Z_{\\mathrm{output}}\\)\n\n\\(Z_{\\mathrm{input}}\\), \\(q_{\\mathrm{weight}}\\), \\(q_{\\mathrm{bias}}\\)ëŠ” ì¶”ë¡  ì „ì— ê²°ì •ë˜ë¯€ë¡œ,\n\n\\(Q_{\\mathrm{bias}} = q_{\\mathrm{bias}} - \\mathrm{CONV}[Z_{\\mathrm{input}}, q_{\\mathrm{weight}}]\\)\n\në¡œ ì„¤ì •í•˜ë©´,\n\n\\(q_{\\mathrm{output}} = (\\mathrm{Linear}[q_{\\mathrm{input}}, q_{\\mathrm{weight}}] + Q_{\\mathrm{bias}})\\cdot (S_{\\mathrm{input}} \\cdot S_{\\mathrm{weight}} / S_{\\mathrm{output}}) + Z_{\\mathrm{output}}\\)\n\n\nQuestion 6 (5 pts)\nbiasë¥¼ linear quantizingí•˜ëŠ” í•¨ìˆ˜ë¥¼ ì™„ì„±í•˜ì„¸ìš”.\nHint:\nìœ„ì˜ ì¶”ë¡ ê³¼ì •ì—ì„œ ì•„ë˜ì™€ ê°™ì€ ìˆ˜ì‹ì„ ì–»ì—ˆìŠµë‹ˆë‹¤.\n\n\\(Z_{\\mathrm{bias}} = 0\\)\n\\(S_{\\mathrm{bias}} = S_{\\mathrm{input}} \\cdot S_{\\mathrm{weight}}\\)\n\n\ndef linear_quantize_bias_per_output_channel(bias, weight_scale, input_scale):\n    \"\"\"\n    linear quantization for single bias tensor\n        quantized_bias = fp_bias / bias_scale\n    :param bias: [torch.FloatTensor] bias weight to be quantized\n    :param weight_scale: [float or torch.FloatTensor] weight scale tensor\n    :param input_scale: [float] input scale\n    :return:\n        [torch.IntTensor] quantized bias tensor\n    \"\"\"\n    assert(bias.dim() == 1)\n    assert(bias.dtype == torch.float)\n    assert(isinstance(input_scale, float))\n    if isinstance(weight_scale, torch.Tensor):\n        assert(weight_scale.dtype == torch.float)\n        weight_scale = weight_scale.view(-1)\n        assert(bias.numel() == weight_scale.numel())\n\n    ############### YOUR CODE STARTS HERE ###############\n    bias_scale = weight_scale * input_scale\n    ############### YOUR CODE ENDS HERE #################\n\n    quantized_bias = linear_quantize(bias, 32, bias_scale,\n                                     zero_point=0, dtype=torch.int32)\n    return quantized_bias, bias_scale, 0\n\n\n\nQuantized Fully-Connected Layer\nì–‘ìí™”ëœ fully-connected layerì˜ ê²½ìš°, \\(Q_{\\mathrm{bias}}\\)ë¥¼ ë¨¼ì € ê³„ì‚°í•©ë‹ˆë‹¤. \\(Q_{\\mathrm{bias}} = q_{\\mathrm{bias}} - \\mathrm{Linear}[Z_{\\mathrm{input}}, q_{\\mathrm{weight}}]\\)ë¥¼ ê¸°ì–µí•˜ì„¸ìš”.\n\ndef shift_quantized_linear_bias(quantized_bias, quantized_weight, input_zero_point):\n    \"\"\"\n    shift quantized bias to incorporate input_zero_point for nn.Linear\n        shifted_quantized_bias = quantized_bias - Linear(input_zero_point, quantized_weight)\n    :param quantized_bias: [torch.IntTensor] quantized bias (torch.int32)\n    :param quantized_weight: [torch.CharTensor] quantized weight (torch.int8)\n    :param input_zero_point: [int] input zero point\n    :return:\n        [torch.IntTensor] shifted quantized bias tensor\n    \"\"\"\n    assert(quantized_bias.dtype == torch.int32)\n    assert(isinstance(input_zero_point, int))\n    return quantized_bias - quantized_weight.sum(1).to(torch.int32) * input_zero_point\n\n\nQuestion 7 (15 pts)\nì•„ë˜ì˜ ì–‘ìí™”ëœ fully-connected layer inference functionë¥¼ ì™„ì„±í•˜ì„¸ìš”.\nHint:\n\n\\(q_{\\mathrm{output}} = (\\mathrm{Linear}[q_{\\mathrm{input}}, q_{\\mathrm{weight}}] + Q_{\\mathrm{bias}})\\cdot (S_{\\mathrm{input}} S_{\\mathrm{weight}} / S_{\\mathrm{output}}) + Z_{\\mathrm{output}}\\)\n\n\ndef quantized_linear(input, weight, bias, feature_bitwidth, weight_bitwidth,\n                     input_zero_point, output_zero_point,\n                     input_scale, weight_scale, output_scale):\n    \"\"\"\n    quantized fully-connected layer\n    :param input: [torch.CharTensor] quantized input (torch.int8)\n    :param weight: [torch.CharTensor] quantized weight (torch.int8)\n    :param bias: [torch.IntTensor] shifted quantized bias or None (torch.int32)\n    :param feature_bitwidth: [int] quantization bit width of input and output\n    :param weight_bitwidth: [int] quantization bit width of weight\n    :param input_zero_point: [int] input zero point\n    :param output_zero_point: [int] output zero point\n    :param input_scale: [float] input feature scale\n    :param weight_scale: [torch.FloatTensor] weight per-channel scale\n    :param output_scale: [float] output feature scale\n    :return:\n        [torch.CharIntTensor] quantized output feature (torch.int8)\n    \"\"\"\n    assert(input.dtype == torch.int8)\n    assert(weight.dtype == input.dtype)\n    assert(bias is None or bias.dtype == torch.int32)\n    assert(isinstance(input_zero_point, int))\n    assert(isinstance(output_zero_point, int))\n    assert(isinstance(input_scale, float))\n    assert(isinstance(output_scale, float))\n    assert(weight_scale.dtype == torch.float)\n\n    # Step 1: integer-based fully-connected (8-bit multiplication with 32-bit accumulation)\n    if 'cpu' in input.device.type:\n        # use 32-b MAC for simplicity\n        output = torch.nn.functional.linear(input.to(torch.int32), weight.to(torch.int32), bias)\n    else:\n        # current version pytorch does not yet support integer-based linear() on GPUs\n        output = torch.nn.functional.linear(input.float(), weight.float(), bias.float())\n\n    ############### YOUR CODE STARTS HERE ###############\n    # Step 2: scale the output\n    #         hint: 1. scales are floating numbers, we need to convert output to float as well\n    #               2. the shape of weight scale is [oc, 1, 1, 1] while the shape of output is [batch_size, oc]\n    real_scale = input_scale * weight_scale.view(-1) / output_scale\n    output = output.float() * real_scale\n\n    # Step 3: Shift output by output_zero_point\n    output += output_zero_point\n    ############### YOUR CODE STARTS HERE ###############\n\n    # Make sure all value lies in the bitwidth-bit range\n    output = output.round().clamp(*get_quantized_range(feature_bitwidth)).to(torch.int8)\n    return output\n\nLetâ€™s verify the functionality of defined quantized fully connected layer.\n\ntest_quantized_fc()\n\n* Test quantized_fc()\n    target bitwidth: 2 bits\n      batch size: 4\n      input channels: 8\n      output channels: 8\n* Test passed.\n\n\n\n\n\n\n\n\n\n\n\n\nQuantized Convolution\nì–‘ìí™”ëœ ì»¨ë³¼ë£¨ì…˜ ë ˆì´ì–´ì˜ ê²½ìš°, ë¨¼ì € \\(Q_{\\mathrm{bias}}\\)ë¥¼ ê³„ì‚°í•©ë‹ˆë‹¤. \\(Q_{\\mathrm{bias}} = q_{\\mathrm{bias}} - \\mathrm{CONV}[Z_{\\mathrm{input}}, q_{\\mathrm{weight}}]\\)ë¥¼ ê¸°ì–µí•˜ì„¸ìš”.\n\ndef shift_quantized_conv2d_bias(quantized_bias, quantized_weight, input_zero_point):\n    \"\"\"\n    shift quantized bias to incorporate input_zero_point for nn.Conv2d\n        shifted_quantized_bias = quantized_bias - Conv(input_zero_point, quantized_weight)\n    :param quantized_bias: [torch.IntTensor] quantized bias (torch.int32)\n    :param quantized_weight: [torch.CharTensor] quantized weight (torch.int8)\n    :param input_zero_point: [int] input zero point\n    :return:\n        [torch.IntTensor] shifted quantized bias tensor\n    \"\"\"\n    assert(quantized_bias.dtype == torch.int32)\n    assert(isinstance(input_zero_point, int))\n    return quantized_bias - quantized_weight.sum((1,2,3)).to(torch.int32) * input_zero_point\n\n\nQuestion 8 (15 pts)\nì•„ë˜ì˜ quantized convolution functionì„ ì™„ì„±í•˜ì„¸ìš”.\nHint: &gt; \\(q_{\\mathrm{output}} = (\\mathrm{CONV}[q_{\\mathrm{input}}, q_{\\mathrm{weight}}] + Q_{\\mathrm{bias}}) \\cdot (S_{\\mathrm{input}}S_{\\mathrm{weight}} / S_{\\mathrm{output}}) + Z_{\\mathrm{output}}\\)\n\ndef quantized_conv2d(input, weight, bias, feature_bitwidth, weight_bitwidth,\n                     input_zero_point, output_zero_point,\n                     input_scale, weight_scale, output_scale,\n                     stride, padding, dilation, groups):\n    \"\"\"\n    quantized 2d convolution\n    :param input: [torch.CharTensor] quantized input (torch.int8)\n    :param weight: [torch.CharTensor] quantized weight (torch.int8)\n    :param bias: [torch.IntTensor] shifted quantized bias or None (torch.int32)\n    :param feature_bitwidth: [int] quantization bit width of input and output\n    :param weight_bitwidth: [int] quantization bit width of weight\n    :param input_zero_point: [int] input zero point\n    :param output_zero_point: [int] output zero point\n    :param input_scale: [float] input feature scale\n    :param weight_scale: [torch.FloatTensor] weight per-channel scale\n    :param output_scale: [float] output feature scale\n    :return:\n        [torch.(cuda.)CharTensor] quantized output feature\n    \"\"\"\n    assert(len(padding) == 4)\n    assert(input.dtype == torch.int8)\n    assert(weight.dtype == input.dtype)\n    assert(bias is None or bias.dtype == torch.int32)\n    assert(isinstance(input_zero_point, int))\n    assert(isinstance(output_zero_point, int))\n    assert(isinstance(input_scale, float))\n    assert(isinstance(output_scale, float))\n    assert(weight_scale.dtype == torch.float)\n\n    # Step 1: calculate integer-based 2d convolution (8-bit multiplication with 32-bit accumulation)\n    input = torch.nn.functional.pad(input, padding, 'constant', input_zero_point)\n    if 'cpu' in input.device.type:\n        # use 32-b MAC for simplicity\n        output = torch.nn.functional.conv2d(input.to(torch.int32), weight.to(torch.int32), None, stride, 0, dilation, groups)\n    else:\n        # current version pytorch does not yet support integer-based conv2d() on GPUs\n        output = torch.nn.functional.conv2d(input.float(), weight.float(), None, stride, 0, dilation, groups)\n        output = output.round().to(torch.int32)\n    if bias is not None:\n        output = output + bias.view(1, -1, 1, 1)\n\n    ############### YOUR CODE STARTS HERE ###############\n    # hint: this code block should be the very similar to quantized_linear()\n\n    # Step 2: scale the output\n    #         hint: 1. scales are floating numbers, we need to convert output to float as well\n    #               2. the shape of weight scale is [oc, 1, 1, 1] while the shape of output is [batch_size, oc, height, width]\n    real_scale = input_scale * weight_scale.view(-1) / output_scale\n    output = output.float() * real_scale.unsqueeze(1).unsqueeze(2)\n\n    # Step 3: shift output by output_zero_point\n    #         hint: one line of code\n    output += output_zero_point\n    ############### YOUR CODE STARTS HERE ###############\n\n    # Make sure all value lies in the bitwidth-bit range\n    output = output.round().clamp(*get_quantized_range(feature_bitwidth)).to(torch.int8)\n    return output"
  },
  {
    "objectID": "posts/labs/lab02.html#question-9-10-pts",
    "href": "posts/labs/lab02.html#question-9-10-pts",
    "title": "ğŸ‘©â€ğŸ’» Lab 2",
    "section": "Question 9 (10 pts)",
    "text": "Question 9 (10 pts)\në§ˆì§€ë§‰ìœ¼ë¡œ ëª¨ë“  ê²ƒì„ ì¢…í•©í•˜ì—¬ ëª¨ë¸ì— ëŒ€í•œ í›ˆë ¨ í›„ int8 ì–‘ìí™”ë¥¼ ìˆ˜í–‰í•©ë‹ˆë‹¤. ëª¨ë¸ì˜ ì»¨ë³¼ë£¨ì…˜ ë ˆì´ì–´ì™€ ì„ í˜• ë ˆì´ì–´ë¥¼ í•˜ë‚˜ì”© ì–‘ìí™”ëœ ë²„ì „ìœ¼ë¡œ ë³€í™˜í•©ë‹ˆë‹¤.\n\në¨¼ì €, BatchNorm ê³„ì¸µì„ ì´ì „ convolutional layerì— ìœµí•©í•  ê²ƒì´ë©°, ì´ëŠ” ì–‘ìí™” ì „ì— í•˜ëŠ” í‘œì¤€ ê´€í–‰ì…ë‹ˆë‹¤. BatchNormì„ ìœµí•©í•˜ë©´ ì¶”ë¡  ì¤‘ì— ì¶”ê°€ ê³±ì…ˆì´ ì¤„ì–´ë“­ë‹ˆë‹¤.\n\nìœµí•© ëª¨ë¸ì¸ model_fusedê°€ ì›ë˜ ëª¨ë¸ê³¼ ë™ì¼í•œ ì •í™•ë„ë¥¼ ê°–ëŠ”ì§€ë„ ê²€ì¦í•  ì˜ˆì •ì…ë‹ˆë‹¤(BN fusionì€ ë„¤íŠ¸ì›Œí¬ ê¸°ëŠ¥ì„ ë³€ê²½í•˜ì§€ ì•ŠëŠ” ë™ë“±í•œ ë³€í™˜ì…ë‹ˆë‹¤).\n\ndef fuse_conv_bn(conv, bn):\n    # modified from https://mmcv.readthedocs.io/en/latest/_modules/mmcv/cnn/utils/fuse_conv_bn.html\n    assert conv.bias is None\n\n    factor = bn.weight.data / torch.sqrt(bn.running_var.data + bn.eps)\n    conv.weight.data = conv.weight.data * factor.reshape(-1, 1, 1, 1)\n    conv.bias = nn.Parameter(- bn.running_mean.data * factor + bn.bias.data)\n\n    return conv\n\nprint('Before conv-bn fusion: backbone length', len(model.backbone))\n#  fuse the batchnorm into conv layers\nrecover_model()\nmodel_fused = copy.deepcopy(model)\nfused_backbone = []\nptr = 0\nwhile ptr &lt; len(model_fused.backbone):\n    if isinstance(model_fused.backbone[ptr], nn.Conv2d) and \\\n        isinstance(model_fused.backbone[ptr + 1], nn.BatchNorm2d):\n        fused_backbone.append(fuse_conv_bn(\n            model_fused.backbone[ptr], model_fused.backbone[ptr+ 1]))\n        ptr += 2\n    else:\n        fused_backbone.append(model_fused.backbone[ptr])\n        ptr += 1\nmodel_fused.backbone = nn.Sequential(*fused_backbone)\n\nprint('After conv-bn fusion: backbone length', len(model_fused.backbone))\n# sanity check, no BN anymore\nfor m in model_fused.modules():\n    assert not isinstance(m, nn.BatchNorm2d)\n\n#  the accuracy will remain the same after fusion\nfused_acc = evaluate(model_fused, dataloader['test'])\nprint(f'Accuracy of the fused model={fused_acc:.2f}%')\n\nBefore conv-bn fusion: backbone length 29\nAfter conv-bn fusion: backbone length 21\nAccuracy of the fused model=92.95%\n\n\n\n\n\n\nê° íŠ¹ì§• ë§µì˜ ë²”ìœ„ë¥¼ ì–»ê¸° ìœ„í•´ ì¼ë¶€ ìƒ˜í”Œ ë°ì´í„°ë¡œ ëª¨ë¸ì„ ì‹¤í–‰í•˜ì—¬ íŠ¹ì§• ë§µì˜ ë²”ìœ„ë¥¼ ì–»ê³ , í•´ë‹¹ ìŠ¤ì¼€ì¼ë§ íŒ©í„°ì™€ ì œë¡œ í¬ì¸íŠ¸ë¥¼ ê³„ì‚°í•  ìˆ˜ ìˆìŠµë‹ˆë‹¤.\n\n\n# add hook to record the min max value of the activation\ninput_activation = {}\noutput_activation = {}\n\ndef add_range_recoder_hook(model):\n    import functools\n    def _record_range(self, x, y, module_name):\n        x = x[0]\n        input_activation[module_name] = x.detach()\n        output_activation[module_name] = y.detach()\n\n    all_hooks = []\n    for name, m in model.named_modules():\n        if isinstance(m, (nn.Conv2d, nn.Linear, nn.ReLU)):\n            all_hooks.append(m.register_forward_hook(\n                functools.partial(_record_range, module_name=name)))\n    return all_hooks\n\nhooks = add_range_recoder_hook(model_fused)\nsample_data = iter(dataloader['train']).__next__()[0]\nmodel_fused(sample_data.cuda())\n\n# remove hooks\nfor h in hooks:\n    h.remove()\n\n\në§ˆì§€ë§‰ìœ¼ë¡œ ëª¨ë¸ ì–‘ìí™”ë¥¼ í•´ë³´ê² ìŠµë‹ˆë‹¤. ë‹¤ìŒê³¼ ê°™ì€ ë§¤í•‘ìœ¼ë¡œ ëª¨ë¸ì„ ë³€í™˜í•©ë‹ˆë‹¤.\n\nnn.Conv2d: QuantizedConv2d,\nnn.Linear: QuantizedLinear,\n# the following twos are just wrappers, as current\n# torch modules do not support int8 data format;\n# we will temporarily convert them to fp32 for computation\nnn.MaxPool2d: QuantizedMaxPool2d,\nnn.AvgPool2d: QuantizedAvgPool2d,\n\nclass QuantizedConv2d(nn.Module):\n    def __init__(self, weight, bias,\n                 input_zero_point, output_zero_point,\n                 input_scale, weight_scale, output_scale,\n                 stride, padding, dilation, groups,\n                 feature_bitwidth=8, weight_bitwidth=8):\n        super().__init__()\n        # current version Pytorch does not support IntTensor as nn.Parameter\n        self.register_buffer('weight', weight)\n        self.register_buffer('bias', bias)\n\n        self.input_zero_point = input_zero_point\n        self.output_zero_point = output_zero_point\n\n        self.input_scale = input_scale\n        self.register_buffer('weight_scale', weight_scale)\n        self.output_scale = output_scale\n\n        self.stride = stride\n        self.padding = (padding[1], padding[1], padding[0], padding[0])\n        self.dilation = dilation\n        self.groups = groups\n\n        self.feature_bitwidth = feature_bitwidth\n        self.weight_bitwidth = weight_bitwidth\n\n\n    def forward(self, x):\n        return quantized_conv2d(\n            x, self.weight, self.bias,\n            self.feature_bitwidth, self.weight_bitwidth,\n            self.input_zero_point, self.output_zero_point,\n            self.input_scale, self.weight_scale, self.output_scale,\n            self.stride, self.padding, self.dilation, self.groups\n            )\n\nclass QuantizedLinear(nn.Module):\n    def __init__(self, weight, bias,\n                 input_zero_point, output_zero_point,\n                 input_scale, weight_scale, output_scale,\n                 feature_bitwidth=8, weight_bitwidth=8):\n        super().__init__()\n        # current version Pytorch does not support IntTensor as nn.Parameter\n        self.register_buffer('weight', weight)\n        self.register_buffer('bias', bias)\n\n        self.input_zero_point = input_zero_point\n        self.output_zero_point = output_zero_point\n\n        self.input_scale = input_scale\n        self.register_buffer('weight_scale', weight_scale)\n        self.output_scale = output_scale\n\n        self.feature_bitwidth = feature_bitwidth\n        self.weight_bitwidth = weight_bitwidth\n\n    def forward(self, x):\n        return quantized_linear(\n            x, self.weight, self.bias,\n            self.feature_bitwidth, self.weight_bitwidth,\n            self.input_zero_point, self.output_zero_point,\n            self.input_scale, self.weight_scale, self.output_scale\n            )\n\nclass QuantizedMaxPool2d(nn.MaxPool2d):\n    def forward(self, x):\n        # current version PyTorch does not support integer-based MaxPool\n        return super().forward(x.float()).to(torch.int8)\n\nclass QuantizedAvgPool2d(nn.AvgPool2d):\n    def forward(self, x):\n        # current version PyTorch does not support integer-based AvgPool\n        return super().forward(x.float()).to(torch.int8)\n\n# we use int8 quantization, which is quite popular\nfeature_bitwidth = weight_bitwidth = 8\nquantized_model = copy.deepcopy(model_fused)\nquantized_backbone = []\nptr = 0\nwhile ptr &lt; len(quantized_model.backbone):\n    if isinstance(quantized_model.backbone[ptr], nn.Conv2d) and \\\n        isinstance(quantized_model.backbone[ptr + 1], nn.ReLU):\n        conv = quantized_model.backbone[ptr]\n        conv_name = f'backbone.{ptr}'\n        relu = quantized_model.backbone[ptr + 1]\n        relu_name = f'backbone.{ptr + 1}'\n\n        input_scale, input_zero_point = \\\n            get_quantization_scale_and_zero_point(\n                input_activation[conv_name], feature_bitwidth)\n\n        output_scale, output_zero_point = \\\n            get_quantization_scale_and_zero_point(\n                output_activation[relu_name], feature_bitwidth)\n\n        quantized_weight, weight_scale, weight_zero_point = \\\n            linear_quantize_weight_per_channel(conv.weight.data, weight_bitwidth)\n        quantized_bias, bias_scale, bias_zero_point = \\\n            linear_quantize_bias_per_output_channel(\n                conv.bias.data, weight_scale, input_scale)\n        shifted_quantized_bias = \\\n            shift_quantized_conv2d_bias(quantized_bias, quantized_weight,\n                                        input_zero_point)\n\n        quantized_conv = QuantizedConv2d(\n            quantized_weight, shifted_quantized_bias,\n            input_zero_point, output_zero_point,\n            input_scale, weight_scale, output_scale,\n            conv.stride, conv.padding, conv.dilation, conv.groups,\n            feature_bitwidth=feature_bitwidth, weight_bitwidth=weight_bitwidth\n        )\n\n        quantized_backbone.append(quantized_conv)\n        ptr += 2\n    elif isinstance(quantized_model.backbone[ptr], nn.MaxPool2d):\n        quantized_backbone.append(QuantizedMaxPool2d(\n            kernel_size=quantized_model.backbone[ptr].kernel_size,\n            stride=quantized_model.backbone[ptr].stride\n            ))\n        ptr += 1\n    elif isinstance(quantized_model.backbone[ptr], nn.AvgPool2d):\n        quantized_backbone.append(QuantizedAvgPool2d(\n            kernel_size=quantized_model.backbone[ptr].kernel_size,\n            stride=quantized_model.backbone[ptr].stride\n            ))\n        ptr += 1\n    else:\n        raise NotImplementedError(type(quantized_model.backbone[ptr]))  # should not happen\nquantized_model.backbone = nn.Sequential(*quantized_backbone)\n\n# finally, quantized the classifier\nfc_name = 'classifier'\nfc = model.classifier\ninput_scale, input_zero_point = \\\n    get_quantization_scale_and_zero_point(\n        input_activation[fc_name], feature_bitwidth)\n\noutput_scale, output_zero_point = \\\n    get_quantization_scale_and_zero_point(\n        output_activation[fc_name], feature_bitwidth)\n\nquantized_weight, weight_scale, weight_zero_point = \\\n    linear_quantize_weight_per_channel(fc.weight.data, weight_bitwidth)\nquantized_bias, bias_scale, bias_zero_point = \\\n    linear_quantize_bias_per_output_channel(\n        fc.bias.data, weight_scale, input_scale)\nshifted_quantized_bias = \\\n    shift_quantized_linear_bias(quantized_bias, quantized_weight,\n                                input_zero_point)\n\nquantized_model.classifier = QuantizedLinear(\n    quantized_weight, shifted_quantized_bias,\n    input_zero_point, output_zero_point,\n    input_scale, weight_scale, output_scale,\n    feature_bitwidth=feature_bitwidth, weight_bitwidth=weight_bitwidth\n)\n\nì–‘ìí™” ê³¼ì •ì´ ì™„ë£Œë˜ì—ˆìŠµë‹ˆë‹¤! ëª¨ë¸ ì•„í‚¤í…ì²˜ë¥¼ ì¸ì‡„í•˜ê³  ì‹œê°í™”í•˜ë©° ì–‘ìí™”ëœ ëª¨ë¸ì˜ ì •í™•ì„±ë„ ê²€ì¦í•´ ë³´ê² ìŠµë‹ˆë‹¤.\n\nQuestion 9.1 (5 pts)\nì–‘ìí™”ëœ ëª¨ë¸ì„ ì‹¤í–‰í•˜ê¸° ìœ„í•´ì„œëŠ” (0, 1) ë²”ìœ„ì˜ ì…ë ¥ ë°ì´í„°ë¥¼ (-128, 127) ë²”ìœ„ì˜ int8 ë²”ìœ„ë¡œ ë§¤í•‘í•˜ëŠ” ì¶”ê°€ì ì¸ ì „ì²˜ë¦¬ê°€ í•„ìš”í•©ë‹ˆë‹¤. ì´ ì „ì²˜ë¦¬ë¥¼ ì§„í–‰í•˜ëŠ” ì•„ë˜ ì½”ë“œë¥¼ ì™„ì„±í•˜ì„¸ìš”.\nHint: ì–‘ìí™”ëœ ëª¨ë¸ì€ fp32 ëª¨ë¸ê³¼ ê±°ì˜ ë™ì¼í•œ ì •í™•ë„ë¥¼ ê°€ì§€ê³  ìˆìŠµë‹ˆë‹¤.\n\nprint(quantized_model)\n\ndef extra_preprocess(x):\n    # hint: you need to convert the original fp32 input of range (0, 1)\n    #  into int8 format of range (-128, 127)\n    ############### YOUR CODE STARTS HERE ###############\n    x_scaled = x * 255\n    x_shifted = x_scaled - 128\n    return x_shifted.clamp(-128, 127).to(torch.int8)\n    ############### YOUR CODE ENDS HERE #################\n\nint8_model_accuracy = evaluate(quantized_model, dataloader['test'],\n                               extra_preprocess=[extra_preprocess])\nprint(f\"int8 model has accuracy={int8_model_accuracy:.2f}%\")\n\nVGG(\n  (backbone): Sequential(\n    (0): QuantizedConv2d()\n    (1): QuantizedConv2d()\n    (2): QuantizedMaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n    (3): QuantizedConv2d()\n    (4): QuantizedConv2d()\n    (5): QuantizedMaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n    (6): QuantizedConv2d()\n    (7): QuantizedConv2d()\n    (8): QuantizedMaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n    (9): QuantizedConv2d()\n    (10): QuantizedConv2d()\n    (11): QuantizedMaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n    (12): QuantizedAvgPool2d(kernel_size=2, stride=2, padding=0)\n  )\n  (classifier): QuantizedLinear()\n)\nint8 model has accuracy=92.90%"
  },
  {
    "objectID": "posts/labs/lab02.html#question-9.2-bonus-question-5-pts",
    "href": "posts/labs/lab02.html#question-9.2-bonus-question-5-pts",
    "title": "ğŸ‘©â€ğŸ’» Lab 2",
    "section": "Question 9.2 (Bonus Question; 5 pts)",
    "text": "Question 9.2 (Bonus Question; 5 pts)\nlinear quantized modelì— ReLU ì¸µì´ ì—†ëŠ” ì´ìœ ë¥¼ ì„¤ëª…í•˜ì„¸ìš”.\nYour Answer:\nì„ í˜•(Linear) ì–‘ìí™” ëª¨ë¸ì—ì„œ ReLU(Rectified Linear Unit) ì¸µì´ ì—†ëŠ” ì´ìœ ëŠ” ì£¼ë¡œ ì–‘ìí™” ê³¼ì •ì—ì„œì˜ ë°ì´í„° í‘œí˜„ ë°©ì‹ê³¼ ì—°ì‚°ì˜ íš¨ìœ¨ì„±ê³¼ ê´€ë ¨ì´ ìˆìŠµë‹ˆë‹¤. ì–‘ìí™”ëŠ” ëª¨ë¸ì˜ ê°€ì¤‘ì¹˜ë‚˜ í™œì„±í™”ë¥¼ ê³ ì •ëœ ë¹„íŠ¸ ë„ˆë¹„(ì˜ˆ: 8ë¹„íŠ¸)ì˜ ì •ìˆ˜ë¡œ ì œí•œí•˜ì—¬ ì €ì¥í•˜ê³  ê³„ì‚°í•˜ëŠ” ê¸°ìˆ ì…ë‹ˆë‹¤. ì´ëŸ¬í•œ ì œí•œì€ ëª¨ë¸ì˜ í¬ê¸°ë¥¼ ì¤„ì´ê³ , ê³„ì‚° ì†ë„ë¥¼ í–¥ìƒì‹œí‚¤ë©°, ì €ì „ë ¥ ì¥ì¹˜ì—ì„œì˜ ì‹¤í–‰ì„ ìš©ì´í•˜ê²Œ í•©ë‹ˆë‹¤. ê·¸ëŸ¬ë‚˜ ì´ ê³¼ì •ì—ì„œ ë°ì´í„°ì˜ ì •ë°€ë„ê°€ ì†ì‹¤ë  ìˆ˜ ìˆìœ¼ë©°, ì´ëŠ” ëª¨ë¸ ì„±ëŠ¥ì— ì˜í–¥ì„ ë¯¸ì¹  ìˆ˜ ìˆìŠµë‹ˆë‹¤.\nReLU í™œì„±í™” í•¨ìˆ˜ëŠ” ì…ë ¥ì´ ì–‘ìˆ˜ì¼ ê²½ìš° ê·¸ëŒ€ë¡œ ì¶œë ¥í•˜ê³ , ìŒìˆ˜ì¼ ê²½ìš° 0ìœ¼ë¡œ ë§Œë“œëŠ” ê°„ë‹¨í•˜ê³  íš¨ìœ¨ì ì¸ ë¹„ì„ í˜• í•¨ìˆ˜ì…ë‹ˆë‹¤. ReLUëŠ” ë”¥ëŸ¬ë‹ ëª¨ë¸ì—ì„œ ë„ë¦¬ ì‚¬ìš©ë˜ë©°, íŠ¹íˆ ì€ë‹‰ì¸µì—ì„œ ë¹„ì„ í˜•ì„±ì„ ì¶”ê°€í•˜ì—¬ ëª¨ë¸ì˜ í‘œí˜„ë ¥ì„ í–¥ìƒì‹œí‚¤ëŠ” ë° ì¤‘ìš”í•œ ì—­í• ì„ í•©ë‹ˆë‹¤.\nì„ í˜• ì–‘ìí™” ëª¨ë¸ì—ì„œ ReLU ì¸µì´ ì—†ëŠ” ì£¼ëœ ì´ìœ ëŠ” ë‹¤ìŒê³¼ ê°™ìŠµë‹ˆë‹¤:\n\nì–‘ìí™”ëœ ë°ì´í„°ì˜ ë²”ìœ„ ì œí•œ: ì •ìˆ˜ ì–‘ìí™” ê³¼ì •ì—ì„œëŠ” ë°ì´í„°ê°€ íŠ¹ì • ë²”ìœ„ ë‚´ì˜ ê°’ìœ¼ë¡œ ì œí•œë©ë‹ˆë‹¤. ì˜ˆë¥¼ ë“¤ì–´, 8ë¹„íŠ¸ ì–‘ìí™”ì—ì„œëŠ” ê°’ì´ -128ë¶€í„° 127ê¹Œì§€ì˜ ì •ìˆ˜ ë²”ìœ„ë¥¼ ê°€ì§‘ë‹ˆë‹¤. ì´ëŸ¬í•œ ë²”ìœ„ ë‚´ì—ì„œ ReLUë¥¼ ì ìš©í•˜ë©´ ìŒìˆ˜ ê°’ì´ ëª¨ë‘ 0ìœ¼ë¡œ ë³€í™˜ë˜ì–´, ì–‘ìˆ˜ ê°’ë§Œ ë‚¨ê²Œ ë©ë‹ˆë‹¤. ì´ ê³¼ì •ì—ì„œ ë°ì´í„°ì˜ ë²”ìœ„ê°€ ë”ìš± ì œí•œë˜ì–´, ì–‘ìí™”ëœ ëª¨ë¸ì˜ í‘œí˜„ë ¥ì´ ë”ìš± ê°ì†Œí•  ìˆ˜ ìˆìŠµë‹ˆë‹¤.\níš¨ìœ¨ì„±: ì–‘ìí™”ëœ ëª¨ë¸ì€ ê°€ëŠ¥í•œ í•œ ê³„ì‚°ì„ ê°„ë‹¨í•˜ê²Œ ìœ ì§€í•˜ì—¬ ë¹ ë¥¸ ì¶”ë¡  ì†ë„ì™€ ë‚®ì€ ì „ë ¥ ì†Œëª¨ë¥¼ ë‹¬ì„±í•˜ë ¤ê³  í•©ë‹ˆë‹¤. ReLUì™€ ê°™ì€ ë¹„ì„ í˜• í•¨ìˆ˜ë¥¼ ì¶”ê°€í•˜ë©´, ì¶”ë¡  ê³¼ì •ì—ì„œ ì¶”ê°€ì ì¸ ê³„ì‚°ì´ í•„ìš”í•˜ê²Œ ë©ë‹ˆë‹¤. ì–´ë–¤ ê²½ìš°ì—ëŠ” ëª¨ë¸ì˜ êµ¬ì¡°ë‚˜ ëª©ì ì— ë”°ë¼ ì´ëŸ¬í•œ ì¶”ê°€ ê³„ì‚° ì—†ì´ë„ ì¶©ë¶„í•œ ì„±ëŠ¥ì„ ë‹¬ì„±í•  ìˆ˜ ìˆìœ¼ë¯€ë¡œ, ReLU ì¸µì„ ìƒëµí•  ìˆ˜ ìˆìŠµë‹ˆë‹¤.\nëª¨ë¸ ì„¤ê³„ì™€ ëª©ì : íŠ¹ì • ì–‘ìí™” ëª¨ë¸ì—ì„œëŠ” ì„±ëŠ¥ ìœ ì§€ë¥¼ ìœ„í•´ ReLU ëŒ€ì‹  ë‹¤ë¥¸ ê¸°ë²•ì´ë‚˜ í™œì„±í™” í•¨ìˆ˜ë¥¼ ì‚¬ìš©í•  ìˆ˜ ìˆìŠµë‹ˆë‹¤. ì˜ˆë¥¼ ë“¤ì–´, ì–‘ìí™” ì „ ëª¨ë¸ì—ì„œ ReLUë¥¼ ì‚¬ìš©í•˜ëŠ” ëŒ€ì‹ , ì–‘ìí™” ê³¼ì •ì—ì„œ ìµœì í™”ëœ í™œì„±í™” í•¨ìˆ˜ë¥¼ ì„ íƒí•˜ê±°ë‚˜, ReLUì˜ íš¨ê³¼ë¥¼ ëª¨ë°©í•  ìˆ˜ ìˆëŠ” ë‹¤ë¥¸ ë°©ë²•ì„ ëª¨ìƒ‰í•  ìˆ˜ ìˆìŠµë‹ˆë‹¤.\n\nê²°ë¡ ì ìœ¼ë¡œ, ì„ í˜• ì–‘ìí™” ëª¨ë¸ì—ì„œ ReLU ì¸µì˜ ë¶€ì¬ëŠ” ë°ì´í„°ì˜ ë²”ìœ„ ì œí•œ, ê³„ì‚° íš¨ìœ¨ì„±, ê·¸ë¦¬ê³  íŠ¹ì • ëª¨ë¸ ì„¤ê³„ì™€ ëª©ì ì— ê¸°ì¸í•  ìˆ˜ ìˆìŠµë‹ˆë‹¤. ëª¨ë¸ì˜ ì„¤ê³„ìëŠ” ì„±ëŠ¥, ì†ë„, í¬ê¸° ë“±ì˜ ìš”êµ¬ ì‚¬í•­ì„ ê· í˜• ìˆê²Œ ê³ ë ¤í•˜ì—¬ ìµœì ì˜ ëª¨ë¸ êµ¬ì¡°ë¥¼ ê²°ì •í•´ì•¼ í•©ë‹ˆë‹¤."
  },
  {
    "objectID": "posts/labs/lab00.html",
    "href": "posts/labs/lab00.html",
    "title": "ğŸ‘©â€ğŸ’» Lab 0",
    "section": "",
    "text": "Lab 0 PyTorch Tutorial\nIn this tutorial, we will explore how to train a neural network with PyTorch.\n\nSetup\nWe will first install a few packages that will be used in this tutorial:\n\n!pip install torchprofile 1&gt;/dev/null\n\nWe will then import a few libraries:\n\nimport random\nfrom collections import OrderedDict, defaultdict\n\nimport numpy as np\nimport torch\nfrom matplotlib import pyplot as plt\nfrom torch import nn\nfrom torch.optim import *\nfrom torch.optim.lr_scheduler import *\nfrom torch.utils.data import DataLoader\nfrom torchprofile import profile_macs\nfrom torchvision.datasets import *\nfrom torchvision.transforms import *\nfrom tqdm.auto import tqdm\n\nTo ensure the reproducibility, we will control the seed of random generators:\n\nrandom.seed(0)\nnp.random.seed(0)\ntorch.manual_seed(0)\n\n\n\nData\nIn this tutorial, we will use CIFAR-10 as our target dataset. This dataset contains images from 10 classes, where each image is of size 3x32x32, i.e.Â 3-channel color images of 32x32 pixels in size.\n\ntransforms = {\n  \"train\": Compose([\n    RandomCrop(32, padding=4),\n    RandomHorizontalFlip(),\n    ToTensor(),\n  ]),\n  \"test\": ToTensor(),\n}\n\ndataset = {}\nfor split in [\"train\", \"test\"]:\n  dataset[split] = CIFAR10(\n    root=\"data/cifar10\",\n    train=(split == \"train\"),\n    download=True,\n    transform=transforms[split],\n  )\n\nWe can visualize a few images in the dataset and their corresponding class labels:\n\nsamples = [[] for _ in range(10)]\nfor image, label in dataset[\"test\"]:\n  if len(samples[label]) &lt; 4:\n    samples[label].append(image)\n\nplt.figure(figsize=(20, 9))\nfor index in range(40):\n  label = index % 10\n  image = samples[label][index // 10]\n\n  # Convert from CHW to HWC for visualization\n  image = image.permute(1, 2, 0)\n\n  # Convert from class index to class name\n  label = dataset[\"test\"].classes[label]\n\n  # Visualize the image\n  plt.subplot(4, 10, index + 1)\n  plt.imshow(image)\n  plt.title(label)\n  plt.axis(\"off\")\nplt.show()\n\nTo train a neural network, we will need to feed data in batches. We create data loaders with batch size of 512:\n\ndataflow = {}\nfor split in ['train', 'test']:\n  dataflow[split] = DataLoader(\n    dataset[split],\n    batch_size=512,\n    shuffle=(split == 'train'),\n    num_workers=0,\n    pin_memory=True,\n  )\n\nWe can print the data type and shape from the training data loader:\n\nfor inputs, targets in dataflow[\"train\"]:\n  print(\"[inputs] dtype: {}, shape: {}\".format(inputs.dtype, inputs.shape))\n  print(\"[targets] dtype: {}, shape: {}\".format(targets.dtype, targets.shape))\n  break\n\n\n\nModel\nIn this tutorial, we will use a variant of VGG-11 (with fewer downsamples and a smaller classifier) as our model.\n\nclass VGG(nn.Module):\n  ARCH = [64, 128, 'M', 256, 256, 'M', 512, 512, 'M', 512, 512, 'M']\n\n  def __init__(self) -&gt; None:\n    super().__init__()\n\n    layers = []\n    counts = defaultdict(int)\n\n    def add(name: str, layer: nn.Module) -&gt; None:\n      layers.append((f\"{name}{counts[name]}\", layer))\n      counts[name] += 1\n\n    in_channels = 3\n    for x in self.ARCH:\n      if x != 'M':\n        # conv-bn-relu\n        add(\"conv\", nn.Conv2d(in_channels, x, 3, padding=1, bias=False))\n        add(\"bn\", nn.BatchNorm2d(x))\n        add(\"relu\", nn.ReLU(True))\n        in_channels = x\n      else:\n        # maxpool\n        add(\"pool\", nn.MaxPool2d(2))\n\n    self.backbone = nn.Sequential(OrderedDict(layers))\n    self.classifier = nn.Linear(512, 10)\n\n  def forward(self, x: torch.Tensor) -&gt; torch.Tensor:\n    # backbone: [N, 3, 32, 32] =&gt; [N, 512, 2, 2]\n    x = self.backbone(x)\n\n    # avgpool: [N, 512, 2, 2] =&gt; [N, 512]\n    x = x.mean([2, 3])\n\n    # classifier: [N, 512] =&gt; [N, 10]\n    x = self.classifier(x)\n    return x\n\nmodel = VGG().cuda()\n\nIts backbone is composed of eight conv-bn-relu blocks interleaved with four maxpoolâ€™s to downsample the feature map by 2^4 = 16 times:\n\nprint(model.backbone)\n\nAfter the feature map is pooled, its classifier predicts the final output with a linear layer:\n\nprint(model.classifier)\n\nAs this course focuses on efficiency, we will then inspect its model size and (theoretical) computation cost.\n\nThe model size can be estimated by the number of trainable parameters:\n\n\nnum_params = 0\nfor param in model.parameters():\n  if param.requires_grad:\n    num_params += param.numel()\nprint(\"#Params:\", num_params)\n\n\nThe computation cost can be estimated by the number of multiplyâ€“accumulate operations (MACs) using TorchProfile:\n\n\nnum_macs = profile_macs(model, torch.zeros(1, 3, 32, 32).cuda())\nprint(\"#MACs:\", num_macs)\n\nThis model has 9.2M parameters and requires 606M MACs for inference. We will work together in the next few labs to improve its efficiency.\n\n\nOptimization\nAs we are working on a classification problem, we will apply cross entropy as our loss function to optimize the model:\n\ncriterion = nn.CrossEntropyLoss()\n\nOptimization will be carried out using stochastic gradient descent (SGD) with momentum:\n\noptimizer = SGD(\n  model.parameters(),\n  lr=0.4,\n  momentum=0.9,\n  weight_decay=5e-4,\n)\n\nThe learning rate will be modulated using the following scheduler (which is adapted from this blog series):\n\nnum_epochs = 20\nsteps_per_epoch = len(dataflow[\"train\"])\n\n# Define the piecewise linear scheduler\nlr_lambda = lambda step: np.interp(\n  [step / steps_per_epoch],\n  [0, num_epochs * 0.3, num_epochs],\n  [0, 1, 0]\n)[0]\n\n# Visualize the learning rate schedule\nsteps = np.arange(steps_per_epoch * num_epochs)\nplt.plot(steps, [lr_lambda(step) * 0.4 for step in steps])\nplt.xlabel(\"Number of Steps\")\nplt.ylabel(\"Learning Rate\")\nplt.grid(\"on\")\nplt.show()\n\nscheduler = LambdaLR(optimizer, lr_lambda)\n\n\n\nTraining\nWe first define the training function that optimizes the model for one epoch (i.e., a pass over the training set):\n\ndef train(\n  model: nn.Module,\n  dataflow: DataLoader,\n  criterion: nn.Module,\n  optimizer: Optimizer,\n  scheduler: LambdaLR,\n) -&gt; None:\n  model.train()\n\n  for inputs, targets in tqdm(dataflow, desc='train', leave=False):\n    # Move the data from CPU to GPU\n    inputs = inputs.cuda()\n    targets = targets.cuda()\n\n    # Reset the gradients (from the last iteration)\n    optimizer.zero_grad()\n\n    # Forward inference\n    outputs = model(inputs)\n    loss = criterion(outputs, targets)\n\n    # Backward propagation\n    loss.backward()\n\n    # Update optimizer and LR scheduler\n    optimizer.step()\n    scheduler.step()\n\nWe then define the evaluation function that calculates the metric (i.e., accuracy in our case) on the test set:\n\n@torch.inference_mode()\ndef evaluate(\n  model: nn.Module,\n  dataflow: DataLoader\n) -&gt; float:\n  model.eval()\n\n  num_samples = 0\n  num_correct = 0\n\n  for inputs, targets in tqdm(dataflow, desc=\"eval\", leave=False):\n    # Move the data from CPU to GPU\n    inputs = inputs.cuda()\n    targets = targets.cuda()\n\n    # Inference\n    outputs = model(inputs)\n\n    # Convert logits to class indices\n    outputs = outputs.argmax(dim=1)\n\n    # Update metrics\n    num_samples += targets.size(0)\n    num_correct += (outputs == targets).sum()\n\n  return (num_correct / num_samples * 100).item()\n\nWith training and evaluation functions, we can finally start training the model! This will take around 10 minutes.\n\nfor epoch_num in tqdm(range(1, num_epochs + 1)):\n  train(model, dataflow[\"train\"], criterion, optimizer, scheduler)\n  metric = evaluate(model, dataflow[\"test\"])\n  print(f\"epoch {epoch_num}:\", metric)\n\nIf everything goes well, your trained model should be able to achieve &gt;92.5% of accuracy!\n\n\nVisualization\nWe can visualize the modelâ€™s prediction to see how the model truly performs:\n\nplt.figure(figsize=(20, 10))\nfor index in range(40):\n  image, label = dataset[\"test\"][index]\n\n  # Model inference\n  model.eval()\n  with torch.inference_mode():\n    pred = model(image.unsqueeze(dim=0).cuda())\n    pred = pred.argmax(dim=1)\n\n  # Convert from CHW to HWC for visualization\n  image = image.permute(1, 2, 0)\n\n  # Convert from class indices to class names\n  pred = dataset[\"test\"].classes[pred]\n  label = dataset[\"test\"].classes[label]\n\n  # Visualize the image\n  plt.subplot(4, 10, index + 1)\n  plt.imshow(image)\n  plt.title(f\"pred: {pred}\" + \"\\n\" + f\"label: {label}\")\n  plt.axis(\"off\")\nplt.show()"
  },
  {
    "objectID": "about.html",
    "href": "about.html",
    "title": "TinyML Study Group",
    "section": "",
    "text": "@Gueltto9th"
  },
  {
    "objectID": "about.html#lec-4lab",
    "href": "about.html#lec-4lab",
    "title": "TinyML Study Group",
    "section": "21Lec + 4Lab",
    "text": "21Lec + 4Lab\n\nLec1/2ì™€ Lab0ì€ ì œì™¸\nê°•ì˜ë¥¼ ë“£ê³  1ëª…ì”© ëŒì•„ê°€ë©´ì„œ ê°•ì˜ ë³µìŠµ recap ë°œí‘œ\në‹¤ë¥¸ ì‚¬ëŒë“¤ì€ ì§ˆë¬¸/ë””ìŠ¤ì»¤ì…˜ í† í”½ ê°€ì ¸ì˜¤ê¸°\nì£¼ 1íšŒ (ì•½ 16ì£¼ - 4ê°œì›” ì´ë‚´ ì™„ë£Œ ëª©í‘œ)\n\n\n\n[Chapter I: Efficient Inference]\n\nLecture 3: Pruning and Sparsity (Part I) @ooshyun\nLecture 4: Pruning and Sparsity (Part II) @curieuxjy\nLab 1 @CastleFlag\nLecture 5: Quantization (Part I) @ooshyun\nLecture 6: Quantization (Part II) @ooshyun\nLab 2 @curieuxjy\nLecture 7: Neural Architecture Search (Part I) @CastleFlag\nLecture 8: Neural Architecture Search (Part II) @CastleFlag\nLab 3 @ooshyun\nLecture 9: Knowledge Distillation @ooshyun\nLecture 10: MCUNet: TinyML on Microcontrollers @curieuxjy\nLecture 11: TinyEngine and Parallel Processing @curieuxjy\n\n\n\n[Chapter II: Domain-Specific Optimization]\n\nLecture 12: Transformer and LLM (Part I) @CastleFlag\nLecture 13: Transformer and LLM (Part II) @CastleFlag\nLecture 14: Vision Transformer @ooshyun\nLab 4 @ooshyun\nLecture 15: GAN, Video, and Point Cloud @curieuxjy\nLecture 16: Diffusion Model @curieuxjy\nLecture 17: Distributed Training (Part I) @CastleFlag\nLecture 18: Distributed Training (Part II) @CastleFlag\nLab 5 @ooshyun\nLecture 19: On-Device Training and Transfer Learning @ooshyun\nLecture 20: Efficient Fine-tuning and Prompt Engineering @curieuxjy\n\n\n\n[Chapter IV: Advanced Topics]\n\nLecture 21: Basics of Quantum Computing @curieuxjy\nLecture 22: Quantum Machine Learning @CastleFlag\nLecture 23: Noise Robust Quantum ML @CastleFlag"
  },
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "Study Archive",
    "section": "",
    "text": "ìŠ¤í„°ë”” ìë£Œì™€ ê´€ë ¨í•´ì„œ ì–´ë–¤ í† ì˜ë‚˜ ì˜ê²¬ ëª¨ë‘ ê°ì‚¬í•©ë‹ˆë‹¤! Github Discussionì— ê¸€ì„ ë‚¨ê²¨ì£¼ì…”ë„ ì¢‹ê³  ê° í¬ìŠ¤íŒ… í•˜ë‹¨ì— ìˆëŠ” Giscus ëŒ“ê¸€ì°½ì— ì½”ë©˜íŠ¸ë“¤ì„ ë‚¨ê²¨ì£¼ì‹œë©´ ë©ë‹ˆë‹¤.\nWelcome to any comments or opinions on our content! You can leave messages using the direct Discussion or the Giscus window on each post.\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nğŸ‘©â€ğŸ’» Lab 3\n\n\n\n\n\n\nNAS\n\n\nTinyML\n\n\nlab\n\n\n\nNeural Architecture Search(NAS) Experiment\n\n\n\n\n\nMar 16, 2024\n\n\nSeunghyun Oh\n\n\n\n\n\n\n\n\n\n\n\n\nğŸ‘©â€ğŸ’» Lab 2\n\n\n\n\n\n\nlab\n\n\nquantization\n\n\nlinear\n\n\nkmeans\n\n\n\nK-means & Linear Quantization\n\n\n\n\n\nMar 6, 2024\n\n\nJung Yeon Lee\n\n\n\n\n\n\n\n\n\n\n\n\nğŸ§‘â€ğŸ« Lecture 5-6\n\n\n\n\n\n\nlecture\n\n\nquantization\n\n\n\nQuantization\n\n\n\n\n\nMar 5, 2024\n\n\nSeunghyun Oh\n\n\n\n\n\n\n\n\n\n\n\n\nğŸ‘©â€ğŸ’» Lab 1\n\n\n\n\n\n\nlab\n\n\npruning\n\n\nfine-grained\n\n\nchannel\n\n\n\nFine-grained & Channel Pruning\n\n\n\n\n\nMar 1, 2024\n\n\ncastleflag\n\n\n\n\n\n\n\n\n\n\n\n\nğŸ§‘â€ğŸ« Lecture 4\n\n\n\n\n\n\npruning\n\n\nlecture\n\n\n\nPruning and Sparsity (Part II)\n\n\n\n\n\nFeb 18, 2024\n\n\nJung Yeon Lee\n\n\n\n\n\n\n\n\n\n\n\n\nğŸ§‘â€ğŸ« Lecture 3\n\n\n\n\n\n\npruning\n\n\nlecture\n\n\n\nPruning and Sparsity (Part I)\n\n\n\n\n\nJan 28, 2024\n\n\nSeunghyun Oh\n\n\n\n\n\n\n\n\n\n\n\n\nğŸ‘©â€ğŸ’» Lab 0\n\n\n\n\n\n\nlab\n\n\npytorch\n\n\nbasic\n\n\n\nPyTorch Tutorial\n\n\n\n\n\nJan 11, 2024\n\n\nJung Yeon Lee\n\n\n\n\n\n\nNo matching items"
  },
  {
    "objectID": "posts/labs/lab01.html",
    "href": "posts/labs/lab01.html",
    "title": "ğŸ‘©â€ğŸ’» Lab 1",
    "section": "",
    "text": "ì´ë²ˆ Lab 1 Pruningì€ ë‹¤ìŒê³¼ ê°™ì€ ëª©í‘œì™€ ë‚´ìš©ìœ¼ë¡œ êµ¬ì„±ë˜ì–´ ìˆìŠµë‹ˆë‹¤. ì•„ë˜ ë²„íŠ¼ì„ ëˆŒëŸ¬ Colaboratoryì—ì„œ ë°”ë¡œ ì‹¤í–‰í•  ìˆ˜ ìˆìŠµë‹ˆë‹¤.\n\n\n\n\npruningì˜ ê¸°ë³¸ ê°œë…ì„ ì´í•´í•©ë‹ˆë‹¤.\nfine-grained pruningì„ êµ¬í˜„í•˜ê³  ì ìš©í•©ë‹ˆë‹¤.\nchannel pruningì„ êµ¬í˜„í•˜ê³  ì ìš©í•©ë‹ˆë‹¤.\npruningìœ¼ë¡œë¶€í„°ì˜ ì„±ëŠ¥ ê°œì„ (ì˜ˆ: ì†ë„ í–¥ìƒ)ì— ëŒ€í•œ ê¸°ë³¸ì ì¸ ì´í•´ë¥¼ ì–»ìŠµë‹ˆë‹¤.\nì´ëŸ¬í•œ pruning ì ‘ê·¼ ë°©ì‹ ê°„ì˜ ì°¨ì´ì ê³¼ tradeoffsë¥¼ ì´í•´í•©ë‹ˆë‹¤.\n\n\n\n\nì´ ì‹¤ìŠµì—ëŠ” Fine-grained Pruningê³¼ Channel Pruningì˜ ë‘ ê°€ì§€ ì£¼ìš” ì„¹ì…˜ì´ ìˆìŠµë‹ˆë‹¤.\nì´ 9ê°œì˜ ì§ˆë¬¸ì´ ìˆìŠµë‹ˆë‹¤:\n\nFine-grained Pruningì— ëŒ€í•´ì„œëŠ” 5ê°œì˜ ì§ˆë¬¸ì´ ìˆìŠµë‹ˆë‹¤ (Question 1-5).\nChannel Pruningì— ëŒ€í•´ì„œëŠ” 3ê°œì˜ ì§ˆë¬¸ì´ ìˆìŠµë‹ˆë‹¤ (Question 6-8).\nQuestion 9ëŠ” fine-grained pruningê³¼ channel pruningì„ ë¹„êµí•©ë‹ˆë‹¤.\n\n\nì‹¤ìŠµë…¸íŠ¸ì— ëŒ€í•œ ì„¤ì • ë¶€ë¶„(Setup)ì€ Colaboratory Noteë¥¼ ì—´ë©´ í™•ì¸í•˜ì‹¤ ìˆ˜ ìˆìŠµë‹ˆë‹¤. í¬ìŠ¤íŒ…ì—ì„œëŠ” ë³´ë‹¤ ì‹¤ìŠµë‚´ìš©ì— ì§‘ì¤‘í•  ìˆ˜ ìˆë„ë¡ ìƒëµë˜ì–´ ìˆìŠµë‹ˆë‹¤."
  },
  {
    "objectID": "posts/labs/lab01.html#goals",
    "href": "posts/labs/lab01.html#goals",
    "title": "ğŸ‘©â€ğŸ’» Lab 1",
    "section": "",
    "text": "pruningì˜ ê¸°ë³¸ ê°œë…ì„ ì´í•´í•©ë‹ˆë‹¤.\nfine-grained pruningì„ êµ¬í˜„í•˜ê³  ì ìš©í•©ë‹ˆë‹¤.\nchannel pruningì„ êµ¬í˜„í•˜ê³  ì ìš©í•©ë‹ˆë‹¤.\npruningìœ¼ë¡œë¶€í„°ì˜ ì„±ëŠ¥ ê°œì„ (ì˜ˆ: ì†ë„ í–¥ìƒ)ì— ëŒ€í•œ ê¸°ë³¸ì ì¸ ì´í•´ë¥¼ ì–»ìŠµë‹ˆë‹¤.\nì´ëŸ¬í•œ pruning ì ‘ê·¼ ë°©ì‹ ê°„ì˜ ì°¨ì´ì ê³¼ tradeoffsë¥¼ ì´í•´í•©ë‹ˆë‹¤."
  },
  {
    "objectID": "posts/labs/lab01.html#contents",
    "href": "posts/labs/lab01.html#contents",
    "title": "ğŸ‘©â€ğŸ’» Lab 1",
    "section": "",
    "text": "ì´ ì‹¤ìŠµì—ëŠ” Fine-grained Pruningê³¼ Channel Pruningì˜ ë‘ ê°€ì§€ ì£¼ìš” ì„¹ì…˜ì´ ìˆìŠµë‹ˆë‹¤.\nì´ 9ê°œì˜ ì§ˆë¬¸ì´ ìˆìŠµë‹ˆë‹¤:\n\nFine-grained Pruningì— ëŒ€í•´ì„œëŠ” 5ê°œì˜ ì§ˆë¬¸ì´ ìˆìŠµë‹ˆë‹¤ (Question 1-5).\nChannel Pruningì— ëŒ€í•´ì„œëŠ” 3ê°œì˜ ì§ˆë¬¸ì´ ìˆìŠµë‹ˆë‹¤ (Question 6-8).\nQuestion 9ëŠ” fine-grained pruningê³¼ channel pruningì„ ë¹„êµí•©ë‹ˆë‹¤.\n\n\nì‹¤ìŠµë…¸íŠ¸ì— ëŒ€í•œ ì„¤ì • ë¶€ë¶„(Setup)ì€ Colaboratory Noteë¥¼ ì—´ë©´ í™•ì¸í•˜ì‹¤ ìˆ˜ ìˆìŠµë‹ˆë‹¤. í¬ìŠ¤íŒ…ì—ì„œëŠ” ë³´ë‹¤ ì‹¤ìŠµë‚´ìš©ì— ì§‘ì¤‘í•  ìˆ˜ ìˆë„ë¡ ìƒëµë˜ì–´ ìˆìŠµë‹ˆë‹¤."
  },
  {
    "objectID": "posts/labs/lab01.html#question-1-10-pts",
    "href": "posts/labs/lab01.html#question-1-10-pts",
    "title": "ğŸ‘©â€ğŸ’» Lab 1",
    "section": "Question 1 (10 pts)",
    "text": "Question 1 (10 pts)\nìœ„ weight íˆìŠ¤í† ê·¸ë¨ë“¤ì„ ë³´ê³  ë‹¤ìŒ ì§ˆë¬¸ì— ë‹µí•´ ì£¼ì„¸ìš”.\n\nQuestion 1.1 (5 pts)\nê°ê¸° ë‹¤ë¥¸ ê³„ì¸µì—ì„œ weight ë¶„í¬ë“¤ì˜ ê³µí†µì ì¸ íŠ¹ì„±ì€ ë¬´ì—‡ì¸ê°€ìš”?\nYour Answer:\nmeanì´ 0ì¸ normal ë¶„í¬ë¥¼ ë”°ë¥´ê³  ìˆë‹¤ (backboneì˜ ê²½ìš°, classifier ì œì™¸)\n\n\nQuestion 1.2 (5 pts)\nì´ëŸ¬í•œ íŠ¹ì„±ë“¤ì´ pruningì— ì–´ë–»ê²Œ ë„ì›€ì´ ë˜ë‚˜ìš”?\nYour Answer:\n0ì´ ë§ìœ¼ë¯€ë¡œ, ê³„ì‚°í•˜ì§€ ì•Šê±°ë‚˜ ì—†ì•¨ ìˆ˜ ìˆë‹¤."
  },
  {
    "objectID": "posts/labs/lab01.html#magnitude-based-pruning",
    "href": "posts/labs/lab01.html#magnitude-based-pruning",
    "title": "ğŸ‘©â€ğŸ’» Lab 1",
    "section": "Magnitude-based Pruning",
    "text": "Magnitude-based Pruning\nFine-grained pruningì— ìˆì–´ì„œ ë„ë¦¬ ì‚¬ìš©ë˜ëŠ” importance(ì¤‘ìš”ë„)ëŠ” weight ê°’ì˜ í¬ê¸°, ì¦‰,\n\\(Importance=|W|\\)\nì…ë‹ˆë‹¤. Magnitude-based Pruningìœ¼ë¡œ ì•Œë ¤ì ¸ ìˆìŠµë‹ˆë‹¤ (Learning both Weights and Connections for Efficient Neural Networks ì°¸ì¡°).\n\n\nQuestion 2 (15 pts)\në‹¤ìŒ magnitude-based fine-grained pruning í•¨ìˆ˜ë¥¼ ì™„ì„±í•´ ì£¼ì„¸ìš”.\nHint:\n\n1ë‹¨ê³„ì—ì„œëŠ” pruning í›„ì— 0ì˜ ê°œìˆ˜(num_zeros)ë¥¼ ê³„ì‚°í•©ë‹ˆë‹¤. num_zerosëŠ” ì •ìˆ˜ì—¬ì•¼ í•©ë‹ˆë‹¤. ë¶€ë™ ì†Œìˆ˜ì  ìˆ«ìë¥¼ ì •ìˆ˜ë¡œ ë³€í™˜í•˜ê¸° ìœ„í•´ round() ë˜ëŠ” int()ë¥¼ ì‚¬ìš©í•  ìˆ˜ ìˆìŠµë‹ˆë‹¤. ì—¬ê¸°ì„œëŠ” round()ë¥¼ ì‚¬ìš©í•©ë‹ˆë‹¤.\n2ë‹¨ê³„ì—ì„œëŠ” ê°€ì¤‘ì¹˜ í…ì„œì˜ importanceë¥¼ ê³„ì‚°í•©ë‹ˆë‹¤. PytorchëŠ” torch.abs(), torch.Tensor.abs(), torch.Tensor.abs_() APIë¥¼ ì œê³µí•©ë‹ˆë‹¤.\n3ë‹¨ê³„ì—ì„œëŠ” thresholdë¥¼ ê³„ì‚°í•˜ì—¬ thresholdë³´ë‹¤ ì¤‘ìš”ë„ê°€ ë‚®ì€ ëª¨ë“  synapsesê°€ ì œê±°ë˜ë„ë¡ í•©ë‹ˆë‹¤. PytorchëŠ” torch.kthvalue(), torch.Tensor.kthvalue(), torch.topk() APIë¥¼ ì œê³µí•©ë‹ˆë‹¤.\n4ë‹¨ê³„ì—ì„œëŠ” thresholdë¥¼ ê¸°ë°˜ìœ¼ë¡œ pruning maskë¥¼ ê³„ì‚°í•©ë‹ˆë‹¤. maskì—ì„œ 1ì€ synapseê°€ ìœ ì§€ë¨ì„ ë‚˜íƒ€ë‚´ê³ , 0ì€ synapseê°€ ì œê±°ë¨ì„ ë‚˜íƒ€ëƒ…ë‹ˆë‹¤. mask = importance &gt; threshold. PytorchëŠ” torch.gt() APIë¥¼ ì œê³µí•©ë‹ˆë‹¤.\n\n\ndef fine_grained_prune(tensor: torch.Tensor, sparsity : float) -&gt; torch.Tensor:\n    \"\"\"\n    magnitude-based pruning for single tensor\n    :param tensor: torch.(cuda.)Tensor, weight of conv/fc layer\n    :param sparsity: float, pruning sparsity\n        sparsity = #zeros / #elements = 1 - #nonzeros / #elements\n    :return:\n        torch.(cuda.)Tensor, mask for zeros\n    \"\"\"\n    sparsity = min(max(0.0, sparsity), 1.0)\n    if sparsity == 1.0:\n        tensor.zero_()\n        return torch.zeros_like(tensor)\n    elif sparsity == 0.0:\n        return torch.ones_like(tensor)\n\n    num_elements = tensor.numel()\n\n    ##################### YOUR CODE STARTS HERE #####################\n    # Step 1: calculate the #zeros (please use round())\n    num_zeros = round(num_elements * sparsity)\n    # Step 2: calculate the importance of weight\n    importance = torch.abs(tensor)\n    # Step 3: calculate the pruning threshold\n    threshold = torch.kthvalue(torch.flatten(importance), num_zeros)[0]\n    # Step 4: get binary mask (1 for nonzeros, 0 for zeros)\n    mask = importance &gt; threshold\n\n    ##################### YOUR CODE ENDS HERE #######################\n\n    # Step 5: apply mask to prune the tensor\n    tensor.mul_(mask)\n\n    return mask\n\nìœ„ì—ì„œ ì •ì˜í•œ fine-grained pruning ê¸°ëŠ¥ì„ í™•ì¸í•˜ê¸° ìœ„í•´, ë”ë¯¸ í…ì„œì— ìœ„ í•¨ìˆ˜ë¥¼ ì ìš©í•´ ë´…ì‹œë‹¤.\n\ntest_fine_grained_prune()\n\n\n\n\n\n\n\n\n* Test fine_grained_prune()\n    target sparsity: 0.75\n        sparsity before pruning: 0.04\n        sparsity after pruning: 0.76\n        sparsity of pruning mask: 0.76\n* Test passed.\n\n\n\n\nQuestion 3 (5 pts)\në§ˆì§€ë§‰ ì…€ì€ pruning ì „í›„ì˜ í…ì„œë¥¼ ê·¸ë¦½ë‹ˆë‹¤. 0ì´ ì•„ë‹Œ ê°’ì€ íŒŒë€ìƒ‰ìœ¼ë¡œ, 0ì€ íšŒìƒ‰ìœ¼ë¡œ í‘œì‹œë©ë‹ˆë‹¤. ë‹¤ìŒ ì½”ë“œ ì…€ì—ì„œ target_sparsityì˜ ê°’ì„ ìˆ˜ì •í•˜ì—¬ pruning í›„ sparse í…ì„œì— 0ì´ ì•„ë‹Œ ê°’ì´ 10ê°œë§Œ ë‚¨ë„ë¡ í•´ ì£¼ì„¸ìš”.\n\n##################### YOUR CODE STARTS HERE #####################\n# sparsity:=#Zeros/#ğ‘Š=1âˆ’#Nonzeros/#ğ‘Š\n# 1 - 10/25\ntarget_sparsity = 0.6 # please modify the value of target_sparsit\n##################### YOUR CODE ENDS HERE #####################\ntest_fine_grained_prune(target_sparsity=target_sparsity, target_nonzeros=10)\n\n\n\n\n\n\n\n\n* Test fine_grained_prune()\n    target sparsity: 0.60\n        sparsity before pruning: 0.04\n        sparsity after pruning: 0.60\n        sparsity of pruning mask: 0.60\n* Test passed.\n\n\nì´ì œ fine-grained pruning í•¨ìˆ˜ë¥¼ ì „ì²´ ëª¨ë¸ì„ pruningí•˜ëŠ” í´ë˜ìŠ¤ë¡œ ë˜í•‘í•©ë‹ˆë‹¤. FineGrainedPruner í´ë˜ìŠ¤ì—ì„œëŠ” ëª¨ë¸ ê°€ì¤‘ì¹˜ê°€ ë³€ê²½ë  ë•Œë§ˆë‹¤ ë§ˆìŠ¤í¬ë¥¼ ì ìš©í•˜ì—¬ ëª¨ë¸ì´ í•­ìƒ sparse ìƒíƒœë¥¼ ìœ ì§€í•  ìˆ˜ ìˆë„ë¡ pruning ë§ˆìŠ¤í¬ ê¸°ë¡ì„ ê°€ì§€ê³  ìˆì–´ì•¼ í•©ë‹ˆë‹¤.\n\nclass FineGrainedPruner:\n    def __init__(self, model, sparsity_dict):\n        self.masks = FineGrainedPruner.prune(model, sparsity_dict)\n\n    @torch.no_grad()\n    def apply(self, model):\n        for name, param in model.named_parameters():\n            if name in self.masks:\n                param *= self.masks[name]\n\n    @staticmethod\n    @torch.no_grad()\n    def prune(model, sparsity_dict):\n        masks = dict()\n        for name, param in model.named_parameters():\n            if param.dim() &gt; 1: # we only prune conv and fc weights\n                masks[name] = fine_grained_prune(param, sparsity_dict[name])\n        return masks"
  },
  {
    "objectID": "posts/labs/lab01.html#sensitivity-scan",
    "href": "posts/labs/lab01.html#sensitivity-scan",
    "title": "ğŸ‘©â€ğŸ’» Lab 1",
    "section": "Sensitivity Scan",
    "text": "Sensitivity Scan\nê° ë ˆì´ì–´ëŠ” ëª¨ë¸ ì„±ëŠ¥ì— ëŒ€í•´ ê°ê° ë‹¤ë¥´ê²Œ ê¸°ì—¬í•©ë‹ˆë‹¤. ê° ë ˆì´ì–´ì— ì ì ˆí•œ sparsityë¥¼ ê²°ì •í•˜ëŠ” ê²ƒì€ ì–´ë ¤ìš´ ì¼ì…ë‹ˆë‹¤. ë„ë¦¬ ì‚¬ìš©ë˜ëŠ” ì ‘ê·¼ ë°©ì‹ì€ sensitivity scanì…ë‹ˆë‹¤.\nsensitivity scan ë™ì•ˆ, ê° ì‹œê°„ë§ˆë‹¤ í•˜ë‚˜ì˜ ë ˆì´ì–´ë§Œì„ pruneí•˜ì—¬ accuracy ì €í•˜ë¥¼ ê´€ì°°í•©ë‹ˆë‹¤. ë‹¤ì–‘í•œ sparsitiesë¥¼ ìŠ¤ìº”í•¨ìœ¼ë¡œì¨, í•´ë‹¹ ë ˆì´ì–´ì˜ sensitivity curve (ì¦‰, ì •í™•ë„ ëŒ€ë¹„ sparsity)ë¥¼ ê·¸ë¦´ ìˆ˜ ìˆìŠµë‹ˆë‹¤.\në‹¤ìŒì€ sensitivity curvesì˜ ì˜ˆì‹œ ê·¸ë¦¼ì…ë‹ˆë‹¤. xì¶•ì€ sparsity ë˜ëŠ” #parametersê°€ ê°ì†Œí•œ ë¹„ìœ¨ (ì¦‰, sparsity)ì…ë‹ˆë‹¤. yì¶•ì€ ê²€ì¦ ì •í™•ë„ì…ë‹ˆë‹¤. (Learning both Weights and Connections for Efficient Neural Networksì˜ Figure 6)\n\në‹¤ìŒ ì½”ë“œ ì…€ì€ ìŠ¤ìº”ëœ sparsitiesì™€ ê° ê°€ì¤‘ì¹˜ê°€ pruneë  ë•Œì˜ ì •í™•ë„ ë¦¬ìŠ¤íŠ¸ë¥¼ ë°˜í™˜í•˜ëŠ” sensitivity scan í•¨ìˆ˜ë¥¼ ì •ì˜í•©ë‹ˆë‹¤.\n\n@torch.no_grad()\ndef sensitivity_scan(model, dataloader, scan_step=0.1, scan_start=0.4, scan_end=1.0, verbose=True):\n    sparsities = np.arange(start=scan_start, stop=scan_end, step=scan_step)\n    accuracies = []\n    named_conv_weights = [(name, param) for (name, param) \\\n                          in model.named_parameters() if param.dim() &gt; 1]\n    for i_layer, (name, param) in enumerate(named_conv_weights):\n        param_clone = param.detach().clone()\n        accuracy = []\n        for sparsity in tqdm(sparsities, desc=f'scanning {i_layer}/{len(named_conv_weights)} weight - {name}'):\n            fine_grained_prune(param.detach(), sparsity=sparsity)\n            acc = evaluate(model, dataloader, verbose=False)\n            if verbose:\n                print(f'\\r    sparsity={sparsity:.2f}: accuracy={acc:.2f}%', end='')\n            # restore\n            param.copy_(param_clone)\n            accuracy.append(acc)\n        if verbose:\n            print(f'\\r    sparsity=[{\",\".join([\"{:.2f}\".format(x) for x in sparsities])}]: accuracy=[{\", \".join([\"{:.2f}%\".format(x) for x in accuracy])}]', end='')\n        accuracies.append(accuracy)\n    return sparsities, accuracies\n\në‹¤ìŒ ì…€ë“¤ì„ ì‹¤í–‰í•˜ì—¬ sensitivity curvesë¥¼ ê·¸ë ¤ì£¼ì„¸ìš”. ì™„ë£Œí•˜ëŠ” ë° ì•½ 2ë¶„ ì •ë„ ê±¸ë¦´ ê²ƒì…ë‹ˆë‹¤.\n\nsparsities, accuracies = sensitivity_scan(\n    model, dataloader['test'], scan_step=0.1, scan_start=0.4, scan_end=1.0)\n\n\n\n\n    sparsity=[0.40,0.50,0.60,0.70,0.80,0.90]: accuracy=[92.42%, 91.19%, 87.55%, 83.39%, 69.41%, 31.81%]    sparsity=[0.40,0.50,0.60,0.70,0.80,0.90]: accuracy=[92.93%, 92.88%, 92.71%, 92.40%, 91.32%, 84.78%]    sparsity=[0.40,0.50,0.60,0.70,0.80,0.90]: accuracy=[92.94%, 92.64%, 92.46%, 91.77%, 89.85%, 78.56%]    sparsity=[0.40,0.50,0.60,0.70,0.80,0.90]: accuracy=[92.86%, 92.72%, 92.23%, 91.09%, 85.35%, 51.31%]    sparsity=[0.40,0.50,0.60,0.70,0.80,0.90]: accuracy=[92.88%, 92.68%, 92.22%, 89.47%, 76.86%, 38.78%]    sparsity=[0.40,0.50,0.60,0.70,0.80,0.90]: accuracy=[92.92%, 92.71%, 92.63%, 91.88%, 89.90%, 82.19%]    sparsity=[0.40,0.50,0.60,0.70,0.80,0.90]: accuracy=[92.94%, 92.86%, 92.65%, 92.10%, 90.58%, 83.65%]    sparsity=[0.40,0.50,0.60,0.70,0.80,0.90]: accuracy=[92.94%, 92.92%, 92.88%, 92.81%, 92.63%, 91.34%]    sparsity=[0.40,0.50,0.60,0.70,0.80,0.90]: accuracy=[92.91%, 92.83%, 92.81%, 92.97%, 92.68%, 92.52%]\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\ndef plot_sensitivity_scan(sparsities, accuracies, dense_model_accuracy):\n    lower_bound_accuracy = 100 - (100 - dense_model_accuracy) * 1.5\n    fig, axes = plt.subplots(3, int(math.ceil(len(accuracies) / 3)),figsize=(15,8))\n    axes = axes.ravel()\n    plot_index = 0\n    for name, param in model.named_parameters():\n        if param.dim() &gt; 1:\n            ax = axes[plot_index]\n            curve = ax.plot(sparsities, accuracies[plot_index])\n            line = ax.plot(sparsities, [lower_bound_accuracy] * len(sparsities))\n            ax.set_xticks(np.arange(start=0.4, stop=1.0, step=0.1))\n            ax.set_ylim(80, 95)\n            ax.set_title(name)\n            ax.set_xlabel('sparsity')\n            ax.set_ylabel('top-1 accuracy')\n            ax.legend([\n                'accuracy after pruning',\n                f'{lower_bound_accuracy / dense_model_accuracy * 100:.0f}% of dense model accuracy'\n            ])\n            ax.grid(axis='x')\n            plot_index += 1\n    fig.suptitle('Sensitivity Curves: Validation Accuracy vs. Pruning Sparsity')\n    fig.tight_layout()\n    fig.subplots_adjust(top=0.925)\n    plt.show()\n\nplot_sensitivity_scan(sparsities, accuracies, dense_model_accuracy)\n\n\n\n\n\n\n\n\n\nQuestion 4 (15 pts)\nìœ„ sensitivity curvesì˜ ì •ë³´ë¥¼ ì‚¬ìš©í•˜ì—¬ ë‹¤ìŒ ì§ˆë¬¸ì— ë‹µí•´ ì£¼ì„¸ìš”.\n\nQuestion 4.1 (5 pts)\npruning sparsityì™€ ëª¨ë¸ ì •í™•ë„ ì‚¬ì´ì˜ ê´€ê³„ëŠ” ë¬´ì—‡ì¸ê°€ìš”? (ì¦‰, sparsityê°€ ë†’ì•„ì§ˆ ë•Œ ì •í™•ë„ê°€ ì¦ê°€í•˜ë‚˜ìš”, ì•„ë‹ˆë©´ ê°ì†Œí•˜ë‚˜ìš”?)\nYour Answer:\npruning sparsityê°€ ë†’ì•„ì§ˆ ìˆ˜ë¡, model accuracyëŠ” ê°ì†Œí•˜ëŠ” ê²½í–¥ì„ ë³´ì¸ë‹¤\n\n\nQuestion 4.2 (5 pts)\nëª¨ë“  ë ˆì´ì–´ê°€ ê°™ì€ sensitivityë¥¼ ê°€ì§€ê³  ìˆë‚˜ìš”?\nYour Answer:\nì–´ë–¤ ë ˆì´ì–´ëŠ” sensitiveí•˜ì§€ ì•Šê³ (classifier), ì–´ë–¤ ë ˆì´ì–´ëŠ” sensitiveí•˜ë‹¤(conv0) ëŒ€ì²´ë¡œ, ì•ìª½ ë ˆì´ì–´(0~1..)ì´ ë¯¼ê°í•´ë³´ì¸ë‹¤\n\n\nQuestion 4.3 (5 pts)\nì–´ë–¤ ë ˆì´ì–´ê°€ pruning sparsityì— ê°€ì¥ ë¯¼ê°í•œê°€ìš”?\nYour Answer:\nconv0 layer"
  },
  {
    "objectID": "posts/labs/lab01.html#parameters-of-each-layer",
    "href": "posts/labs/lab01.html#parameters-of-each-layer",
    "title": "ğŸ‘©â€ğŸ’» Lab 1",
    "section": "#Parameters of each layer",
    "text": "#Parameters of each layer\nì •í™•ë„ë¿ë§Œ ì•„ë‹ˆë¼ ê° ë ˆì´ì–´ì˜ ë§¤ê°œë³€ìˆ˜(parameter) ìˆ˜ë„ sparsity ì„ íƒì— ì˜í–¥ì„ ë¯¸ì¹©ë‹ˆë‹¤. ë§¤ê°œë³€ìˆ˜ê°€ ë” ë§ì€ ë ˆì´ì–´ëŠ” ë” í° sparsitiesë¥¼ ìš”êµ¬í•©ë‹ˆë‹¤.\në‹¤ìŒ ì½”ë“œ ì…€ì„ ì‹¤í–‰í•˜ì—¬ ì „ì²´ ëª¨ë¸ì—ì„œ #parametersì˜ ë¶„í¬ë¥¼ ê·¸ë ¤ì£¼ì„¸ìš”.\n\ndef plot_num_parameters_distribution(model):\n    num_parameters = dict()\n    for name, param in model.named_parameters():\n        if param.dim() &gt; 1:\n            num_parameters[name] = param.numel()\n    fig = plt.figure(figsize=(8, 6))\n    plt.grid(axis='y')\n    plt.bar(list(num_parameters.keys()), list(num_parameters.values()))\n    plt.title('#Parameter Distribution')\n    plt.ylabel('Number of Parameters')\n    plt.xticks(rotation=60)\n    plt.tight_layout()\n    plt.show()\n\nplot_num_parameters_distribution(model)"
  },
  {
    "objectID": "posts/labs/lab01.html#sensitivity-curvesì™€-parameters-ë¶„í¬ë¥¼-ê¸°ë°˜ìœ¼ë¡œ-sparsity-ì„ íƒí•˜ê¸°",
    "href": "posts/labs/lab01.html#sensitivity-curvesì™€-parameters-ë¶„í¬ë¥¼-ê¸°ë°˜ìœ¼ë¡œ-sparsity-ì„ íƒí•˜ê¸°",
    "title": "ğŸ‘©â€ğŸ’» Lab 1",
    "section": "Sensitivity Curvesì™€ #Parameters ë¶„í¬ë¥¼ ê¸°ë°˜ìœ¼ë¡œ Sparsity ì„ íƒí•˜ê¸°",
    "text": "Sensitivity Curvesì™€ #Parameters ë¶„í¬ë¥¼ ê¸°ë°˜ìœ¼ë¡œ Sparsity ì„ íƒí•˜ê¸°\n\nQuestion 5 (10 pts)\nsensitivity curvesì™€ ëª¨ë¸ì˜ #parameters ë¶„í¬ë¥¼ ê¸°ë°˜ìœ¼ë¡œ ê° ë ˆì´ì–´ì˜ sparsityë¥¼ ì„ íƒí•´ ì£¼ì„¸ìš”.\npruned ëª¨ë¸ì˜ ì „ì²´ ì••ì¶• ë¹„ìœ¨ì€ ëŒ€ì²´ë¡œ #parametersê°€ í° ë ˆì´ì–´ì— ì£¼ë¡œ ì˜ì¡´í•˜ë©°, ë‹¤ë¥¸ ë ˆì´ì–´ëŠ” pruningì— ëŒ€í•œ sensitivityê°€ ë‹¤ë¦…ë‹ˆë‹¤(Question 4 ì°¸ì¡°).\npruning í›„ì— sparse ëª¨ë¸ì´ dense ëª¨ë¸ì˜ í¬ê¸°ì˜ 25%ì´ë©°, finetuning í›„ì— ê²€ì¦ ì •í™•ë„ê°€ 92.5% ì´ìƒì¸ì§€ í™•ì¸í•˜ì„¸ìš”.\nHint:\n\n#parametersê°€ ë” ë§ì€ ë ˆì´ì–´ëŠ” ë” í° sparsityë¥¼ ê°€ì ¸ì•¼ í•©ë‹ˆë‹¤. (Figure #Parameter Distribution ì°¸ì¡°)\npruning sparsityì— ë¯¼ê°í•œ ë ˆì´ì–´(ì¦‰, sparsityê°€ ë†’ì•„ì§ˆìˆ˜ë¡ ì •í™•ë„ê°€ ë¹ ë¥´ê²Œ ë–¨ì–´ì§€ëŠ” ë ˆì´ì–´)ëŠ” ë” ì‘ì€ sparsityë¥¼ ê°€ì ¸ì•¼ í•©ë‹ˆë‹¤. (Figure Sensitivity Curves ì°¸ì¡°)\n\n\nrecover_model()\n\nsparsity_dict = {\n##################### YOUR CODE STARTS HERE #####################\n    # please modify the sparsity value of each layer\n    # please DO NOT modify the key of sparsity_dict\n    'backbone.conv0.weight': 0,\n    'backbone.conv1.weight': 0.5,\n    'backbone.conv2.weight': 0.5,\n    'backbone.conv3.weight': 0.5,\n    'backbone.conv4.weight': 0.5,\n    'backbone.conv5.weight': 0.8,\n    'backbone.conv6.weight': 0.8,\n    'backbone.conv7.weight': 0.9,\n    'classifier.weight': 0\n##################### YOUR CODE ENDS HERE #######################\n}\n\nì •ì˜ëœ sparsity_dictì— ë”°ë¼ ëª¨ë¸ì„ pruneí•˜ê³  sparse ëª¨ë¸ì˜ ì •ë³´ë¥¼ ì¶œë ¥í•˜ê¸° ìœ„í•´ ë‹¤ìŒ ì…€ì„ ì‹¤í–‰í•´ ì£¼ì„¸ìš”.\n\npruner = FineGrainedPruner(model, sparsity_dict)\nprint(f'After pruning with sparsity dictionary')\nfor name, sparsity in sparsity_dict.items():\n    print(f'  {name}: {sparsity:.2f}')\nprint(f'The sparsity of each layer becomes')\nfor name, param in model.named_parameters():\n    if name in sparsity_dict:\n        print(f'  {name}: {get_sparsity(param):.2f}')\n\nsparse_model_size = get_model_size(model, count_nonzero_only=True)\nprint(f\"Sparse model has size={sparse_model_size / MiB:.2f} MiB = {sparse_model_size / dense_model_size * 100:.2f}% of dense model size\")\nsparse_model_accuracy = evaluate(model, dataloader['test'])\nprint(f\"Sparse model has accuracy={sparse_model_accuracy:.2f}% before fintuning\")\n\nplot_weight_distribution(model, count_nonzero_only=True)\n\nAfter pruning with sparsity dictionary\n  backbone.conv0.weight: 0.00\n  backbone.conv1.weight: 0.50\n  backbone.conv2.weight: 0.50\n  backbone.conv3.weight: 0.50\n  backbone.conv4.weight: 0.50\n  backbone.conv5.weight: 0.80\n  backbone.conv6.weight: 0.80\n  backbone.conv7.weight: 0.90\n  classifier.weight: 0.00\nThe sparsity of each layer becomes\n  backbone.conv0.weight: 0.00\n  backbone.conv1.weight: 0.50\n  backbone.conv2.weight: 0.50\n  backbone.conv3.weight: 0.50\n  backbone.conv4.weight: 0.50\n  backbone.conv5.weight: 0.80\n  backbone.conv6.weight: 0.80\n  backbone.conv7.weight: 0.90\n  classifier.weight: 0.00\nSparse model has size=8.63 MiB = 24.50% of dense model size\nSparse model has accuracy=87.00% before fintuning"
  },
  {
    "objectID": "posts/labs/lab01.html#finetune-the-fine-grained-pruned-model",
    "href": "posts/labs/lab01.html#finetune-the-fine-grained-pruned-model",
    "title": "ğŸ‘©â€ğŸ’» Lab 1",
    "section": "Finetune the fine-grained pruned model",
    "text": "Finetune the fine-grained pruned model\nì´ì „ ì…€ì˜ ì¶œë ¥ì—ì„œ ë³¼ ìˆ˜ ìˆë“¯ì´, fine-grained pruningì´ ëª¨ë¸ ê°€ì¤‘ì¹˜ì˜ ëŒ€ë¶€ë¶„ì„ ì¤„ì´ì§€ë§Œ ëª¨ë¸ì˜ ì •í™•ë„ë„ ë–¨ì–´ì¡ŒìŠµë‹ˆë‹¤. ë”°ë¼ì„œ, sparse ëª¨ë¸ì˜ ì •í™•ë„ë¥¼ íšŒë³µí•˜ê¸° ìœ„í•´ finetuneí•´ì•¼ í•©ë‹ˆë‹¤.\nsparse ëª¨ë¸ì„ finetuneí•˜ê¸° ìœ„í•´ ë‹¤ìŒ ì…€ì„ ì‹¤í–‰í•´ ì£¼ì„¸ìš”. ì™„ë£Œí•˜ëŠ” ë° ì•½ 3ë¶„ ì •ë„ ê±¸ë¦´ ê²ƒì…ë‹ˆë‹¤.\n\nnum_finetune_epochs = 5\noptimizer = torch.optim.SGD(model.parameters(), lr=0.01, momentum=0.9, weight_decay=1e-4)\nscheduler = torch.optim.lr_scheduler.CosineAnnealingLR(optimizer, num_finetune_epochs)\ncriterion = nn.CrossEntropyLoss()\n\nbest_sparse_model_checkpoint = dict()\nbest_accuracy = 0\nprint(f'Finetuning Fine-grained Pruned Sparse Model')\nfor epoch in range(num_finetune_epochs):\n    # At the end of each train iteration, we have to apply the pruning mask\n    #    to keep the model sparse during the training\n    train(model, dataloader['train'], criterion, optimizer, scheduler,\n          callbacks=[lambda: pruner.apply(model)])\n    accuracy = evaluate(model, dataloader['test'])\n    is_best = accuracy &gt; best_accuracy\n    if is_best:\n        best_sparse_model_checkpoint['state_dict'] = copy.deepcopy(model.state_dict())\n        best_accuracy = accuracy\n    print(f'    Epoch {epoch+1} Accuracy {accuracy:.2f}% / Best Accuracy: {best_accuracy:.2f}%')\n\nFinetuning Fine-grained Pruned Sparse Model\n    Epoch 1 Accuracy 92.66% / Best Accuracy: 92.66%\n    Epoch 2 Accuracy 92.77% / Best Accuracy: 92.77%\n    Epoch 3 Accuracy 92.80% / Best Accuracy: 92.80%\n    Epoch 4 Accuracy 92.68% / Best Accuracy: 92.80%\n    Epoch 5 Accuracy 92.77% / Best Accuracy: 92.80%\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nbest finetuned sparse ëª¨ë¸ì˜ ì •ë³´ë¥¼ ë³´ê¸° ìœ„í•´ ë‹¤ìŒ ì…€ì„ ì‹¤í–‰í•´ ì£¼ì„¸ìš”.\n\n# load the best sparse model checkpoint to evaluate the final performance\nmodel.load_state_dict(best_sparse_model_checkpoint['state_dict'])\nsparse_model_size = get_model_size(model, count_nonzero_only=True)\nprint(f\"Sparse model has size={sparse_model_size / MiB:.2f} MiB = {sparse_model_size / dense_model_size * 100:.2f}% of dense model size\")\nsparse_model_accuracy = evaluate(model, dataloader['test'])\nprint(f\"Sparse model has accuracy={sparse_model_accuracy:.2f}% after fintuning\")\n\nSparse model has size=8.63 MiB = 24.50% of dense model size\nSparse model has accuracy=92.80% after fintuning"
  },
  {
    "objectID": "posts/labs/lab01.html#remove-channel-weights",
    "href": "posts/labs/lab01.html#remove-channel-weights",
    "title": "ğŸ‘©â€ğŸ’» Lab 1",
    "section": "Remove Channel Weights",
    "text": "Remove Channel Weights\nFine-grained pruningê³¼ ë‹¬ë¦¬, channel pruningì—ì„œëŠ” í…ì„œì—ì„œ ê°€ì¤‘ì¹˜ë¥¼ ì™„ì „íˆ ì œê±°í•  ìˆ˜ ìˆìŠµë‹ˆë‹¤. ì¦‰, ì¶œë ¥ ì±„ë„ì˜ ìˆ˜ê°€ ì¤„ì–´ë“­ë‹ˆë‹¤:\n\n\\(\\#\\mathrm{out\\_channels}_{\\mathrm{new}} = \\#\\mathrm{out\\_channels}_{\\mathrm{origin}} \\cdot (1 - \\mathrm{sparsity})\\)\n\nChannel pruning í›„ì—ë„ ê°€ì¤‘ì¹˜ í…ì„œ \\(W\\)ëŠ” ì—¬ì „íˆ denseí•©ë‹ˆë‹¤. ë”°ë¼ì„œ, sparsityë¥¼ prune ratioë¼ê³  í•©ë‹ˆë‹¤.\nFine-grained pruningì²˜ëŸ¼, ë‹¤ë¥¸ ë ˆì´ì–´ì— ëŒ€í•´ ë‹¤ë¥¸ pruning ë¹„ìœ¨ì„ ì‚¬ìš©í•  ìˆ˜ ìˆìŠµë‹ˆë‹¤. í•˜ì§€ë§Œ ì§€ê¸ˆì€ ëª¨ë“  ë ˆì´ì–´ì— ëŒ€í•´ ê· ì¼í•œ pruning ë¹„ìœ¨ì„ ì‚¬ìš©í•©ë‹ˆë‹¤. ìš°ë¦¬ëŠ” ëŒ€ëµ 30%ì˜ ê· ì¼í•œ pruning ë¹„ìœ¨ë¡œ 2ë°°ì˜ ê³„ì‚° ê°ì†Œë¥¼ ëª©í‘œë¡œ í•©ë‹ˆë‹¤(ì™œ ê·¸ëŸ°ì§€ ìƒê°í•´ ë³´ì„¸ìš”).\nì´ ì„¹ì…˜ì˜ ëì—ì„œ ë ˆì´ì–´ë³„ë¡œ ë‹¤ë¥¸ pruning ë¹„ìœ¨ì„ ì‹œë„í•´ ë³¼ ìˆ˜ ìˆìŠµë‹ˆë‹¤. channel_prune í•¨ìˆ˜ì— ë¹„ìœ¨ ë¦¬ìŠ¤íŠ¸ë¥¼ ì „ë‹¬í•  ìˆ˜ ìˆìŠµë‹ˆë‹¤.\n\nQuestion 6 (10 pts)\nChannel pruningì„ ìœ„í•œ ë‹¤ìŒ í•¨ìˆ˜ë¥¼ ì™„ì„±í•´ ì£¼ì„¸ìš”.\nì—¬ê¸°ì„œ ìš°ë¦¬ëŠ” ì²« ë²ˆì§¸ \\(\\#\\mathrm{out\\_channels}_{\\mathrm{new}}\\) ì±„ë„ì„ ì œì™¸í•œ ëª¨ë“  ì¶œë ¥ ì±„ë„ì„ ë‹¨ìˆœíˆ pruneí•©ë‹ˆë‹¤.\n\ndef get_num_channels_to_keep(channels: int, prune_ratio: float) -&gt; int:\n    \"\"\"A function to calculate the number of layers to PRESERVE after pruning\n    Note that preserve_rate = 1. - prune_ratio\n    \"\"\"\n    ##################### YOUR CODE STARTS HERE #####################\n    return int(round((1-prune_ratio)*channels))\n    ##################### YOUR CODE ENDS HERE #####################\n\n@torch.no_grad()\ndef channel_prune(model: nn.Module,\n                  prune_ratio: Union[List, float]) -&gt; nn.Module:\n    \"\"\"Apply channel pruning to each of the conv layer in the backbone\n    Note that for prune_ratio, we can either provide a floating-point number,\n    indicating that we use a uniform pruning rate for all layers, or a list of\n    numbers to indicate per-layer pruning rate.\n    \"\"\"\n    # sanity check of provided prune_ratio\n    assert isinstance(prune_ratio, (float, list))\n    n_conv = len([m for m in model.backbone if isinstance(m, nn.Conv2d)])\n    # note that for the ratios, it affects the previous conv output and next\n    # conv input, i.e., conv0 - ratio0 - conv1 - ratio1-...\n    if isinstance(prune_ratio, list):\n        assert len(prune_ratio) == n_conv - 1\n    else:  # convert float to list\n        prune_ratio = [prune_ratio] * (n_conv - 1)\n\n    # we prune the convs in the backbone with a uniform ratio\n    model = copy.deepcopy(model)  # prevent overwrite\n    # we only apply pruning to the backbone features\n    all_convs = [m for m in model.backbone if isinstance(m, nn.Conv2d)]\n    all_bns = [m for m in model.backbone if isinstance(m, nn.BatchNorm2d)]\n    # apply pruning. we naively keep the first k channels\n    assert len(all_convs) == len(all_bns)\n    for i_ratio, p_ratio in enumerate(prune_ratio):\n        prev_conv = all_convs[i_ratio]\n        prev_bn = all_bns[i_ratio]\n        next_conv = all_convs[i_ratio + 1]\n        original_channels = prev_conv.out_channels  # same as next_conv.in_channels\n        n_keep = get_num_channels_to_keep(original_channels, p_ratio)\n\n        # prune the output of the previous conv and bn\n        prev_conv.weight.set_(prev_conv.weight.detach()[:n_keep])\n        prev_bn.weight.set_(prev_bn.weight.detach()[:n_keep])\n        prev_bn.bias.set_(prev_bn.bias.detach()[:n_keep])\n        prev_bn.running_mean.set_(prev_bn.running_mean.detach()[:n_keep])\n        prev_bn.running_var.set_(prev_bn.running_var.detach()[:n_keep])\n\n        # prune the input of the next conv (hint: just one line of code)\n        ##################### YOUR CODE STARTS HERE #####################\n        next_conv.weight.set_(next_conv.weight.detach()[:, :n_keep])\n        ##################### YOUR CODE ENDS HERE #####################\n\n    return model\n\nêµ¬í˜„ì´ ì˜¬ë°”ë¥¸ì§€ í™•ì¸í•˜ê¸° ìœ„í•´ ë‹¤ìŒ ì…€ì„ ì‹¤í–‰í•˜ì—¬ í™•ì¸í•˜ì„¸ìš”.\n\ndummy_input = torch.randn(1, 3, 32, 32).cuda()\npruned_model = channel_prune(model, prune_ratio=0.3)\npruned_macs = get_model_macs(pruned_model, dummy_input)\nassert pruned_macs == 305388064\nprint('* Check passed. Right MACs for the pruned model.')\n\n* Check passed. Right MACs for the pruned model.\n\n\nì´ì œ 30% pruning ë¹„ìœ¨ì„ ê°€ì§„ ê· ì¼ channel pruning í›„ ëª¨ë¸ì˜ ì„±ëŠ¥ì„ í‰ê°€í•´ ë´…ì‹œë‹¤.\nì§ì ‘ì ìœ¼ë¡œ 30%ì˜ ì±„ë„ì„ ì œê±°í•˜ëŠ” ê²ƒì€ ë‚®ì€ ì •í™•ë„ë¡œ ì´ì–´ì§‘ë‹ˆë‹¤.\n\npruned_model_accuracy = evaluate(pruned_model, dataloader['test'])\nprint(f\"pruned model has accuracy={pruned_model_accuracy:.2f}%\")\n\n\n\n\npruned model has accuracy=28.14%"
  },
  {
    "objectID": "posts/labs/lab01.html#ranking-channels-by-importance",
    "href": "posts/labs/lab01.html#ranking-channels-by-importance",
    "title": "ğŸ‘©â€ğŸ’» Lab 1",
    "section": "Ranking Channels by Importance",
    "text": "Ranking Channels by Importance\në³´ì‹œë‹¤ì‹œí”¼, ëª¨ë“  ë ˆì´ì–´ì—ì„œ ì²« 30%ì˜ ì±„ë„ì„ ì œê±°í•˜ë©´ ì •í™•ë„ê°€ í¬ê²Œ ê°ì†Œí•©ë‹ˆë‹¤. ì´ ë¬¸ì œë¥¼ í•´ê²°í•˜ëŠ” í•œ ê°€ì§€ ê°€ëŠ¥í•œ ë°©ë²•ì€ ëœ ì¤‘ìš”í•œ ì±„ë„ ê°€ì¤‘ì¹˜ë¥¼ ì°¾ì•„ì„œ ì œê±°í•˜ëŠ” ê²ƒì…ë‹ˆë‹¤. ì¤‘ìš”ë„ë¥¼ í‰ê°€í•˜ëŠ” ì¸ê¸° ìˆëŠ” ê¸°ì¤€ì€ ê° ì…ë ¥ ì±„ë„ì— í•´ë‹¹í•˜ëŠ” ê°€ì¤‘ì¹˜ì˜ Frobenius normì„ ì‚¬ìš©í•˜ëŠ” ê²ƒì…ë‹ˆë‹¤:\n\n\\(importance_{i} = \\|W_{i}\\|_2, \\;\\; i = 0, 1, 2,\\cdots, \\#\\mathrm{in\\_channels}-1\\)\n\nìš°ë¦¬ëŠ” ì±„ë„ ê°€ì¤‘ì¹˜ë¥¼ ë” ì¤‘ìš”í•œ ê²ƒì—ì„œ ëœ ì¤‘ìš”í•œ ê²ƒìœ¼ë¡œ ì •ë ¬í•œ ë‹¤ìŒ, ê° ë ˆì´ì–´ì— ëŒ€í•´ ì²˜ìŒ \\(k\\)ê°œì˜ ì±„ë„ì„ ìœ ì§€í•  ìˆ˜ ìˆìŠµë‹ˆë‹¤.\n\nQuestion 7 (15 pts)\nFrobenius normì— ê¸°ë°˜í•˜ì—¬ ê°€ì¤‘ì¹˜ í…ì„œë¥¼ ì •ë ¬í•˜ëŠ” ë‹¤ìŒ í•¨ìˆ˜ë¥¼ ì™„ì„±í•´ ì£¼ì„¸ìš”.\nHint:\n\ní…ì„œì˜ Frobenius normì„ ê³„ì‚°í•˜ê¸° ìœ„í•´, PytorchëŠ” torch.norm APIë¥¼ ì œê³µí•©ë‹ˆë‹¤.\n\n\n# function to sort the channels from important to non-important\ndef get_input_channel_importance(weight):\n    in_channels = weight.shape[1]\n    importances = []\n    # compute the importance for each input channel\n    for i_c in range(weight.shape[1]):\n        channel_weight = weight.detach()[:, i_c]\n        ##################### YOUR CODE STARTS HERE #####################\n        importance = torch.norm(channel_weight, p=2)\n        ##################### YOUR CODE ENDS HERE #####################\n        importances.append(importance.view(1))\n    return torch.cat(importances)\n\n@torch.no_grad()\ndef apply_channel_sorting(model):\n    model = copy.deepcopy(model)  # do not modify the original model\n    # fetch all the conv and bn layers from the backbone\n    all_convs = [m for m in model.backbone if isinstance(m, nn.Conv2d)]\n    all_bns = [m for m in model.backbone if isinstance(m, nn.BatchNorm2d)]\n    # iterate through conv layers\n    for i_conv in range(len(all_convs) - 1):\n        # each channel sorting index, we need to apply it to:\n        # - the output dimension of the previous conv\n        # - the previous BN layer\n        # - the input dimension of the next conv (we compute importance here)\n        prev_conv = all_convs[i_conv]\n        prev_bn = all_bns[i_conv]\n        next_conv = all_convs[i_conv + 1]\n        # note that we always compute the importance according to input channels\n        importance = get_input_channel_importance(next_conv.weight)\n        # sorting from large to small\n        sort_idx = torch.argsort(importance, descending=True)\n\n        # apply to previous conv and its following bn\n        prev_conv.weight.copy_(torch.index_select(\n            prev_conv.weight.detach(), 0, sort_idx))\n        for tensor_name in ['weight', 'bias', 'running_mean', 'running_var']:\n            tensor_to_apply = getattr(prev_bn, tensor_name)\n            tensor_to_apply.copy_(\n                torch.index_select(tensor_to_apply.detach(), 0, sort_idx)\n            )\n\n        # apply to the next conv input (hint: one line of code)\n        ##################### YOUR CODE STARTS HERE #####################\n        next_conv.weight.copy_(torch.index_select(next_conv.weight.detach(), 1, sort_idx))\n        ##################### YOUR CODE ENDS HERE #####################\n\n    return model\n\nì´ì œ ë‹¤ìŒ ì…€ì„ ì‹¤í–‰í•˜ì—¬ ê²°ê³¼ê°€ ì˜¬ë°”ë¥¸ì§€ í™•ì¸í•˜ì„¸ìš”.\n\nprint('Before sorting...')\ndense_model_accuracy = evaluate(model, dataloader['test'])\nprint(f\"dense model has accuracy={dense_model_accuracy:.2f}%\")\n\nprint('After sorting...')\nsorted_model = apply_channel_sorting(model)\nsorted_model_accuracy = evaluate(sorted_model, dataloader['test'])\nprint(f\"sorted model has accuracy={sorted_model_accuracy:.2f}%\")\n\n# make sure accuracy does not change after sorting, since it is\n# equivalent transform\nassert abs(sorted_model_accuracy - dense_model_accuracy) &lt; 0.1\nprint('* Check passed.')\n\nBefore sorting...\ndense model has accuracy=92.95%\nAfter sorting...\nsorted model has accuracy=92.95%\n* Check passed.\n\n\n\n\n\n\n\n\në§ˆì§€ë§‰ìœ¼ë¡œ í”„ë£¨ë‹ëœ ëª¨ë¸ì˜ ì •í™•ë„ë¥¼ ì •ë ¬í•  ë•Œì™€ ê·¸ë ‡ì§€ ì•Šì„ ë•Œë¥¼ ë¹„êµí•©ë‹ˆë‹¤.\n\nchannel_pruning_ratio = 0.3  # pruned-out ratio\n\nprint(\" * Without sorting...\")\npruned_model = channel_prune(model, channel_pruning_ratio)\npruned_model_accuracy = evaluate(pruned_model, dataloader['test'])\nprint(f\"pruned model has accuracy={pruned_model_accuracy:.2f}%\")\n\n\nprint(\" * With sorting...\")\nsorted_model = apply_channel_sorting(model)\npruned_model = channel_prune(sorted_model, channel_pruning_ratio)\npruned_model_accuracy = evaluate(pruned_model, dataloader['test'])\nprint(f\"pruned model has accuracy={pruned_model_accuracy:.2f}%\")\n\n * Without sorting...\npruned model has accuracy=28.14%\n * With sorting...\npruned model has accuracy=36.81%\n\n\n\n\n\n\n\n\në³´ì‹œë‹¤ì‹œí”¼ channel sortingì€ pruned modelì˜ ì •í™•ë„ë¥¼ ì•½ê°„ í–¥ìƒì‹œí‚¬ ìˆ˜ ìˆì§€ë§Œ ì—¬ì „íˆ channel pruningì— ë§¤ìš° ì¼ë°˜ì ì¸ í° ì €í•˜ê°€ ìˆëŠ” ê±¸ ì•Œ ìˆ˜ ìˆìŠµë‹ˆë‹¤. ì´ëŸ¬í•œ ì •í™•ë„ ì €í•˜ë¥¼ íšŒë³µí•˜ê¸° ìœ„í•´ fine-tuningì„ ìˆ˜í–‰í•  ìˆ˜ ìˆìŠµë‹ˆë‹¤.\n\nnum_finetune_epochs = 5\noptimizer = torch.optim.SGD(pruned_model.parameters(), lr=0.01, momentum=0.9, weight_decay=1e-4)\nscheduler = torch.optim.lr_scheduler.CosineAnnealingLR(optimizer, num_finetune_epochs)\ncriterion = nn.CrossEntropyLoss()\n\nbest_accuracy = 0\nfor epoch in range(num_finetune_epochs):\n    train(pruned_model, dataloader['train'], criterion, optimizer, scheduler)\n    accuracy = evaluate(pruned_model, dataloader['test'])\n    is_best = accuracy &gt; best_accuracy\n    if is_best:\n        best_accuracy = accuracy\n    print(f'Epoch {epoch+1} Accuracy {accuracy:.2f}% / Best Accuracy: {best_accuracy:.2f}%')\n\n\n\n\n\n\n\nEpoch 1 Accuracy 91.66% / Best Accuracy: 91.66%\nEpoch 2 Accuracy 92.10% / Best Accuracy: 92.10%\nEpoch 3 Accuracy 92.01% / Best Accuracy: 92.10%\nEpoch 4 Accuracy 92.18% / Best Accuracy: 92.18%\nEpoch 5 Accuracy 92.16% / Best Accuracy: 92.18%"
  },
  {
    "objectID": "posts/labs/lab01.html#measure-acceleration-from-pruning",
    "href": "posts/labs/lab01.html#measure-acceleration-from-pruning",
    "title": "ğŸ‘©â€ğŸ’» Lab 1",
    "section": "Measure acceleration from pruning",
    "text": "Measure acceleration from pruning\nfine-tuningì´ ëë‚˜ë©´ ëª¨ë¸ì€ ì •í™•ë„ë¥¼ ê±°ì˜ íšŒë³µí•©ë‹ˆë‹¤. channel pruningëŠ” fine-grained pruningì— ë¹„í•´ ì¼ë°˜ì ìœ¼ë¡œ ì •í™•ë„ë¥¼ íšŒë³µí•˜ê¸°ê°€ ë” ì–´ë µë‹¤ëŠ” ê²ƒì„ ì´ë¯¸ ì•Œê³  ê³„ì‹¤ ìˆ˜ë„ ìˆìŠµë‹ˆë‹¤. ê·¸ëŸ¬ë‚˜ specialized model formatì´ ì—†ìœ¼ë©´ ì§ì ‘ì ìœ¼ë¡œ ëª¨ë¸ í¬ê¸°ê°€ ì‘ì•„ì§€ê³  ê³„ì‚°ì´ ì‘ì•„ì§‘ë‹ˆë‹¤. GPUì—ì„œë„ ë” ë¹ ë¥´ê²Œ ì‹¤í–‰ë  ìˆ˜ ìˆìŠµë‹ˆë‹¤.\nì´ì œ pruned modelì˜ ëª¨ë¸ í¬ê¸°, ê³„ì‚° ë° ì§€ì—° ì‹œê°„ì„ ë¹„êµí•´ë´…ì‹œë‹¤.\n\n# helper functions to measure latency of a regular PyTorch models.\n#   Unlike fine-grained pruning, channel pruning\n#   can directly leads to model size reduction and speed up.\n@torch.no_grad()\ndef measure_latency(model, dummy_input, n_warmup=20, n_test=100):\n    model.eval()\n    # warmup\n    for _ in range(n_warmup):\n        _ = model(dummy_input)\n    # real test\n    t1 = time.time()\n    for _ in range(n_test):\n        _ = model(dummy_input)\n    t2 = time.time()\n    return (t2 - t1) / n_test  # average latency\n\ntable_template = \"{:&lt;15} {:&lt;15} {:&lt;15} {:&lt;15}\"\nprint (table_template.format('', 'Original','Pruned','Reduction Ratio'))\n\n# 1. measure the latency of the original model and the pruned model on CPU\n#   which simulates inference on an edge device\ndummy_input = torch.randn(1, 3, 32, 32).to('cpu')\npruned_model = pruned_model.to('cpu')\nmodel = model.to('cpu')\n\npruned_latency = measure_latency(pruned_model, dummy_input)\noriginal_latency = measure_latency(model, dummy_input)\nprint(table_template.format('Latency (ms)',\n                            round(original_latency * 1000, 1),\n                            round(pruned_latency * 1000, 1),\n                            round(original_latency / pruned_latency, 1)))\n\n# 2. measure the computation (MACs)\noriginal_macs = get_model_macs(model, dummy_input)\npruned_macs = get_model_macs(pruned_model, dummy_input)\nprint(table_template.format('MACs (M)',\n                            round(original_macs / 1e6),\n                            round(pruned_macs / 1e6),\n                            round(original_macs / pruned_macs, 1)))\n\n# 3. measure the model size (params)\noriginal_param = get_num_parameters(model)\npruned_param = get_num_parameters(pruned_model)\nprint(table_template.format('Param (M)',\n                            round(original_param / 1e6, 2),\n                            round(pruned_param / 1e6, 2),\n                            round(original_param / pruned_param, 1)))\n\n# put model back to cuda\npruned_model = pruned_model.to('cuda')\nmodel = model.to('cuda')\n\n                Original        Pruned          Reduction Ratio\nLatency (ms)    24.2            13.0            1.9            \nMACs (M)        606             305             2.0            \nParam (M)       9.23            5.01            1.8            \n\n\n\nQuestion 8 (10 pts)\nì´ì „ ì½”ë“œì…€ì˜ ì •ë³´ë¥¼ ì´ìš©í•˜ì—¬ ë‹¤ìŒ ì§ˆë¬¸ì— ë‹µë³€í•´ ì£¼ì‹œê¸° ë°”ëë‹ˆë‹¤.\n\nQuestion 8.1 (5 pts)\n30%ì˜ ì±„ë„ì„ ì œê±°í•˜ë©´ ëŒ€ëµ 50%ì˜ ê³„ì‚° ì ˆê° íš¨ê³¼ê°€ ë°œìƒí•˜ëŠ” ì´ìœ ë¥¼ ì„¤ëª…í•˜ì„¸ìš”.\nYour Answer:\nMACì€ 2ë°°, Paramì€ 1.8ë°° ì¤„ì–´ë“¤ì—ˆì§€ë§Œ, latencyëŠ” 1.7ë°°ë§Œ ë” ë¹¨ë¼ì¡Œë‹¤ ë©”ëª¨ë¦¬ ê´€ë ¨ëœ ì´ìœ ë¼ê³  ì¶”ì •ë¨\n\n\nQuestion 8.2 (5 pts)\nì§€ì—° ì‹œê°„ ê°ì†Œ ë¹„ìœ¨(latency reduction ratio)ì´ ê³„ì‚° ê°ì†Œ(computation reduction)ë³´ë‹¤ ì•½ê°„ ì‘ì€ ì´ìœ ë¥¼ ì„¤ëª…í•˜ì„¸ìš”.\nYour Answer:\n0.7^2 = 0.49 ë‹ˆê¹Œ, ì–¼ì¶” 2ë°°. íŒŒë¼ë¯¸í„° ìˆ˜ê°€ ì¤„ì–´ë“¤ìˆ˜ë¡ latencyëŠ” quadraticí•˜ê²Œ ì¤„ì–´ë“ ë‹¤"
  },
  {
    "objectID": "posts/labs/lab01.html#question-9-10-pts",
    "href": "posts/labs/lab01.html#question-9-10-pts",
    "title": "ğŸ‘©â€ğŸ’» Lab 1",
    "section": "Question 9 (10 pts)",
    "text": "Question 9 (10 pts)\nì´ë²ˆ ë©ì—ì„œ ëª¨ë“  ì‹¤í—˜ì„ í•œ í›„ì—ëŠ” fine-grained pruningì™€ channel pruningì— ìµìˆ™í•´ì§ˆ ìˆ˜ ìˆìŠµë‹ˆë‹¤.\nlectureì™€ ì´ë²ˆ labì—ì„œ ë°°ìš´ ë‚´ìš©ì„ í™œìš©í•˜ì—¬ ë‹¤ìŒ ì§ˆë¬¸ì— ë‹µë³€í•´ ì£¼ì‹œê¸° ë°”ëë‹ˆë‹¤.\n\nQuestion 9.1 (5 pts)\nfine-grained pruningì™€ channel pruningì˜ ì¥ë‹¨ì ì€ ë¬´ì—‡ì…ë‹ˆê¹Œ?\ncompression ratio, accuracy, latency, hardware support(i.e., ì „ë¬¸ í•˜ë“œì›¨ì–´ ê°€ì†ê¸° í•„ìš”) ë“±ì˜ ê´€ì ì—ì„œ ë…¼ì˜í•  ìˆ˜ ìˆìŠµë‹ˆë‹¤.\nYour Answer:\n\nfine-grained\n\n\nì¥ì \n\nì •í™•ë„ê°€ ë†’ìŒ\nUsually larger compression ratio since we can flexibly find â€œredundantâ€ weights\n\në‹¨ì \n\ncpu overhead\nmemory overhead\nhardware support í•„ìš”(eieâ€¦)\n\n\n\nchannel pruning\n\n\nì¥ì \n\në¹ ë¥¸ inference\n\në‹¨ì \n\nsmaller compression ratio\n\n\n\n\nQuestion 9.2 (5 pts)\nìŠ¤ë§ˆíŠ¸í°ì—ì„œ ëª¨ë¸ì„ ë” ë¹¨ë¦¬ ì‹¤í–‰ì‹œí‚¤ê³  ì‹¶ë‹¤ë©´, ì–´ë–¤ ê°€ì§€ì¹˜ê¸° ë°©ë²•ì„ ì‚¬ìš©í•  ê²ƒì¸ê°€ìš”? ê·¸ ì´ìœ ëŠ” ë¬´ì—‡ì¸ê°€ìš”?\nYour Answer:\níŠ¹ë³„í•œ í•˜ë“œì›¨ì–´ ì„œí¬íŠ¸ê°€ í•„ìš”í•˜ì§€ì•Šê³ , inference timeì´ ë¹ ë¥¸ channel pruning."
  },
  {
    "objectID": "posts/labs/lab03.html",
    "href": "posts/labs/lab03.html",
    "title": "ğŸ‘©â€ğŸ’» Lab 3",
    "section": "",
    "text": "ì´ë²ˆ ì‹œê°„ì€ Neural Architecture Search(NAS)ì—ì„œ ì‹¤ìŠµì„ í•´ë³´ëŠ” ì‹œê°„ì´ì˜€ì–´ìš”. íŒŒë¼ë¯¸í„°ë¥¼ ê°€ì§€ê³  ë„¤íŠ¸ì›Œí¬ë¥¼ ë” ê¹Šê²Œ ë§Œë“¤ê±°ë‚˜, ì±„ë„ì„ ë” í¬ê²Œ ë§Œë“œëŠ” ë°©ë²•ì— ëŒ€í•´ ì‹¤ì œë¡œ ì½”ë“œ ì˜ˆì‹œê°€ ì¹œì ˆí•˜ê²Œ ë¼ ìˆì–´, ì‹¤í—˜ê²°ê³¼ë¥¼ ìì„¸íˆ ë³´ê¸° ì¢‹ì•˜ë˜ ì˜ˆì œì…ë‹ˆë‹¤. ì˜ì–´ë¡œ ëœ ì„¤ëª…ì€ NAS ê°•ì˜ì— ë‚˜ì˜¤ëŠ” ìë£Œë¼ ê¼­ ì½ìœ¼ì‹¤ í•„ìš”ëŠ” ì—†ì–´ìš”. ê·¸ë¦¬ê³  ì¤‘ê°„ì¤‘ê°„ì— ì´í•´ë¥¼ ë•ê¸° ìœ„í•œ ë‹¤ì´ì–´ê·¸ë¨ì´ë‚˜ ì„¤ëª…ì´ Getting Started ë¶€ë¶„ì— ìˆì–´ì„œ ì°¸ê³ í•˜ì‹œë©´ ì¢‹ì„ ê²ƒ ê°™ì•„ìš”.\nê·¸ëŸ¼ ì‹œì‘í•´ë³´ì‹œì£ !"
  },
  {
    "objectID": "posts/labs/lab03.html#introduction",
    "href": "posts/labs/lab03.html#introduction",
    "title": "ğŸ‘©â€ğŸ’» Lab 3",
    "section": "Introduction",
    "text": "Introduction\nì²˜ìŒì—ëŠ” ì—¬ëŸ¬ ì—°êµ¬ë“¤ê³¼ ì—°êµ¬ì— í•´ë‹¹í•˜ëŠ” ëª¨ë¸ì„ ì–¸ê¸‰í•©ë‹ˆë‹¤. ì €í¬ê°€ ì˜¤ëŠ˜ ì‹¤ìŠµí•  ëª¨ë¸ì€ Once for All(OFA) MCUNet ì´ë‹ˆ ì°¸ê³ í•´ì£¼ì„¸ìš”.\n\n\nì˜¤ëŠ˜ OFA MCUNetì—ì„œëŠ” ì•„ë˜ ê·¸ë¦¼ì²˜ëŸ¼ ì´ë¯¸ í›ˆë ¨í•œ ëª¨ë¸ì„ ê°€ì§€ê³  ì±„ë„ ìˆ˜ë¥¼ ì¤„ì´ê±°ë‚˜, ë ˆì´ì–´ì˜ ìˆ˜ë¥¼ ì¡°ì ˆí•˜ëŠ” ê²ƒê³¼ ê°™ì´ íŒŒë¼ë¯¸í„°ë¥¼ ì¡°ì •í•œ â€œsubsetâ€ë“¤ì„ ê°€ì§€ê³  ë©”ëª¨ë¦¬ì™€ ì—°ì‚°ì†ë„(MAC)ë¥¼ í‰ê°€í•  ê²ë‹ˆë‹¤. ê·¸ë¦¬ê³  ì €í¬ê°€ ì›í•˜ëŠ” MACê³¼ Peak Memoryë¥¼ ê°€ì§„ ëª¨ë¸ì„ ì°¾ì•„ë³´ëŠ” ê²ƒì´ì£ .\n\nì–´ë–»ê²Œ Constraintì— ë§ëŠ” ëª¨ë¸ì„ ì°¾ì„ ê²ƒì´ëƒí•˜ë©´, ë°”ë¡œ Accuracy Predictor ëª¨ë¸ì„ ë§Œë“œëŠ” ê²ë‹ˆë‹¤. ëª¨ë¸êµ¬ì¡°ì™€ Accuracyì— ëŒ€í•œ ë°ì´í„°ë¥¼ OFA MCUNetì—ì„œ ëª¨ì€ ë‹¤ìŒ, ê·¸ ë°ì´í„°ë¥¼ ê°€ì§€ê³  ëª¨ë¸ì„ ë§Œë“­ë‹ˆë‹¤. ê·¸ ëª¨ë¸ì— ëª¨ë¸ íŒŒë¼ë¯¸í„°ë¥¼ ë„£ìœ¼ë©´ ëª¨ë¸ ì •í™•ë„ê°€ ë‚˜ì˜¤ëŠ”, ê·¸ëŸ° ëª¨ë¸ì´ ë˜ëŠ”ê±°ì£ . ë§ˆì§€ë§‰ìœ¼ë¡œ Accuracy Predictorë¥¼ ê°€ì§€ê³  ì„ì˜ë¡œ ëª¨ë¸ íŒŒë¼ë¯¸í„°ì— ëŒ€í•œ ìƒ˜í”Œì„ ëª¨ì•„ ì›í•˜ëŠ” Constraintì— ë§ëŠ” ëª¨ë¸ì„ ì°¾ëŠ” ê²ë‹ˆë‹¤. ê°•ì˜ì—ì„œ NASë¥¼ ì†Œê°œí•˜ëŠ” ëª©ì ì€ â€œí° ëª¨ë¸ì—ì„œ ì‘ì€ ë””ë°”ì´ìŠ¤ì— ë„£ê¸° ìœ„í•´ì„œ Sub-Networkë¥¼ ì›í•˜ëŠ” ìŠ¤í™ì— ë§ê²Œ ì°¾ì•„ ë„£ëŠ”ë‹¤.â€ ì¸ ê²ë‹ˆë‹¤. ê·¸ëŸ° ì‘ì€ ë””ë°”ì´ìŠ¤ì— ëŒ€í•œ ì˜ˆì‹œë¡œ MCU, Alexa, Google Homeì„ ì•„ë˜ ê·¸ë¦¼ì²˜ëŸ¼ ë³´ì—¬ì£¼ì£ .\n\nBut the tight memory budget (50,000x smaller than GPUs) makes deep learning deployment difficult.\n\nThere are 2 main sections: accuracy & efficiency predictors and architecture search.\n\nFor predictors, there are 4 questions in total. There is one question (5 pts) in the Getting Started section and the other three questions (30 pts) are in the Predictors section.\nFor architecture search, there are 6 questions in total.\n\nì´ì œ ê°ì„¤í•˜ê³  í•˜ë‚˜ì”© ì‹¤í—˜í•´ë³¼ê²Œìš”! íŒ¨í‚¤ì§€ëŠ” ì•„ë˜ì™€ ê°™ì´ ì„¤ì¹˜í•˜ì‹œë©´ ë©ë‹ˆë‹¤.\nFirst, install the required packages and download the Visual Wake Words dataset that will be used in this lab.\n\n# print(\"Cleanning up workspace ...\")\n# # !rm -rf *\n# print(\"Installing graphviz ...\")\n# # !sudo apt-get install graphviz 1&gt;/adev/null\n# print(\"Downloading MCUNet codebase ...\")\n# !wget https://www.dropbox.com/s/3y2n2u3mfxczwcb/mcunetv2-dev-main.zip?dl=0 &gt;/dev/null\n# !unzip mcunetv2-dev-main.zip* 1&gt;/dev/null\n# !mv mcunetv2-dev-main/* . 1&gt;/dev/null\n# print(\"Downloading VWW dataset ...\")\n# !wget https://www.dropbox.com/s/169okcuuv64d4nn/data.zip?dl=0 &gt;/dev/null\n# print(\"Unzipping VWW dataset ...\")\n# !unzip data.zip* 1&gt;/dev/null\n# print(\"Installing thop and onnx ...\")\n# !pip install thop 1&gt;/dev/null\n# !pip install onnx 1&gt;/dev/null\n\n\nimport argparse\nimport json\nfrom PIL import Image\nfrom tqdm import tqdm\nimport copy\nimport math\nimport numpy as np\nimport os\nimport random\nimport torch\nfrom torch import nn\nfrom torchvision import datasets, transforms\nfrom mcunet.tinynas.search.accuracy_predictor import (\n    AccuracyDataset,\n    MCUNetArchEncoder,\n)\n\nfrom mcunet.tinynas.elastic_nn.networks.ofa_mcunets import OFAMCUNets\nfrom mcunet.utils.mcunet_eval_helper import calib_bn, validate\nfrom mcunet.utils.arch_visualization_helper import draw_arch\n\n\n%matplotlib inline\nfrom matplotlib import pyplot as plt\nimport warnings\nwarnings.filterwarnings('ignore')"
  },
  {
    "objectID": "posts/labs/lab03.html#getting-started-super-network-and-the-vww-dataset",
    "href": "posts/labs/lab03.html#getting-started-super-network-and-the-vww-dataset",
    "title": "ğŸ‘©â€ğŸ’» Lab 3",
    "section": "Getting Started: Super Network and the VWW dataset",
    "text": "Getting Started: Super Network and the VWW dataset\nì‹¤í—˜ì—ì„œëŠ” ì´ë¯¸ í›ˆë ¨í•œ MCUNetV2 super network ë¥¼ ê°€ì ¸ì˜µë‹ˆë‹¤. ë°ì´í„°ì…‹ì„ ê°€ì ¸ì˜¤ëŠ” ë¶€ë¶„, OFA MCUNet í´ë˜ìŠ¤ë¥¼ ê°€ì ¸ì˜¤ëŠ” ë¶€ë¶„, ê·¸ë¦¬ê³  Sub-networkë¥¼ ê°€ì ¸ì™€ ëª¨ë¸ êµ¬ì¡°ë¥¼ ì‹œê°í™”í•˜ê³  ì €í¬ê°€ ì›í•˜ëŠ” Constraint(ë©”ëª¨ë¦¬, ì—°ì‚°ì†ë„)ì— ë§ëŠ” ëª¨ë¸ì„ ì°¾ëŠ” ì½”ë“œ ì˜ˆì œì…ë‹ˆë‹¤.\n\nMCUNetV2 is a family of efficiency neural networks tailored for resource-constrained microntrollers. It utilizes patch-based inference, receptive field redistribution and system-NN co-design and greatly improves the accuracy-efficiency tradeoff of MCUNet.\n\n\ndef build_val_data_loader(data_dir, resolution, batch_size=128, split=0):\n    # split = 0: real val set, split = 1: holdout validation set\n    assert split in [0, 1]\n    normalize = transforms.Normalize(mean=[0.5, 0.5, 0.5], std=[0.5, 0.5, 0.5])\n    kwargs = {\"num_workers\": min(8, os.cpu_count()), \"pin_memory\": False}\n\n    val_transform = transforms.Compose(\n        [\n            transforms.Resize(\n                (resolution, resolution)\n            ),  # if center crop, the person might be excluded\n            transforms.ToTensor(),\n            normalize,\n        ]\n    )\n    val_dataset = datasets.ImageFolder(data_dir, transform=val_transform)\n\n    val_dataset = torch.utils.data.Subset(\n        val_dataset, list(range(len(val_dataset)))[split::2]\n    )\n        \n    val_loader = torch.utils.data.DataLoader(\n        val_dataset, batch_size=batch_size, shuffle=False, **kwargs\n    )\n    return val_loader\n\n\ndata_dir = \"data/vww-s256/val\"\n\nval_data_loader = build_val_data_loader(data_dir, resolution=128, batch_size=1)\n\nvis_x, vis_y = 2, 3\nfig, axs = plt.subplots(vis_x, vis_y)\n\nnum_images = 0\nfor data, label in val_data_loader:\n    img = np.array((((data + 1) / 2) * 255).numpy(), dtype=np.uint8)\n    img = img[0].transpose(1, 2, 0)\n    if label.item() == 0:\n        label_text = \"No person\"\n    else:\n        label_text = \"Person\"\n    axs[num_images // vis_y][num_images % vis_y].imshow(img)\n    axs[num_images // vis_y][num_images % vis_y].set_title(f\"Label: {label_text}\")\n    axs[num_images // vis_y][num_images % vis_y].set_xticks([])\n    axs[num_images // vis_y][num_images % vis_y].set_yticks([])\n    num_images += 1\n    if num_images &gt; vis_x * vis_y - 1:\n        break\n\nplt.show()\n\n\n\n\n\n\n\n\nì—¬ê¸°ì„œ OFA MCUNetì˜ Design Spaceê°€ \\(&gt;10^{19}\\) ë‚˜ ëœë‹¤ê³  í•˜ë„¤ìš”. ì–´ë§ˆì–´ë§ˆí•˜ì£ ? Subnetì€ inverted MobileNet blocksë¡œ êµ¬ì„±ë¼ ìˆìœ¼ë©´ì„œ ëª¨ë¸ êµ¬ì¡°ë¥¼ ë°”ê¾¸ëŠ” íŒŒë¼ë¯¸í„°ë¡œëŠ” kernel sizes (3, 5, 7), expand ratios (3, 4, 6), depth, global channel scaling (0.5x, 0.75x, 1.0x) (specified by width_mult_list) ê°€ ìˆìŠµë‹ˆë‹¤. ìì„¸í•œ ì„¤ëª…ì€ ì´ë”°ê°€ ê³„ì†í• ê²Œìš”.\n\ndevice = \"cuda:0\"\nofa_network = OFAMCUNets(\n    n_classes=2,\n    bn_param=(0.1, 1e-3),\n    dropout_rate=0.0,\n    base_stage_width=\"mcunet384\",\n    width_mult_list=[0.5, 0.75, 1.0],\n    ks_list=[3, 5, 7],\n    expand_ratio_list=[3, 4, 6],\n    depth_list=[0, 1, 2],\n    base_depth=[1, 2, 2, 2, 2],\n    fuse_blk1=True,\n    se_stages=[False, [False, True, True, True], True, True, True, False],\n)\n\nofa_network.load_state_dict(\n    torch.load(\"vww_supernet.pth\", map_location=\"cpu\")[\"state_dict\"], strict=True\n)\n\nofa_network = ofa_network.to(device)\n\n\nfrom mcunet.utils.pytorch_utils import count_peak_activation_size, count_net_flops, count_parameters\n\ndef evaluate_sub_network(ofa_network, cfg, image_size=None):\n    if \"image_size\" in cfg:\n        image_size = cfg[\"image_size\"]\n    batch_size = 128\n    # step 1. sample the active subnet with the given config.\n    ofa_network.set_active_subnet(**cfg)\n    # step 2. extract the subnet with corresponding weights.\n    subnet = ofa_network.get_active_subnet().to(device)\n    # step 3. calculate the efficiency stats of the subnet.\n    peak_memory = count_peak_activation_size(subnet, (1, 3, image_size, image_size))\n    macs = count_net_flops(subnet, (1, 3, image_size, image_size))\n    params = count_parameters(subnet)\n    # step 4. perform BN parameter re-calibration.\n    calib_bn(subnet, data_dir, batch_size, image_size)\n    # step 5. define the validation dataloader.\n    val_loader = build_val_data_loader(data_dir, image_size, batch_size)\n    # step 6. validate the accuracy.\n    acc = validate(subnet, val_loader)\n    return acc, peak_memory, macs, params\n\nWe also provide a handly helper function to visualize the architecture of the subnets. The function takes in the configuration of the subnet and returns an image representing the architecture.\n\ndef visualize_subnet(cfg):\n    draw_arch(cfg[\"ks\"], cfg[\"e\"], cfg[\"d\"], cfg[\"image_size\"], out_name=\"viz/subnet\")\n    im = Image.open(\"viz/subnet.png\")\n    im = im.rotate(90, expand=1)\n    fig = plt.figure(figsize=(im.size[0] / 250, im.size[1] / 250))\n    plt.axis(\"off\")\n    plt.imshow(im)\n    plt.show()\n\nìœ„ ì½”ë“œë¥¼ ì´ìš©í•´ì„œ ëª¨ë¸êµ¬ì¡° ì‹œê°í™”ë„ í• í…ë°ìš”, MBConv3-3x3ê³¼ ê°™ì€ ì´ë¦„ì´ ë‚˜ì˜¬ê±°ì—ìš”. ê°ê° expand ratio eì™€ kernel size of the depthwise convolution layer kë¡œ MBConv{e}-{k}x{k}ê°€ ë‚˜íƒ€ë‚˜ë‹ˆ ì°¸ê³ í•˜ì‹œë©´ ì¢‹ì„ ê²ƒ ê°™ì•„ìš”.\n\nMore Explanation to understand OFA-MCUNet\nê³¼ì œë¥¼ ë“¤ì–´ê°€ê¸° ì•ì„œ OFA-MCUNetì— ëŒ€í•´ì„œ ì¡°ê¸ˆ ì„¤ëª…í• ê¹Œí•´ìš”. ë‚´ë ¤ê°€ë©´ì„œ ëª¨ë¸ êµ¬ì¡°ì— íŒŒë¼ë¯¸í„°ë“¤ì´ ë‚˜ì˜¤ëŠ” ë° ê° ì˜ë¯¸ë¥¼ ì•Œë©´ ì´í•´í•˜ê¸°ê°€ ë” ìˆ˜ì›”í•  ê²ë‹ˆë‹¤.\nëª¨ë¸ì€ ì´ first_conv, blocks, feature_mix_layer, classifier ìœ¼ë¡œ êµ¬ì„±í•´ìš”. blockì—ì„œë„ ì²« ë²ˆì§¸, ë§ˆì§€ë§‰ blockì„ ì œì™¸í•œ ì´ 6ê°œì˜ blockì—ì„œ kernel size, expand ratio, depth, width multiplyë¥¼ íŒŒë¼ë¯¸í„°ë¡œ í•´ì„œ ëª¨ë¸ì„ í‚¤ìš°ê±°ë‚˜, ì¤„ì´ì£ . ê°ê°ì˜ íŒŒë¼ë¯¸í„°ë¥¼ ì¢€ ë” ì‚´í´ë³´ì£ !\n\n1. Kernel size\nkernel sizeëŠ” Convolutionì— ë‚˜ì˜¤ëŠ” ê·¸ kernelì´ ë§ìŠµë‹ˆë‹¤. ì˜ˆì œì—ì„œëŠ” 3x3, 5x5, 7x7ë¡œ ê°€ì§ˆ ìˆ˜ ìˆì–´ìš”.\n\n\n2. Width multiply, Depth\nOFA MCUNetì„ ë¸”ëŸ­ìœ¼ë¡œ í‘œí˜„í•˜ë©´ ì•„ë˜ì™€ ê°™ì£ . ê·¸ì¤‘ ì´ˆë¡ìƒ‰ìœ¼ë¡œ ì¹ í•´ì§„ Blockì„ ë³´ì‹œë©´, Blockìœ¼ë¡œ ë“¤ì–´ì˜¤ëŠ” Input Channelê³¼ Output Channelì´ ìˆì–´ìš”. ë°”ë¡œ ê·¸ ë‘˜ì„ ì–¼ë§ˆë‚˜ ì¤„ì¼ ê²ƒì¸ê°€, ìœ ì§€í•  ê²ƒì¸ê°€ê°€ Width multiplyì…ë‹ˆë‹¤.\në‘ ë²ˆì§¸ë¡œ í•˜ë‚˜ì˜ Blockì€ MBConv(MobileNet Conv)ë¡œ êµ¬ì„±ë©ë‹ˆë‹¤. ê·¸ëŸ¬ë©´ ì´ MBConvê°€ ëª‡ ê°œê°€ ë“¤ì–´ ê°ˆ ê²ƒì´ëƒê°€ ê´€ê±´ì¼ í…ë°ìš”, ì´ê±¸ ì •í•˜ëŠ” ê²ƒì´ Depthì…ë‹ˆë‹¤. íŒŒë¼ë¯¸í„°ì—ì„œëŠ” depth_listì™€ base_depthë¡œ ë‚˜ëˆ ì„œ ê° blockë³„ë¡œ base_depthë¥¼ ê¸°ì¤€ìœ¼ë¡œ depth_listì— ë‚˜ì˜¤ëŠ” ê°œìˆ˜ ë§Œí¼ ë” MBConvì´ ì¶”ê°€ë˜ì£ .\n\në§ˆì§€ë§‰ì€ expand ratio ì…ë‹ˆë‹¤. ì´ íŒŒë¼ë¯¸í„°ëŠ” MBConv ë‚´ì—ì„œ ìˆì–´ìš”, ì—­ì‹œë‚˜ ê·¸ë¦¼ì„ ë³´ì‹œì£ . MBConvëŠ” MobileNet Convolution, Separable Convolution,\nSE-Block, ê·¸ë¦¬ê³  ë‹¤ì‹œ MobileNet Convolutionìœ¼ë¡œ êµ¬ì„±ë˜ìš”. ê·¸ ì¤‘, ì²˜ìŒ ì…ë ¥ì˜ ì±„ë„ê³¼ ì²« MobileNet Convolutionì„ ê±°ì¹˜ê³  ë‚˜ì˜¨ ì¶œë ¥ ì±„ë„ì˜ ë¹„ë¥¼ Expand ratioë¼ê³  í•©ë‹ˆë‹¤.\n\n\n\n-\n\n\n\n# OFAMCUNets\n# constitutes: first_conv, blocks, feature_mix_layer, classifier\n# total 9 block (first_conv, first block, blocks, last block)\n# 1. first_conv = 1x1 channel inc conv (3 -&gt; X)\n# 2. first block = MB InvertedConvLayer\n\n# 3. blocks\n# - depth = num block\n# - 1 block = MobileInvertedResidualBlock = MBConvLayer + Residual\n#############################################################\n# Dynamic MBConvLayer = 2 times channel expansion           #\n#                  fuse_blk1    se_stage                    #\n# MBConvLayer + SeparableConv + SEBlock + MBConvLayer       #\n#############################################################\n# SEblock: conv 1x1 (reduce) -&gt; act -&gt; conv 1x1 (expand) -&gt; h_sigmoid\n# -&gt; SENet(Squeeze-and-Excitation Network)\n\n# 4. Last block = Mobile Inverted Residual Blcok\n# 5. feature_mix_layer = 1x1 channel dec conv\n# 6. classifier = linear layer\n\n# Parameters (sample_active_subnet)\n# kernel size, expand ratio, depth, width multiplY\n\nì½”ë“œ ì¤‘ make_divisibleì´ë¼ëŠ” ë©”ì„œë“œê°€ ìˆìŠµë‹ˆë‹¤. ì—¬ê¸°ì„  ì±„ë„ì„ ëŠ˜ë¦¬ê±°ë‚˜ ì¤„ì¼ ë•Œ 8ë¡œ ë‚˜ëˆŒ ìˆ˜ ìˆê²Œ í•©ë‹ˆë‹¤. tensorflowì—ì„œë„ ì‚¬ìš©í•œë‹¤ê³  í•˜ëŠ”ë°, ì´ìœ ëŠ” ì•„ì§ ëª¨ë¥´ê² ë„¤ìš”!\ndef make_divisible(v, divisor, min_val=None):\n    \"\"\"\n    This function is taken from the original tf repo.\n    It ensures that all layers have a channel number that is divisible by 8\n    It can be seen here:\n    https://github.com/tensorflow/models/blob/master/research/slim/nets/mobilenet/mobilenet.py\n    :param v:\n    :param divisor:\n    :param min_val:\n    :return:\n    \"\"\"\n    if min_val is None:\n        min_val = divisor\n    new_v = max(min_val, int(v + divisor / 2) // divisor * divisor)\n    # Make sure that round down does not go down by more than 10%.\n    if new_v &lt; 0.9 * v:\n        new_v += divisor\n    return new_v\n\n\n\nTL;DR. Summary\nì‹¤í—˜ì€ ì´ 4 ë‹¨ê³„ë¡œ ë‚˜ë‰©ë‹ˆë‹¤. ì—¬ê¸°ì„œ íŒŒë¼ë¯¸í„°ëŠ” kernel size, expand ratio, depth, width multiplyì£ .\n\n1. OFA-MCUNet\nì²˜ìŒì€ í›ˆë ¨ëœ vww_supernetì„ ê°€ì§€ê³  íŒŒë¼ë¯¸í„°ë§ˆë‹¤ accuracy ì¡°í•©ì„ êµ¬í•©ë‹ˆë‹¤. ê·¸ë¦¬ê³  ê° ê²°ê³¼ë§ˆë‹¤ ì´í›„ì— constraint ë²”ìœ„ ë‚´ì— ë“¤ì–´ì˜¤ëŠ” ëª¨ë¸êµ¬ì¡°ë¥¼ ì°¾ê¸° ìœ„í•´ MACê³¼ Peak memory ë˜í•œ êµ¬í•  ê²ë‹ˆë‹¤.\n\n\n2. Accuracy Predictor\nì•ì„œì„œ êµ¬í•œ íŒŒë¼ë¯¸í„°ë§ˆë‹¤ Accuracyë¥¼ ê°€ì§€ê³ , ì´ë²ˆì—” ë°˜ëŒ€ë¡œ ì´ ë°ì´í„°ë¥¼ ê°€ì§€ê³  Accuracyë¥¼ ì˜ˆì¸¡í•˜ëŠ” ëª¨ë¸ì„ í›ˆë ¨ì‹œí‚¬ê²ë‹ˆë‹¤. ëª¨ë¸ì€ Linear Layerê°€ ì„¸ì¸µìœ¼ë¡œ ìŒ“ì—¬ìˆëŠ” ê°„ë‹¨í•œ ëª¨ë¸ì„ ì‚¬ìš©í•˜ì£ . í•˜ì§€ë§Œ íŒŒíƒ€ë¯¸í„° ì¡°í•©ì„ Embedding vectorë¡œ ë§Œë“¤ê¸° ìœ„í•´ encoderê°€ ë“¤ì–´ê°‘ë‹ˆë‹¤.\n\n\n3. Encoding: MCUNetArchEncoder\nê·¸ ê³¼ì •ì—ì„œ íŒŒë¼ë¯¸í„° ì¡°í•©ì„ Embedding vectorë¡œ Encodingì„ í•©ë‹ˆë‹¤. ì˜ˆë¥¼ ë“¤ì–´, Kenral sizeê°€ 3x3, 5x5, 7x7 ì´ ìˆëŠ” ê²½ìš° ê°ê°ì„ (0, 0, 1), (0, 1, 0), (1, 0, 0) ì´ë ‡ê²Œ encoding í•˜ëŠ” ê±°ì£ . ì´ encodingì´ ë“¤ì–´ê°„ Accuracy Predictor ëª¨ë¸ì„ í›ˆë ¨ì‹œí‚µë‹ˆë‹¤. í›ˆë ¨ì‹œí‚¨ ëª¨ë¸ì˜ Predictionê³¼ Label ê°„ì˜ ìƒê´€ê´€ê³„ê°€ Linearí•˜ê²Œ ë‚˜ì˜¤ëŠ” ê²ƒ ë˜í•œ ë³´ì—¬ì¤„ ê²ë‹ˆë‹¤.\n\n\n4. Random Search and Evolutionary Search\në§ˆì§€ë§‰ ë‹¨ê³„ëŠ” Constraint, ì¦‰ ë©”ëª¨ë¦¬ì™€ MACì— í•´ë‹¹í•˜ëŠ” ëª¨ë¸ì˜ íŒŒë¼ë¯¸í„°ë¥¼ ì°¾ëŠ” ë‹¨ê³„ì…ë‹ˆë‹¤. Random Mutate ë°©ì‹ê³¼ Crossover ë°©ì‹ì„ ì‚¬ìš©í•˜ëŠ”ë°, ìì„¸í•œ ë‚´ìš©ì€ ì½”ë“œë¥¼ ì°¸ê³ í•˜ì‹œëŠ”ê²Œ ì´í•´í•˜ê¸° ë” ìˆ˜ì›”í•  ê²ë‹ˆë‹¤! ì°¸ê³ ë¡œ ë§ˆì§€ë§‰ì— Question 10ì—ì„œ â€œThe activation size of the subnet is at most 64 KBâ€ ì˜ ì¡°ê±´ì„ ê°€ì§„ ëª¨ë¸ì˜ êµ¬ì¡°ëŠ” ëª»ì°¾ì•˜ìŠµë‹ˆë‹¤. í˜¹ì‹œ ì°¾ê²Œ ëœë‹¤ë©´, í˜¹ì€ ì°¾ì§€ ëª»í•˜ëŠ” ì´ìœ ë¥¼ ì•„ì‹œê²Œëœë‹¤ë©´ ê³µìœ í•´ì£¼ì„¸ìš”!\n\n\n\nOFA_networkâ€™s forward\nëª¨ë¸ ì‹¤í—˜í•˜ê¸°ì— ì•ì„œì„œ, ì±„ë„ì„ ë§Œì•½ ì¤„ì¸ë‹¤ë©´ ì–´ë–¤ì‹ìœ¼ë¡œ í• ì§€ Convolution Networkì—ì„œ ë‚˜ì˜¨ ì½”ë“œë¥¼ ê°€ì ¸ì™€ë´¤ì–´ìš”. íŒŒë¼ë¯¸í„°ì— ë§ê²Œ ê²°ì •í•œ out_channel, in_channelì„ ì•„ë˜ ì½”ë“œ ì²˜ëŸ¼ ì˜ë¼ active subnetì´ë¼ê³  ë¶€ë¥¼ ê±°ì—ìš”. ì‹¤í—˜ì€ ì œê°€ ì„ì˜ë¡œ ì´ë¯¸ì§€ ì‚¬ì´ì¦ˆë¥¼ 48, 96, 128, 256, 384, 512ë¡œ í‚¤ì›Œë‚˜ê°€ë©´ì„œ í–ˆê³ , sub networkë¡œ ìƒ˜í”Œë§í•˜ëŠ” ë°©ë²•ìœ¼ë¡œëŠ” random, max, minìœ¼ë¡œ í–ˆìŠµë‹ˆë‹¤.\nfilters = self.conv.weight[:out_channel, :in_channel, :, :].contiguous()\npadding = get_same_padding(self.kernel_size)\ny = F.conv2d(x, filters, None, self.stride, padding, self.dilation, 1)\ní¥ë¯¸ë¡œì› ë˜ ê±´ ì´ë¯¸ì§€ê°€ ì»¤ì§€ë©´ ì»¤ì§ˆìˆ˜ë¡ AccuracyëŠ” ê³„ì† ì˜¬ë¼ê°€ë‹¤ê°€ 512ì—ì„œ ë¶€í„° ë–¨ì–´ì§€ë”ë¼êµ¬ìš”. ì‹¤í—˜ê²°ê³¼ëŠ” ì•„ë˜ë¥¼ ì°¸ê³ ë°”ëë‹ˆë‹¤.\n\n# sample_active_subnet\n# kernel size, expand ratio, depth, width mult\n\nimage_size = 48\n\ncfg = ofa_network.sample_active_subnet(sample_function=random.choice, image_size=image_size)\nacc, _, _, params = evaluate_sub_network(ofa_network, cfg)\nvisualize_subnet(cfg)\nprint(f\"The accuracy of the sampled subnet: #params={params/1e6: .1f}M, accuracy={acc: .1f}%.\")\n\nlargest_cfg = ofa_network.sample_active_subnet(sample_function=max, image_size=image_size)\nacc, _, _, params = evaluate_sub_network(ofa_network, largest_cfg)\nvisualize_subnet(largest_cfg)\nprint(f\"The largest subnet: #params={params/1e6: .1f}M, accuracy={acc: .1f}%.\")\n\nsmallest_cfg = ofa_network.sample_active_subnet(sample_function=min, image_size=image_size)\nacc, peak_memory, macs, params = evaluate_sub_network(ofa_network, smallest_cfg)\nvisualize_subnet(smallest_cfg)\nprint(f\"The smallest subnet: #params={params/1e6: .1f}M, accuracy={acc: .1f}%.\")\n\nValidate: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 32/32 [00:00&lt;00:00, 51.09it/s, loss=0.603, top1=65.9]\nValidate: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 32/32 [00:00&lt;00:00, 53.97it/s, loss=0.625, top1=64.2]\nValidate: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 32/32 [00:00&lt;00:00, 51.76it/s, loss=0.718, top1=59.3]\n\n\n\n\n\n\n\n\n\nThe accuracy of the sampled subnet: #params= 1.6M, accuracy= 65.9%.\nThe largest subnet: #params= 2.5M, accuracy= 64.2%.\nThe smallest subnet: #params= 0.3M, accuracy= 59.3%.\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nimage_size = 96\n\ncfg = ofa_network.sample_active_subnet(sample_function=random.choice, image_size=image_size)\nacc, _, _, params = evaluate_sub_network(ofa_network, cfg)\nvisualize_subnet(cfg)\nprint(f\"The accuracy of the sampled subnet: #params={params/1e6: .1f}M, accuracy={acc: .1f}%.\")\n\nlargest_cfg = ofa_network.sample_active_subnet(sample_function=max, image_size=image_size)\nacc, _, _, params = evaluate_sub_network(ofa_network, largest_cfg)\nvisualize_subnet(largest_cfg)\nprint(f\"The largest subnet: #params={params/1e6: .1f}M, accuracy={acc: .1f}%.\")\n\nsmallest_cfg = ofa_network.sample_active_subnet(sample_function=min, image_size=image_size)\nacc, peak_memory, macs, params = evaluate_sub_network(ofa_network, smallest_cfg)\nvisualize_subnet(smallest_cfg)\nprint(f\"The smallest subnet: #params={params/1e6: .1f}M, accuracy={acc: .1f}%.\")\n\nValidate: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 32/32 [00:00&lt;00:00, 35.68it/s, loss=0.321, top1=86.4]\nValidate: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 32/32 [00:00&lt;00:00, 42.76it/s, loss=0.29, top1=88.6] \nValidate: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 32/32 [00:00&lt;00:00, 44.92it/s, loss=0.379, top1=83.4]\n\n\n\n\n\n\n\n\n\nThe accuracy of the sampled subnet: #params= 0.6M, accuracy= 86.4%.\nThe largest subnet: #params= 2.5M, accuracy= 88.6%.\nThe smallest subnet: #params= 0.3M, accuracy= 83.4%.\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nimage_size = 128\n\n# sample_active_subnet\n# kernel size, expand ratio, depth, width mult\n\ncfg = ofa_network.sample_active_subnet(sample_function=random.choice, image_size=image_size)\nacc, _, _, params = evaluate_sub_network(ofa_network, cfg)\nvisualize_subnet(cfg)\nprint(f\"The accuracy of the sampled subnet: #params={params/1e6: .1f}M, accuracy={acc: .1f}%.\")\n\nlargest_cfg = ofa_network.sample_active_subnet(sample_function=max, image_size=image_size)\nacc, _, _, params = evaluate_sub_network(ofa_network, largest_cfg)\nvisualize_subnet(largest_cfg)\nprint(f\"The largest subnet: #params={params/1e6: .1f}M, accuracy={acc: .1f}%.\")\n\nsmallest_cfg = ofa_network.sample_active_subnet(sample_function=min, image_size=image_size)\nacc, peak_memory, macs, params = evaluate_sub_network(ofa_network, smallest_cfg)\nvisualize_subnet(smallest_cfg)\nprint(f\"The smallest subnet: #params={params/1e6: .1f}M, accuracy={acc: .1f}%.\")\n\nValidate: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 32/32 [00:00&lt;00:00, 39.53it/s, loss=0.228, top1=91.3]\nValidate: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 32/32 [00:01&lt;00:00, 30.92it/s, loss=0.21, top1=92.3] \nValidate: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 32/32 [00:00&lt;00:00, 40.69it/s, loss=0.307, top1=87.3]\n\n\n\n\n\n\n\n\n\nThe accuracy of the sampled subnet: #params= 1.3M, accuracy= 91.3%.\nThe largest subnet: #params= 2.5M, accuracy= 92.3%.\nThe smallest subnet: #params= 0.3M, accuracy= 87.3%.\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nimage_size = 256\n\n# sample_active_subnet\n# kernel size, expand ratio, depth, width mult\n\ncfg = ofa_network.sample_active_subnet(sample_function=random.choice, image_size=image_size)\nacc, _, _, params = evaluate_sub_network(ofa_network, cfg)\nvisualize_subnet(cfg)\nprint(f\"The accuracy of the sampled subnet: #params={params/1e6: .1f}M, accuracy={acc: .1f}%.\")\n\nlargest_cfg = ofa_network.sample_active_subnet(sample_function=max, image_size=image_size)\nacc, _, _, params = evaluate_sub_network(ofa_network, largest_cfg)\nvisualize_subnet(largest_cfg)\nprint(f\"The largest subnet: #params={params/1e6: .1f}M, accuracy={acc: .1f}%.\")\n\nsmallest_cfg = ofa_network.sample_active_subnet(sample_function=min, image_size=image_size)\nacc, peak_memory, macs, params = evaluate_sub_network(ofa_network, smallest_cfg)\nvisualize_subnet(smallest_cfg)\nprint(f\"The smallest subnet: #params={params/1e6: .1f}M, accuracy={acc: .1f}%.\")\n\nValidate: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 32/32 [00:01&lt;00:00, 19.93it/s, loss=0.187, top1=93.5]\nValidate: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 32/32 [00:03&lt;00:00, 10.12it/s, loss=0.177, top1=93.9] \nValidate: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 32/32 [00:01&lt;00:00, 25.67it/s, loss=0.258, top1=90.2]\n\n\n\n\n\n\n\n\n\nThe accuracy of the sampled subnet: #params= 0.6M, accuracy= 93.5%.\nThe largest subnet: #params= 2.5M, accuracy= 93.9%.\nThe smallest subnet: #params= 0.3M, accuracy= 90.2%.\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nimage_size = 256+128\n\n# sample_active_subnet\n# kernel size, expand ratio, depth, width mult\n\ncfg = ofa_network.sample_active_subnet(sample_function=random.choice, image_size=image_size)\nacc, _, _, params = evaluate_sub_network(ofa_network, cfg)\nvisualize_subnet(cfg)\nprint(f\"The accuracy of the sampled subnet: #params={params/1e6: .1f}M, accuracy={acc: .1f}%.\")\n\nlargest_cfg = ofa_network.sample_active_subnet(sample_function=max, image_size=image_size)\nacc, _, _, params = evaluate_sub_network(ofa_network, largest_cfg)\nvisualize_subnet(largest_cfg)\nprint(f\"The largest subnet: #params={params/1e6: .1f}M, accuracy={acc: .1f}%.\")\n\nsmallest_cfg = ofa_network.sample_active_subnet(sample_function=min, image_size=image_size)\nacc, peak_memory, macs, params = evaluate_sub_network(ofa_network, smallest_cfg)\nvisualize_subnet(smallest_cfg)\nprint(f\"The smallest subnet: #params={params/1e6: .1f}M, accuracy={acc: .1f}%.\")\n\nValidate: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 32/32 [00:03&lt;00:00,  8.16it/s, loss=0.241, top1=91.1] \nValidate: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 32/32 [00:06&lt;00:00,  4.60it/s, loss=0.263, top1=90.5] \nValidate: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 32/32 [00:02&lt;00:00, 12.13it/s, loss=0.34, top1=85.4] \n\n\n\n\n\n\n\n\n\nThe accuracy of the sampled subnet: #params= 1.1M, accuracy= 91.1%.\nThe largest subnet: #params= 2.5M, accuracy= 90.5%.\nThe smallest subnet: #params= 0.3M, accuracy= 85.4%.\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nimage_size = 512\n\n# sample_active_subnet\n# kernel size, expand ratio, depth, width mult\n\ncfg = ofa_network.sample_active_subnet(sample_function=random.choice, image_size=image_size)\nacc, _, _, params = evaluate_sub_network(ofa_network, cfg)\nvisualize_subnet(cfg)\nprint(f\"The accuracy of the sampled subnet: #params={params/1e6: .1f}M, accuracy={acc: .1f}%.\")\n\nlargest_cfg = ofa_network.sample_active_subnet(sample_function=max, image_size=image_size)\nacc, _, _, params = evaluate_sub_network(ofa_network, largest_cfg)\nvisualize_subnet(largest_cfg)\nprint(f\"The largest subnet: #params={params/1e6: .1f}M, accuracy={acc: .1f}%.\")\n\nsmallest_cfg = ofa_network.sample_active_subnet(sample_function=min, image_size=image_size)\nacc, peak_memory, macs, params = evaluate_sub_network(ofa_network, smallest_cfg)\nvisualize_subnet(smallest_cfg)\nprint(f\"The smallest subnet: #params={params/1e6: .1f}M, accuracy={acc: .1f}%.\")\n\nValidate: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 32/32 [00:06&lt;00:00,  5.31it/s, loss=0.376, top1=83.1]\nValidate: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 32/32 [00:11&lt;00:00,  2.67it/s, loss=0.413, top1=81]   \nValidate: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 32/32 [00:04&lt;00:00,  7.23it/s, loss=0.489, top1=76.1]\n\n\n\n\n\n\n\n\n\nThe accuracy of the sampled subnet: #params= 0.5M, accuracy= 83.1%.\nThe largest subnet: #params= 2.5M, accuracy= 81.0%.\nThe smallest subnet: #params= 0.3M, accuracy= 76.1%.\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nQuestion 1: Design space exploration.\nTry manually sample different subnets by running the cell above multiple times. You can also vary the input resolution. Talk about your findings.\nHint: which dimension plays the most important role for the accuracy?\nAnswer: Image resolution plays the most important role for classification accuracy.\në„¤, ì§ˆë¬¸ì—ì„œ ì‚¬ì‹¤ íŒíŠ¸ë¥¼ ì–»ì–´ ì‹¤í—˜ì„ í–ˆìŠµë‹ˆë‹¤. â€œImage resolutionì— ë”°ë¥¸ Accuracy ë³€í™”â€ë¥¼ ì•Œ ìˆ˜ ìˆì—ˆìŠµë‹ˆë‹¤."
  },
  {
    "objectID": "posts/labs/lab03.html#part-1.-predictors",
    "href": "posts/labs/lab03.html#part-1.-predictors",
    "title": "ğŸ‘©â€ğŸ’» Lab 3",
    "section": "Part 1. Predictors",
    "text": "Part 1. Predictors\nì´ì œ ë‘ë²ˆì§¸ ë‹¨ê³„ëŠ” ì•ì„œì„œ ëª¨ë¸ì„ í†µí•´ ì–»ì€ VWW datasetìœ¼ë¡œ Accuracyë¥¼ ì˜ˆì¸¡í•˜ëŠ” ëª¨ë¸ì„ ë§Œë“¤ê²ë‹ˆë‹¤. ëª¨ë¸ì€ ìƒê°ëª¨ë‹¤ ê°„ë‹¨í•´ìš”, Linear ì„¸ ì¸µìœ¼ë¡œ êµ¬ì„±ë¼ ìˆì£ . ì•„ë˜ ê·¸ë¨ì€ ê¶ê·¹ì ìœ¼ë¡œ Constraintì— í•´ë‹¹í•˜ëŠ” ëª¨ë¸ì„ ìš°ë¦¬ëŠ” êµ¬í• ê±°ë‹¤, ì´ëŸ° ë‚´ìš©ì…ë‹ˆë‹¤.\n\nEfficiency predictorëŠ” ëª¨ë¸ êµ¬ì¡°ê°€ ê²°ì •ë˜ë©´ Accuracyì™€ í•¨ê»˜ ë‚˜ì˜¬ê±°ì—ìš”. ì•ì„  ì˜ˆì œì—ì„œ í–ˆìœ¼ë‹ˆ ê¸°ì–µì´ ì•ˆë‚˜ì‹ ë‹¤ë©´ ì´ì „ ì˜ˆì œë¡œ!\n\nQuestion 2: Implement the efficiency predictor.\nì²˜ìŒì€ â€œAnalyticalEfficiencyPredictorâ€ë¼ëŠ” í´ë˜ìŠ¤ë¥¼ ë§Œë“­ë‹ˆë‹¤. ì´ë¯¸ì§€ í¬ê¸°ì— ë”°ë¼ MACê³¼ ë©”ëª¨ë¦¬ë¥¼ ê³„ì‚°í•´ì£¼ê³ (get_efficiency), ì´ ë‘ê°€ì§€ê°€ íƒ€ê²Ÿí•˜ê³  ë¶€í•©í•˜ëŠ”ì§€ë„ ì•Œë ¤ì£¼ëŠ” í•¨ìˆ˜(satisfy_constraint)ë„ ë§Œë“­ë‹ˆë‹¤. FLOPê³¼ ë©”ëª¨ë¦¬ ê³„ì‚°ì€ êµìˆ˜ë‹˜ì´ ì¹œì ˆí•˜ê²Œ ë§Œë“¤ì–´ ë†“ìœ¼ì‹  count_net_flopsê³¼ count_peak_activation_sizeë¥¼ ì‚¬ìš©í•˜ë©´ ë©ë‹ˆë‹¤.\n\nclass AnalyticalEfficiencyPredictor:\n    def __init__(self, net):\n        self.net = net\n\n    def get_efficiency(self, spec: dict):\n        self.net.set_active_subnet(**spec)\n        subnet = self.net.get_active_subnet()\n        if torch.cuda.is_available():\n            subnet = subnet.cuda()\n        ############### YOUR CODE STARTS HERE ###############\n        # Hint: take a look at the `evaluate_sub_network` function above.\n        # Hint: the data shape is (batch_size, input_channel, image_size, image_size)\n        data_shape = (1, 3, spec[\"image_size\"], spec[\"image_size\"])\n        macs = count_net_flops(subnet, data_shape)\n        peak_memory = count_peak_activation_size(subnet, data_shape)\n        ################ YOUR CODE ENDS HERE ################\n\n        return dict(millionMACs=macs / 1e6, KBPeakMemory=peak_memory / 1024)\n\n    def satisfy_constraint(self, measured: dict, target: dict):\n        for key in measured:\n            # if the constraint is not specified, we just continue\n            if key not in target:\n                continue\n            # if we exceed the constraint, just return false.\n            if measured[key] &gt; target[key]:\n                return False\n        # no constraint violated, return true.\n        return True\n\nLetâ€™s test your implementation for the analytical efficiency predictor by examining the returned values for the smallest and largest subnets we just evaluated a while ago. The results from the efficiency predictor should match with the previous results.\n\nefficiency_predictor = AnalyticalEfficiencyPredictor(ofa_network)\n\nimage_size = 96\n# Print out the efficiency of the smallest subnet.\nsmallest_cfg = ofa_network.sample_active_subnet(sample_function=min, image_size=image_size)\neff_smallest = efficiency_predictor.get_efficiency(smallest_cfg)\n\n# Print out the efficiency of the largest subnet.\nlargest_cfg = ofa_network.sample_active_subnet(sample_function=max, image_size=image_size)\neff_largest = efficiency_predictor.get_efficiency(largest_cfg)\n\nprint(\"Efficiency stats of the smallest subnet:\", eff_smallest)\nprint(\"Efficiency stats of the largest subnet:\", eff_largest)\n\nEfficiency stats of the smallest subnet: {'millionMACs': 8.302128, 'KBPeakMemory': 72.0}\nEfficiency stats of the largest subnet: {'millionMACs': 79.416432, 'KBPeakMemory': 270.0}\n\n\n\n\nQuestion 3: Implement the accuracy predictor.\nì´ì œ Accuracy predictorë¥¼ ë§Œë“¤ì–´ì•¼ì£ ? ê·¸ì „ì—, ë°ì´í„°ì…‹ìœ¼ë¡œ ì£¼ì–´ì§„ ê±¸ ì‚´í´ë³´ë‹ˆ íŒŒë¼ë¯¸í„°ê°„ ì¡°í•©ìœ¼ë¡œ ë³´ì…ë‹ˆë‹¤. ì´ë¥¼ ë°ì´í„°ë¡œì¨ ì“°ê¸° ìœ„í•´ ì„ë² ë”©ì„ í•´ì•¼í•˜ëŠ”ë° ê·¸ ì—­í• ì„ ë°”ë¡œ MCUNetArchEncoderê°€ í•©ë‹ˆë‹¤. ì—­ì‹œë‚˜ êµìˆ˜ë‹˜ì´ ì¹œì ˆí•˜ê²Œ ë§Œë“¤ì–´ì£¼ì…¨êµ°ìš”. ê·¸ë¦¬ê³  Accuracy predictor ëª¨ë¸ êµ¬ì¡°ëŠ” MLP (multi-layer perception)ë¥¼ ì‚¬ìš©í• ê²ë‹ˆë‹¤.\nThe accuracy predictor takes in the architecture of a sub-network and predicts its accuracy on the VWW dataset. Since it is an MLP network, the sub-network must be encoded into a vector. In this lab, we provide a class MCUNetArchEncoder to perform such conversion from sub-network architecture to a binary vector.\n\nimage_size_list = [96, 112, 128, 144, 160]\narch_encoder = MCUNetArchEncoder(\n    image_size_list=image_size_list,\n    base_depth=ofa_network.base_depth,\n    depth_list=ofa_network.depth_list,\n    expand_list=ofa_network.expand_ratio_list,\n    width_mult_list=ofa_network.width_mult_list,\n)\n\nWe generated an accuracy dataset beforehand, which is a collection of [architecture, accuracy] pairs stored under the acc_datasets folder.\nWith the architecture encoder, you are now required define the accuracy predictor, which is a multi-layer perception (MLP) network with 400 channels per intermediate layer. For simplicity, we fix the number of layers to be 3. Please implement this MLP network in the following cell.\n\nclass AccuracyPredictor(nn.Module):\n    def __init__(\n        self,\n        arch_encoder,\n        hidden_size=400,\n        n_layers=3,\n        checkpoint_path=None,\n        device=\"cuda:0\",\n    ):\n        super(AccuracyPredictor, self).__init__()\n        self.arch_encoder = arch_encoder\n        self.hidden_size = hidden_size\n        self.n_layers = n_layers\n        self.device = device\n\n        layers = []\n        \n        ############### YOUR CODE STARTS HERE ###############\n        # Let's build an MLP with n_layers layers. \n        # Each layer (nn.Linear) has hidden_size channels and \n        # uses nn.ReLU as the activation function.\n        # Hint: You can assume that n_layers is fixed to be 3, for simplicity.\n        # Hint: the input dimension of the first layer is not hidden_size.\n        for i in range(self.n_layers):\n            layers.append(\n                nn.Sequential(\n                    nn.Linear(\n                        self.arch_encoder.n_dim if i == 0 else self.hidden_size,\n                        self.hidden_size,\n                    ),\n                    nn.ReLU(inplace=True),\n                )\n            )\n        ################ YOUR CODE ENDS HERE ################\n        layers.append(nn.Linear(self.hidden_size, 1, bias=False))\n        self.layers = nn.Sequential(*layers)\n        self.base_acc = nn.Parameter(\n            torch.zeros(1, device=self.device), requires_grad=False\n        )\n\n        if checkpoint_path is not None and os.path.exists(checkpoint_path):\n            checkpoint = torch.load(checkpoint_path, map_location=\"cpu\")\n            if \"state_dict\" in checkpoint:\n                checkpoint = checkpoint[\"state_dict\"]\n            self.load_state_dict(checkpoint)\n            print(\"Loaded checkpoint from %s\" % checkpoint_path)\n\n        self.layers = self.layers.to(self.device)\n\n    def forward(self, x):\n        y = self.layers(x).squeeze()\n        return y + self.base_acc\n\n    def predict_acc(self, arch_dict_list):\n        X = [self.arch_encoder.arch2feature(arch_dict) for arch_dict in arch_dict_list]\n        X = torch.tensor(np.array(X)).float().to(self.device)\n        return self.forward(X)\n\nLetâ€™s print out the architecture of the AccuracyPredictor you just defined.\n\nos.makedirs(\"pretrained\", exist_ok=True)\nacc_pred_checkpoint_path = (\n    f\"pretrained/{ofa_network.__class__.__name__}_acc_predictor.pth\"\n)\nacc_predictor = AccuracyPredictor(\n    arch_encoder,\n    hidden_size=400,\n    n_layers=3,\n    checkpoint_path=None,\n    device=device,\n)\nprint(acc_predictor)\n\nAccuracyPredictor(\n  (layers): Sequential(\n    (0): Sequential(\n      (0): Linear(in_features=128, out_features=400, bias=True)\n      (1): ReLU(inplace=True)\n    )\n    (1): Sequential(\n      (0): Linear(in_features=400, out_features=400, bias=True)\n      (1): ReLU(inplace=True)\n    )\n    (2): Sequential(\n      (0): Linear(in_features=400, out_features=400, bias=True)\n      (1): ReLU(inplace=True)\n    )\n    (3): Linear(in_features=400, out_features=1, bias=False)\n  )\n)\n\n\në°ì´í„° ì…‹ì€ ì´ 4ë§Œê°œì˜ í›ˆë ¨ë°ì´í„°ì™€ ë§Œê°œì˜ í…ŒìŠ¤íŠ¸ ë°ì´í„°ë¡œ ìˆê³ , AccuracyëŠ” ëª¨ë¸ íŒŒë¼ë¯¸í„°(architecture)ì™€ ìŒì„ ì´ë£°ê±°ë¼ëŠ”, ë‚´ìš©ì…ë‹ˆë‹¤. í•˜ë‚˜ ë”, íŒŒë¼ë¯¸í„°ë¥¼ one-hot representation ë¡œ ë°”ê¾¸ëŠ” ê³¼ì •ë„ ìŠì§€ë§ˆì‹œì£ ! ë‹¤ìŒ ê²°ê³¼ë¥¼ ë³´ì‹œë©´ â€œkernel size embedding: [0 1 0] =&gt; kernel size: 5; expand ratio embedding: [0 1 0] =&gt; expand ratio: 4â€ ì´ëŸ¬ë©´ì„œ ëª¨ë¸ êµ¬ì¡°ê°€ ì„ë² ë”©ëœ ê±¸ í™•ì¸í•˜ì‹¤ ìˆ˜ ìˆì–´ìš”\nLetâ€™s first visualize some samples in the accuracy dataset in the following cell.\nThe accuracy dataset is composed of 50,000 [architecture, accuracy] pairs, where 40,000 of them are used as the training set and the rest 10,000 are used as validation set.\nFor accuracy, We calculate the average accuracy of all [architecture, accuracy] pairs on the accuracy dataset and define it as base_acc. For the accuracy predictor, instead of directly regressing the accuracy of each architecture, its training target is accuracy - base_acc. Since accuracy - base_acc is usually much smaller than accuracy itself, this can make training easier.\nFor architecture, each subnet within the design space is uniquely represented by a binary vector. The binary vector is a concatenation of the one-hot representation for both global parameters (e.g. input resolution, width multiplier) and parameters of each inverted MobileNet block (e.g. kernel sizes and expand ratios). Note that we prefer one-hot representations over numerical representations because all design hyperparameters are discrete values.\nFor example, our design space supports\nkernel_size = [3, 5, 7]\nexpand_ratio = [3, 4, 6]\nThen, we represent kernel_size=3 as [1, 0, 0], kernel_size=5 as [0, 1, 0], and kernel_size=7 as [0, 0, 1]. Similarly, for expand_ratio=3, it is written as [1, 0, 0]; expand_ratio=4 is written as [0, 1, 0] and expand_ratio=6 is written as [0, 0, 1]. The representation for each inverted MobileNet block is obtained by concatenating the kernel size embedding with the expand ratio embedding. Note that for skipped blocks, we use [0, 0, 0] to represent their kernel sizes and expand ratios. You will see a detailed explanation of the architecture-embedding correspondence after running the following cell.\n\nacc_dataset = AccuracyDataset(\"acc_datasets\")\ntrain_loader, valid_loader, base_acc = acc_dataset.build_acc_data_loader(\n    arch_encoder=arch_encoder\n)\n\nprint(f\"The basic accuracy (mean accuracy of all subnets within the dataset is: {(base_acc * 100): .1f}%.\")\n\n# Let's print one sample in the training set\nsampled = 0\nfor (data, label) in train_loader:\n    data = data.to(device)\n    label = label.to(device)\n    print(\"=\" * 100)\n    # dummy pass to print the divided encoding\n    arch_encoding = arch_encoder.feature2arch(data[0].int().cpu().numpy(), verbose=False)\n    # print out the architecture encoding process in detail\n    arch_encoding = arch_encoder.feature2arch(data[0].int().cpu().numpy(), verbose=True)\n    visualize_subnet(arch_encoding)\n    print(f\"The accuracy of this subnet on the holdout validation set is: {(label[0] * 100): .1f}%.\")\n    sampled += 1\n    if sampled == 1:\n        break\n\nLoading data: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 50000/50000 [00:00&lt;00:00, 228025.66it/s]\n\n\nTrain Size: 40000, Valid Size: 10000\nThe basic accuracy (mean accuracy of all subnets within the dataset is:  90.3%.\n====================================================================================================\nnetwork embedding: [1 0 0 0 0 | 0 1 0 | 0 1 0 | 0 1 0 | 1 0 0 | 0 0 1 | 1 0 0 | 1 0 0 | 0 0 1 | 1 0 0 | 0 1 0 | 0 1 0 | 0 0 1 | 0 0 1 | 0 0 0 | 0 0 0 | 0 1 0 | 0 0 1 | 0 1 0 | 0 0 1 | 0 1 0 | 0 1 0 | 0 1 0 | 0 0 1 | 1 0 0 | 1 0 0 | 0 1 0 | 0 1 0 | 0 0 1 | 0 0 1 | 0 1 0 | 0 0 1 | 0 0 1 | 1 0 0 | 0 1 0 | 0 0 1 | 0 0 0 | 0 0 0 | 0 0 0 | 0 0 0 | 0 1 0 | 0 0 1]\nimage resolution embedding: [1 0 0 0 0] =&gt; image resolution: 96\nwidth multiplier embedding: [0 1 0] =&gt; width multiplier: 0.75\n**************************************************Stage1**************************************************\nkernel size embedding: [0 1 0] =&gt; kernel size: 5; expand ratio embedding: [0 1 0] =&gt; expand ratio: 4\nkernel size embedding: [1 0 0] =&gt; kernel size: 3; expand ratio embedding: [0 0 1] =&gt; expand ratio: 6\nkernel size embedding: [1 0 0] =&gt; kernel size: 3; expand ratio embedding: [1 0 0] =&gt; expand ratio: 3\n**************************************************Stage2**************************************************\nkernel size embedding: [0 0 1] =&gt; kernel size: 7; expand ratio embedding: [1 0 0] =&gt; expand ratio: 3\nkernel size embedding: [0 1 0] =&gt; kernel size: 5; expand ratio embedding: [0 1 0] =&gt; expand ratio: 4\nkernel size embedding: [0 0 1] =&gt; kernel size: 7; expand ratio embedding: [0 0 1] =&gt; expand ratio: 6\nkernel size embedding: [0 0 0] expand ratio embedding: [0 0 0] =&gt; layer skipped.\n**************************************************Stage3**************************************************\nkernel size embedding: [0 1 0] =&gt; kernel size: 5; expand ratio embedding: [0 0 1] =&gt; expand ratio: 6\nkernel size embedding: [0 1 0] =&gt; kernel size: 5; expand ratio embedding: [0 0 1] =&gt; expand ratio: 6\nkernel size embedding: [0 1 0] =&gt; kernel size: 5; expand ratio embedding: [0 1 0] =&gt; expand ratio: 4\nkernel size embedding: [0 1 0] =&gt; kernel size: 5; expand ratio embedding: [0 0 1] =&gt; expand ratio: 6\n**************************************************Stage4**************************************************\nkernel size embedding: [1 0 0] =&gt; kernel size: 3; expand ratio embedding: [1 0 0] =&gt; expand ratio: 3\nkernel size embedding: [0 1 0] =&gt; kernel size: 5; expand ratio embedding: [0 1 0] =&gt; expand ratio: 4\nkernel size embedding: [0 0 1] =&gt; kernel size: 7; expand ratio embedding: [0 0 1] =&gt; expand ratio: 6\nkernel size embedding: [0 1 0] =&gt; kernel size: 5; expand ratio embedding: [0 0 1] =&gt; expand ratio: 6\n**************************************************Stage5**************************************************\nkernel size embedding: [0 0 1] =&gt; kernel size: 7; expand ratio embedding: [1 0 0] =&gt; expand ratio: 3\nkernel size embedding: [0 1 0] =&gt; kernel size: 5; expand ratio embedding: [0 0 1] =&gt; expand ratio: 6\nkernel size embedding: [0 0 0] expand ratio embedding: [0 0 0] =&gt; layer skipped.\nkernel size embedding: [0 0 0] expand ratio embedding: [0 0 0] =&gt; layer skipped.\n**************************************************Stage6**************************************************\nkernel size embedding: [0 1 0] =&gt; kernel size: 5; expand ratio embedding: [0 0 1] =&gt; expand ratio: 6\nThe accuracy of this subnet on the holdout validation set is:  88.7%.\n\n\n\n\n\n\n\n\n\n\n\nQuestion 4: Complete the code for accuracy predictor training.\ní›ˆë ¨í•  ì‹œê°„ì…ë‹ˆë‹¤!\n\ncriterion = torch.nn.L1Loss().to(device)\noptimizer = torch.optim.Adam(acc_predictor.parameters())\n# the default value is zero\nacc_predictor.base_acc.data += base_acc\nfor epoch in tqdm(range(10)):\n    acc_predictor.train()\n    for (data, label) in tqdm(train_loader, desc=\"Epoch%d\" % (epoch + 1), position=0, leave=True):\n        # step 1. Move the data and labels to device (cuda:0).\n        data = data.to(device)\n        label = label.to(device)\n        ############### YOUR CODE STARTS HERE ###############\n        # step 2. Run forward pass.\n        pred = acc_predictor(data)\n        # step 3. Calculate the loss.\n        loss = criterion(pred, label)\n        # step 4. Perform the backward pass.\n        optimizer.zero_grad()\n        loss.backward()\n        optimizer.step()\n        ################ YOUR CODE ENDS HERE ################\n\n    acc_predictor.eval()\n    with torch.no_grad():\n        with tqdm(total=len(valid_loader), desc=\"Val\", position=0, leave=True) as t:\n            for (data, label) in valid_loader:\n                # step 1. Move the data and labels to device (cuda:0).\n                data = data.to(device)\n                label = label.to(device)\n                ############### YOUR CODE STARTS HERE ###############\n                # step 2. Run forward pass.\n                pred = acc_predictor(data)\n                # step 3. Calculate the loss.\n                loss = criterion(pred, label)\n                ############### YOUR CODE ENDS HERE ###############\n                t.set_postfix({\"loss\": loss.item()})\n                t.update(1)\n\nif not os.path.exists(acc_pred_checkpoint_path):\n    torch.save(acc_predictor.cpu().state_dict(), acc_pred_checkpoint_path)\n\nEpoch1: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 157/157 [00:00&lt;00:00, 362.86it/s]\nVal: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 40/40 [00:00&lt;00:00, 109.00it/s, loss=0.00374]\nEpoch2: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 157/157 [00:00&lt;00:00, 262.66it/s]\nVal: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 40/40 [00:00&lt;00:00, 141.77it/s, loss=0.0026]\nEpoch3: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 157/157 [00:00&lt;00:00, 241.87it/s]\nVal: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 40/40 [00:00&lt;00:00, 118.13it/s, loss=0.00251]\nEpoch4: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 157/157 [00:00&lt;00:00, 336.42it/s]\nVal: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 40/40 [00:00&lt;00:00, 119.41it/s, loss=0.00259]\nEpoch5: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 157/157 [00:00&lt;00:00, 331.75it/s]\nVal: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 40/40 [00:00&lt;00:00, 117.39it/s, loss=0.00242]\nEpoch6: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 157/157 [00:00&lt;00:00, 341.96it/s]\nVal: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 40/40 [00:00&lt;00:00, 96.35it/s, loss=0.00235]\nEpoch7: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 157/157 [00:00&lt;00:00, 321.68it/s]\nVal: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 40/40 [00:00&lt;00:00, 122.19it/s, loss=0.0023]\nEpoch8: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 157/157 [00:00&lt;00:00, 307.33it/s]\nVal: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 40/40 [00:00&lt;00:00, 121.72it/s, loss=0.00178]\nEpoch9: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 157/157 [00:00&lt;00:00, 329.76it/s]\nVal: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 40/40 [00:00&lt;00:00, 119.59it/s, loss=0.00203]\nEpoch10: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 157/157 [00:00&lt;00:00, 308.76it/s]\nVal: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 40/40 [00:00&lt;00:00, 99.72it/s, loss=0.00195] \n100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 10/10 [00:08&lt;00:00,  1.17it/s]\n\n\ní›ˆë ¨í•œ ëª¨ë¸ì˜ Predictionê³¼ ì‹¤ì œ ìˆ˜ì¹˜ì™€ Corrleationì´ ê·¸ë˜í”„ë¡œ ë³´ì´ë„¤ìš”. â€œLinearâ€ í•©ë‹ˆë‹¤.\n\npredicted_accuracies = []\nground_truth_accuracies = []\nacc_predictor = acc_predictor.to(\"cuda:0\")\nacc_predictor.eval()\nwith torch.no_grad():\n    with tqdm(total=len(valid_loader), desc=\"Val\") as t:\n        for (data, label) in valid_loader:\n            data = data.to(device)\n            label = label.to(device)\n            pred = acc_predictor(data)\n            predicted_accuracies += pred.cpu().numpy().tolist()\n            ground_truth_accuracies += label.cpu().numpy().tolist()\n            if len(predicted_accuracies) &gt; 200:\n                break\nplt.scatter(predicted_accuracies, ground_truth_accuracies)\n# draw y = x\nmin_acc, max_acc = min(predicted_accuracies), max(predicted_accuracies)\nprint(min_acc, max_acc)\nplt.plot([min_acc, max_acc], [min_acc, max_acc], c=\"red\", linewidth=2)\nplt.xlabel(\"Predicted accuracy\")\nplt.ylabel(\"Measured accuracy\")\nplt.title(\"Correlation between predicted accuracy and real accuracy\")\n\nVal:   0%|          | 0/40 [00:00&lt;?, ?it/s]\n\n\n0.8604847192764282 0.9356203079223633\n\n\nText(0.5, 1.0, 'Correlation between predicted accuracy and real accuracy')"
  },
  {
    "objectID": "posts/labs/lab03.html#part-2.-neural-architecture-search",
    "href": "posts/labs/lab03.html#part-2.-neural-architecture-search",
    "title": "ğŸ‘©â€ğŸ’» Lab 3",
    "section": "Part 2. Neural Architecture Search",
    "text": "Part 2. Neural Architecture Search\në“œë””ì–´ ë§ˆì§€ë§‰ ë‹¨ê³„ì…ë‹ˆë‹¤. ì›í•˜ëŠ” ëª¨ë¸ì„ ì°¾ì•„ë³´ì£ ! ë‘ ê°€ì§€ Search ë°©ë²•ì„ ì´ìš©í•  ê±´ë°, í•˜ë‚˜ëŠ” Random Searchì´ê³ , ë‹¤ë¥¸ í•˜ë‚˜ëŠ” evolutionary Search ì´ìš©í•œ Neural Architecture Searchì…ë‹ˆë‹¤(ë“œë””ì–´ NAS!).\n\n\nQuestion 5: Complete the following random search agent.\nRandom SearchëŠ” ì—´ì‹¬íˆ constraintì— í•´ë‹¹í•˜ëŠ” Sampleì„ ëª¨ì•„ì„œ ìµœê³ ì˜ Accuracyë¥¼ ê°€ì§„ ëª¨ë¸ êµ¬ì¡°ë¥¼ ê³ ë¥´ë©´ ë©ë‹ˆë‹¤.\n\nclass RandomSearcher:\n    def __init__(self, efficiency_predictor, accuracy_predictor):\n        self.efficiency_predictor = efficiency_predictor\n        self.accuracy_predictor = accuracy_predictor\n\n    def random_valid_sample(self, constraint):\n        # randomly sample subnets until finding one that satisfies the constraint \n        while True:\n            sample = self.accuracy_predictor.arch_encoder.random_sample_arch()\n            efficiency = self.efficiency_predictor.get_efficiency(sample)\n            if self.efficiency_predictor.satisfy_constraint(efficiency, constraint):\n                return sample, efficiency\n\n    def run_search(self, constraint, n_subnets=100):\n        subnet_pool = []\n        # sample subnets\n        for _ in tqdm(range(n_subnets)):\n            sample, efficiency = self.random_valid_sample(constraint)\n            subnet_pool.append(sample)\n        # predict the accuracy of subnets\n        accs = self.accuracy_predictor.predict_acc(subnet_pool)\n        ############### YOUR CODE STARTS HERE ###############\n        # hint: one line of code\n        # get the index of the best subnet\n        best_idx = accs.argmax()\n        ############### YOUR CODE ENDS HERE #################\n        # return the best subnet\n        return accs[best_idx], subnet_pool[best_idx]\n\n\n\nQuestion 6: Complete the following function.\nNote: MACs 100M results lower than MACs 50M, Prof.Â Han says this might not be intuitive.\n\ndef search_and_measure_acc(agent, constraint, **kwargs):\n    ############### YOUR CODE STARTS HERE ###############\n    # hint: call the search function\n    best_info = agent.run_search(constraint=constraint, **kwargs)\n    ############### YOUR CODE ENDS HERE #################\n    # get searched subnet\n    \n    print(\"Best info: \", best_info)\n\n    ofa_network.set_active_subnet(**best_info[1])\n    subnet = ofa_network.get_active_subnet().to(device)\n    # calibrate bn\n    calib_bn(subnet, data_dir, 128, best_info[1][\"image_size\"])  # ?\n    # build val loader\n    val_loader = build_val_data_loader(data_dir, best_info[1][\"image_size\"], 128)\n    # measure accuracy\n    acc = validate(subnet, val_loader)\n    # print best_info\n    print(f\"Accuracy of the selected subnet: {acc}\")\n    # visualize model architecture\n    visualize_subnet(best_info[1])\n    return acc, subnet\n\n\nrandom.seed(1)\nnp.random.seed(1)\nnas_agent = RandomSearcher(efficiency_predictor, acc_predictor)\n# MACs-constrained search\nsubnets_rs_macs = {}\nfor millonMACs in [50, 100]:\n    search_constraint = dict(millonMACs=millonMACs)\n    print(f\"Random search with constraint: MACs &lt;= {millonMACs}M\")\n    subnets_rs_macs[millonMACs] = search_and_measure_acc(nas_agent, search_constraint, n_subnets=300)\n\n# memory-constrained search\nsubnets_rs_memory = {}\nfor KBPeakMemory in [256, 512]:\n    search_constraint = dict(KBPeakMemory=KBPeakMemory)\n    print(f\"Random search with constraint: Peak memory &lt;= {KBPeakMemory}KB\")\n    subnets_rs_memory[KBPeakMemory] = search_and_measure_acc(nas_agent, search_constraint, n_subnets=300)\n\nRandom search with constraint: MACs &lt;= 50M\nBest info:  (tensor(0.9327, device='cuda:0', grad_fn=&lt;SelectBackward0&gt;), {'ks': [5, 7, 5, 3, 3, 7, 5, 3, 7, 3, 3, 3, 7, 5, 5, 5, 7, 5, 3, 7], 'e': [4, 3, 3, 6, 4, 3, 6, 6, 4, 3, 3, 4, 4, 6, 6, 4, 3, 4, 4, 3], 'd': [2, 2, 1, 1, 2, 0], 'image_size': 160, 'wid': 1})\nAccuracy of the selected subnet: 93.27543427346657\nRandom search with constraint: MACs &lt;= 100M\nBest info:  (tensor(0.9329, device='cuda:0', grad_fn=&lt;SelectBackward0&gt;), {'ks': [3, 3, 5, 7, 7, 5, 5, 7, 7, 3, 5, 5, 5, 3, 7, 5, 7, 5, 5, 3], 'e': [4, 3, 6, 3, 3, 6, 4, 4, 4, 4, 3, 3, 4, 4, 4, 4, 4, 6, 6, 3], 'd': [2, 2, 1, 1, 2, 2], 'image_size': 160, 'wid': 1})\nAccuracy of the selected subnet: 93.47394543971969\nRandom search with constraint: Peak memory &lt;= 256KB\nBest info:  (tensor(0.9248, device='cuda:0', grad_fn=&lt;SelectBackward0&gt;), {'ks': [7, 7, 5, 7, 7, 5, 3, 5, 3, 5, 3, 5, 7, 3, 3, 5, 7, 7, 5, 3], 'e': [4, 4, 6, 4, 6, 3, 6, 4, 6, 6, 6, 4, 4, 4, 4, 6, 3, 6, 4, 4], 'd': [1, 1, 1, 1, 1, 1], 'image_size': 160, 'wid': 0})\nAccuracy of the selected subnet: 92.8287840963889\nRandom search with constraint: Peak memory &lt;= 512KB\nBest info:  (tensor(0.9328, device='cuda:0', grad_fn=&lt;SelectBackward0&gt;), {'ks': [3, 3, 3, 3, 5, 5, 5, 7, 5, 7, 5, 5, 7, 5, 7, 7, 7, 7, 3, 3], 'e': [4, 3, 6, 4, 6, 6, 4, 6, 4, 4, 4, 6, 4, 4, 3, 6, 4, 4, 3, 6], 'd': [2, 1, 2, 0, 2, 0], 'image_size': 160, 'wid': 1})\nAccuracy of the selected subnet: 93.15136479455839\n\n\n100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 300/300 [00:19&lt;00:00, 15.43it/s]\nValidate: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 32/32 [00:01&lt;00:00, 30.42it/s, loss=0.187, top1=93.3]\n100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 300/300 [00:19&lt;00:00, 15.05it/s]\nValidate: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 32/32 [00:01&lt;00:00, 30.58it/s, loss=0.186, top1=93.5]\n100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 300/300 [00:43&lt;00:00,  6.83it/s]\nValidate: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 32/32 [00:00&lt;00:00, 34.29it/s, loss=0.204, top1=92.8]\n100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 300/300 [00:22&lt;00:00, 13.54it/s]\nValidate: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 32/32 [00:01&lt;00:00, 29.29it/s, loss=0.19, top1=93.2] \n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nQuestion 7: Complete the following evolutionary search agent.\nEvolutionary SearchëŠ” ì—¬ê¸°ì„œ í•œ ë‹¨ê³„ê°€ ë” ë“¤ì–´ê°€ìš”. ë°”ë¡œ â€œCrossoverâ€ì´ë¼ëŠ” ë‹¨ê³„ì¸ë°, ëœë¤ìœ¼ë¡œ ë½‘ì€ ë‘ ìƒ˜í”Œì—ì„œ ë˜ ë‹¤ì‹œ ëª¨ë¸ êµ¬ì¡°ì—ì„œ ë¶€ë¶„ì„ ë‚˜ëˆ  ëœë¤ìœ¼ë¡œ ë‘˜ ì¤‘ì— í•˜ë‚˜ë¥¼ ë½‘ì•„ì„œ í•©ì¹˜ëŠ” ê³¼ì •ì…ë‹ˆë‹¤. sub-networkê°€ ë½‘íˆëŠ” ê°œìˆ˜ë¥¼ populationì´ë¼ê³  í•˜ë©´ì„œ ëœë¤ ìƒ˜í”Œë§ì„ í•˜ëŠ” íšŸìˆ˜ë¥¼ generation, ê·¸ë¦¬ê³  max_time_budgetì„ ë‘¬ì„œ ëª‡ë²ˆì˜ generationì„ ê±°ì¹  ê²ƒì¸ê°€ë„ ì œí•œí•©ë‹ˆë‹¤. ë§¤ generationë§ˆë‹¤ ëª¨ì´ëŠ” populationì„ ì •ë ¬í•  ê±°ê±°ë“ ìš”.\n\nNow you have succesfully implemented the random search algorithm. In this part, we will implement a more sample-efficient search algorithm, evolutionary search. Evolutionary search is inspired by the evolution algorithm (or genetic algorithm). A population of sub-networks are first sampled from the design space. Then, in each generation, we perform random mutation and crossover operations as is shown in the figure above. The sub-networks with highest accuracy will be kept, and this process will be repeated until the number of generations reaches max_time_budget. Similar to the random search, throughout the search process, all sub-networks that cannot satisfy the efficiency constraint will be discarded.\n\nclass EvolutionSearcher:\n    def __init__(self, efficiency_predictor, accuracy_predictor, **kwargs):\n        self.efficiency_predictor = efficiency_predictor\n        self.accuracy_predictor = accuracy_predictor\n\n        # evolution hyper-parameters\n        self.arch_mutate_prob = kwargs.get(\"arch_mutate_prob\", 0.1)\n        self.resolution_mutate_prob = kwargs.get(\"resolution_mutate_prob\", 0.5)\n        self.population_size = kwargs.get(\"population_size\", 100)\n        self.max_time_budget = kwargs.get(\"max_time_budget\", 500)\n        self.parent_ratio = kwargs.get(\"parent_ratio\", 0.25)\n        self.mutation_ratio = kwargs.get(\"mutation_ratio\", 0.5)\n\n    def update_hyper_params(self, new_param_dict):\n        self.__dict__.update(new_param_dict)\n\n    def random_valid_sample(self, constraint):\n        # randomly sample subnets until finding one that satisfies the constraint \n        while True:\n            sample = self.accuracy_predictor.arch_encoder.random_sample_arch()\n            efficiency = self.efficiency_predictor.get_efficiency(sample)\n            if self.efficiency_predictor.satisfy_constraint(efficiency, constraint):\n                return sample, efficiency\n\n    def mutate_sample(self, sample, constraint):\n        while True:\n            new_sample = copy.deepcopy(sample)\n\n            self.accuracy_predictor.arch_encoder.mutate_resolution(new_sample, self.resolution_mutate_prob)\n            self.accuracy_predictor.arch_encoder.mutate_width(new_sample, self.arch_mutate_prob)\n            self.accuracy_predictor.arch_encoder.mutate_arch(new_sample, self.arch_mutate_prob)\n\n            efficiency = self.efficiency_predictor.get_efficiency(new_sample)\n            if self.efficiency_predictor.satisfy_constraint(efficiency, constraint):\n                return new_sample, efficiency\n\n    def crossover_sample(self, sample1, sample2, constraint):\n        while True:\n            new_sample = copy.deepcopy(sample1)\n            for key in new_sample.keys():\n                if not isinstance(new_sample[key], list):\n                    ############### YOUR CODE STARTS HERE ###############\n                    # hint: randomly choose the value from sample1[key] and sample2[key], random.choice\n                    new_sample[key] = random.choice([sample1[key], sample2[key]])\n                    ############### YOUR CODE ENDS HERE #################\n                else:\n                    for i in range(len(new_sample[key])):\n                        ############### YOUR CODE STARTS HERE ###############\n                        new_sample[key][i] = random.choice([sample1[key][i], sample2[key][i]])\n                        ############### YOUR CODE ENDS HERE #################\n\n            efficiency = self.efficiency_predictor.get_efficiency(new_sample)\n            if self.efficiency_predictor.satisfy_constraint(efficiency, constraint):\n                return new_sample, efficiency\n\n    def run_search(self, constraint, **kwargs):\n        self.update_hyper_params(kwargs)\n\n        mutation_numbers = int(round(self.mutation_ratio * self.population_size))\n        parents_size = int(round(self.parent_ratio * self.population_size))\n\n        best_valids = [-100]\n        population = []  # (acc, sample) tuples\n        child_pool = []\n        best_info = None\n        # generate random population\n        for _ in range(self.population_size):\n            sample, efficiency = self.random_valid_sample(constraint)\n            child_pool.append(sample)\n\n        accs = self.accuracy_predictor.predict_acc(child_pool)\n        for i in range(self.population_size):\n            population.append((accs[i].item(), child_pool[i]))\n\n        # evolving the population\n        with tqdm(total=self.max_time_budget) as t:\n            for i in range(self.max_time_budget):\n                ############### YOUR CODE STARTS HERE ###############\n                # hint: sort the population according to the acc (descending order)\n                population = sorted(population, key=lambda x: x[0], reverse=True)\n                ############### YOUR CODE ENDS HERE #################\n\n                ############### YOUR CODE STARTS HERE ###############\n                # hint: keep topK samples in the population, K = parents_size\n                # the others are discarded.\n                population = population[:parents_size]\n                ############### YOUR CODE ENDS HERE #################\n\n                # update best info\n                acc = population[0][0]\n                if acc &gt; best_valids[-1]:\n                    best_valids.append(acc)\n                    best_info = population[0]\n                else:\n                    best_valids.append(best_valids[-1])\n\n                child_pool = []\n                for j in range(mutation_numbers):\n                    # randomly choose a sample\n                    par_sample = population[np.random.randint(parents_size)][1]\n                    # mutate this sample\n                    new_sample, efficiency = self.mutate_sample(par_sample, constraint)\n                    child_pool.append(new_sample)\n\n                for j in range(self.population_size - mutation_numbers):\n                    # randomly choose two samples\n                    par_sample1 = population[np.random.randint(parents_size)][1]\n                    par_sample2 = population[np.random.randint(parents_size)][1]\n                    # crossover\n                    new_sample, efficiency = self.crossover_sample(\n                        par_sample1, par_sample2, constraint\n                    )\n                    child_pool.append(new_sample)\n                # predict accuracy with the accuracy predictor\n                accs = self.accuracy_predictor.predict_acc(child_pool)\n                for j in range(self.population_size):\n                    population.append((accs[j].item(), child_pool[j]))\n\n                t.update(1)\n\n        return best_info\n\n\n\nQuestion 8: Run evolutionary search and tune evo_params to optimize the results. Describe your findings.\në‚¨ì€ ë¶€ë¶„ì€ ì‹¤í—˜, ì‹¤í—˜, ì‹¤í—˜ì…ë‹ˆë‹¤. ê´€ì°°í•´ë³´ì‹œì£ !\nAnswer: - The default population size and time budget are too small. Increasing them can effectively improves the final results. But it also increases the search cost. - Increasing the probability of resolution mutation can improve the final results. (hint: Question 1)\n\nrandom.seed(1)\nnp.random.seed(1)\n\nevo_params = {\n    'arch_mutate_prob': 0.1, # The probability of architecture mutation in evolutionary search\n    'resolution_mutate_prob': 0.1, # The probability of resolution mutation in evolutionary search\n    'population_size': 10,  # The size of the population\n    'max_time_budget': 10,  \n    'parent_ratio': 0.1,\n    'mutation_ratio': 0.1,\n}\n\nnas_agent = EvolutionSearcher(efficiency_predictor, acc_predictor, **evo_params)\n# MACs-constrained search\nsubnets_evo_macs = {}\nfor millonMACs in [50, 100]:\n    search_constraint = dict(millionMACs=millonMACs)\n    print(f\"Evolutionary search with constraint: MACs &lt;= {millonMACs}M\")\n    subnets_evo_macs[millonMACs] = search_and_measure_acc(nas_agent, search_constraint)\n\n# memory-constrained search\nsubnets_evo_memory = {}\nfor KBPeakMemory in [256, 512]:\n    search_constraint = dict(KBPeakMemory=KBPeakMemory)\n    print(f\"Evolutionary search with constraint: Peak memory &lt;= {KBPeakMemory}KB\")\n    subnets_evo_memory[KBPeakMemory] = search_and_measure_acc(nas_agent, search_constraint)\n\nEvolutionary search with constraint: MACs &lt;= 50M\nAccuracy of the selected subnet: 92.28287844220107\nEvolutionary search with constraint: MACs &lt;= 100M\nAccuracy of the selected subnet: 92.33250616939725\nEvolutionary search with constraint: Peak memory &lt;= 256KB\nAccuracy of the selected subnet: 92.45657571267253\nEvolutionary search with constraint: Peak memory &lt;= 512KB\nAccuracy of the selected subnet: 93.05210921143184\n\n\n100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 10/10 [00:05&lt;00:00,  1.72it/s]\nValidate: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 32/32 [00:00&lt;00:00, 35.68it/s, loss=0.214, top1=92.3]\n100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 10/10 [00:07&lt;00:00,  1.42it/s]\nValidate: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 32/32 [00:00&lt;00:00, 36.25it/s, loss=0.206, top1=92.3]\n100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 10/10 [00:07&lt;00:00,  1.35it/s]\nValidate: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 32/32 [00:00&lt;00:00, 35.39it/s, loss=0.21, top1=92.5] \n100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 10/10 [00:05&lt;00:00,  1.74it/s]\nValidate: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 32/32 [00:00&lt;00:00, 32.62it/s, loss=0.193, top1=93.1]\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nQuestion 9: Run evolutionary search under real-world constraints.\nIn real-world applications, we may have multiple efficiency constraints: https://blog.tensorflow.org/2019/10/visual-wake-words-with-tensorflow-lite_30.html. Use evolutionary search to find models that satisfy the following constraints: - [15 pts] 250 KB, 60M MACs (acc &gt;= 92.5% to get the full credit) - [10 pts, bonus] 200KB, 30M MACs (acc &gt;= 90% to get the full credit)\nHint: You do not have to use the same evo_params for these two tasks.\n\nrandom.seed(1)\nnp.random.seed(1)\nevo_params = {\n    'arch_mutate_prob': 0.1, # The probability of architecture mutation in evolutionary search\n    'resolution_mutate_prob': 0.5, # The probability of resolution mutation in evolutionary search\n    'population_size': 50,  # The size of the population\n    'max_time_budget': 20,  \n    'parent_ratio': 0.25,\n    'mutation_ratio': 0.3,\n}\n\nnas_agent = EvolutionSearcher(efficiency_predictor, acc_predictor, **evo_params)\n\n(millionMACs, KBPeakMemory) = [60, 250]\nprint(f\"Evolution search with constraint: MACs &lt;= {millionMACs}M, peak memory &lt;= {KBPeakMemory}KB\")\nsearch_and_measure_acc(nas_agent, dict(millionMACs=millionMACs, KBPeakMemory=KBPeakMemory))\nprint(\"Evolution search finished!\")\n\nEvolution search with constraint: MACs &lt;= 60M, peak memory &lt;= 250KB\nAccuracy of the selected subnet: 92.60545903435415\nEvolution search finished!\n\n\n100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 20/20 [01:13&lt;00:00,  3.66s/it]\nValidate: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 32/32 [00:00&lt;00:00, 36.77it/s, loss=0.201, top1=92.6]\n\n\n\n\n\n\n\n\n\n\nrandom.seed(1)\nnp.random.seed(1)\nevo_params = {\n    'arch_mutate_prob': 0.1, # The probability of architecture mutation in evolutionary search\n    'resolution_mutate_prob': 0.5, # The probability of resolution mutation in evolutionary search\n    'population_size': 50,  # The size of the population\n    'max_time_budget': 30,  \n    'parent_ratio': 0.25,\n    'mutation_ratio': 0.3,\n}\nnas_agent = EvolutionSearcher(efficiency_predictor, acc_predictor, **evo_params)\n\n(millionMACs, KBPeakMemory) = [30, 200]\nprint(f\"Evolution search with constraint: MACs &lt;= {millionMACs}M, peak memory &lt;= {KBPeakMemory}KB\")\nsearch_and_measure_acc(nas_agent, dict(millionMACs=millionMACs, KBPeakMemory=KBPeakMemory))\nprint(\"Evolution search finished!\")\n\nEvolution search with constraint: MACs &lt;= 30M, peak memory &lt;= 200KB\nAccuracy of the selected subnet: 90.54590573748644\nEvolution search finished!\n\n\n100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 30/30 [01:45&lt;00:00,  3.52s/it]\nValidate: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 32/32 [00:00&lt;00:00, 37.25it/s, loss=0.241, top1=90.5]\n\n\n\n\n\n\n\n\n\n\nrandom.seed(1)\nnp.random.seed(1)\nevo_params = {\n    'arch_mutate_prob': 0.1, # The probability of architecture mutation in evolutionary search\n    'resolution_mutate_prob': 0.5, # The probability of resolution mutation in evolutionary search\n    'population_size': 50,  # The size of the population\n    'max_time_budget': 20,  \n    'parent_ratio': 0.25,\n    'mutation_ratio': 0.3,\n}\n\nnas_agent = EvolutionSearcher(efficiency_predictor, acc_predictor, **evo_params)\n\n(millionMACs, KBPeakMemory) = [15, 256]\nprint(f\"Evolution search with constraint: MACs &lt;= {millionMACs}M, peak memory &lt;= {KBPeakMemory}KB\")\nsearch_and_measure_acc(nas_agent, dict(millionMACs=millionMACs, KBPeakMemory=KBPeakMemory))\nprint(\"Evolution search finished!\")\n\nEvolution search with constraint: MACs &lt;= 15M, peak memory &lt;= 256KB\nBest info:  (0.8773346543312073, {'ks': [7, 5, 5, 3, 5, 3, 5, 7, 5, 5, 7, 5, 3, 7, 3, 5, 3, 3, 7, 5], 'e': [4, 6, 6, 6, 4, 3, 3, 4, 3, 4, 4, 6, 6, 4, 6, 4, 3, 4, 3, 3], 'd': [0, 1, 0, 1, 1, 1], 'image_size': 96, 'wid': 0})\nAccuracy of the selected subnet: 86.35235731631296\nEvolution search finished!\n\n\n100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 20/20 [01:29&lt;00:00,  4.48s/it]\nValidate: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 32/32 [00:00&lt;00:00, 45.67it/s, loss=0.319, top1=86.4]\n\n\n\n\n\n\n\n\n\n\nrandom.seed(1)\nnp.random.seed(1)\nevo_params = {\n    'arch_mutate_prob': 0.1, # The probability of architecture mutation in evolutionary search\n    'resolution_mutate_prob': 0.5, # The probability of resolution mutation in evolutionary search\n    'population_size': 50,  # The size of the population\n    'max_time_budget': 20,  \n    'parent_ratio': 0.25,\n    'mutation_ratio': 0.3,\n}\n\nnas_agent = EvolutionSearcher(efficiency_predictor, acc_predictor, **evo_params)\n\n(millionMACs, KBPeakMemory) = [60, 64]\nprint(f\"Evolution search with constraint: MACs &lt;= {millionMACs}M, peak memory &lt;= {KBPeakMemory}KB\")\nsearch_and_measure_acc(nas_agent, dict(millionMACs=millionMACs, KBPeakMemory=KBPeakMemory))\nprint(\"Evolution search finished!\")\n\nEvolution search with constraint: MACs &lt;= 60M, peak memory &lt;= 64KB\n\n\nKeyboardInterrupt: \n\n\n\nrandom.seed(1)\nnp.random.seed(1)\nevo_params = {\n    'arch_mutate_prob': 0.1, # The probability of architecture mutation in evolutionary search\n    'resolution_mutate_prob': 0.5, # The probability of resolution mutation in evolutionary search\n    'population_size': 50,  # The size of the population\n    'max_time_budget': 20,  \n    'parent_ratio': 0.25,\n    'mutation_ratio': 0.3,\n}\n\nnas_agent = EvolutionSearcher(efficiency_predictor, acc_predictor, **evo_params)\n\n(millionMACs, KBPeakMemory) = [10, 64]\nprint(f\"Evolution search with constraint: MACs &lt;= {millionMACs}M, peak memory &lt;= {KBPeakMemory}KB\")\nsearch_and_measure_acc(nas_agent, dict(millionMACs=millionMACs, KBPeakMemory=KBPeakMemory))\nprint(\"Evolution search finished!\")\n\nEvolution search with constraint: MACs &lt;= 10M, peak memory &lt;= 64KB\n\n\n\n\nQuestion 10: Is it possible to find a subnet with the following efficiency constraints in the current design space?\n\nA: The activation size of the subnet is at most 256KB and the MACs of the subnet is at most 15M.\nB: The activation size of the subnet is at most 64 KB.\n\nAnswer:\n\nA: yes\nB: no"
  },
  {
    "objectID": "posts/lecs/lec04.html",
    "href": "posts/lecs/lec04.html",
    "title": "ğŸ§‘â€ğŸ« Lecture 4",
    "section": "",
    "text": "ì´ì „ í¬ìŠ¤íŒ…ì—ì„œ Pruningì— ëŒ€í•´ì„œ ë°°ì› ì—ˆë‹¤. ì´ë²ˆì—ëŠ” Pruningì— ëŒ€í•œ ë‚¨ì€ ì´ì•¼ê¸°ì¸ Pruning Ratioë¥¼ ì •í•˜ëŠ” ë°©ë²•, Fine-tuning ê³¼ì •ì— ëŒ€í•´ ì•Œì•„ë³´ê³ , ë§ˆì§€ë§‰ìœ¼ë¡œ Sparsityë¥¼ ìœ„í•œ System Supportì— ëŒ€í•´ ì•Œì•„ë³´ê³ ì í•œë‹¤."
  },
  {
    "objectID": "posts/lecs/lec04.html#pruning-ratio",
    "href": "posts/lecs/lec04.html#pruning-ratio",
    "title": "ğŸ§‘â€ğŸ« Lecture 4",
    "section": "1. Pruning Ratio",
    "text": "1. Pruning Ratio\nPruningì„ í•˜ê¸° ìœ„í•´ì„œ ì–´ëŠ ì •ë„ Pruningì„ í•´ì•¼ í• ì§€ ì–´ë–»ê²Œ ì •í•´ì•¼ í• ê¹Œ?\nì¦‰, ë‹¤ì‹œ ë§í•´ì„œ ëª‡ % ì •ë„ ê·¸ë¦¬ê³  ì–´ë–»ê²Œ Pruningì„ í•´ì•¼ ì¢‹ì„ê¹Œ?\n\n\n\nPruning ë°©ì‹ ë¹„êµ\n\n\nìš°ì„  Channel ë³„ Pruningì„ í•  ë•Œ, Channel êµ¬ë¶„ ì—†ì´ ë™ì¼í•œ Pruning ë¹„ìœ¨(Uniform)ì„ ì ìš©í•˜ë©´ ì„±ëŠ¥ì´ ì¢‹ì§€ ì•Šë‹¤. ì˜¤ë¥¸ìª½ ê·¸ë˜í”„ì—ì„œ ì§€í–¥í•´ì•¼ í•˜ëŠ” ë°©í–¥ì€ LatencyëŠ” ì ê²Œ, AccuracyëŠ” ë†’ê²Œì´ë¯€ë¡œ ì™¼ìª½ ìƒë‹¨ì˜ ì˜ì—­ì´ ë˜ë„ë¡ Pruningì„ ì§„í–‰í•´ì•¼ í•œë‹¤. ê·¸ë ‡ë‹¤ë©´ ê²°ë¡ ì€ Channel ë³„ êµ¬ë¶„ì„ í•´ì„œ ì–´ë–¤ Channelì€ Pruning ë¹„ìœ¨ì„ ë†’ê²Œ, ì–´ë–¤ Channelì€ Pruning ë¹„ìœ¨ì„ ë‚®ê²Œ í•´ì•¼ í•œë‹¤ëŠ” ì´ì•¼ê¸°ê°€ ëœë‹¤.\n\n1.1 Sensitiviy Analysis\nChannel ë³„ êµ¬ë¶„ì„ í•´ì„œ Pruningì„ í•œë‹¤ëŠ” ê¸°ë³¸ ì•„ì´ë””ì–´ëŠ” ì•„ë˜ì™€ ê°™ë‹¤.\n\nAccuracyì— ì˜í–¥ì„ ë§ì´ ì£¼ëŠ” LayerëŠ” Pruningì„ ì ê²Œ í•´ì•¼ í•œë‹¤.\nAccuracyì— ì˜í–¥ì„ ì ê²Œ ì£¼ëŠ” LayerëŠ” Pruningì„ ë§ì´ í•´ì•¼ í•œë‹¤.\n\nAccuracyë¥¼ ë˜ë„ë¡ì´ë©´ ì›ë˜ì˜ ëª¨ë¸ë³´ë‹¤ ëœ ë–¨ì–´ì§€ê²Œ ë§Œë“¤ë©´ì„œ Pruningì„ í•´ì„œ ëª¨ë¸ì„ ê°€ë³ê²Œ ë§Œë“œëŠ” ê²ƒì´ ëª©í‘œì´ê¸° ë•Œë¬¸ì— ë‹¹ì—°í•œ ì•„ì´ë””ì–´ì¼ ê²ƒì´ë‹¤. Accuracyì— ì˜í–¥ì„ ë§ì´ ì¤€ë‹¤ëŠ” ë§ì€ Sensitiveí•œ Layerì´ë‹¤ë¼ëŠ” í‘œí˜„ìœ¼ë¡œ ë‹¤ë¥´ê²Œ ë§í•  ìˆ˜ ìˆë‹¤. ë”°ë¼ì„œ ê° Layerì˜ Senstivityë¥¼ ì¸¡ì •í•´ì„œ Sensitiveí•œ LayerëŠ” Pruning Ratioë¥¼ ë‚®ê²Œ ì„¤ê³„í•˜ë©´ ëœë‹¤.\nLayerì˜ Sensitivityë¥¼ ì¸¡ì •í•˜ê¸° ìœ„í•´ Sensitivity Analysisë¥¼ ì§„í–‰í•´ë³´ì. ë‹¹ì—°íˆ íŠ¹ì • Layerì˜ Pruning Ratioê°€ ë†’ì„ ìˆ˜ë¡ weightê°€ ë§ì´ ê°€ì§€ì¹˜ê¸° ëœ ê²ƒì´ë¯€ë¡œ AccuracyëŠ” ë–¨ì–´ì§€ê²Œ ëœë‹¤.\n\n\n\nL0 Pruning Rate ê·¸ë˜í”„\n\n\n\n\n\n\n\n\nPruningì„ í•˜ëŠ” WeightëŠ” ì–´ë–»ê²Œ ê²°ì •í•˜ë‚˜ìš”?\n\n\n\n\n\nPruning Ratioì— ì˜í•´ Pruned ë˜ëŠ” weightëŠ” ì´ì „ ê°•ì˜ì—ì„œ ë°°ìš´ â€œImportance(weightì˜ ì ˆëŒ“ê°’ í¬ê¸°)â€ì— ë”°ë¼ ì„ íƒëœë‹¤.\n\n\n\nìœ„ì˜ ê·¸ë¦¼ì—ì„œ ì²˜ëŸ¼ Layer 0(L0)ë§Œì„ ê°€ì§€ê³  Pruning Ratioë¥¼ ë†’ì—¬ê°€ë©´ì„œ ê´€ì°°í•´ë³´ë©´, ì•½ 70% ì´í›„ë¶€í„°ëŠ” Accuracyê°€ ê¸‰ê²©í•˜ê²Œ ë–¨ì–´ì§€ëŠ” ê²ƒì„ ë³¼ ìˆ˜ ìˆë‹¤. L0ì—ì„œ Ratioë¥¼ ë†’ì—¬ê°€ë©° Accuracyì˜ ë³€í™”ë¥¼ ê´€ì°°í•œ ê²ƒì²˜ëŸ¼ ë‹¤ë¥¸ Layerë“¤ë„ ê´€ì°°í•´ë³´ì.\n\n\n\nLayerë³„ Sensitivity ë¹„êµ\n\n\nL1ì€ ë‹¤ë¥¸ Layerë“¤ì— ë¹„í•´ ìƒëŒ€ì ìœ¼ë¡œ Pruning Ratioë¥¼ ë†’ì—¬ê°€ë„ Accuracyì˜ ë–¨ì–´ì§€ëŠ” ì •ë„ê°€ ì•½í•œ ë°˜ë©´, L0ëŠ” ë‹¤ë¥¸ Layerë“¤ì— ë¹„í•´ ìƒëŒ€ì ìœ¼ë¡œ Pruning Ratioë¥¼ ë†’ì—¬ê°€ë©´ Accuracyì˜ ë–¨ì–´ì§€ëŠ” ì •ë„ê°€ ì‹¬í•œ ê²ƒì„ í™•ì¸í•  ìˆ˜ ìˆë‹¤. ë”°ë¼ì„œ L1ì€ Sensitivityê°€ ë†’ë‹¤ê³  ë³¼ ìˆ˜ ìˆìœ¼ë©° Pruningì„ ì ê²Œí•´ì•¼ í•˜ê³ , L0ì€ Sensitivityê°€ ë‚®ë‹¤ê³  ë³¼ ìˆ˜ ìˆìœ¼ë©° Pruningì„ ë§ê²Œí•´ì•¼ í•¨ì„ ì•Œ ìˆ˜ ìˆë‹¤.\nì—¬ê¸°ì„œ Sensitivity Analysisì—ì„œ ê³ ë ¤í•´ì•¼í•  ëª‡ê°€ì§€ ì‚¬í•­ë“¤ì— ëŒ€í•´ì„œ ì§šê³  ë„˜ì–´ê°€ì.\n\nSensitivity Analysisì—ì„œ ëª¨ë“  Layerë“¤ì´ ë…ë¦½ì ìœ¼ë¡œ ì‘ë™í•œë‹¤ëŠ” ê²ƒì„ ì „ì œë¡œ í•œë‹¤. ì¦‰, L0ì˜ Pruningì´ L1ì˜ íš¨ê³¼ì— ì˜í–¥ì„ ì£¼ì§€ ì•ŠëŠ” ë…ë¦½ì„±ì„ ê°€ì§„ë‹¤ëŠ” ê²ƒì„ ì „ì œë¡œ í•œë‹¤.\nLayerì˜ Pruning Ratioê°€ ë™ì¼í•˜ë‹¤ê³  í•´ì„œ Pruned Weightìˆ˜ê°€ ê°™ìŒì„ ì˜ë¯¸í•˜ì§€ ì•ŠëŠ”ë‹¤.\n\n100ê°œì˜ weightê°€ ìˆëŠ” layerì˜ 10% Pruning Ratio ì ìš©ì€ 10ê°œì˜ weightê°€ pruned ë˜ì—ˆìŒì„ ì˜ë¯¸í•˜ê³ , 500ê°œì˜ weightê°€ ìˆëŠ” layerì˜ 10% Pruning Ratio ì ìš©ì€ 50ê°œì˜ weightê°€ pruned ë˜ì—ˆìŒì„ ì˜ë¯¸í•œë‹¤.\nLayerì˜ ì „ì²´ í¬ê¸°ì— ë”°ë¼ Pruning Ratioì˜ ì ìš© íš¨ê³¼ëŠ” ë‹¤ë¥¼ ìˆ˜ ìˆë‹¤.\n\n\nSensitivity Analysisê¹Œì§€ ì§„í–‰í•œ í›„ì—ëŠ” ë³´í†µ ì‚¬ëŒì´ Accuracyê°€ ë–¨ì–´ì§€ëŠ” ì •ë„, thresholdë¥¼ ì •í•´ì„œ Pruning Ratioë¥¼ ì •í•œë‹¤.\n\n\n\nThreshold ì •í•˜ê¸°\n\n\nìœ„ ê·¸ë˜í”„ì—ì„œëŠ” Accuracyê°€ ì•½ 75%ìˆ˜ì¤€ìœ¼ë¡œ ìœ ì§€ë˜ëŠ” threhsold \\(T\\) ìˆ˜í‰ì„ ì„ ê¸°ì¤€ìœ¼ë¡œ L0ëŠ” ì•½ 74%, L4ëŠ” ì•½ 80%, L3ëŠ” ì•½ 82%, L2ëŠ” 90%ê¹Œì§€ Pruningì„ ì§„í–‰í•´ì•¼ ê² ë‹¤ê³  ì •í•œ ì˜ˆì‹œë¥¼ ë³´ì—¬ì¤€ë‹¤. ë¯¼ê°í•œ layerì¸ L0ëŠ” ìƒëŒ€ì ìœ¼ë¡œ Pruningì„ ì ê²Œ, ëœ ë¯¼ê°í•œ layerì¸ L2ëŠ” Pruningì„ ë§ê²Œ í•˜ëŠ” ê²ƒì„ í™•ì¸í•  ìˆ˜ ìˆë‹¤.\në¬¼ë¡  ì‚¬ëŒì´ ì •í•˜ëŠ” thresholdëŠ” ê°œì„ ì˜ ì—¬ì§€ê°€ ë¬¼ë¡  ìˆë‹¤. Pruning Ratioë¥¼ ì¢€ ë” Automaticí•˜ê²Œ ì°¾ëŠ” ë°©ë²•ì— ëŒ€í•´ ì•Œì•„ë³´ì.\n\n\n1.2 AMC\nAMCëŠ” AutoML for Model Compressionì˜ ì•½ìë¡œ, ê°•í™”í•™ìŠµ(Reinforcement Learning) ë°©ë²•ìœ¼ë¡œ ìµœì ì˜ Pruning Ratioë¥¼ ì°¾ë„ë¡ í•˜ëŠ” ë°©ë²•ì´ë‹¤.\n\n\n\nAMC ì „ì²´ êµ¬ì¡°\n\n\nAMCì˜ êµ¬ì¡°ëŠ” ìœ„ ê·¸ë¦¼ê³¼ ê°™ë‹¤. ê°•í™”í•™ìŠµ ì•Œê³ ë¦¬ì¦˜ ê³„ì—´ ì¤‘, Actor-Critic ê³„ì—´ì˜ ì•Œê³ ë¦¬ì¦˜ì¸ Deep Deterministic Policy Gradient(DDPG)ì„ í™œìš©í•˜ì—¬ Pruning Ratioë¥¼ ì •í•˜ëŠ” Actionì„ ì„ íƒí•˜ë„ë¡ í•™ìŠµí•œë‹¤. ìì„¸í•œ MDP(Markov Decision Process) ì„¤ê³„ëŠ” ì•„ë˜ì™€ ê°™ë‹¤.\n\n\n\nAMCì˜ MDP\n\n\nê°•í™”í•™ìŠµ Agentì˜ í•™ìŠµ ë°©í–¥ì„ ê²°ì •í•˜ëŠ” ì¤‘ìš”í•œ Reward Functionì€ ëª¨ë¸ì˜ Accuracyë¥¼ ê³ ë ¤í•´ì„œ Errorë¥¼ ì¤„ì´ë„ë¡ ìœ ë„í•  ë¿ë§Œ ì•„ë‹ˆë¼ Latencyë¥¼ ê°„ì ‘ì ìœ¼ë¡œ ê³ ë ¤í•  ìˆ˜ ìˆë„ë¡ ëª¨ë¸ì˜ ì—°ì‚°ëŸ‰ì„ ë‚˜íƒ€ë‚´ëŠ” FLOPë¥¼ ì ê²Œ í•˜ë„ë¡ ìœ ë„í•˜ë„ë¡ ì„¤ê³„í•œë‹¤. ì˜¤ë¥¸ìª½ì— ëª¨ë¸ë“¤ì˜ ì—°ì‚°ëŸ‰ ë³„(Operations) Top-1 Accuracy ê·¸ë˜í”„ë¥¼ ë³´ë©´ ì—°ì‚°ëŸ‰ì´ ë§ì„ìˆ˜ë¡ ë¡œê·¸í•¨ìˆ˜ì²˜ëŸ¼ Accuracyê°€ ì¦ê°€í•˜ëŠ” ê²ƒì„ ë³´ê³  ì´ë¥¼ ë³´ê³  ë°˜ì˜í•œ ë¶€ë¶„ì´ë¼ê³  ë³¼ ìˆ˜ ìˆë‹¤.\n\n\n\nAMCì˜ Reward Function\n\n\nì´ë ‡ê²Œ AMCë¡œ Pruningì„ ì§„í–‰í–ˆì„ ë•Œ, Human Expertê°€ Pruning í•œ ê²ƒê³¼ ë¹„êµí•´ë³´ì. ì•„ë˜ ëª¨ë¸ ì„¹ì…˜ë³„ Density íˆìŠ¤í† ê·¸ë¨ ê·¸ë˜í”„ì—ì„œ Totalì„ ë³´ë©´, ë™ì¼ Accuracyê°€ ë‚˜ì˜¤ë„ë¡ Pruningì„ ì§„í–‰í–ˆì„ ë•Œ AMCë¡œ Pruningì„ ì§„í–‰í•œ ê²ƒ(ì£¼í™©ìƒ‰)ì´ Human Expert Pruning ëª¨ë¸(íŒŒë€ìƒ‰)ë³´ë‹¤ Densityê°€ ë‚®ì€ ê²ƒì„ í™•ì¸í•  ìˆ˜ ìˆë‹¤. ì¦‰, AMCë¡œ Pruning ì§„í–‰í–ˆì„ ë•Œ ë” ë§ì€ weightë¥¼ Pruning ë” ê°€ë²¼ìš´ ëª¨ë¸ì„ ê°€ì§€ê³ ë„ Accuracyë¥¼ ìœ ì§€í–ˆë‹¤ê³  ë³¼ ìˆ˜ ìˆë‹¤.\n\n\n\nAMCì˜ Density Graph\n\n\në‘ë²ˆì§¸ êº¾ì€ ì„  ê·¸ë˜í”„ì—ì„œ AMCë¥¼ ê°€ì§€ê³  Pruningê³¼ Fine-tuningì„ ë²ˆê°ˆì•„ ê°€ë©° ì—¬ëŸ¬ ìŠ¤í…ìœ¼ë¡œ ì§„í–‰í•´ê°€ë©´ì„œ ê´€ì°°í•œ ê²ƒì„ ì¡°ê¸ˆ ë” ìì„¸íˆ ì‚´í´ë³´ì. ê° Iteration(Pruning+Fine-tuning)ì„ stage1, 2, 3, 4ë¡œ ë‚˜íƒ€ë‚´ì–´ plotí•œ ê²ƒì„ ë³´ë©´, 1x1 convë³´ë‹¤ 3x3 convì—ì„œ Densityê°€ ë” ë‚®ì€ ê²ƒì„ í™•ì¸í•  ìˆ˜ ìˆë‹¤. ì¦‰, 3x3 convì—ì„œ 1x1 convë³´ë‹¤ Pruningì„ ë§ì´ í•œ ê²ƒì„ ë³¼ ìˆ˜ ìˆë‹¤. ì´ë¥¼ í•´ì„í•´ë³´ìë©´, AMCê°€ 3x3 convì„ Pruningí•˜ë©´ 9ê°œì˜ weightë¥¼ pruningí•˜ê³  ì´ëŠ” 1x1 conv pruningí•´ì„œ 1ê°œì˜ weightë¥¼ ì—†ì• ëŠ” ê²ƒë³´ë‹¤ í•œë²ˆì— ë” ë§ì€ weight ìˆ˜ë¥¼ ì¤„ì¼ ìˆ˜ ìˆê¸° ë•Œë¬¸ì— 3x3 conv pruningì„ ì ê·¹ í™œìš©í–ˆì„ ê²ƒìœ¼ë¡œ ë³¼ ìˆ˜ ìˆë‹¤.\n\n\n\nAMC Result\n\n\nì´ AMC ì‹¤í—˜ ê²°ê³¼í‘œì—ì„œ ë³´ë©´, FLOPì™€ Time ê°ê° 50%ë¡œ ì¤„ì¸ AMC ëª¨ë¸ ë‘˜ë‹¤ Top-1 Accuracyê°€ ê¸°ì¡´ì˜ 1.0 MobileNetì˜ Accuracyë³´ë‹¤ ì•½ 0.1~0.4% ì •ë„ë§Œ ì¤„ê³  Latencyë‚˜ SpeedUpì´ íš¨ìœ¨ì ìœ¼ë¡œ ì¡°ì •ëœ ê²ƒì„ í™•ì¸í•  ìˆ˜ ìˆë‹¤.\n\n\n\n\n\n\nAMC ì‹¤í—˜ ê²°ê³¼í‘œì—ì„œ 0.75 MobileNetì˜ SpeedUpì´ ì™œ 1.7x ì¸ê°€ìš”?\n\n\n\n\n\nê²°ê³¼í‘œì—ì„œ 0.75 MobileNetì€ 25%ì˜ weightë¥¼ ê°ì†Œì‹œí‚¨ ê²ƒì´ê¸° ë•Œë¬¸ì— SpeedUpì´ \\(\\frac{4}{3} \\simeq 1.3\\)xì´ì–´ì•¼ í•œë‹¤ê³  ìƒê°í•  ìˆ˜ ìˆë‹¤. í•˜ì§€ë§Œ ì—°ì‚°ëŸ‰ì€ quadraticí•˜ê²Œ ê°ì†Œí•˜ê²Œ ë˜ê¸° ë•Œë¬¸ì— \\(\\frac{4}{3} \\cdot \\frac{4}{3} \\simeq 1.7\\)xë¡œ SpeedUpì´ ëœë‹¤.\n\n\n\n\n\n1.3 NetAdapt\në˜ ë‹¤ë¥¸ Pruning Ratioë¥¼ ì •í•˜ëŠ” ê¸°ë²•ìœ¼ë¡œ NetAdaptì´ ìˆë‹¤. Latency Constraintë¥¼ ê°€ì§€ê³  layerë§ˆë‹¤ pruningì„ ì ìš©í•´ë³¸ë‹¤. ì˜ˆë¥¼ ë“¤ì–´, ì¤„ì¼ ëª©í‘œ latency ëŸ‰ì„ lmsë¡œ ì •í•˜ë©´, 10ms â†’ 9msë¡œ ì¤„ ë•Œê¹Œì§€ layerì˜ pruning ratioë¥¼ ë†’ì—¬ê°€ëŠ” ë°©ë²•ì´ë‹¤.\n\n\n\nNetAdapt\n\n\nNetAdaptì˜ ì „ì²´ì ì¸ ê³¼ì •ì€ ì•„ë˜ì™€ ê°™ì´ ì§„í–‰ëœë‹¤. ê¸°ì¡´ ëª¨ë¸ì—ì„œ ê° layerë¥¼ Latency Constraintì— ë„ë‹¬í•˜ë„ë¡ Pruningí•˜ë©´ì„œ Accuracy(\\(Acc_A\\)ë“±)ì„ ë°˜ë³µì ìœ¼ë¡œ ì¸¡ì •í•œë‹¤.\n\nê° layerì˜ pruning ratioë¥¼ ì¡°ì ˆí•œë‹¤.\nShort-term fine tuningì„ ì§„í–‰í•œë‹¤.\nLatency Constraintì— ë„ë‹¬í–ˆëŠ”ì§€ í™•ì¸í•œë‹¤.\nLatency Constraint ë„ë‹¬í•˜ë©´ í•´ë‹¹ layerì˜ ìµœì ì˜ Pruning ratioë¡œ íŒë‹¨í•œë‹¤.\nê° layerì˜ ìµœì  Pruning ratioê°€ ì •í•´ì¡Œë‹¤ë©´ ë§ˆì§€ë§‰ìœ¼ë¡œ Long-term fine tuningì„ ì§„í–‰í•œë‹¤.\n\n\n\n\nNetAdapt ê³¼ì •\n\n\nì´ì™€ ê°™ì´ NetAdaptì˜ ê³¼ì •ì„ ì§„í–‰í•˜ë©´ ì•„ë˜ì™€ ê°™ì€ ì‹¤í—˜ ê²°ê³¼ë¥¼ ë³¼ ìˆ˜ ìˆë‹¤. Uniformí•˜ê²Œ Pruningì„ ì§„í–‰í•œ Multipilersë³´ë‹¤ NetAdaptê°€ 1.7x ë” ë¹ ë¥´ê³  ì˜¤íˆë ¤ AccuracyëŠ” ì•½ 0.3% ì •ë„ ë†’ì€ ê²ƒì„ ì•Œ ìˆ˜ ìˆë‹¤.\n\n\n\nNetAdaptì˜ Latency / Top-1 Accuracy ê·¸ë˜í”„"
  },
  {
    "objectID": "posts/lecs/lec04.html#fine-tuningtrain",
    "href": "posts/lecs/lec04.html#fine-tuningtrain",
    "title": "ğŸ§‘â€ğŸ« Lecture 4",
    "section": "2. Fine-tuning/Train",
    "text": "2. Fine-tuning/Train\nPrunned ëª¨ë¸ì˜ í¼í¬ë¨¼ìŠ¤ë¥¼ í–¥ìƒí•˜ê¸° ìœ„í•´ì„œëŠ” Pruningë¥¼ ì§„í–‰í•˜ê³  ë‚˜ì„œ Fine-tuning ê³¼ì •ì´ í•„ìš”í•˜ë‹¤.\n\n2.1 Iterative Pruning\në³´í†µ Pruned ëª¨ë¸ì˜ Fine-tuning ê³¼ì •ì—ì„œëŠ” ê¸°ì¡´ì— í•™ìŠµí–ˆë˜ learning rateë³´ë‹¤ ì‘ì€ rateë¥¼ ì‚¬ìš©í•œë‹¤. ì˜ˆë¥¼ë“¤ì–´ ê¸°ì¡´ì˜ ëª¨ë¸ì„ í•™ìŠµí•  ë•Œ ì‚¬ìš©í•œ learning rateì˜ \\(1/100\\) ë˜ëŠ” \\(1/10\\)ì„ ì‚¬ìš©í•œë‹¤. ë˜í•œ Pruning ê³¼ì •ê³¼ Fine-tuning ê³¼ì •ì€ 1ë²ˆë§Œ ì§„í–‰í•˜ê¸°ë³´ë‹¤ ì ì°¨ì ìœ¼ë¡œ pruning ratioë¥¼ ëŠ˜ë ¤ê°€ë©° Pruning, Fine-tuningì„ ë²ˆê°ˆì•„ê°€ë©° ì—¬ëŸ¬ë²ˆ ì§„í–‰í•˜ëŠ”ê²Œ ë” ì¢‹ë‹¤.\n\n\n\nIterative Pruning + Fine-tuning ë¹„êµ ê·¸ë˜í”„\n\n\n\n\n2.2 Regularization\nTinyMLì˜ ëª©í‘œëŠ” ê°€ëŠ¥í•œ ë§ì€ weightë“¤ì„ 0ìœ¼ë¡œ ë§Œë“œëŠ” ê²ƒìœ¼ë¡œ ìƒê°í•  ìˆ˜ ìˆë‹¤. ê·¸ë˜ì•¼ ëª¨ë¸ì„ ê°€ë³ê²Œ ë§Œë“¤ ìˆ˜ ìˆê¸° ë•Œë¬¸ì´ë‹¤. ê·¸ë˜ì„œ Regularization ê¸°ë²•ì„ ì´ìš©í•´ì„œ ëª¨ë¸ì˜ weightë“¤ì„ 0ìœ¼ë¡œ, í˜¹ì€ 0ê³¼ ê°€ê¹ê²Œ ì‘ì€ ê°’ì„ ê°€ì§€ë„ë¡ ë§Œë“ ë‹¤. ì‘ì€ ê°’ì˜ weightê°€ ë˜ë„ë¡ í•˜ëŠ” ì´ìœ ëŠ” 0ê³¼ ê°€ê¹Œìš´ ì‘ì€ ê°’ë“¤ì€ ë‹¤ìŒ layerë“¤ë¡œ ë„˜ì–´ê°€ë©´ì„œ 0ì´ ë  ê°€ëŠ¥ì„±ì´ ë†’ì•„ì§€ê¸° ë•Œë¬¸ì´ë‹¤. ê¸°ì¡´ì˜ ë”¥ëŸ¬ë‹ ëª¨ë¸ë“¤ì˜ ê³¼ì í•©(Overfitting)ì„ ë§‰ê¸° ìœ„í•œ Regularization ê¸°ë²•ë“¤ê³¼ ë‹¤ë¥´ì§€ ì•Šìœ¼ë‚˜ ì˜ë„ì™€ ëª©ì ì€ ë‹¤ë¥¸ ê²ƒì„ ì§šì–´ë³¼ ìˆ˜ ìˆë‹¤.\n\n\n\nPruningì„ ìœ„í•œ Regularization\n\n\n\n\n2.3 The Lottery Ticket Hypothesis\n2019ë…„ ICLRì—ì„œ ë°œí‘œëœ ë…¼ë¬¸ì—ì„œ Jonathan Frankleê³¼ Michael Carbinì´ ì†Œê°œí•œ The Lottery Ticket Hypothesis(LTH)ì€ ì‹¬ì¸µ ì‹ ê²½ë§(DNN) í›ˆë ¨ì— ëŒ€í•œ í¥ë¯¸ë¡œìš´ ì•„ì´ë””ì–´ë¥¼ ì œì•ˆí•œë‹¤. ë¬´ì‘ìœ„ë¡œ ì´ˆê¸°í™”ëœ ëŒ€ê·œëª¨ ì‹ ê²½ë§ ë‚´ì— ë” ì‘ì€ í•˜ìœ„ ë„¤íŠ¸ì›Œí¬(Winning Ticket)ê°€ ì¡´ì¬í•œë‹¤ëŠ” ê²ƒì„ ë§í•œë‹¤. ì´ í•˜ìœ„ ë„¤íŠ¸ì›Œí¬ëŠ” ì²˜ìŒë¶€í„° ë³„ë„ë¡œ í›ˆë ¨í•  ë•Œ ì›ë˜ ë„¤íŠ¸ì›Œí¬ì˜ ì„±ëŠ¥ì— ë„ë‹¬í•˜ê±°ë‚˜ ëŠ¥ê°€í•  ìˆ˜ ìˆë‹¤. ì´ ê°€ì„¤ì€ ì´ëŸ¬í•œ Winning Ticketì´ í•™ìŠµí•˜ëŠ” ë° ì í•©í•œ ì´ˆê¸° ê°€ì¤‘ì¹˜ë¥¼ ê°–ëŠ”ë‹¤ê³  ê°€ì •í•œë‹¤.\n\n\n\nLTH ì„¤ëª… ê·¸ë¦¼"
  },
  {
    "objectID": "posts/lecs/lec04.html#system-support-for-sparsity",
    "href": "posts/lecs/lec04.html#system-support-for-sparsity",
    "title": "ğŸ§‘â€ğŸ« Lecture 4",
    "section": "3. System Support for Sparsity",
    "text": "3. System Support for Sparsity\nDNNì„ ê°€ì†í™” ì‹œí‚¤ëŠ” ë°©ë²•ì€ í¬ê²Œ 3ê°€ì§€, Sparse Weight, Sparse Activation, Weight Sharingì´ ìˆë‹¤. Sparse Weight, Sparse Activationì€ Pruningì´ê³  Weight Sharingì€ Quantizationì˜ ë°©ë²•ì´ë‹¤.\n\n\n\nDNNì„ ê°€ì†í™” ì‹œí‚¤ëŠ” ë°©ë²•\n\n\n\nSparse Weight: Weightë¥¼ Pruningí•˜ì—¬ Computationì€ Pruning Ratioì— ëŒ€ì‘í•˜ì—¬ ë¹¨ë¼ì§„ë‹¤. í•˜ì§€ë§Œ MemoryëŠ” Pruningëœ weightì˜ ìœ„ì¹˜ë¥¼ ê¸°ì–µí•˜ê¸° ìœ„í•œ memory ìš©ëŸ‰ì´ í•„ìš”í•˜ë¯€ë¡œ Pruning Ratioì— ë¹„ë¡€í•˜ì—¬ ì¤„ì§„ ì•ŠëŠ”ë‹¤.\nSparse Activation: Weightë¥¼ Pruningí•˜ëŠ” ê²ƒê³¼ ë‹¤ë¥´ê²Œ Activationì€ Test Inputì— ë”°ë¼ dynamic í•˜ë¯€ë¡œ Weightë¥¼ Pruningí•˜ëŠ” ê²ƒë³´ë‹¤ Computationì´ ëœ ì¤„ì–´ë“ ë‹¤.\nWeight Sharing: Quantization ë°©ë²•ìœ¼ë¡œ 32-bit dataë¥¼ 4-bit dataë¡œ ë³€ê²½í•¨ìœ¼ë¡œì¨ 8ë°°ì˜ memory ì ˆì•½ì„ í•  ìˆ˜ ìˆë‹¤.\n\n\n3.1 EIE\nEfficient Inference Engineì€ ê¸°ê³„ í•™ìŠµ ëª¨ë¸ì„ ì‹¤ì‹œê°„ìœ¼ë¡œ ì‹¤í–‰í•˜ê¸° ìœ„í•´ ìµœì í™”ëœ ì†Œí”„íŠ¸ì›¨ì–´ ë¼ì´ë¸ŒëŸ¬ë¦¬ë‚˜ í”„ë ˆì„ì›Œí¬ë¥¼ ë§í•œë‹¤. Processing Elements(PE)ì˜ êµ¬ì¡°\n\n\n\nPE ì—°ì‚° Logically / Physically ë¶„ì„\n\n\nì•„ë˜ ê·¸ë¦¼ì—ì„œ Inputë³„(\\(\\vec{a}\\)) ì—°ì‚°ì€ ì•„ë˜ì™€ ê°™ì´ Inputì´ 0ì¼ ë•ŒëŠ” skipë˜ê³  0ì´ ì•„ë‹ ë•ŒëŠ” prunning ë˜ì§€ ì•Šì€ weightì™€ ì—°ì‚°ì´ ì§„í–‰ëœë‹¤.\n\n\n\nInputë³„ ì—°ì‚° ê³¼ì •\n\n\nEIE ì‹¤í—˜ì€ ê°€ì¥ lossê°€ ì ì€ data ìë£Œí˜•ì¸ 16 bit Intí˜•ì„ ì‚¬ìš©í–ˆë‹¤.(0.5% loss) AlexNetì´ë‚˜ VGGì™€ ê°™ì´ ReLU Activationì´ ë§ì´ ì‚¬ìš©ë˜ëŠ” ëª¨ë¸ë“¤ì€ ê²½ëŸ‰í™”ê°€ ë§ì´ ëœ ë°˜ë©´, RNNì™€ LSTMì´ ì‚¬ìš©ëœ NeuralTalk ëª¨ë¸ë“¤ ê°™ì€ ê²½ìš°ì—ëŠ” ReLUë¥¼ ì‚¬ìš©í•˜ì§€ ì•Šì•„ ê²½ëŸ‰í™”ë  ìˆ˜ ìˆëŠ” ë¶€ë¶„ì´ ì—†ì–´ Activation Densityê°€ 100%ì¸ ê²ƒì„ í™•ì¸í•  ìˆ˜ ìˆë‹¤.\n\n\n\nEIE ì‹¤í—˜ ê²°ê³¼\n\n\n\n\n3.2 M:N Weight Sparsity\nì´ ë°©ë²•ì€ Nvidia í•˜ë“œì›¨ì–´ì˜ ì§€ì›ì´ í•„ìš”í•œ ë°©ë²•ìœ¼ë¡œ ë³´í†µ 2:4 Weight Sparsityë¥¼ ì‚¬ìš©í•œë‹¤. ì™¼ìª½ì˜ Sparse Matrixë¥¼ ì¬ë°°ì¹˜í•´ì„œ Non-zero data matrixì™€ ì¸ë±ìŠ¤ë¥¼ ì €ì¥í•˜ëŠ” Index matrixë¥¼ ë”°ë¡œ ë§Œë“¤ì–´ì„œ ì €ì¥í•œë‹¤.\n\n\n\n2:4 Weight Sparsity\n\n\nM:N Weight Sparsity ì ìš©í•˜ì§€ ì•Šì€ Dense GEMMê³¼ ì ìš©í•œ Sparse GEMMì„ ê³„ì‚°í•  ë•ŒëŠ” ì•„ë˜ì˜ ê·¸ë¦¼ê³¼ ê°™ì€ ê³¼ì •ìœ¼ë¡œ ì—°ì‚°ì´ ì§„í–‰ëœë‹¤.\n\n\n\nDense VS. Sparse GEMM\n\n\n\n\n3.3 Sparse Convolution\nSubmanifold Sparse Convolutional Networks (SSCN)ì€ ê³ ì°¨ì› ë°ì´í„°ì—ì„œ íš¨ìœ¨ì ì¸ ê³„ì‚°ì„ ê°€ëŠ¥í•˜ê²Œ í•˜ëŠ” ì‹ ê²½ë§ ì•„í‚¤í…ì²˜ì˜ í•œ í˜•íƒœì´ë‹¤. ì´ ê¸°ìˆ ì€ íŠ¹íˆ 3D í¬ì¸íŠ¸ í´ë¼ìš°ë“œ ë˜ëŠ” ê³ í•´ìƒë„ ì´ë¯¸ì§€ì™€ ê°™ì´ ëŒ€ê·œëª¨ ë° ê³ ì°¨ì› ë°ì´í„°ë¥¼ ì²˜ë¦¬í•  ë•Œ ì¤‘ìš”í•˜ë‹¤. SSCNì˜ í•µì‹¬ ì•„ì´ë””ì–´ëŠ” ë°ì´í„°ì˜ Sparcityì„ í™œìš©í•˜ì—¬ ê³„ì‚°ê³¼ ë©”ëª¨ë¦¬ ì‚¬ìš©ëŸ‰ì„ í¬ê²Œ ì¤„ì´ëŠ” ê²ƒì´ë‹¤.\n\n\n\nì¶œì²˜: Submanifold Sparse Convolutional Networks\n\n\nì´ëŸ¬í•œ Sparse Convolutionì€ ê¸°ë³¸ Convolutionê³¼ ë¹„êµí–ˆì„ ë•Œ ì•„ë˜ ê·¸ë¦¼ê³¼ ê°™ì´ ë‚˜íƒ€ë‚´ë³¼ ìˆ˜ ìˆë‹¤.\n\n\n\nConventional VS. Sparse Convolution\n\n\nì—°ì‚° ê³¼ì •ì„ ë¹„êµí•´ë³´ê¸° ìœ„í•´ Input Point Cloud(\\(P\\)), Feature Map(\\(W\\)), Ouput Point Cloud(\\(Q\\))ë¥¼ ì•„ë˜ì™€ ê°™ì´ ìˆë‹¤ê³  í•˜ì. ê¸°ì¡´ì˜ Convolutionê³¼ Sparse Convolutionì„ ë¹„êµí•´ë³´ë©´ ì—°ì‚°ëŸ‰ì´ 9:2ë¡œ ë§¤ìš° ì ì€ ì—°ì‚°ë§Œ í•„ìš”í•œ ê²ƒì„ ì•Œ ìˆ˜ ìˆë‹¤.\n\n\n\nConventional VS. Sparse ì—°ì‚°ëŸ‰ ë¹„êµ\n\n\nFeature Map(\\(W\\))ì„ ê¸°ì¤€ìœ¼ë¡œ ê° weight ë§ˆë‹¤ í•„ìš”í•œ Input dataì˜ í¬ê¸°ê°€ ë‹¤ë¥´ë‹¤. ì˜ˆë¥¼ ë“¤ì–´ \\(W_{-1, 0}\\)ì€ \\(P1\\)ê³¼ ë§Œì˜ ì—°ì‚°ì´ ì§„í–‰ë˜ë¯€ë¡œ \\(P1\\)ë§Œ ì—°ì‚°ì‹œ ë¶ˆëŸ¬ë‚´ê²Œ ëœë‹¤.\n\n\n\nSparse Convolution ê³„ì‚° ê³¼ì •\n\n\në”°ë¼ì„œ Feature Mapì˜ \\(W\\)ì— ë”°ë¼ í•„ìš”í•œ Input dataë¥¼ í‘œí˜„í•˜ê³  ë”°ë¡œ computationì„ ì§„í–‰í•˜ë©´ ì•„ë˜ì™€ ê°™ì´ ê³ ë¥´ì§€ ëª»í•œ ì—°ì‚°ëŸ‰ ë¶„ë°°ê°€ ì§„í–‰ë˜ëŠ”ë°(ì™¼ìª½ ê·¸ë¦¼) ì´ëŠ” computationì— overheadëŠ” ì—†ì§€ë§Œ regularityê°€ ì¢‹ì§€ ì•Šë‹¤. ë˜ëŠ” ê°€ì¥ computationì´ ë§ì€ ê²ƒì„ ê¸°ì¤€ìœ¼ë¡œ Batch ë‹¨ìœ„ë¡œ ê³„ì‚°í•˜ê²Œ ëœë‹¤ë©´(ê°€ìš´ë° ê·¸ë¦¼) ì ì€ computation weightì—ì„œì˜ ë¹„íš¨ìœ¨ì ì¸ ê³„ì‚° ëŒ€ê¸°ì‹œê°„ì´ ìƒê¸°ë¯€ë¡œ overheadê°€ ìƒê¸´ë‹¤. ë”°ë¼ì„œ ì ì ˆíˆ ë¹„ìŠ·í•œ ì—°ì‚°ëŸ‰ì„ ê°€ì§€ëŠ” groupingì„ ì§„í–‰í•œ ë’¤ batchë¡œ ë¬¶ìœ¼ë©´ ì ì ˆíˆ computationì„ ì§„í–‰í•  ìˆ˜ ìˆë‹¤.(ì˜¤ë¥¸ìª½ ê·¸ë¦¼)\n\n\n\nGrouping Computation\n\n\nì´ëŸ° Groupingì„ ì ìš©í•œ í›„ Sparse Convolutionì„ ì§„í–‰í•˜ë©´ Adaptive Groupingì´ ì ìš©ë˜ì–´ ì•„ë˜ì™€ ê°™ì´ ì§„í–‰ëœë‹¤.\n\n\n\nSparse Convolution ì˜ˆì‹œ\n\n\nì—¬ê¸°ê¹Œì§€ê°€ 2023ë…„ë„ ê°•ì˜ì—ì„œ ë§ˆì§€ë§‰ Sparse Convolutionì— ëŒ€í•´ ì„¤ëª…í•œ ë¶€ë¶„ì„ ì •ë¦¬í•œ ë¶€ë¶„ì´ë‹¤. í•˜ì§€ë§Œ ê°•ì˜ì—ì„œ ì„¤ëª…ì´ ë§ì´ ìƒëµë˜ì–´ ìˆìœ¼ë¯€ë¡œ ì¢€ ë” ìì„¸í•œ ë‚´ìš©ì€ Youtube ë°œí‘œ ì˜ìƒì´ë‚˜ 2022ë…„ë„ ê°•ì˜ë¥¼ ì°¸ê³ í•˜ëŠ” ê²ƒì„ ê¶Œì¥í•œë‹¤."
  },
  {
    "objectID": "posts/lecs/lec04.html#reference",
    "href": "posts/lecs/lec04.html#reference",
    "title": "ğŸ§‘â€ğŸ« Lecture 4",
    "section": "4. Reference",
    "text": "4. Reference\n\nMIT-TinyML-lecture04-Pruning-2\nAMC: Automl for Model Compression and Acceleration on Mobile Devices, 2018\nContinuous control with deep reinforcement learning\nFLOPsë€? ë”¥ëŸ¬ë‹ ì—°ì‚°ëŸ‰ì— ëŒ€í•´ì„œ\nNetAdapt: Platform-Aware Neural Network Adaptation for Mobile Applications\nThe Lottery Ticket Hypothesis: Finding Sparse, Trainable Neural Networks\nThe Lottery Ticket Hypothesis Finding Sparse, Trainable Neural Networks ë…¼ë¬¸ ë¦¬ë·°\nLLM Inference - HW/SW Optimizations\nAccelerating Sparse Deep Neural Networks\nSubmanifold Sparse Convolutional Networks\nTorchSparse: Efficient Point Cloud Inference Engine\nTorchSparse++: Efficient Training and Inference Framework for Sparse Convolution on GPUs\nmit-han-lab/torchsparse\nMLSysâ€™22 TorchSparse: Efficient Point Cloud Inference Engine"
  }
]