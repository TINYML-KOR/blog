[
  {
    "objectID": "about.html",
    "href": "about.html",
    "title": "TinyML Study Group",
    "section": "",
    "text": "@Gueltto9th"
  },
  {
    "objectID": "about.html#lec-4lab",
    "href": "about.html#lec-4lab",
    "title": "TinyML Study Group",
    "section": "21Lec + 4Lab",
    "text": "21Lec + 4Lab\n\nLec1/2ì™€ Lab0ì€ ì œì™¸\nê°•ì˜ë¥¼ ë“£ê³  1ëª…ì”© ëŒì•„ê°€ë©´ì„œ ê°•ì˜ ë³µìŠµ recap ë°œí‘œ\në‹¤ë¥¸ ì‚¬ëŒë“¤ì€ ì§ˆë¬¸/ë””ìŠ¤ì»¤ì…˜ í† í”½ ê°€ì ¸ì˜¤ê¸°\nì£¼ 1íšŒ (ì•½ 16ì£¼ - 4ê°œì›” ì´ë‚´ ì™„ë£Œ ëª©í‘œ)\n\n\n\n[Chapter I: Efficient Inference]\n\nLecture 3: Pruning and Sparsity (Part I) [P1]\nLecture 4: Pruning and Sparsity (Part II) [P2]\nâ€” Lab 1 â†’ 2ì£¼ ë¶„ëŸ‰ [P3]\nLecture 5: Quantization (Part I) [P1]\nLecture 6: Quantization (Part II) [P2]\nâ€” Lab 2 [P3]\nLecture 7: Neural Architecture Search (Part I) [P1]\nLecture 8: Neural Architecture Search (Part II) [P2]\nâ€” Lab 3 [P3]\nLecture 9: Knowledge Distillation [P1]\nLecture 10: MCUNet: TinyML on Microcontrollers\nLecture 11: TinyEngine and Parallel Processing [P2]\n\n\n\n[Chapter II: Domain-Specific Optimization]\n\nLecture 12: Transformer and LLM (Part I)\nLecture 13: Transformer and LLM (Part II)\nLecture 14: Vision Transformer\nâ€” Lab 4\nLecture 15: GAN, Video, and Point Cloud\nLecture 16: Diffusion Model\nLecture 17: Distributed Training (Part I)\nLecture 18: Distributed Training (Part II)\nâ€” Lab 5\nLecture 19: On-Device Training and Transfer Learning\nLecture 20: Efficient Fine-tuning and Prompt Engineering\n\n\n\n[Chapter IV: Advanced Topics]\n\nLecture 21: Basics of Quantum Computing\nLecture 22: Quantum Machine Learning\nLecture 23: Noise Robust Quantum ML"
  },
  {
    "objectID": "posts/labs/lab00.html",
    "href": "posts/labs/lab00.html",
    "title": "ğŸ‘©â€ğŸ’» Lab 0",
    "section": "",
    "text": "Lab 0 PyTorch Tutorial\nIn this tutorial, we will explore how to train a neural network with PyTorch.\n\nSetup\nWe will first install a few packages that will be used in this tutorial:\n\n!pip install torchprofile 1>/dev/null\n\nWe will then import a few libraries:\n\nimport random\nfrom collections import OrderedDict, defaultdict\n\nimport numpy as np\nimport torch\nfrom matplotlib import pyplot as plt\nfrom torch import nn\nfrom torch.optim import *\nfrom torch.optim.lr_scheduler import *\nfrom torch.utils.data import DataLoader\nfrom torchprofile import profile_macs\nfrom torchvision.datasets import *\nfrom torchvision.transforms import *\nfrom tqdm.auto import tqdm\n\nTo ensure the reproducibility, we will control the seed of random generators:\n\nrandom.seed(0)\nnp.random.seed(0)\ntorch.manual_seed(0)\n\n\n\nData\nIn this tutorial, we will use CIFAR-10 as our target dataset. This dataset contains images from 10 classes, where each image is of size 3x32x32, i.e.Â 3-channel color images of 32x32 pixels in size.\n\ntransforms = {\n  \"train\": Compose([\n    RandomCrop(32, padding=4),\n    RandomHorizontalFlip(),\n    ToTensor(),\n  ]),\n  \"test\": ToTensor(),\n}\n\ndataset = {}\nfor split in [\"train\", \"test\"]:\n  dataset[split] = CIFAR10(\n    root=\"data/cifar10\",\n    train=(split == \"train\"),\n    download=True,\n    transform=transforms[split],\n  )\n\nWe can visualize a few images in the dataset and their corresponding class labels:\n\nsamples = [[] for _ in range(10)]\nfor image, label in dataset[\"test\"]:\n  if len(samples[label]) < 4:\n    samples[label].append(image)\n\nplt.figure(figsize=(20, 9))\nfor index in range(40):\n  label = index % 10\n  image = samples[label][index // 10]\n\n  # Convert from CHW to HWC for visualization\n  image = image.permute(1, 2, 0)\n\n  # Convert from class index to class name\n  label = dataset[\"test\"].classes[label]\n\n  # Visualize the image\n  plt.subplot(4, 10, index + 1)\n  plt.imshow(image)\n  plt.title(label)\n  plt.axis(\"off\")\nplt.show()\n\nTo train a neural network, we will need to feed data in batches. We create data loaders with batch size of 512:\n\ndataflow = {}\nfor split in ['train', 'test']:\n  dataflow[split] = DataLoader(\n    dataset[split],\n    batch_size=512,\n    shuffle=(split == 'train'),\n    num_workers=0,\n    pin_memory=True,\n  )\n\nWe can print the data type and shape from the training data loader:\n\nfor inputs, targets in dataflow[\"train\"]:\n  print(\"[inputs] dtype: {}, shape: {}\".format(inputs.dtype, inputs.shape))\n  print(\"[targets] dtype: {}, shape: {}\".format(targets.dtype, targets.shape))\n  break\n\n\n\nModel\nIn this tutorial, we will use a variant of VGG-11 (with fewer downsamples and a smaller classifier) as our model.\n\nclass VGG(nn.Module):\n  ARCH = [64, 128, 'M', 256, 256, 'M', 512, 512, 'M', 512, 512, 'M']\n\n  def __init__(self) -> None:\n    super().__init__()\n\n    layers = []\n    counts = defaultdict(int)\n\n    def add(name: str, layer: nn.Module) -> None:\n      layers.append((f\"{name}{counts[name]}\", layer))\n      counts[name] += 1\n\n    in_channels = 3\n    for x in self.ARCH:\n      if x != 'M':\n        # conv-bn-relu\n        add(\"conv\", nn.Conv2d(in_channels, x, 3, padding=1, bias=False))\n        add(\"bn\", nn.BatchNorm2d(x))\n        add(\"relu\", nn.ReLU(True))\n        in_channels = x\n      else:\n        # maxpool\n        add(\"pool\", nn.MaxPool2d(2))\n\n    self.backbone = nn.Sequential(OrderedDict(layers))\n    self.classifier = nn.Linear(512, 10)\n\n  def forward(self, x: torch.Tensor) -> torch.Tensor:\n    # backbone: [N, 3, 32, 32] => [N, 512, 2, 2]\n    x = self.backbone(x)\n\n    # avgpool: [N, 512, 2, 2] => [N, 512]\n    x = x.mean([2, 3])\n\n    # classifier: [N, 512] => [N, 10]\n    x = self.classifier(x)\n    return x\n\nmodel = VGG().cuda()\n\nIts backbone is composed of eight conv-bn-relu blocks interleaved with four maxpoolâ€™s to downsample the feature map by 2^4 = 16 times:\n\nprint(model.backbone)\n\nAfter the feature map is pooled, its classifier predicts the final output with a linear layer:\n\nprint(model.classifier)\n\nAs this course focuses on efficiency, we will then inspect its model size and (theoretical) computation cost.\n\nThe model size can be estimated by the number of trainable parameters:\n\n\nnum_params = 0\nfor param in model.parameters():\n  if param.requires_grad:\n    num_params += param.numel()\nprint(\"#Params:\", num_params)\n\n\nThe computation cost can be estimated by the number of multiplyâ€“accumulate operations (MACs) using TorchProfile:\n\n\nnum_macs = profile_macs(model, torch.zeros(1, 3, 32, 32).cuda())\nprint(\"#MACs:\", num_macs)\n\nThis model has 9.2M parameters and requires 606M MACs for inference. We will work together in the next few labs to improve its efficiency.\n\n\nOptimization\nAs we are working on a classification problem, we will apply cross entropy as our loss function to optimize the model:\n\ncriterion = nn.CrossEntropyLoss()\n\nOptimization will be carried out using stochastic gradient descent (SGD) with momentum:\n\noptimizer = SGD(\n  model.parameters(),\n  lr=0.4,\n  momentum=0.9,\n  weight_decay=5e-4,\n)\n\nThe learning rate will be modulated using the following scheduler (which is adapted from this blog series):\n\nnum_epochs = 20\nsteps_per_epoch = len(dataflow[\"train\"])\n\n# Define the piecewise linear scheduler\nlr_lambda = lambda step: np.interp(\n  [step / steps_per_epoch],\n  [0, num_epochs * 0.3, num_epochs],\n  [0, 1, 0]\n)[0]\n\n# Visualize the learning rate schedule\nsteps = np.arange(steps_per_epoch * num_epochs)\nplt.plot(steps, [lr_lambda(step) * 0.4 for step in steps])\nplt.xlabel(\"Number of Steps\")\nplt.ylabel(\"Learning Rate\")\nplt.grid(\"on\")\nplt.show()\n\nscheduler = LambdaLR(optimizer, lr_lambda)\n\n\n\nTraining\nWe first define the training function that optimizes the model for one epoch (i.e., a pass over the training set):\n\ndef train(\n  model: nn.Module,\n  dataflow: DataLoader,\n  criterion: nn.Module,\n  optimizer: Optimizer,\n  scheduler: LambdaLR,\n) -> None:\n  model.train()\n\n  for inputs, targets in tqdm(dataflow, desc='train', leave=False):\n    # Move the data from CPU to GPU\n    inputs = inputs.cuda()\n    targets = targets.cuda()\n\n    # Reset the gradients (from the last iteration)\n    optimizer.zero_grad()\n\n    # Forward inference\n    outputs = model(inputs)\n    loss = criterion(outputs, targets)\n\n    # Backward propagation\n    loss.backward()\n\n    # Update optimizer and LR scheduler\n    optimizer.step()\n    scheduler.step()\n\nWe then define the evaluation function that calculates the metric (i.e., accuracy in our case) on the test set:\n\n@torch.inference_mode()\ndef evaluate(\n  model: nn.Module,\n  dataflow: DataLoader\n) -> float:\n  model.eval()\n\n  num_samples = 0\n  num_correct = 0\n\n  for inputs, targets in tqdm(dataflow, desc=\"eval\", leave=False):\n    # Move the data from CPU to GPU\n    inputs = inputs.cuda()\n    targets = targets.cuda()\n\n    # Inference\n    outputs = model(inputs)\n\n    # Convert logits to class indices\n    outputs = outputs.argmax(dim=1)\n\n    # Update metrics\n    num_samples += targets.size(0)\n    num_correct += (outputs == targets).sum()\n\n  return (num_correct / num_samples * 100).item()\n\nWith training and evaluation functions, we can finally start training the model! This will take around 10 minutes.\n\nfor epoch_num in tqdm(range(1, num_epochs + 1)):\n  train(model, dataflow[\"train\"], criterion, optimizer, scheduler)\n  metric = evaluate(model, dataflow[\"test\"])\n  print(f\"epoch {epoch_num}:\", metric)\n\nIf everything goes well, your trained model should be able to achieve >92.5% of accuracy!\n\n\nVisualization\nWe can visualize the modelâ€™s prediction to see how the model truly performs:\n\nplt.figure(figsize=(20, 10))\nfor index in range(40):\n  image, label = dataset[\"test\"][index]\n\n  # Model inference\n  model.eval()\n  with torch.inference_mode():\n    pred = model(image.unsqueeze(dim=0).cuda())\n    pred = pred.argmax(dim=1)\n\n  # Convert from CHW to HWC for visualization\n  image = image.permute(1, 2, 0)\n\n  # Convert from class indices to class names\n  pred = dataset[\"test\"].classes[pred]\n  label = dataset[\"test\"].classes[label]\n\n  # Visualize the image\n  plt.subplot(4, 10, index + 1)\n  plt.imshow(image)\n  plt.title(f\"pred: {pred}\" + \"\\n\" + f\"label: {label}\")\n  plt.axis(\"off\")\nplt.show()"
  },
  {
    "objectID": "posts/lecs/lec03.html",
    "href": "posts/lecs/lec03.html",
    "title": "ğŸ§‘â€ğŸ« Lecture 3",
    "section": "",
    "text": "ì´ë¯¸ì§€ëŠ” Typora ì´ë¯¸ì§€ ê²½ë¡œ ì„¤ì •í•˜ë©´ ë°”ë¡œ ë„£ì„ ìˆ˜ ìˆìŠµë‹ˆë‹¤!\n\n\n\nresult\n\n\n\nì´ë¯¸ì§€ í¬ê¸° ì¡°ì ˆì€\n<center>\n<img src=\"../../images/ì´ë¯¸ì§€_ì´ë¦„.png\" width=\"50%\" />\n<figcaption>ì´ë¯¸ì§€ ìº¡ì…˜</figcaption>\n</center>"
  },
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "Study Archive",
    "section": "",
    "text": "ğŸ‘©â€ğŸ’» Lab 0\n\n\n\n\n\n\n\nlab\n\n\npytorch\n\n\nbasic\n\n\n\n\nPyTorch Tutorial\n\n\n\n\n\n\nJan 11, 2024\n\n\nJung Yeon Lee\n\n\n\n\n\n\n  \n\n\n\n\nğŸ§‘â€ğŸ« Lecture 3\n\n\n\n\n\n\n\npruning\n\n\nlec\n\n\n\n\nIntroduction - Machine Learning for Graphs\n\n\n\n\n\n\nJan 6, 2024\n\n\nwho\n\n\n\n\n\n\nNo matching items"
  }
]