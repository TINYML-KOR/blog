---
toc: true
title: "ğŸ§‘â€ğŸ« Lecture 5-6"
description:  Quantization
author: "Seunghyun Oh(ooshyun)"
date: "2024-03-05"
categories: [TinyML, EdgeAI]
---
<style>
p {
  line-height: 2; /* Adjust the line-height property as needed */
  text-align: justify;
  text-justify: inter-word; /* Adjust the justification mode as needed */
}

.image-container {
  text-align: center; /* Center-align the contents horizontally */
}

.centered-text {
  text-align: center; /* Center-align the text within the container */
}
</style>

ì´ë²ˆ ê¸€ì—ì„œëŠ” MIT HAN LABì—ì„œ ê°•ì˜í•˜ëŠ” [TinyML and Efficient Deep Learning Computing](https://www.youtube.com/playlist?list=PL80kAHvQbh-ocildRaxjjBy6MR1ZsNCU7)ì— ë‚˜ì˜¤ëŠ” Quantization ë°©ë²•ì„ ì†Œê°œí•˜ë ¤ í•œë‹¤. Quantization(ì–‘ìí™”) ì‹ í˜¸ì™€ ì´ë¯¸ì§€ì—ì„œ ì•„ë‚ ë¡œê·¸ë¥¼ ë””ì§€í„¸ë¡œ ë³€í™˜í•˜ëŠ” ê³¼ì •ì—ì„œ ì‚¬ìš©í•˜ëŠ” ê°œë…ì´ë‹¤. ì•„ë˜ ê·¸ë¦¼ê³¼ ê°™ì´ ì—°ì†ì ì¸ ì„¼ì„œë¡œ ë¶€í„° ë“¤ì–´ì˜¤ëŠ” ì•„ë‚ ë¡œê·¸ ë°ì´í„° ë‚˜ ì´ë¯¸ì§€ë¥¼ í‘œí˜„í•˜ê¸° ìœ„í•´ ë‹¨ìœ„ ì‹œê°„ì— ëŒ€í•´ì„œ ë°ì´í„°ë¥¼ ìƒ˜í”Œë§í•˜ì—¬ ë°ì´í„°ë¥¼ ìˆ˜ì§‘í•œë‹¤.

![Reference. MIT-TinyML-lecture5-Quantization-1](../../images/lec05/1/intro.png)

ë””ì§€í„¸ë¡œ ë°ì´í„°ë¥¼ ë³€í™˜í•˜ê¸° ìœ„í•´ ë°ì´í„° íƒ€ì…ì„ ì •í•˜ë©´ì„œ ì´ë¥¼ í•˜ë‚˜ì”© ì–‘ìí™”í•œë‹¤. ì–‘ìˆ˜ì™€ ìŒìˆ˜ë¥¼ í‘œí˜„í•˜ê¸° ìœ„í•´ Unsigned Integer ì—ì„œ Signed Integer, Signedì—ì„œë„ Sign-Magnitude ë°©ì‹ê³¼ Two's Complementë°©ì‹ìœ¼ë¡œ, ê·¸ë¦¬ê³  ë” ë§ì€ ì†Œìˆ«ì  ìë¦¬ë¥¼ í‘œí˜„í•˜ê¸° ìœ„í•´ Fixed-pointì—ì„œ Floating pointë¡œ ë°ì´í„° íƒ€ì…ì—ì„œ ìˆ˜ì˜ ë²”ì£¼ë¥¼ í™•ì¥ì‹œí‚¨ë‹¤. ì°¸ê³ ë¡œ Deviceì˜ Computationalityì™€ ML ëª¨ë¸ì˜ ì„±ëŠ¥ì§€í‘œì¤‘ í•˜ë‚˜ì¸ FLOPì´ ë°”ë¡œ floating point operations per secondì´ë‹¤. 

![](../../images/lec05/1/comp-bitwidth-fix-float.png)

![Reference. MIT-TinyML-lecture5-Quantization-1](../../images/lec05/1/comp-memory-fix-float.png)


[ì´ ê¸€](https://ooshyun.github.io/2023/02/24/Fixed-point-vs-Floating-point.html)ì—ì„œ floating pointë¥¼ ì´í•´í•˜ë©´, fixed pointë¥¼ ì‚¬ìš©í•˜ëŠ” ê²ƒì´ ë§¤ëª¨ë¦¬ì—ì„œ, ê·¸ë¦¬ê³  ì—°ì‚°ì—ì„œ ë” íš¨ìœ¨ì ì¼ ê²ƒì´ë¼ê³  ì˜ˆìƒí•´ë³¼ ìˆ ìˆ˜ ìˆë‹¤. MLëª¨ë¸ì„ í´ë¼ìš°ë“œ ì„œë²„ì—ì„œ ëŒë¦´ ë•ŒëŠ” í¬ê²Œ ë¬¸ì œë˜ì§€ ì•Šì•˜ì§€ë§Œ ì•„ë˜ ë‘ ê°€ì§€ í‘œë¥¼ ë³´ë©´ ì—ë„ˆì§€ì†Œëª¨, ì¦‰ ë°°í„°ë¦¬ íš¨ìœ¨ì—ì„œ í¬ê²Œ ì°¨ì´ê°€ ë³´ì¸ë‹¤. ê·¸ë ‡ê¸° ë•Œë¬¸ì— ëª¨ë¸ì—ì„œ Floating pointë¥¼ fixed pointë¡œ ë” ë§ì´ ë°”ê¾¸ë ¤ê³  í•˜ëŠ”ë° ì´ ë°©ë²•ìœ¼ë¡œ ë‚˜ì˜¨ ê²ƒì´ ë°”ë¡œ Quatizationì´ë‹¤. 

ì´ë²ˆ ê¸€ì—ì„œëŠ” Quntization ì¤‘ì—ì„œ Quantization ë°©ë²•ê³¼ ê·¸ ì¤‘ Linearí•œ ë°©ë²•ì— ëŒ€í•´ ë” ìì„¸í•˜ê²Œ, ê·¸ë¦¬ê³  Post-training Quantizationê¹Œì§€ ë‹¤ë£¨ê³ , ë‹¤ìŒ ê¸€ì—ì„œëŠ” Quantization-Aware Training, Binary/Tenary Quantization, Mixed Precision Quantizationê¹Œì§€ ë‹¤ë£¨ë ¤ê³  í•œë‹¤.

## 1. Common Network Quantization
ì•ì„œì„œ ì†Œê°œí•œ ê²ƒì²˜ëŸ¼ Neural Netoworkë¥¼ ìœ„í•œ Quantizationì€ ë‹¤ìŒê³¼ ê°™ì´ ë‚˜ëˆŒ ìˆ˜ ìˆë‹¤. Quantization ë°©ë²•ì„ í•˜ë‚˜ì”© ì•Œì•„ë³´ì.

![Reference. MIT-TinyML-lecture5-Quantization-1](../../images/lec05/1/intro.png)
<p>
    <img src="../../images/lec05/1/quantization-method.png" width="500" height="300" class="projects__article__img__center">
    <p align="center">
    <em class="projects__img__caption"> Reference. MIT-TinyML-lecture5-Quantization-1 in https://efficientml.ai </em>
    </p> 
</p>

### 1.1 K-Means-based Quantization
ê·¸ ì¤‘ ì²« ë²ˆì§¸ë¡œ K-means-based Quantizationì´ ìˆë‹¤. [Deep Compression [HanÂ et al., ICLR 2016]](https://arxiv.org/abs/1510.00149) ë…¼ë¬¸ì— ì†Œê°œí–ˆë‹¤ëŠ” ì´ ë°©ë²•ì€ ì¤‘ì‹¬ê°’ì„ ê¸°ì¤€ìœ¼ë¡œ clusteringì„ í•˜ëŠ” ë°©ë²•ì´ë‹¤. ì˜ˆì œë¥¼ ë´ë³´ì.

![Reference. MIT-TinyML-lecture5-Quantization-1](../../images/lec05/1/k-mean-quantization.png)

ìœ„ ì˜ˆì œëŠ” weightë¥¼ codebookì—ì„œ -1, 0, 1.5, 2ë¡œ ë‚˜ëˆ  ê°ê°ì— ë§ëŠ” ì¸ë±ìŠ¤ë¡œ í‘œê¸°í•œë‹¤. ì´ë ‡ê²Œ ì—°ì‚°ì„ í•˜ë©´ ê¸°ì¡´ì— 64bytesë¥¼ ì‚¬ìš©í–ˆë˜ weightê°€ 20bytesë¡œ ì¤„ì–´ë“ ë‹¤. codebookìœ¼ë¡œ ì˜ˆì œëŠ” 2bitë¡œ ë‚˜ëˆ´ì§€ë§Œ, ì´ë¥¼ N-bitë§Œí¼ ì¤„ì¸ë‹¤ë©´ ìš°ë¦¬ëŠ” ì´ 32/Në°°ì˜ ë©”ëª¨ë¦¬ë¥¼ ì¤„ì¼ ìˆ˜ ìˆë‹¤. í•˜ì§€ë§Œ ì´ ê³¼ì •ì—ì„œ quantizatio error, ì¦‰ quantizationì„ í•˜ê¸° ì „ê³¼ í•œ í›„ì— ì˜¤ì°¨ê°€ ìƒê¸°ëŠ” ê²ƒì„ ìœ„ ì˜ˆì œì—ì„œ ë³¼ ìˆ˜ ìˆë‹¤. ë©”ëª¨ë¦¬ ì‚¬ìš©ëŸ‰ì„ ì¤„ì´ëŠ” ê²ƒë„ ì¢‹ì§€ë§Œ, ì´ ë•Œë¬¸ì— ì„±ëŠ¥ì— ì˜¤ì°¨ê°€ ìƒê¸°ì§€ ì•Šê²Œ í•˜ê¸°ìœ„í•´ ì´ ì˜¤ì°¨ë¥¼ ì¤„ì´ëŠ” ê²ƒ ë˜í•œ ì¤‘ìš”í•˜ë‹¤.

![Reference. MIT-TinyML-lecture5-Quantization-1](../../images/lec05/1/k-mean-quantization-finetuning.png)

ì´ë¥¼ ë³´ì™„í•˜ê¸° ìœ„í•´ Quantizedí•œ Weightë¥¼ ìœ„ì— ê·¸ë¦¼ì²˜ëŸ¼ Fine-tuningí•˜ê¸°ë„ í•œë‹¤. centroidë¥¼ fine-tuningí•œë‹¤ê³  ìƒê°í•˜ë©´ ë˜ëŠ”ë°, ê° centroidì—ì„œ ìƒê¸°ëŠ” ì˜¤ì°¨ë¥¼ í‰ê· ë‚´ tuningí•˜ëŠ” ë°©ë²•ì´ë‹¤. ì´ ë°©ë²•ì„ ì œì•ˆí•œ [ë…¼ë¬¸](https://arxiv.org/abs/1510.00149) ì—ì„œëŠ” Convolution ë ˆì´ì–´ì—ì„œëŠ” 4bitê¹Œì§€ centroidë¥¼ ê°€ì¡Œì„ ë•Œ, Full-Connected layerì—ì„œëŠ” 2 bitê¹Œì§€ centroidë¥¼ ê°€ì¡Œì„ ë•Œ ì„±ëŠ¥ì— í•˜ë½ì´ ì—†ë‹¤ê³  ë§í•˜ê³  ìˆì—ˆë‹¤.

![Reference. Deep Compression [Han et al., ICLR 2016]](../../images/lec05/1/continuous-data.png)

ì´ë ‡ê²Œ Quantization ëœ WeightëŠ” ìœ„ì²˜ëŸ¼ ì—°ì†ì ì¸ ê°’ì—ì„œ ì•„ë˜ì²˜ëŸ¼ Discreteí•œ ê°’ìœ¼ë¡œ ë°”ë€ë‹¤. 

![Reference. Deep Compression [Han et al., ICLR 2016]](../../images/lec05/1/discrete-data.png)

ë…¼ë¬¸ì€ ì´ë ‡ê²Œ Quantizationí•œ weightë¥¼ í•œ ë²ˆ ë” Huffman codingë¥¼ ì´ìš©í•´ ìµœì í™”ì‹œí‚¨ë‹¤. ì§§ê²Œ ì„¤ëª…í•˜ìë©´, ë¹ˆë„ìˆ˜ê°€ ë†’ì€ ë¬¸ìëŠ” ì§§ì€ ì´ì§„ì½”ë“œë¥¼, ë¹ˆë„ ìˆ˜ê°€ ë‚®ì€ ë¬¸ìì—ëŠ” ê¸´ ì´ì§„ì½”ë“œë¥¼ ì“°ëŠ” ë°©ë²•ì´ë‹¤. ì••ì¶• ê²°ê³¼ë¡œ Generalí•œ ëª¨ë¸ê³¼ ì••ì¶• ë¹„ìœ¨ì´ ê½¤ í° SqueezeNetì„ ì˜ˆë¡œ ë“ ë‹¤. ìì„¸í•œ ë‚´ìš©ì€ ë…¼ë¬¸ì„ ì°¸ê³ í•˜ëŠ” ê±¸ë¡œ. 

![Reference. Deep Compression [Han et al., ICLR 2016]](../../images/lec05/1/deep-compression.png)

![Reference. Deep Compression [Han et al., ICLR 2016]](../../images/lec05/1/deep-compression-result.png)

inferenceë¥¼ ìœ„í•´ weightë¥¼ Decodingí•˜ëŠ” ê³¼ì •ì€ inferenceê³¼ì •ì—ì„œ ì €ì¥í•œ clusterì˜ ì¸ë±ìŠ¤ë¥¼ ì´ìš©í•´ codebookì—ì„œ í•´ë‹¹í•˜ëŠ” ê°’ì„ ì°¾ì•„ë‚´ëŠ” ê²ƒì´ë‹¤. ì´ ë°©ë²•ì€ ì €ì¥ ê³µê°„ì„ ì¤„ì¼ ìˆ˜ëŠ” ìˆì§€ë§Œ, floating point Computationì´ë‚˜ ë©”ëª¨ë¦¬ ì ‘ê·¼í•˜ëŠ” ë°©ì‹ìœ¼ë¡œ centroidë¥¼ ì“°ëŠ” í•œê³„ê°€ ìˆì„ ìˆ˜ ë°–ì— ì—†ë‹¤.

![Reference. MIT-TinyML-lecture5-Quantization-1](../../images/lec05/1/intro.png)
<p>
    <img src="../../images/lec05/1/decoding-deep-compression.png" width="500" height="200" class="projects__article__img__center">
    <p align="center">
    <em class="projects__img__caption"> Reference. Deep Compression [Han et al., ICLR 2016] </em>
    </p>
</p>


### 1.2 Linear Quantization

ë‘ ë²ˆì§¸ ë°©ë²•ì€ Linear Quatizationì´ë‹¤. floating-pointì¸ weightë¥¼ N-bitì˜ ì •ìˆ˜ë¡œ affine mappingì„ ì‹œí‚¤ëŠ” ë°©ë²•ì´ë‹¤. ê°„ë‹¨í•˜ê²Œ ì‹ìœ¼ë¡œ ë³´ëŠ” ê²Œ ë” ì´í•´ê°€ ì‰½ë‹¤.

![Reference. MIT-TinyML-lecture5-Quantization-1](../../images/lec05/1/linear-quantization-eq.png)

ì—¬ê¸°ì„œ S(Scale of Linear Quantization)ì™€ Z(Zero point of Linear Quantization)ê°€ ìˆëŠ”ë° ì´ ë‘˜ì´ quantization parameter ë¡œì¨ tuningì„ í•  ìˆ˜ ìˆëŠ” ê°’ì¸ ê²ƒì´ë‹¤.

![Reference. MIT-TinyML-lecture5-Quantization-1](../../images/lec05/1/linear-quantization-img.png)

### 1.3 Scale and Zero point

![Reference. MIT-TinyML-lecture5-Quantization-1](../../images/lec05/1/scale-zero-point.png)

ì´ Scaleê³¼ Zero point ë‘ íŒŒë¼ë¯¸í„°ë¥¼ ì´ìš©í•´ì„œ affine mappingì€ ìœ„ ê·¸ë¦¼ê³¼ ê°™ë‹¤. Bit ìˆ˜(Bit Width)ê°€ ë‚®ì•„ì§€ë©´ ë‚®ì•„ì§ˆ ìˆ˜ë¡, floating pointì—ì„œ í‘œí˜„í•  ìˆëŠ” ìˆ˜ ë˜í•œ ì¤„ì–´ë“¤ ê²ƒì´ë‹¤. ê·¸ë ‡ë‹¤ë©´ Scaleì™€ Zero pointëŠ” ê°ê° ì–´ë–»ê²Œ ê³„ì‚°í• ê¹Œ?

ìš°ì„  floating-point ì¸ ìˆ«ìì˜ ë²”ìœ„ ì¤‘ ìµœëŒ€ê°’ê³¼ ìµœì†Ÿê°’ì— ë§ê²Œ ë‘ ì‹ì„ ì„¸ìš°ê³  ì´ë¥¼ ì—°ë¦½ë°©ì •ì‹ìœ¼ë¡œ Scaleê³¼ Zero pointì„ êµ¬í•  ìˆ˜ ìˆë‹¤.

- Scale point
    $$
    r_{max} = S(q_{max}-Z) 
    $$ 
    $$
    r_{min} = S(q_{min}-Z) 
    $$

    $$ 
    r_{max} - r_{min} = S(q_{max} - q_{min})
    $$

    $$ 
    S = \dfrac{r_{max}-r_{min}}{q_{max}-q_{min}}
    $$

- Zero point
    $$
    r_{min} = S(q_{min}-Z) 
    $$

    $$ 
    Z=q_{min}-\dfrac{r_{min}}{S} 
    $$

    $$ 
    Z = round\Big(q_{min}-\dfrac{r_{min}}{S}\Big)
    $$

ì˜ˆë¥¼ ë“¤ì–´, ì•„ë˜ì™€ ê°™ì€ ì˜ˆì œì—ì„œ $r_{max}$ ëŠ”$2.12$ ì´ê³  $r_{min}$ ì€ $-1.08$ ë¡œ Scaleì„ ê³„ì‚°í•˜ë©´ ì•„ë˜ ê·¸ë¦¼ì²˜ëŸ¼ ëœë‹¤. Zero pointëŠ” $-1$ ë¡œ ê³„ì‚°í•  ìˆ˜ ìˆë‹¤.

![](../../images/lec05/1/scale-zero-point-ex1.png)

![Reference. MIT-TinyML-lecture5-Quantization-1](../../images/lec05/1/scale-zero-point-ex2.png)

ê·¸ëŸ¼ Symmetricí•˜ê²Œ rì˜ ë²”ìœ„ë¥¼ ì œí•œí•˜ëŠ” ê²ƒê³¼ ê°™ì€ ë‹¤ë¥¸ Linear Quantizationì€ ì—†ì„ê¹Œ? ì´ë¥¼ ì•ì„œ, Quatizedëœ ê°’ë“¤ì´ Matrix Multiplicationì„ í•˜ë©´ì„œ ë¯¸ë¦¬ ê³„ì‚°ë  ìˆ˜ ìˆëŠ” ìˆ˜ (Quantized Weight, Scale, Zero point)ê°€ ìˆìœ¼ë‹ˆ inferenceì‹œ ì—°ì‚°ëŸ‰ì„ ì¤„ì´ê¸° ìœ„í•´ ë¯¸ë¦¬ ê³„ì‚°í•  ìˆ˜ ìˆëŠ” íŒŒë¼ë¯¸í„°ëŠ” ì—†ì„ê¹Œ? 

### 1.4 Quantized Matrix Multiplication
ì…ë ¥ X, Weight W, ê²°ê³¼ Yê°€ Matrix Multiplicationì„ í–ˆë‹¤ê³  í•  ë•Œ ì‹ì„ ê³„ì‚°í•´ë³´ì.

$$
Y=WX
$$

$$
S_Y(q_Y-Z_Y) = S_W(q_W-Z_W) \cdot S_X(q_X-Z_X
$$

$$
\vdots
$$

![Reference. MIT-TinyML-lecture5-Quantization-1](../../images/lec05/1/quantized-matrix-multi.png)

ì—¬ê¸°ì„œ ë§ˆì§€ë§‰ ì •ë¦¬í•œ ì‹ì„ ì‚´í´ë³´ë©´,

$Z_x$ ì™€ $q_w, Z_w, Z_X$ ì˜ ê²½ìš°ëŠ” ë¯¸ë¦¬ ì—°ì‚°ì´ ê°€ëŠ¥í•˜ë‹¤. ë˜ $S_wS_X/S_Y$ ì˜ ê²½ìš° í•­ìƒ ìˆ˜ì˜ ë²”ìœ„ê°€ $(0, 1)$ ë¡œ $2^{-n}M_0$ , $M_0 \in [0.5, 1)$ ë¡œ ë³€í˜•í•˜ë©´ N-bit Integerë¡œ Fixed-point í˜•íƒœë¡œ í‘œí˜„ ê°€ëŠ¥í•˜ë‹¤. ì—¬ê¸°ì— $Z_w$ê°€ 0ì´ë©´ ì–´ë–¨ê¹Œ? ë˜ ë¯¸ë¦¬ ê³„ì‚°í•  ìˆ˜ ìˆëŠ” í•­ì´ ë³´ì¸ë‹¤.

### 1.5 Symmetric Linear Quantization

![Reference. MIT-TinyML-lecture5-Quantization-1](../../images/lec05/1/sym-linear-quant.png)

$Z_w = 0$ ì´ë¼ê³  í•¨ì€ ë°”ë¡œ ìœ„ì™€ ê°™ì€ Weight ë¶„í¬ì¸ë°, ë°”ë¡œ Symmetricí•œ Linear Quantizationìœ¼ë¡œ $Z_w$ë¥¼ 0ìœ¼ë¡œ ë§Œë“¤ì–´ $Z_w q_x$í•­ì„ 0ìœ¼ë¡œ ë‘˜ ìˆ˜ ìˆì–´ ì—°ì‚°ì„ ë˜ ì¤„ì¼ ìˆ˜ ìˆì„ ê²ƒì´ë‹¤.

Symmetric Linear Quantizationì€ ì£¼ì–´ì§„ ë°ì´í„°ì—ì„œ Full range modeì™€ Restrict range modeë¡œ ë‚˜ë‰œë‹¤. 

ì²« ë²ˆì§¸ Full range mode ëŠ” Scaleì„ real number(ë°ì´í„°, weight)ì—ì„œ ë²”ìœ„ê°€ ë„“ì€ ìª½ì— ë§ì¶”ëŠ” ê²ƒì´ë‹¤. ì˜ˆë¥¼ ë“¤ì–´ ì•„ë˜ì˜ ê²½ìš°, r_minì´ r_maxë³´ë‹¤ ì ˆëŒ“ê°’ì´ ë” í¬ê¸° ë•Œë¬¸ì— r_minì— ë§ì¶° q_minì„ ê°€ì§€ê³  Scaleì„ êµ¬í•œë‹¤. ì´ ë°©ë²•ì€ Pytorch native quantizationê³¼ ONNXì—ì„œ ì‚¬ìš©ëœë‹¤ê³  ê°•ì˜ì—ì„œ ì†Œê°œí•œë‹¤.

![Reference. MIT-TinyML-lecture5-Quantization-1](../../images/lec05/1/sym-full-range.png)

ë‘ ë²ˆì§¸ Restrict range modeëŠ” Scaleì„ real number(ë°ì´í„°, weight)ì—ì„œ ë²”ìœ„ê°€ ì¢ì€ ìª½ì— ë§ì¶”ëŠ” ê²ƒì´ë‹¤. ì˜ˆë¥¼ ë“¤ì–´ ì•„ë˜ì˜ ê²½ìš°, r_minê°€ r_maxë³´ë‹¤ ì ˆëŒ“ê°’ì´ ë” í¬ê¸° ë•Œë¬¸ì— r_minì— ë§ì¶”ë©´ì„œ q_maxì— ë§ë„ë¡ Scaleì„ êµ¬í•œë‹¤. ì´ ë°©ë²•ì€ [TensorFlow](https://www.tensorflow.org/lite/performance/quantization_spec), NVIDIA TensorRT, Intel DNNLì—ì„œ ì‚¬ìš©ëœë‹¤ê³  ê°•ì˜ì—ì„œ ì†Œê°œí•œë‹¤.

![Reference. MIT-TinyML-lecture5-Quantization-1](../../images/lec05/1/sym-restrict-range.png)

ê·¸ë ‡ë‹¤ë©´ ì™œ Symmetric ì¨ì•¼í• ê¹Œ? Asymmetric ë°©ë²•ê³¼ Symmetric ë°©ë²•ì˜ ì°¨ì´ëŠ” ë­˜ê¹Œ? (feat. Neural Network Distiller) ì•„ë˜ ê·¸ë¦¼ì„ ì°¸ê³ í•˜ë©´ ë˜ì§€ë§Œ, ê°€ì¥ í° ì°¨ì´ë¡œ ë³´ì´ëŠ” ê²ƒì€ Computation vs Compactful quantized rangeë¡œ ì´í•´ê°„ë‹¤.

![Reference. MIT-TinyML-lecture5-Quantization-1](../../images/lec05/1/sym-range-comp.png)

### 1.6 Linear Quantization examples
ê·¸ëŸ¼ Quatization ë°©ë²•ì— ëŒ€í•´ ì•Œì•„ë´¤ìœ¼ë‹ˆ ì´ë¥¼ Full-Connected Layer, Convolution Layerì— ì ìš©í•´ë³´ê³  ì–´ë–¤ íš¨ê³¼ê°€ ìˆëŠ”ì§€ ì•Œì•„ë³´ì.

#### 1.6.1 Full-Connected Layer
ì•„ë˜ì²˜ëŸ¼ ì‹ì„ ì „ê°œí•´ë³´ë©´ ë¯¸ë¦¬ ì—°ì‚°í•  ê³„ì‚°í•  ìˆ˜ ìˆëŠ” í•­ê³¼ N-bit integerë¡œ í‘œí˜„í•  ìˆëŠ” í•­ìœ¼ë¡œ ë‚˜ëˆŒ ìˆ˜ ìˆë‹¤(ì „ê°œí•˜ëŠ” ì´ìœ ëŠ” ì•„ë§ˆ ë¯¸ë¦¬ ê³„ì‚°í•  ìˆ˜ ìˆëŠ” í•­ì„ ì•Œì•„ë³´ê¸° ìœ„í•¨ì´ ì•„ë‹ê¹Œ ì‹¶ë‹¤).

$$
Y=WX+b 
$$

$$
\downarrow 
$$

$$
S_Y(q_Y - Z_Y) = S_W(q_W - Z_W) \cdot S_X(q_X - Z_X) + S_b(q_b - Z_b)
$$

$$
\downarrow \ Z_w=0 
$$

$$
S_Y(q_Y - Z_Y) = S_WS_X(q_Wq_X - Z_xq_W) + S_b(q_b - Z_b)
$$

$$
\downarrow \ Z_b=0, S_b=S_WS_X 
$$

$$
S_Y(q_Y - Z_Y) = S_WS_X(q_Wq_X - Z_xq_W+q_b)
$$

$$
\downarrow 
$$

$$
q_Y = \dfrac{S_WS_X}{S_Y}(q_Wq_X+q_b - Z_Xq_W) + Z_Y
$$

$$
\downarrow \ q_{bias}=q_b-Z_xq_W\\
$$

$$
q_Y = \dfrac{S_WS_X}{S_Y}(q_Wq_X+ q_{bias}) + Z_Y\\
$$

ê°„ë‹¨íˆ í‘œê¸°í•˜ê¸° ìœ„í•´ $Z_W=0, Z_b=0, S_b = S_W S_X$ ì´ë¼ê³  ê°€ì •í•œë‹¤.

![Reference. MIT-TinyML-lecture5-Quantization-1](../../images/lec05/1/full-connected-layer.png)

#### 1.6.2 Convolutional Layer
Convolution Layerì˜ ê²½ìš°ëŠ” Weightì™€ Xì˜ ê³±ì˜ ê²½ìš°ë¥¼ Convolutionìœ¼ë¡œ ë°”ê¿”ì„œ ìƒê°í•´ë³´ë©´ ëœë‹¤. ê·¸ë„ ê·¸ëŸ´ ê²ƒì´ Convolutionì€ Kernelê³¼ Inputì˜ ê³±ì˜ í•©ìœ¼ë¡œ ì´ë£¨ì–´ì ¸ ìˆê¸° ë•Œë¬¸ì— Full-Connectedì™€ ê±°ì˜ ìœ ì‚¬í•˜ê²Œ ì „ê°œë  ìˆ˜ ìˆì„ ê²ƒì´ë‹¤. 

![Reference. MIT-TinyML-lecture5-Quantization-1](../../images/lec05/1/conv-layer.png)

## 2. Post-training Quantization (PTQ)
ê·¸ëŸ¼ ì•ì„œì„œ Quantizaedí•œ Layerë¥¼ Fine tuningí•  ì—†ì„ê¹Œ? **"How should we get the optimal linear quantization parameters (S, Z)?"** ì´ ì§ˆë¬¸ì— ëŒ€í•´ì„œ Weight, Activation, Bias ì„¸ ê°€ì§€ì™€ ê·¸ì— ëŒ€í•˜ì—¬ ë…¼ë¬¸ì—ì„œ ë³´ì—¬ì£¼ëŠ” ê²°ê³¼ê¹Œì§€ ì•Œì•„ë³´ì.

### 2.1 Weight quantization
**TL;DR.** ì´ ê°•ì˜ì—ì„œ ì†Œê°œí•˜ëŠ” Weight quantizationì€ Grandularityì— ë”°ë¼ Whole(Per-Tensor), Channel, ê·¸ë¦¬ê³  Layerë¡œ ë“¤ì–´ê°„ë‹¤.

#### 2.1.1 Granularity
Weight quantizationì—ì„œ Granularityì— ë”°ë¼ì„œ Per-Tensor, Per-Channel, Group, ê·¸ë¦¬ê³  Generalized í•˜ëŠ” ë°©ë²•ìœ¼ë¡œ í™•ì¥ì‹œì¼œ Shared Micro-exponent(MX) data typeì„ ì°¨ë¡€ë¡œ ë³´ì—¬ì¤€ë‹¤. Scaleì„ ëª‡ ê°œë‚˜ ë‘˜ ê²ƒì´ëƒ, ê·¸ Scaleì„ ì ìš©í•˜ëŠ” ë²”ìœ„ë¥¼ ì–´ë–»ê²Œ ë‘˜ ê²ƒì´ëƒ, ê·¸ë¦¬ê³  Scaleì„ ì–¼ë§ˆë‚˜ ë””í…Œì¼í•˜ê²Œ(e.g. floating-point)í•  ê²ƒì´ëƒì— ì´ˆì ì„ ë‘”ë‹¤.

![Reference. MIT-TinyML-lecture5-Quantization-2](../../images/lec05/1/granularity.png)

ì²« ë²ˆì§¸ëŠ” **Per-Tensor Quantization** íŠ¹ë³„í•˜ê²Œ ì„¤ëª…í•  ê²ƒ ì—†ì´ ì´ì „ê¹Œì§€ ì„¤ëª…í–ˆë˜ í•˜ë‚˜ì˜ Scaleì„ ì‚¬ìš©í•˜ëŠ” Linear Quantizationì´ë¼ê³  ìƒê°í•˜ë©´ ë˜ê² ë‹¤. íŠ¹ì§•ìœ¼ë¡œëŠ” Large modelì— ëŒ€í•´ì„œëŠ” ì„±ëŠ¥ì´ ê´œì°®ì§€ë§Œ ì‘ì€ ëª¨ë¸ë¡œ ë–¨ì–´ì§€ë©´ ì„±ëŠ¥ì´ ê¸‰ê²©í•˜ê²Œ ë–¨ì–´ì§„ë‹¤ê³  ì„¤ëª…í•œë‹¤. Channelë³„ë¡œ weight ë²”ì£¼ê°€ ë„“ì€ ê²½ìš°ë‚˜ outlier weightê°€ ìˆëŠ” ê²½ìš° quantization ì´í›„ì— ì„±ëŠ¥ì´ í•˜ë½í–ˆë‹¤ê³  ë§í•œë‹¤. 

![Reference. MIT-TinyML-lecture5-Quantization-2](../../images/lec05/1/per-channel-quant.png)

ê·¸ë˜ì„œ ê·¸ í•´ê²°ë°©ì•ˆìœ¼ë¡œ ë‚˜ì˜¤ëŠ” ê²ƒì´ ë‘ ë²ˆì§¸ ë°©ë²•ì¸ **Per-Channel Quantization**ì´ë‹¤. ìœ„ ì˜ˆì œì—ì„œ ë³´ë©´ Channel ë§ˆë‹¤ ìµœëŒ€ê°’ê³¼ ê°ê°ì— ë§ëŠ” Scaleì„ ë”°ë¡œ ê°€ì§€ëŠ” ê²ƒì„ ë³¼ ìˆ˜ ìˆë‹¤. ê·¸ë¦¬ê³  ì ìš©í•œ ê²°ê³¼ì¸ ì•„ë˜ ê·¸ë¦¼ì„ ë³´ë©´ Per-Channelê³¼ Per-Tensorë¥¼ ë¹„êµí•´ë³´ë©´ Per-Channelì´ ê¸°ì¡´ì— floating point weightì™€ì˜ ì°¨ì´ê°€ ë” ì ë‹¤. í•˜ì§€ë§Œ, ë§Œì•½ í•˜ë“œì›¨ì–´ì—ì„œ Per-Channel Quantizationì„ ì§€ì›í•˜ì§€ ì•ŠëŠ”ë‹¤ë©´ ë¶ˆí•„ìš”í•œ ì—°ì‚°ì„ ì¶”ê°€ë¡œ í•´ì•¼í•˜ê¸° ë•Œë¬¸ì— ì´ëŠ” ì í•©í•œ ë°©ë²•ì´ ë  ìˆ˜ ì—†ë‹¤ëŠ” ì ë„ ê³ ë ¤í•´ì•¼í•  ê²ƒì´ë‹¤(ì´ëŠ” ì´ì „ [Tiny Engineì— ëŒ€í•œ ê¸€](https://ooshyun.github.io/2023/12/04/Optimization-for-tiny-engine-1.html)ì—ì„œ Channelë‚´ì— ìºì‹±ì„ ì´ìš©í•œ ìµœì í™”ì™€ ì—°ê´€ì´ ìˆë‹¤). ê·¸ëŸ¼ ë˜ ë‹¤ë¥¸ ë°©ë²•ì€ ì—†ì„ê¹Œ?

![Reference. MIT-TinyML-lecture5-Quantization-2](../../images/lec05/1/per-channel-vs-per-tensor.png)

ì„¸ ë²ˆì§¸ ë°©ë²•ì€ **Group Quantization**ìœ¼ë¡œ ì†Œê°œí•˜ëŠ” **Per-vector Scaled Quantizationì™€ Shared Micro-exponent(MX) data type** ì´ë‹¤. Per-vector Scaled Quantizationì€ 2023ë…„ë„ ê°•ì˜ë¶€í„° ì†Œê°œí•˜ëŠ”ë°, ì´ ë°©ë²•ì€ Scale factorë¥¼ ê·¸ë£¹ë³„ë¡œ í•˜ë‚˜, Per-Tensorë¡œ í•˜ë‚˜ë¡œ ë‘ê°œë¥¼ ë‘ëŠ” ë°©ë²•ì´ë‹¤. ì•„ë˜ì˜ ê·¸ë¦¼ì„ ë³´ë©´,

![Reference. MIT-TinyML-lecture5-Quantization-2](../../images/lec05/1/group-quantization.png)

$$
r=S(q-Z) \rightarrow r=\gamma \cdot S_{q}(q-Z)
$$

$S_q$ ë¡œ vectorë³„ ìŠ¤ì¼€ì¼ë§ì„ í•˜ë‚˜, $\gamma$ ë¡œ Tensorì— ìŠ¤ì¼€ì¼ë§ì„ í•˜ë©° ê°ë§ˆëŠ” floating pointë¡œ í•˜ëŠ” ê²ƒì„ ë³¼ ìˆ˜ìˆë‹¤. ì•„ë¬´ë˜ë„ vectorë‹¨ìœ„ë¡œ ìŠ¤ì¼€ì¼ë§ì„ í•˜ê²Œë˜ë©´ channelê³¼ ë¹„êµí•´ì„œ í•˜ë“œì›¨ì–´ í”Œë«í¼ì— ë§ê²Œ accuracyì˜ trade-offë¥¼ ì¡°ì ˆí•˜ê¸° ë” ìˆ˜ì›”í•  ê²ƒìœ¼ë¡œ ë³´ì¸ë‹¤. 

ì—¬ê¸°ì„œ ê°•ì˜ëŠ” ì§€í‘œì¸ Memory Overheadë¡œ **"Effective Bit Width"**ë¥¼ ì†Œê°œí•œë‹¤. ì´ëŠ” Microsoftì—ì„œ ì œê³µí•˜ëŠ” Quantization Approach MX4, MX6, MX9ê³¼ ì—°ê²°ë¼ ìˆëŠ”ë°, ì´ ë°ì´í„°íƒ€ì…ì€ ì¡°ê¸ˆ ì´í›„ì— ë” ìì„¸íˆ ì„¤ëª…í•  ê²ƒì´ë‹¤. Effective Bit Width? ì˜ˆì‹œ í•˜ë‚˜ë¥¼ ë“¤ì–´ ì´í•´í•´ë³´ì. ë§Œì•½ 4-bit Quatizationì„ 4-bit per-vector scaleì„ 16 elements(4ê°œì˜ weightê°€ ê°ê° 4bitë¥¼ ê°€ì§„ë‹¤ê³  ìƒê°í•˜ë©´ 16 elementë¡œ ê³„ì‚°ëœë‹¤ ìœ ì¶”í•  ìˆë‹¤) ë¼ë©´, Effective Bit WidthëŠ” 4(Scale bit) + 4(Vector Scale bit) / 16(Vector Size) = 4.25ê°€ ëœë‹¤. Elementë‹¹ Scale bitë¼ê³  ê°„ë‹¨í•˜ê²Œ ìƒê°í•  ìˆ˜ë„ ìˆì„ ë“¯ ì‹¶ë‹¤.

ë§ˆì§€ë§‰ Per-vector Scaled Quantizationì„ ì´í•´í•˜ë‹¤ë³´ë©´ ì´ì „ì— Per-Tensor, Per-Channelë„ ê·¸ë£¹ìœ¼ë¡œ ì–¼ë§ˆë§Œí¼ ë¬¶ëŠ” ì°¨ì´ê°€ ìˆê³ , ì´ëŠ” ì´ë“¤ì„ ì¼ë°˜í™”í•  ìˆ˜ ìˆì–´ ë³´ì¸ë‹¤. ê°•ì˜ì—ì„œ ë°”ë¡œ ë‹¤ìŒì— ì†Œê°œí•˜ëŠ” ë°©ë²•ì´ ë°”ë¡œ **Multi-level scaling scheme**ì´ë‹¤. Per-Channel Quantizationì™€ Per-Vector Quantization(VSQ, Vector-Scale Quantization)ë¶€í„° ë´ë³´ì.

![Reference. With Shared Microexponents, A Little Shifting Goes a Long Way [Bita Rouhani et al.]](../../images/lec05/1/multi-level-scaling-scheme-1.png)

Per-Channel QuantizationëŠ” Scale factorê°€ í•˜ë‚˜ë¡œ Effective Bit WidthëŠ” 4ê°€ ëœë‹¤. ê·¸ë¦¬ê³  VSQëŠ” ì´ì „ì— ê³„ì‚°í–ˆ ë“¯ 4.25ê°€ ë  ê²ƒì´ë‹¤(ì°¸ê³ ë¡œ Per Channelë¡œ ì ìš©ë˜ëŠ” Scaleì˜ ê²½ìš° elementì˜ ìˆ˜ê°€ ë§ì•„ì„œ ê·¸ëŸ°ì§€ ë”°ë¡œ Effective Bit Widthë¡œ ê³„ì‚°í•˜ì§€ëŠ” ì•ŠëŠ”ë‹¤). VSQê¹Œì§€ ë³´ë©´ì„œ Effective Bit WidthëŠ”,

```
Effective Bit Width = Scale bit + Group 0 Scale bit / Group 0 Size +...
e.g. VSQ Data type int4 = Scale bit (4) + Group 0 Scale bit(4) / Group 0 Size(16) = 4.25
```

ì´ë ‡ê²Œ ê³„ì‚°í•  ìˆ˜ ìˆë‹¤. ê·¸ë¦¬ê³ , MX4, MX6, MX9ê°€ ë‚˜ì˜¨ë‹¤. ì°¸ê³ ë¡œ SëŠ” Sign bit, Mì€ Mantissa bit, EëŠ” Exponent bitë¥¼ ì˜ë¯¸í•œë‹¤(Mantissaë‚˜ Exponentì— ëŒ€í•œ ìì„¸í•œ ë‚´ìš©ì€ [floating point vs fixed point ê¸€](https://ooshyun.github.io/2023/02/24/Fixed-point-vs-Floating-point.html)ì„ ì°¸ê³ í•˜ì). ì•„ë˜ëŠ” Microsoftì—ì„œ ì œê³µí•˜ëŠ” Quantization Approach MX4, MX6, MX9ì— ëŒ€í•œ í‘œì´ë‹¤.

![Reference. MIT-TinyML-lecture5-Quantization-1](../../images/lec05/1/multi-level-scaling-scheme-2.png)

#### 2.1.2 Weight Equalization
ì—¬ê¸°ê¹Œì§€ Weight Quatizationì—ì„œ ê·¸ë£¹ìœ¼ë¡œ ì–¼ë§ˆë§Œí¼ ë¬¶ëŠ”ì§€ì— ë”°ë¼(ê°•ì˜ì—ì„œëŠ” Granularity) Quatizationì„ í•˜ëŠ” ì—¬ëŸ¬ ë°©ë²•ì„ ì†Œê°œí–ˆë‹¤. ë‹¤ìŒìœ¼ë¡œ ì†Œê°œ í•  ë°©ë²•ì€ Weight Equalizationì´ë‹¤. 2022ë…„ì— ì†Œê°œí•´ì¤€ ë‚´ìš©ì¸ë°, ì´ëŠ” ië²ˆì§¸ layerì˜ output channelë¥¼ scaling down í•˜ë©´ì„œ i+1ë²ˆì§¸ layerì˜ input channelì„ scaling up í•´ì„œ Scaleë¡œ ì¸í•´ Quantization ì „í›„ë¡œ ìƒê¸°ëŠ” Layerê°„ ì°¨ì´ë¥¼ ì¤„ì´ëŠ” ë°©ë²•ì´ë‹¤.

![Reference. Data-Free Quantization Through Weight Equalization and Bias Correction [Markus et al., ICCV 2019]](../../images/lec05/1/weight-equalization.png)

ì˜ˆë¥¼ ë“¤ì–´ ìœ„ì— ê·¸ë¦¼ì²˜ëŸ¼ Layer iì˜ output channelê³¼ Layer i+1ì˜ input channelì´ ìˆë‹¤. ì—¬ê¸°ì„œ ì‹ì„ ì „ê°œí•˜ë©´ ì•„ë˜ì™€ ê°™ì€ë°,

$$
\begin{aligned}
y^{(i+1)}&=f(W^{(i+1)}x^{(i+1)}+b^{(i+1)}) \\
         &=f(W^{(i+1)} \cdot f(W^{(i)}x^{(i)}+b^{(i)}) +b^{(i+1)}) \\
         &=f(W^{(i+1)}S \cdot f(S^{-1}(W^{(i)}x^{(i)}+S^{-1}b^{(i)})) +b^{(i+1)})
\end{aligned}
$$

where $S = diag(s)$ , $s_j$ is the weight equalization scale factor of output channel $j$

ì—¬ê¸°ì„œ Scale(S)ê°€ i+1ë²ˆì§¸ layerì˜ weightì—, ië²ˆì§¸ weightì— 1/S ë¡œ Scaleë  ë–„ ê¸°ì¡´ì— Scale í•˜ì§€ ì•Šì€ ì‹ê³¼ ìœ ì‚¬í•˜ê²Œ ìœ ì§€í•  ìˆëŠ” ê²ƒì„ ë³¼ ìˆ˜ ìˆë‹¤. ì¦‰,

$$
r^{(i)}_{oc_j} / s = r^{(i+1)}_{ic_j} \cdot s
$$

$$
s_j = \dfrac{1}{r^{(i+1)}_{ic=j}}\sqrt{r_{oc=j}^{(i)}\cdot r_{ic=j}^{(i+1)}} 
$$

$$
r^{(i)}_{oc_j} = r^{(i)}_{oc_j} / s = \sqrt{r_{oc=j}^{(i)}\cdot r_{ic=j}^{(i+1)}} 
$$

$$
r^{(i)}_{ic_j} =r^{(i)}_{ic_j} \cdot s = \sqrt{r_{oc=j}^{(i)}\cdot r_{ic=j}^{(i+1)}} 
$$

ì´ë ‡ê²Œ í•˜ë©´ ië²ˆì§¸ layerì˜ output channelê³¼ i+1ë²ˆì§¸ layerì˜ input channelì˜ Scaleì„ ê°ê° $S$ ì™€ $1/S$ ë¡œí•˜ë©° weightê°„ì˜ ê²©ì°¨ë¥¼ ì¤„ì¼ ìˆ˜ ìˆë‹¤.

#### 2.1.3 Adaptive rounding

![Reference. MIT-TinyML-lecture5-Quantization-1](../../images/lec05/1/adaptive-rounding.png)

ë§ˆì§€ë§‰ ì†Œê°œí•˜ëŠ” ë°©ë²•ì€ Adaptive rounding ì´ë‹¤. ë°˜ì˜¬ë¦¼ì€ Round-to-nearestìœ¼ë¡œ ë¶ˆë¦¬ëŠ” ì¼ë°˜ì ì¸ ë°˜ì˜¬ë¦¼ì„ ìƒê°í•  ìˆ˜ ìˆê³ , í•˜ë‚˜ì˜ ê¸°ì¤€ì„ ê°€ì§€ê³  ë°˜ì˜¬ë¦¼ì„ í•˜ëŠ” Adaptive Roundë¥¼ ìƒê°í•  í•  ìˆ˜ ìˆë‹¤. ê°•ì˜ì—ì„œëŠ” Round-to-nearestê°€ ìµœì ì˜ ë°©ë²•ì´ ë˜ì§€ ì•ŠëŠ”ë‹¤ê³  ë§í•˜ë©°, Adaptive roundë¡œ weightì— 0ë¶€í„° 1 ì‚¬ì´ì˜ ê°’ì„ ë”í•´ ìˆ˜ì‹ì²˜ëŸ¼ $\tilde{w} = \lfloor\lfloor  w\rfloor + \delta\rceil, \delta \in [0, 1]$ ìµœì ì˜ Optimalí•œ ë°˜ì˜¬ë¦¼ ê°’ì„ êµ¬í•œë‹¤.
$$
\begin{aligned}
&argmin_V\lvert\lvert Wx-\tilde Wx\lvert\lvert ^2_F + \lambda f_{reg}(V) \\

\rightarrow & argmin_V\lvert\lvert Wx-\lfloor\lfloor W \rfloor + h(V) \rceil x\lvert\lvert ^2_F + \lambda f_{reg}(V) 
\end{aligned}
$$
### 2.2 Activation quantization
ë‘ ë²ˆì§¸ë¡œ Activation quantizationì´ ìˆë‹¤. ëª¨ë¸ê²°ê³¼ë¡œ ë‚˜ì˜¤ëŠ” ê²°ê³¼ë¥¼ ì§ì ‘ì ìœ¼ë¡œ ê²°ì •í•˜ëŠ” Activation Quatizationì—ì„œëŠ” ë‘ ê°€ì§€ë¥¼ ê³ ë ¤í•œ ë°©ë²•ì„ ì†Œê°œí•œë‹¤. í•˜ë‚˜ëŠ” Activation ë ˆì´ì–´ì—ì„œ ê²°ê³¼ê°’ì„ Smoothingí•œ ë¶„í¬ë¥¼ ê°€ì§€ê²Œ í•˜ê¸° ìœ„í•´ Exponential Moving Average(EMA)ë¥¼ ì‚¬ìš©í•˜ëŠ” ë°©ë²•ì´ê³ , ë‹¤ë¥¸ í•˜ë‚˜ëŠ” ë‹¤ì–‘í•œ ì…ë ¥ê°’ì„ ê³ ë ¤í•´ batch samplesì„ FP32 ëª¨ë¸ê³¼ calibrationí•˜ëŠ” ë°©ë²•ì´ë‹¤.

Exponential Moving Average (EMA)ì€ ì•„ë˜ ì‹ì—ì„œ $\alpha$ ë¥¼ êµ¬í•˜ëŠ” ë°©ë²•ì´ë‹¤.
$$
\hat r^{(t)}_{max, min} = \alpha r^{(t)}_{max, min} + (1-\alpha) \hat r^{(t)}_{max, min}  
$$
Calibrationì˜ ì»¨ì…‰ì€ ë§ì€ inputì˜ min/max í‰ê· ì„ ì´ìš©í•˜ìëŠ” ê²ƒì´ë‹¤. ê·¸ë˜ì„œ trained FP32 modelê³¼ sample batchë¥¼ ê°€ì§€ê³  quantizedí•œ ëª¨ë¸ì˜ ê²°ê³¼ì™€ calibrationì„ ëŒë¦¬ë©´ì„œ ê·¸ ì°¨ì´ë¥¼ ìµœì†Œí™” ì‹œí‚¤ëŠ”ë°, ì—¬ê¸°ì— ì´ìš©í•˜ëŠ” ì§€í‘œëŠ” loss of informationì™€ Newton-Raphson methodë¥¼ ì‚¬ìš©í•œ Mean Square Error(MSE)ê°€ ìˆë‹¤.
$$
MSE = \underset{\lvert r \lvert_{max}}{min}\ \mathbb{E}[(X-Q(X))^2]
$$
$$
KL\ divergence=D_{KL}(P\lvert\lvert Q) = \sum_i^N P(x_i)log\dfrac{P(x_i)}{Q(x_i)}
$$
### 2.3 Quanization Bias Correction

ë§ˆì§€ë§‰ìœ¼ë¡œ Quatizationìœ¼ë¡œ biased errorë¥¼ ì¡ëŠ”ë‹¤ëŠ” ê²ƒì„ ì†Œê°œí•œë‹¤. $\epsilon = Q(W)-W$ ì´ë¼ê³  ë‘ê³  ì•„ë˜ì²˜ëŸ¼ ì‹ì´ ì „ê°œì‹œí‚¤ë©´ ë§ˆì§€ë§‰ í•­ì—ì„œ ë³´ì´ëŠ” $-\epsilon\mathbb{E}[x]$ ë¶€ë¶„ì´ biasë¥¼ quatizationì„ í•  ë•Œ ì œê±° ëœë‹¤ê³  í•œë‹¤(ì´ ë¶€ë¶„ì€ 2023ë…„ì—ëŠ” ì†Œê°œí•˜ì§„ ì•ŠëŠ”ë°, ë‹¹ì—°í•œ ê²ƒì´ì–´ì„œ ì•ˆí•˜ëŠ”ì§€, í˜¹ì€ ì˜í–¥ì´ í¬ì§€ ì•Šì•„ì„œ ê·¸ëŸ°ì§€ëŠ” ëª¨ë¥´ê² ë‹¤. Bias Quatizationì´í›„ì— MobileNetV2ì—ì„œ í•œ ë ˆì´ì–´ì˜ outputì„ ë³´ë©´ ì–´ëŠì •ë„ ì œê±°ë˜ëŠ” ê²ƒì²˜ëŸ¼ ë³´ì¸ë‹¤).
$$
\begin{aligned}
\mathbb{E}[y] &= \mathbb{E}[Wx] + \mathbb{E}[\epsilon x] - \mathbb{E}[\epsilon x],\ \mathbb{E}[Q(W)x] = \mathbb{E}[Wx] + \mathbb{E}[\epsilon x] \\
\mathbb{E}[y] &= \mathbb{E}[Q(W)x] - \epsilon\mathbb{E}[x]
\end{aligned}
$$

![Reference. MIT-TinyML-lecture5-Quantization-2](../../images/lec05/1/quantization-bias-correction.png)

### 2.4 Post-Training INT8 Linear Quantization Result
ì•ì„  Post-Training Quantizationì„ ì ìš©í•œ ê²°ê³¼ë¥¼ ë³´ì—¬ì¤€ë‹¤. ì´ë¯¸ì§€ê³„ì—´ ëª¨ë¸ì„ ëª¨ë‘ ì‚¬ìš©í–ˆìœ¼ë©°, ì„±ëŠ¥í•˜ë½í­ì€ ì§€í‘œë¡œ ë³´ì—¬ì¤€ë‹¤. ë¹„êµì  í° ëª¨ë¸ë“¤ì˜ ê²½ìš° ì¤€ìˆ˜í•œ ì„±ëŠ¥ì„ ë³´ì—¬ì£¼ì§€ë§Œ MobileNetV1, V2ì™€ ê°™ì€ ì‘ì€ ëª¨ë¸ì€ ìƒê°ë³´ë‹¤ Quantizationìœ¼ë¡œ ë–¨ì–´ì§€ëŠ” ì„±ëŠ¥í­(-11.8%, -2.1%) ì´ í° ê²ƒì„ ë³¼ ìˆ˜ ìˆë‹¤. ê·¸ëŸ¼ ì‘ì€ í¬ê¸°ì˜ ëª¨ë¸ë“¤ì€ ì–´ë–»ê²Œ Training í•´ì•¼í• ê¹Œ?

![Reference. MIT-TinyML-lecture5-Quantization-2](../../images/lec05/1/post-training-result.png)

## 3. Quantization-Aware Training(QAT)

### 3.1 Quantization-Aware Training

![Reference. MIT-TinyML-lecture06-Quantization-2](../../images/lec05/2/Untitled.png)

- Usually, fine-tuning a pre-trained floating point model provides better accuracy than training from scratch.

ì´ì „ì— K-mean Quantizationì—ì„œ Fine-tuningë•Œ Centroidì— gradientë¥¼ ë°˜ì˜í–ˆì—ˆë‹¤. Quantization-Aware Trainingì€ ì´ì™€ ìœ ì‚¬í•˜ê²Œ Quantization - Reconstructionì„ í†µí•´ ë§Œë“¤ì–´ì§„ Weightë¡œ Trainingì„ í•˜ëŠ” ë°©ë²•ì„ ë§í•œë‹¤. ì˜ˆì‹œë¥¼ ë“¤ì–´ì„œ ìì„¸íˆ ì‚´í´ë³´ì.

![Reference. MIT-TinyML-lecture06-Quantization-2](../../images/lec05/2/Untitled%201.png)

- A full precision copy of the weights W is maintained throughout the training.
- **The small gradients are accumulated without loss of precision**
- Once the model is trained, only the quantized weights are used for inference

ìœ„ ê·¸ë¦¼ì—ì„œ Layer Nì´ ë³´ì¸ë‹¤. ì´ Layer Nì€ weightsë¥¼ íŒŒë¼ë¯¸í„°ë¡œ ê°€ì§€ì§€ë§Œ, ì‹¤ì œë¡œ Training ê³¼ì •ì—ì„œ ì“°ì´ëŠ” weightëŠ” â€œweight quantizationâ€ì„ í†µí•´ Quantization - Reconstructionì„ í†µí•´ ë§Œë“¤ì–´ì§„ Weightë¥¼ ê°€ì§€ê³  í›ˆë ¨ì„ í•  ê²ƒì´ë‹¤.

### 3.2 Straight-Through Estimator(STE)

![Reference. MIT-TinyML-lecture06-Quantization-2](../../images/lec05/2/Untitled%202.png)

ê·¸ëŸ¼ í›ˆë ¨ì—ì„œ gradientëŠ” ì–´ë–»ê²Œ ì „ë‹¬í•  ìˆ˜ ìˆì„ê¹Œ? Quantizationì˜ ê°œë…ìƒ, weight quantizationì—ì„œ weightë¡œ ë„˜ì–´ê°€ëŠ” gradientëŠ” ì—†ì„ ìˆ˜ ë°–ì— ì—†ë‹¤. ê·¸ë ‡ê²Œ ë˜ë©´ ì‚¬ì‹¤ìƒ weightë¡œ back propagationì´ ë  ìˆ˜ ì—†ê²Œ ë˜ê³ , ê·¸ë˜ì„œ ì†Œê°œí•˜ëŠ” ê°œë…ì´ **Straight-Through Estimator(STE)** ì…ë‹ˆë‹¤. ë§ì´ ê±°ì°½í•´ì„œ ê·¸ë ‡ì§€, Q(W)ì—ì„œ ë°›ì€ gradientë¥¼ ê·¸ëŒ€ë¡œ weights ë¡œ ë„˜ê²¨ì£¼ëŠ” ë°©ì‹ì´ë‹¤.

- Quantization is discrete-valued, and thus the derivative is 0 almost everywhere â†’ NN will learn nothing!
- **Straight-Through Estimator(STE) simply passes the gradients through the quantization as if it had been the identity function.**
    
    $$
    g_W = \dfrac{\partial L}{\partial  W} = \dfrac{\partial L}{\partial  Q(W)}
    $$
    

![Reference. MIT-TinyML-lecture06-Quantization-2](../../images/lec05/2/Untitled%203.png)

- Reference
    - Neural Networks for Machine Learning [HintonÂ *et al.*, Coursera Video Lecture, 2012]
    - Estimating or Propagating Gradients Through Stochastic Neurons for Conditional Computation [Bengio, arXiv 2013]

ì´ í›ˆë ¨ì˜ ê²°ê³¼ê°€ ê¶ê¸ˆí•˜ì‹œë‹¤ë©´ [ì´ ë…¼ë¬¸](https://arxiv.org/pdf/1806.08342.pdf)ì„ ì°¸ê³ í•˜ì. ì°¸ê³ ë¡œ ë…¼ë¬¸ì—ì„œëŠ” MobileNetV1, V2 ê·¸ë¦¬ê³  NASNet-Mobileì„ ì´ìš©í•´ Post-Training Quantizationê³¼ Quantization-Aware Trainingì„ ë¹„êµí•˜ê³  ìˆë‹¤.

## 4. Binary and Ternary Quantization

ì, ê·¸ëŸ¼ Quantizationì„ ê¶ê·¹ì ìœ¼ë¡œ 2bitë¡œ í•  ìˆ˜ëŠ” ì—†ì„ê¹Œ? ë°”ë¡œ Binary(1, -1)ê³¼ Tenary(1, 0, -1) ì´ë‹¤. 

- **Can we push the quantization precision to 1 bit?**
    
    ![Reference. MIT-TinyML-lecture06-Quantization-2](../../images/lec05/2/Untitled%204.png)
    
    
- Reference
    - BinaryConnect: Training Deep Neural Networks with Binary Weights during Propagations [CourbariauxÂ *et al.*, NeurIPS 2015]
    - XNOR-Net: ImageNet Classification using Binary Convolutional Neural Networks [RastegariÂ *et al.*, ECCV 2016]

ë¨¼ì € Weightë¥¼ 2bitë¡œ Quantizationì„ í•˜ê²Œ ë˜ë©´, ë©”ëª¨ë¦¬ì—ì„œëŠ” 32bitë¥¼ 1bitë¡œ ì¤„ì´ë‹ˆ 32ë°°ë‚˜ ì¤„ì¼ ìˆ˜ ìˆê³ , Computationë„ (8x5)+(-3x2)+(5x0)+(-1x1)ì—ì„œ 5-2+0-1 ë¡œ ì ˆë°˜ì„ ì¤„ì¼ ìˆ˜ ìˆë‹¤.

### 4.1 Binarization: **Deterministic Binarization**

ê·¸ëŸ¼ Binarizationì—ì„œ +1ê³¼ -1ì„ ì–´ë–¤ ê¸°ì¤€ìœ¼ë¡œ í•´ì•¼í• ê¹Œ? ê°€ì¥ ì‰¬ìš´ ë°©ë²•ì€ threholdë¥¼ ê¸°ì¤€ìœ¼ë¡œ +-1ë¡œ ë‚˜ëˆ„ëŠ” ê²ƒì´ë‹¤.

Directly computes the bit value base on a threshold, usually 0 resulting in a sign function.

$$
q = sign(r) = \begin{dcases}
+1, &r \geq 0 \\
-1, &r < 0
 \end{dcases}
$$

### 4.2 Binarization: **Stochastic Binarization**

ë‹¤ë¥¸ ë°©ë²•ìœ¼ë¡œëŠ” outputì—ì„œ hard-sigmoid functionì„ ê±°ì³ì„œ ë‚˜ì˜¨ ê°’ë§Œí¼ í™•ë¥ ì ìœ¼ë¡œ +-1ì´ ë‚˜ì˜¤ë„ë¡ í•˜ëŠ” ê²ƒì´ë‹¤. í•˜ì§€ë§Œ ì´ ë°©ë²•ì€ ë¬´ì‘ìœ„ë¡œ ë¹„íŠ¸ë¥¼ ìƒì„±í•˜ëŠ” í•˜ë“œì›¨ì–´ë¥¼ í•˜ëŠ” ê²ƒì´ ì–´ë µê¸° ë•Œë¬¸ì— ì‚¬ìš©í•˜ì§„ ì•ŠëŠ”ë‹¤ê³  ì–¸ê¸‰í•œë‹¤.

- Use global statistics or the value of input data to determine the probability of being -1 or +1
- In Binary Connect(BC), probability is determined by hard sigmoid function $\sigma(r)$
    
    $$
    q=\begin{dcases}
    +1, &\text{with probability } p=\sigma(r)\\
    -1, & 1-p
    \end{dcases}
    \\
    where\ \sigma(r)=min(max(\dfrac{r+1}{2}, 0), 1)
    $$
    
    ![Reference. MIT-TinyML-lecture06-Quantization-2](../../images/lec05/2/Untitled%205.png)
    
- **Harder to implement** as it requires the hardware to generate random bits when quantizing.

### 4.3 Binarization: Use Scale

ì•ì„  ë°©ë²•ì„ ì´ìš©í•´ì„œ ImageNet Top-1 ì„ í‰ê°€í•´ë³´ë©´ Quantizationì´í›„ -21.2%ë‚˜ ì„±ëŠ¥ì´ í•˜ë½í•˜ëŠ” ê±¸ ë³¼ ìˆ˜ ìˆë‹¤. â€œì–´ë–»ê²Œ ë³´ì™„í•  ìˆ˜ ìˆì„ê¹Œ?â€ í•œ ê²ƒì´ linear qunatizationì—ì„œ ì‚¬ìš©í–ˆë˜ Scale ê°œë…ì´ë‹¤.

- Using **Scale**, Minimizing Quantization Error in Binarization
    
    ![Reference. MIT-TinyML-lecture06-Quantization-2](../../images/lec05/2/Untitled%206.png)
    

ì—¬ê¸°ì„œ Scaleì€ $\dfrac{1}{n}\lvert\lvert W \lvert\lvert_1$ ë¡œ ê³„ì‚°í•  ìˆ˜ ìˆê³ , ì„±ëŠ¥ì€ í•˜ë½ì´ ê±°ì˜ ì—†ëŠ” ê²ƒë„ í™•ì¸í•  ìˆ˜ ìˆë‹¤.  ì™œ $\dfrac{1}{n}\lvert\lvert W \lvert\lvert_1$ì¸ì§€ëŠ” ì•„ë˜ ì¦ëª…ê³¼ì •ì„ ì°¸ê³ í•˜ì!

- Why $\alpha$  is $\dfrac{1}{n}\lvert\lvert W \lvert\lvert_1$?
    
    $$
    \begin{aligned}
    &J(B, \alpha)=\lvert\lvert W-\alpha B\lvert\lvert^2 \\
    &\alpha^*, B^*= \underset{\alpha, B}{argmin}\ J(B, \alpha) \\
    &J(B,\alpha) = \alpha^2B^TB-2\alpha W^T B + W^TW\ since\ B \in \{+1, -1\}^n \\
    &B^TB=n(constant), W^TW= constant(a \ known\ variable) \\
    &J(B,\alpha) = \alpha^2n-2\alpha W^T B + C \\
    &B^* = \underset{B}{argmax} \{W^T B\}\ s.t.\ B\in \{+1,-1 \}^n \\
    &\alpha^*=\dfrac{W^TB^*}{n} \\
    &\alpha^*=\dfrac{W^Tsign(W)}{n} = \dfrac{\lvert W_i \lvert}{n} = \dfrac{1}{n}\lvert\lvert W\lvert\lvert_{l1}
    \end{aligned}
    $$
    
    - Reference. XNOR-Net: ImageNet Classification using Binary Convolutional Neural Networks [RastegariÂ *et al.*, ECCV 2016]
    - B*ëŠ” J(B,$\alpha$)ì—ì„œ ìµœì†Ÿê°’ì„ êµ¬í•´ì•¼í•˜ë¯€ë¡œ $W^T$B ê°€ ìµœëŒ€ì—¬ì•¼í•˜ê³  ê·¸ëŸ¬ê¸° ìœ„í•´ì„œëŠ” Wê°€ ì–‘ìˆ˜ì¼ë•ŒëŠ” Bë„ ì–‘ìˆ˜, Wê°€ ìŒìˆ˜ì¼ ë•ŒëŠ” Bë„ ìŒìˆ˜ì—¬ì•¼ $W^TB=\sum\lvert W \lvert$ ì´ ë˜ë©´ì„œ ìµœëŒ“ê°’ì´ ë  ìˆ˜ ìˆë‹¤.

### 4.4 Binarization: Activation

ê·¸ëŸ¼ Activationê¹Œì§€ Quantizationì„ í•´ë´…ì‹œë‹¤.

**4.4.1 Activation**

![Untitled](../../images/lec05/2/Untitled%207.png)

ì—¬ê¸°ì„œ ì¡°ê¸ˆ ë” ì—°ì‚°ì„ ìµœì í™” í•  ìˆ˜ ìˆì–´ë³´ì´ëŠ” ê²ƒì´ Matrix Muliplicationì´ XOR ì—°ì‚°ê³¼ ë¹„ìŠ·í•˜ê²Œ ë³´ì¸ë‹¤. 

**4.4.2 XNOR bit count**

![Reference. MIT-TinyML-lecture06-Quantization-2](../../images/lec05/2/Untitled%208.png)

- $y_i=-n+ popcount(W_i\ xnor\ x) << 1$ â†’ **popcount returns the number of 1**

ê·¸ë˜ì„œ **popcount**ê³¼ **XNOR**ì„ ì´ìš©í•´ì„œ Computationì—ì„œ ì¢€ ë” ìµœì í™”ë¥¼ ì§„í–‰í•©ë‹ˆë‹¤. ì´ë ‡ê²Œ ìµœì í™”ë¥¼ ì§„í–‰í•˜ê²Œ ë˜ë©´, ë©”ëª¨ë¦¬ëŠ” 32ë°°, Computationì€ 58ë°°ê°€ëŸ‰ ì¤„ì–´ë“¤ ìˆ˜ ìˆë‹¤ê³  ë§í•œë‹¤.

![Reference. MIT-TinyML-lecture06-Quantization-2](../../images/lec05/2/Untitled%209.png)

ì´ë ‡ê²Œ Weight, Scale factor, Activation, ê·¸ë¦¬ê³  XNOR-Bitcout ê¹Œì§€. ì´ ë„¤ ê°€ì§€ ë‹¨ê³„ë¡œ Binary Quantizationì„ ë‚˜ëˆˆë‹¤.  ë‹¤ìŒìœ¼ë¡œëŠ” Ternary Quantizationì€ ì•Œì•„ë³´ì.

![Reference. XNOR-Net: ImageNet Classification using Binary Convolutional Neural Networks [RastegariÂ *et al.*, ECCV 2016]](../../images/lec05/2/Untitled%2010.png)

- (2) Binarizing Input ì˜ ê²½ìš°ëŠ” averageë¥¼ ëª¨ë“  channelì— ê°™ì´ ì ìš©í•  ê²ƒì´ê¸° ë•Œë¬¸ì— ê·¸ cë§Œí¼ì„ average filterë¡œ í•œ ë²ˆì— ì ìš©í•œë‹¤ëŠ” ë§ì´ë‹¤.

### 4.5 **Ternary Weight Networks(TWN)**

TernaryëŠ” Binary Quantizationê³¼ ë‹¨ê³„ëŠ” ëª¨ë‘ ê°™ì§€ë§Œ, ê°€ì§ˆ ìˆ˜ ìˆëŠ” ê°’ìœ¼ë¡œ **0** ì„ ì¶”ê°€í•œë‹¤. ì•„ë˜ ê·¸ë¦¼ì€ Scaleì„ ì´ìš©í•´ì„œ Quantization Errorë¥¼ ì¤„ì´ëŠ” ë°©ë²•ì„ ë§í•˜ê³  ìˆë‹¤.
$$
q = \begin{dcases}
r_t, &r > \Delta \\
0, &\lvert r\lvert \leq \Delta \\
-r_t, &r < -\Delta
 \end{dcases} \\
where\ \Delta = 0.7\times \mathbb{E}(\lvert r \lvert), r_t = \mathbb{E}_{\lvert r \lvert > \Delta}(\lvert r \lvert )
$$
![Reference. Trained Ternary Quantization [ZhuÂ *et al.*, ICLR 2017]](../../images/lec05/2/Untitled%2011.png)
### 4.6 Trained Ternary Quantization(TTQ)

Tenary Quantizationì—ì„œ ë˜ í•œê°€ì§€ ë‹¤ë¥´ê²Œ ì„¤ëª…í•˜ëŠ” ê²ƒì€ 1ê³¼ -1ë¡œë§Œ ì •í•´ì ¸ ìˆë˜ Binary Quantizationê³¼ ë‹¤ë¥´ê²Œ TenaryëŠ” 1, 0, -1ë¡œ Quantizationì„ í•œ í›„, ì¶”ê°€ì ì¸ í›ˆë ¨ì„ í†µí•´ $w_t$ì™€ $-w_t$ë¡œ fine-tuningì„ í•˜ëŠ” ë°©ë²•ë„ ì œì•ˆí•œë‹¤(í•´ë‹¹ [ë…¼ë¬¸](https://arxiv.org/pdf/1612.01064.pdf)ì—ì„œëŠ” ì´ëŸ¬í•œ ê¸°ë²•ì„ ì´ìš©í•´ì„œ í•œ ê²°ê³¼ë¥¼ CIFAR-10 ì´ë¯¸ì§€ ë°ì´í„°ë¥¼ ê°€ì§€ê³  ResNets, AlexNet, ImageNetì—ì„œ ë³´ì—¬ì¤€ë‹¤).
$$
q = \begin{dcases}
w_t, &r > \Delta \\
0, &\lvert r\lvert \leq \Delta \\
-w_t, &r < -\Delta
 \end{dcases}
$$
![Reference. Trained Ternary Quantization [ZhuÂ *et al.*, ICLR 2017]](../../images/lec05/2/Untitled%2012.png)

### 4.7 Accuracy Degradation

Binary, Ternary Quantizationì„ ì‚¬ìš©í•œ ê²°ê³¼ë¥¼ ë³´ì—¬ì¤€ë‹¤(Resnet-18 ê²½ìš°ì—ëŠ” Ternary ê°€ ì˜¤íˆë ¤ Binaryë³´ë‹¤ ì„±ëŠ¥ì´ ë” ë–¨ì–´ì§„ë‹¤!)

- **Binarization**
    
    ![Reference. Binarized Neural Networks: Training Deep Neural Networks with Weights and Activations Constrained to +1 orÂ âˆ’1. [CourbariauxÂ *et al.*, Arxiv 2016], XNOR-Net: ImageNet Classification using Binary Convolutional Neural Networks [RastegariÂ *et al.*, ECCV 2016]](../../images/lec05/2/Untitled%2013.png)

- **Ternary Weight Networks (TWN)**
    
    ![Reference. Ternary Weight Networks [LiÂ *et al.*, Arxiv 2016]](../../images/lec05/2/Untitled%2014.png)
    
- **Trained Ternary Quantization (TTQ)**
    
    ![Reference. Trained Ternary Quantization [ZhuÂ *et al.*, ICLR 2017]](../../images/lec05/2/Untitled%2015.png)

## 5. Low Bit-Width Quantization

ë‚¨ì€ ë¶€ë¶„ë“¤ì€ ì—¬ëŸ¬ê°€ì§€ ì‹¤í—˜ / ì—°êµ¬ë“¤ì„ ì†Œê°œí•˜ê³  ìˆë‹¤.

- Binary Quantizationì€ Quantization Aware Trainingì„ í•  ìˆ˜ ìˆì„ê¹Œ?
- 2,3 bitê³¼ 8bit ê·¸ ì¤‘ê°„ìœ¼ë¡œëŠ” Quantizationì„ í•  ìˆ˜ ì—†ì„ê¹Œ?
- ë ˆì´ì–´ì—ì„œ Quantizationì„ í•˜ì§€ ì•ŠëŠ” ë ˆì´ì–´, ì˜ˆë¥¼ ë“¤ì–´ ê²°ê³¼ì— ì˜í–¥ì„ ì˜ˆë¯¼í•˜ê²Œ ë¯¸ì¹˜ëŠ” ì²« ë²ˆì§¸ ë ˆì´ì–´ê°€ ê°™ì€ ê²½ìš° Quantizationì„ í•˜ì§€ ì•Šìœ¼ë©´ ì–´ë–»ê²Œ ë ê¹Œ?
- Activation í•¨ìˆ˜ë¥¼ ë°”ê¾¸ë©´ ì–´ë–¨ê¹Œ?
- ì˜ˆë¥¼ ë“¤ì–´ ì²«ë²ˆì§¸ ë ˆì´ì–´ì˜ Në°° ë„“ê²Œ í•˜ëŠ” ê²ƒê³¼ ê°™ì´ ëª¨ë¸ êµ¬ì¡°ë¥¼ ë°”ê¾¸ë©´ ì–´ë–»ê²Œ ë ê¹Œ?
- ì¡°ê¸ˆì”© Quantizationì„ í•  ìˆ˜ ì—†ì„ê¹Œ? (20% â†’ 40% â†’ â€¦ â†’ 100%)

ê°•ì˜ì—ì„œëŠ” í¬ê²Œ ì–¸ê¸‰í•˜ì§€ ì•Šê³  ê°„ ë‚´ìš©ë“¤ì´ë¼ ì„¤ëª…ì„ í•˜ì§€ëŠ” ì•Šê² ë‹¤. í•´ë‹¹ ë‚´ìš©ë“¤ì€ ìì„¸í•œ ë‚´ìš©ì„ ì•Œê³ ì‹¶ìœ¼ë©´ ê° íŒŒíŠ¸ì— ì–¸ê¸‰ëœ ë…¼ë¬¸ì„ ì°¸ì¡°í•˜ê¸¸!

### 5.1 Train Binarized Neural Networks From Scratch

- Straight-Through Estimator(STE)

![Reference. MIT-TinyML-lecture06-Quantization-2](../../images/lec05/2/Untitled%2016.png)

- Gradient pass straight to floating-point weights
- Floating-point weight with in [-1, 1]
- Reference. Binarized Neural Networks: Training Deep Neural Networks with Weights and Activations Constrained to +1 orÂ âˆ’1. [Courbariaux et al., Arxiv 2016]

### 5.2 Quantization-Aware Training: DoReFa-Net With Low Bit-Width Gradients

![Reference. MIT-TinyML-lecture06-Quantization-2](../../images/lec05/2/Untitled%2017.png)

- Gradient Quantization
    
    $$
    Q(g) = 2 \cdot max(\lvert G \lvert) \cdot \Large[ \small quantize_k \Large( \small \dfrac{g}{2\cdot max(\lvert G \lvert)} + \dfrac{1}{2} + N(k) \Large ) \small -\dfrac{1}{2} \Large]\small
    $$
    $$
    where\ N(k)=\dfrac{\sigma}{2^k-1} and\ \sigma \thicksim Uniform(-0.5, 0.5)
    $$
    
    - Noise function $N(k)$ is added to compensate the potential bias introduced by gradient quantization.
- Result
    
    ![Reference. DoReFa-Net: Training Low Bitwidth Convolutional Neural Networks with Low Bitwidth Gradients [ZhouÂ *et al.*, arXiv 2016]](../../images/lec05/2/Untitled%2018.png)

### 5.3 Replace the Activation Function: Parameterized Clipping Activation Function

- The most common activation function ReLU is unbounded. The dynamic range of inputs becomes problematic for low bit-width quantization due to very limited range and resolution.
- ReLU is replaced with hard-coded bounded activation functions: ReLU6, ReLU1, etc
- The clipping value per layer can be learned as well: PACT(Parametrized Clipping Activation Function)
    
    ![Reference. PACT: Parameterized Clipping Activation for Quantized Neural Networks [ChoiÂ *et al.*, arXiv 2018]](../../images/lec05/2/Untitled%2019.png)
    $$
    y=PACT(x;\alpha) = 0.5(\lvert x \lvert - \lvert x -\alpha \lvert + \alpha ) = \begin{dcases} 
    0, & x \in [-\infty, 0) \\
    x, & x \in [0, \alpha) \\
    \alpha, & x \in [\alpha, +\infty)
    \end{dcases}
    $$
    
    The upper clipping value of the activation function is a trainable. With STE, the gradient is computed as
    
    $$
    \dfrac{\partial Q(y)}{\partial \alpha} = \dfrac{\partial Q(y)}{\partial y} \cdot \dfrac{\partial y}{\partial \alpha} = \begin{dcases}
    0 & x \in (-\infty, \alpha)\\
    1 & x \in [\alpha, +\infty)\\
    \end{dcases}
    $$
    
    $$
    \rightarrow
    \dfrac{\partial L}{\partial \alpha} = \dfrac{\partial L}{\partial Q(y)} \cdot \dfrac{\partial Q(y)}{\partial \alpha} = \begin{dcases}
    0 & x \in (-\infty, \alpha)\\
    \frac{\partial L}{\partial Q(y)} & x \in [\alpha, +\infty)\\
    \end{dcases}
    $$
    
    The larger $\alpha$, the more the parameterized clipping function resembles a ReLU function
    
    - **To avoid large quantization errors due to a wide dynamic range $[0, \alpha]$, L2-regularizer for $\alpha$** is included in the training loss function.
- Result
    
    ![Reference. PACT: Parameterized Clipping Activation for Quantized Neural Networks [ChoiÂ *et al.*, arXiv 2018]](../../images/lec05/2/Untitled%2020.png)
        
### 5.4 Modify the Neural Network Architecture

1. Widen the neural network to compensate for the loss of information due to quantization
    
    ex. Double the channels, reduce the quantization precision
    
    ![Reference. WRPN: Wide Reduced-Precision Networks [MishraÂ *et al.*, ICLR 2018]](../../images/lec05/2/Untitled%2021.png)
        
2. Replace a single floating-point convolution with multiple binary convolutions.
    - Towards Accurate Binary Convolutional Neural Network [LinÂ *et al.*, NeurIPS 2017]
    - Quantization [Neural Network Distiller]

### 5.5 No Quantization on First and Last Layer

- Because it is more **sensitive** to quantization and **small portion** of the overall computation
- Quantizing these layers to 8-bit integer does not reduce accuracy

### 5.6 Iterative Quantization: Incremental Network Quantization

- Reference. Incremental Network Quantization: Towards Lossless CNNs with Low-precision Weights [ZhouÂ *et al.*, ICLR 2017]

![Reference. Incremental Network Quantization: Towards Lossless CNNs with Low-precision Weights [ZhouÂ *et al.*, ICLR 2017]](../../images/lec05/2/Untitled%2022.png)

- Setting
    - Weight quantization only
    - Quantize weights to $2^n$ for faster computation (**bit shift** instead of multiply)

- Algorithm
    - Start from a pre-trained fp32 model
    - For the remaining fp32 weights
        - Partition into two disjoint groups(e.g., according to magnitude)
        - Quantize the first group (higher magnitude), and re-train the other group to recover accuracy
    - Repeat until all the weights are quantized (a popular stride is {50%, 75%, 87.5%, 100%})
    
    ![Reference. MIT-TinyML-lecture06-Quantization-2](../../images/lec05/2/Iterative-Quantization.gif)    

## 6. Mixed-precision quantization

ë§ˆì§€ë§‰ìœ¼ë¡œ ë ˆì´ì–´ë§ˆë‹¤ Quantization bitë¥¼ ë‹¤ë¥´ê²Œ ê°€ì ¸ê°€ë©´ ì–´ë–¨ì§€ì— ëŒ€í•´ì„œ ì´ì•¼ê¸°í•œë‹¤. í•˜ì§€ë§Œ ê²½ìš°ì˜ ìˆ˜ê°€ 8bit ë³´ë‹¤ ì‘ê±°ë‚˜ ê°™ê²Œ Quantizationì„ í•  ì‹œ, weightì™€ activationë¡œ ê²½ìš°ì˜ ìˆ˜ë¥¼ ê³ ë ¤ë¥¼ í•œë‹¤ë©´ Nê°œ ë ˆì´ì–´ì— ëŒ€í•´ì„œ $(8 \times 8)^N$ë¼ëŠ” ì–´ë§ˆì–´ë§ˆí•œ ê²½ìš°ì˜ ìˆ˜ê°€ ë‚˜ì˜¨ë‹¤. ê·¸ë¦¬ê³  ì´ì— ëŒ€í•´ì„œëŠ” ë‹¤ìŒ íŒŒíŠ¸ì— ë‚˜ê°ˆ Neural Architecture Search(NAS) ì—ì„œ ë‹¤ë£° ë“¯ ì‹¶ë‹¤.

### 6.1 Uniform Quantization

![](../../images/lec05/2/Untitled%2023.png)

### 6.2 Mixed-precision Quantization

![](../../images/lec05/2/Untitled%2024.png)

### 6.3 Huge Design Space and Solution: Design Automation

![](../../images/lec05/2/Untitled%2025.png)

- Design Space: Each of Choices(8x8=64) â†’ $64^n$
    
    ![Reference. HAQ: Hardware-Aware Automated Quantization with Mixed Precision [WangÂ *et al.*, CVPR 2019]](../../images/lec05/2/Untitled%2026.png)
        
- Result in Mixed-Precision Quantized MobileNetV1
    
    ![Reference. HAQ: Hardware-Aware Automated Quantization with Mixed Precision [WangÂ *et al.*, CVPR 2019]](../../images/lec05/2/Untitled%2027.png)
        
    - This paper compares with Model size, Latency and Energy

ê°€ì¥ ë§ˆì§€ë§‰ì— ì–¸ê¸‰í•˜ëŠ” Edgeì™€ í´ë¼ìš°ë“œì—ì„œëŠ” Convolution ë ˆì´ì–´ì˜ ì¢…ë¥˜ ì¤‘ ë”í•˜ê³  ëœ Quantizationí•˜ëŠ” ë ˆì´ì–´ê°€ ê°ê° depthwiseì™€ pointwiseë¡œ ë‹¤ë¥´ë‹¤ê³  ì´ì•¼ê¸°í•œë‹¤. ì´ ë‚´ìš©ì— ëŒ€í•´ì„œ ë” ìì„¸íˆ ì´í•´í•˜ê¸° ìœ„í•´ì„œëŠ” ì•„ë§ˆë„ NASë¡œ ë„˜ì–´ê°€ë´ì•¼ ì•Œ ìˆ˜ ìˆì§€ ì•Šì„ê¹Œ ì‹¶ë‹¤.

- Quantization Policy for Edge and Cloud
    
    ![Reference. HAQ: Hardware-Aware Automated Quantization with Mixed Precision [WangÂ *et al.*, CVPR 2019]](../../images/lec05/2/Untitled%2028.png)
    
## 7. Reference
- [TinyML and Efficient Deep Learning Computing on MIT HAN LAB](https://efficientml.ai/)
- [Youtube for TinyML and Efficient Deep Learning Computing on MIT HAN LAB](https://www.youtube.com/playlist?list=PL80kAHvQbh-ocildRaxjjBy6MR1ZsNCU7)
- [Deep Compression [HanÂ et al., ICLR 2016]](https://arxiv.org/abs/1510.00149)
- [Quantization and Training of Neural Networks for Efficient Integer-Arithmetic-Only Inference [JacobÂ et al., CVPR 2018]](https://arxiv.org/pdf/1712.05877.pdf)
- [With Shared Microexponents, A Little Shifting Goes a Long Way [Bita Rouhani et al.]](https://arxiv.org/pdf/2302.08007.pdf)
- [Data-Free Quantization Through Weight Equalization and Bias Correction [Markus et al., ICCV 2019]](https://arxiv.org/pdf/1906.04721.pdf)
- [Estimating or Propagating Gradients Through Stochastic Neurons for Conditional Computation [Bengio, arXiv 2013]](https://arxiv.org/pdf/1308.3432.pdf)
- [Binarized Neural Networks: Training Deep Neural Networks with Weights and Activations Constrained to +1 orÂ âˆ’1. [CourbariauxÂ et al., Arxiv 2016]](https://arxiv.org/pdf/1602.02830.pdf)
- [XNOR-Net: ImageNet Classification using Binary Convolutional Neural Networks [RastegariÂ et al., ECCV 2016]](https://arxiv.org/pdf/1603.05279.pdf)
- [Ternary Weight Networks [LiÂ et al., Arxiv 2016]](https://arxiv.org/pdf/1605.04711.pdf)
- [Trained Ternary Quantization [ZhuÂ et al., ICLR 2017]](https://arxiv.org/pdf/1612.01064.pdf)
- [DoReFa-Net: Training Low Bitwidth Convolutional Neural Networks with Low Bitwidth Gradients [ZhouÂ et al., arXiv 2016]](https://arxiv.org/pdf/1606.06160.pdf)
- [WRPN: Wide Reduced-Precision Networks [MishraÂ et al., ICLR 2018]](https://arxiv.org/pdf/1709.01134.pdf)
- [PACT: Parameterized Clipping Activation for Quantized Neural Networks [ChoiÂ et al., arXiv 2018]](https://arxiv.org/pdf/1805.06085.pdf)
- [HAQ: Hardware-Aware Automated Quantization with Mixed Precision [WangÂ et al., CVPR 2019]](https://arxiv.org/pdf/1811.08886.pdf)