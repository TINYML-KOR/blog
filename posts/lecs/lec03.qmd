---
toc: true
title: "ğŸ§‘â€ğŸ« Lecture 3"
description:  Pruning and Sparsity (Part I)
author: "Seunghyun Oh"
date: "2024-01-28"
categories: [pruning, lecture]
---

<style>
p {
  line-height: 2; /* Adjust the line-height property as needed */
  text-align: justify;
  text-justify: inter-word; /* Adjust the justification mode as needed */
}

.image-container {
  text-align: center; /* Center-align the contents horizontally */
}

.centered-text {
  text-align: center; /* Center-align the text within the container */
}
</style>

ì•ìœ¼ë¡œ ì´ 5ì¥ì— ê±¸ì³ì„œ ë”¥ëŸ¬ë‹ ëª¨ë¸ ê²½ëŸ‰í™” ê¸°ë²•ë“¤ì— ëŒ€í•´ì„œ ì†Œê°œí•˜ë ¤ê³  í•œë‹¤. ê²½ëŸ‰í™” ê¸°ë²•ìœ¼ë¡œëŠ” Pruning, Quantization, Neural Network Architecture Search, Knowledge Distillation, ê·¸ë¦¬ê³  Tiny Engineì—ì„œ ëŒë¦¬ê¸° ìœ„í•œ ë°©ë²•ì„ ì§„í–‰í•  ì˜ˆì •ì¸ë° ë³¸ ë‚´ìš©ì€ **MITì—ì„œ Song Han êµìˆ˜ë‹˜ì´ Fall 2022ì— í•œ ê°•ì˜ TinyML and Efficient Deep Learning Computing 6.S965**ë¥¼ ë°”íƒ•ìœ¼ë¡œ ì¬ì •ë¦¬í•œ ë‚´ìš©ì´ë‹¤.  ê°•ì˜ ìë£Œì™€ ì˜ìƒì€ ì´ [ë§í¬](https://efficientml.ai)ë¥¼ ì°¸ì¡°í•˜ì! 

ì²« ë²ˆì§¸ ë‚´ìš©ìœ¼ë¡œ **â€œê°€ì§€ì¹˜ê¸°â€**ë¼ëŠ” ì˜ë¯¸ë¥¼ ê°€ì§„ **Pruning**ì— ëŒ€í•´ì„œ ì´ì•¼ê¸°, ì‹œì‘!

## 1. Introduction to Pruning

**Pruning**ì´ë€ ì˜ë¯¸ì²˜ëŸ¼ Neural Networkì—ì„œ ë§¤ê°œë³€ìˆ˜(ë…¸ë“œ)ë¥¼ ì œê±°í•˜ëŠ” ë°©ë²•ì…ë‹ˆë‹¤. ì´ëŠ” Dropoutí•˜ê³  ë¹„ìŠ·í•œ ì˜ë¯¸ë¡œ ë³¼ ìˆ˜ ìˆëŠ”ë°, Dropoutì˜ ê²½ìš° ëª¨ë¸ í›ˆë ¨ ë„ì¤‘ ëœë¤ì ìœ¼ë¡œ íŠ¹ì • ë…¸ë“œë¥¼ ì œì™¸ì‹œí‚¤ê³  í›ˆë ¨ì‹œì¼œ ëª¨ë¸ì˜ Robustnessë¥¼ ë†’ì´ëŠ” ë°©ë²•ìœ¼ë¡œ í›ˆë ¨ì„ í•˜ê³ ë‚˜ì„œë„ ëª¨ë¸ì˜ ë…¸ë“œëŠ” ê·¸ëŒ€ë¡œ ìœ ì§€ê°€ ëœë‹¤. ë°˜ë©´ Pruningì˜ ê²½ìš° í›ˆë ¨ì„ ë§ˆì¹œ í›„ì—, íŠ¹ì • Threshold ì´í•˜ì˜ ë§¤ê°œë³€ìˆ˜(ë…¸ë“œ)ì˜ ê²½ìš° ì‹œ Neural Networkì—ì„œ ì œì™¸ì‹œì¼œ ëª¨ë¸ì˜ í¬ê¸°ë¥¼ ì¤„ì´ë©´ì„œ ë™ì‹œì— ì¶”ë¡  ì†ë„ ë˜í•œ ë†’ì¼ ìˆ˜ ìˆë‹¤.

$$
\underset{W_p}{argmin}\ L(x;W_p), \text{ subject to } \lvert\lvert W_p\lvert\lvert_0\ < N
$$

- **L represents the objective function for neural network training**
- $x$ is input**, $W$** is original weights**, $W_p$** is pruned weights
- $\lvert\lvert W_p\lvert\lvert_0$ calcuates the #nonzeros in $W_p$ and $N$ is the target #nonzeros

ì´ëŠ” ìœ„ì™€ ê°™ì€ ì‹ìœ¼ë¡œ í‘œí˜„í•  ìˆ˜ ìˆë‹¤. íŠ¹ì • W ì˜ ê²½ìš° 0 ìœ¼ë¡œ ë§Œë“¤ì–´ ë…¸ë“œë¥¼ ì—†ì• ëŠ” ê²½ìš°ë¼ê³  ë³¼ ìˆ˜ ìˆê² ìŠµë‹ˆë‹¤. ê·¸ë ‡ê²Œ Pruningí•œ Neural NetworkëŠ” ì•„ë˜ ê·¸ë¦¼ ì²˜ëŸ¼ ëœë‹¤.

<div style="text-align: center;">
![](../../images/lec03/Untitled.png)
  <p style="text-align: center;">Reference. MIT-TinyML-lecture03-Pruning-1</p>
</div>

ê·¸ëŸ¼ ì™œ Pruningì„ í•˜ëŠ” ê±¸ê¹Œ? ê°•ì˜ì—ì„œ Pruningì„ ì‚¬ìš©í•˜ë©´ Latency, Memeoryì™€ ê°™ì€ ë¦¬ì†ŒìŠ¤ë¥¼ í™•ë³´í•  ìˆ˜ ìˆë‹¤ê³  ê´€ë ¨ëœ ì•„ë˜ê°™ì€ ì—°êµ¬ê²°ê³¼ë¥¼ ê°™ì´ ë³´ì—¬ì¤€ë‹¤.

![Reference. MIT-TinyML-lecture03-Pruning-1](../../images/lec03/Untitled%201.png)

Song Han êµìˆ˜ë‹˜ì€ Vision ë”¥ëŸ¬ë‹ ëª¨ë¸ ê²½ëŸ‰í™” ì—°êµ¬ë¥¼ ì£¼ë¡œí•˜ì…”ì„œ, CNNì„ ê¸°ë°˜ìœ¼ë¡œ í•œ ëª¨ë¸ì„ ì˜ˆì‹œë¡œ ë³´ì—¬ì£¼ì‹ ë‹¤. ëª¨ë‘ Pruningì´í›„ì— ëª¨ë¸ ì‚¬ì´ì¦ˆì˜ ê²½ìš° ìµœëŒ€ 12ë°° ì¤„ì–´ ë“¤ë©° ì—°ì‚°ì˜ ê²½ìš° 6.3ë°°ê¹Œì§€ ì¤„ì–´ ë“  ê²ƒì„ ë³¼ ìˆ˜ ë‹¤. 

ê·¸ë ‡ë‹¤ë©´ ì €ë ‡ê²Œ â€œ**í¬ê¸°ê°€ ì¤„ì–´ë“  ëª¨ë¸ì´ ì„±ëŠ¥ì„ ìœ ì§€í•  ìˆ˜ ìˆì„ê¹Œ?"**

![Reference. MIT-TinyML-lecture03-Pruning-1](../../images/lec03/Untitled%202.png)

ê·¸ë˜í”„ì—ì„œ ëª¨ë¸ì˜ Weight ë¶„í¬ë„ë¥¼ ìœ„ ê·¸ë¦¼ì—ì„œ ë³´ë©´, Pruningì„ í•˜ê³  ë‚œ ì´í›„ì— Weight ë¶„í¬ë„ì˜ ì¤‘ì‹¬ì— íŒŒë¼ë¯¸í„°ê°€ ì˜ë ¤ë‚˜ê°„ ê²Œ ë³´ì¸ë‹¤. ì´í›„ Fine Tuningì„ í•˜ê³  ë‚œ ë‹¤ìŒì˜ ë¶„í¬ê°€ ë‚˜ì™€ ìˆëŠ”ë°, ì–´ëŠ ì •ë„ ì •í™•ë„ëŠ” ë–¨ì–´ì§€ì§€ë§Œ ì„±ëŠ¥ì´ ìœ ì§€ë˜ëŠ” ê±¸ ê´€ì°°í•  ìˆ˜ ìˆë‹¤.

![Reference. MIT-TinyML-lecture03-Pruning-1](../../images/lec03/Untitled%203.png)

ê·¸ëŸ° Fine tuningì„ ë°˜ë³µì ìœ¼ë¡œ í•˜ê²Œ ëœë‹¤ë©´(Iterative Pruning and Fine tuning) ê·¸ë˜í”„ì—ì„œëŠ” ìµœëŒ€ 90í”„ë¡œ ì´ìƒì˜ íŒŒë¼ë¯¸í„°ë¥¼ ëœì–´ë‚¼ ìˆ˜ ìˆë‹¤ê³  í•œë‹¤.

ë¬¼ë¡  íŠ¹ì • ëª¨ë¸ì—ì„œ, íŠ¹ì • Taskë¥¼ ëŒ€ìƒìœ¼ë¡œ í•œ ê²ƒì´ë¼ ì¼ë°˜í™”í•  ìˆ˜ëŠ” ì—†ì§€ë§Œ **ë¦¬ì†ŒìŠ¤ë¥¼ ê³ ë ¤í•˜ëŠ” ìƒí™©**ì´ë¼ë©´ ì¶©ë¶„íˆ ì‹œë„í•´ë³¼ ë§Œí•œ ê°€ì¹˜ê°€ ìˆì–´ ë³´ì¸ë‹¤. ê·¸ëŸ¼ ì´ë ‡ê²Œ ì„±ëŠ¥ì„ ìœ ì§€í•˜ë©´ì„œ Pruningì„ í•˜ê¸° ìœ„í•´ì„œ ì–´ë–¤ ìš”ì†Œë¥¼ ê³ ë ¤í•´ì•¼ í• ì§€ ë” ìì„¸íˆ ì´ì•¼ê¸°í•´ë³´ì!

ì†Œê°œí•˜ëŠ” ê³ ë ¤ìš”ì†ŒëŠ” ì•„ë˜ì™€ ê°™ë‹¤. Pruning íŒ¨í„´ë¶€í„° ì°¨ë¡€ëŒ€ë¡œ ì‹œì‘!

- Pruning Granularity â†’ Pruning íŒ¨í„´
- Pruning Criterion â†’ ì–¼ë§ˆë§Œí¼ì— íŒŒë¼ë¯¸í„°ë¥¼ Pruning í•  ê±´ê°€?
- Pruning Ratio â†’ ì „ì²´ íŒŒë¼ë¯¸í„°ì—ì„œ Pruningì„ ì–¼ë§ˆë§Œí¼ì˜ ë¹„ìœ¨ë¡œ?
- Fine Turning â†’ Pruning ì´í›„ì— ì–´ë–»ê²Œ Fine-Tuning í•  ê±´ê°€?
- ADMM â†’ Pruning ì´í›„, ì–´ë–»ê²Œ Convexê°€ ëœë‹¤ê³  í•  ìˆ˜ ìˆì§€?
- Lottery Ticket Hypothesis â†’ Trainingë¶€í„° Pruningê¹Œì§€ ëª¨ë¸ì„ ë§Œë“¤ì–´ ë³´ì!
- System Support â†’ í•˜ë“œì›¨ì–´ë‚˜ ì†Œí”„íŠ¸ì›¨ì–´ì ìœ¼ë¡œ Pruningì„ ì§€ì›í•˜ëŠ” ê²½ìš°ëŠ”?

## 2. Determine the Pruning Granularity

![The case of convolutional layers, red box is preserved and white one is pruned referred from MIT-TinyML-lecture03-Pruning-1](../../images/lec03/Untitled%204.png)

ì—¬ê¸°ì„œ ê³ ë ¤ìš”ì†ŒëŠ” â€œì–¼ë§ˆë§Œí¼ ë‰´ëŸ°ì„ ê·¸ë£¹í™”í•˜ì—¬ ê³ ë ¤í•  ê²ƒì¸ê°€?â€ ì…ë‹ˆë‹¤. Regularí•œ ì •ë¡œë„ ë¶„ë¥˜í•˜ë©´ì„œ Irregularí•œ ê²½ìš°ì™€ Regularí•œ ê²½ìš°ì˜ íŠ¹ì§•ì„ ì•„ë˜ì²˜ëŸ¼ ë§í•©ë‹ˆë‹¤.

- Fine-grained/Unstructured
    - More flexible pruning index choice
    - Hard to accelerate (irregular data expression)
    - Can deliver speed up on some custom hardware

- Coarse-grained/Structured
    - Less flexible pruning index choice (a subset of the fine-grained case)
    - Easy to accelerate

Pruningì„ í•œë‹¤ê³  ëª¨ë¸ ì¶œë ¥ì´ ë‚˜ì˜¤ëŠ” ì‹œê°„ì´ ì§§ì•„ì§€ëŠ” ê²ƒì´ ì•„ë‹˜ë„ ì–¸ê¸‰í•©ë‹ˆë‹¤. Hardware Accelerationì˜ ê°€ëŠ¥ë„ê°€ ìˆëŠ”ë°, ì´ íŠ¹ì§•ì„ ë³´ë©´ ì•Œ ìˆ˜ ìˆë“¯, Pruningì˜ ììœ ë„ì™€ Hardware Accelerationì´ trade-off, **ì¦‰ ê²½ëŸ‰í™” ì •ë„ì™€ Latencyì‚¬ì´ì— trade-off** ê°€ ìˆì„ ê²ƒì´ ì˜ˆì¸¡ë©ë‹ˆë‹¤. í•˜ë‚˜ì”©, ìë£Œë¥¼ ë³´ë©´ì„œ ì‚´í´ ë³´ê² ìŠµë‹ˆë‹¤.

### 2.1 Pattern-based Pruning

Irregularì—ì„œë„ Pattern-based Pruningì€ **ì—°ì†ì ì¸ ë‰´ëŸ° Mê°œ ì¤‘ Nê°œë¥¼ Pruning í•˜ëŠ” ë°©ë²•ì´ë‹¤**. ì¼ë°˜ì ìœ¼ë¡œëŠ” N:M = 2:4 ìœ¼ë¡œ í•œë‹¤ê³  ì†Œê°œí•œë‹¤.

![Reference. Accelerating Inference with Sparsity Using the NVIDIA Ampere Architecture and NVIDIA TensorRT](../../images/lec03/Untitled%205.png)

Reference. Accelerating Inference with Sparsity Using the NVIDIA Ampere Architecture and NVIDIA TensorRT

ì˜ˆì‹œë¥¼ ë“¤ì–´ ë³´ë©´, ìœ„ì™€ ê°™ì€ Matrixì—ì„œ í–‰ì„ ë³´ì‹œë©´ 8ê°œì˜ Weightì¤‘ 4ê°œê°€ Non-zeroì¸ ê²ƒì„ ë³¼ ìˆ˜ ìˆìŠµë‹ˆë‹¤. ì—¬ê¸°ì„œ Zeroì¸ ë¶€ë¶„ì„ ì—†ì• ê³  2bit indexë¡œ í•˜ì—¬ Matrix ì—°ì‚°ì„ í•˜ë©´  Nvidiaâ€™s Ampere GPUì—ì„œ ì†ë„ë¥¼ 2ë°°ê¹Œì§€ ë†’ì¼ ìˆ˜ ìˆë‹¤ê³  í•œë‹¤. ì—¬ê¸°ì„œ **Sparsity**ëŠ” â€œì–¼ë§ˆë§Œí¼ ê²½ëŸ‰í™” ëëŠ”ì§€?â€ ì´ë¼ê³  ìƒê°í•˜ë©´ ëœë‹¤.

- **N:M sparsity** means that in each **contiguous M elements**, **N of them is pruned**
- A classic case is 2:4 sparsity (50% sparsity)
- It is supported by Nvidiaâ€™s Ampere GPU Architecture, which delivers up to 2x speed up and usually maintains accuracy.
    
![Reference. [https://developer.nvidia.com/blog/accelerating-inference-with-sparsity-using-ampere-and-tensorrt/](https://developer.nvidia.com/blog/accelerating-inference-with-sparsity-using-ampere-and-tensorrt/)](../../images/lec03/Untitled%206.png)

### 2.2 Channel-level Pruning

ë°˜ëŒ€ë¡œ íŒ¨í„´ì´ ìƒëŒ€ì ìœ¼ë¡œ regular í•œ ìª½ì¸  Channel-level Pruningì€ ì¶”ë¡ ì‹œê°„ì„ ì¤„ì¼ ìˆ˜ ìˆëŠ” ë°˜ë©´ì— ê²½ëŸ‰í™” ë¹„ìœ¨ì´ ì ë‹¤ê³  ë§í•œë‹¤. ì•„ë˜ ê·¸ë¦¼ì„ ë³´ì‹œë©´ Layerë§ˆë‹¤ Sparsityê°€ ë‹¤ë¥¸ ê±¸ ë³´ì‹¤ ìˆ˜ ìˆë‹¤. 

- Pro: Direct speed up!
- Con: smaller compression ratio

![Reference AMC: Automl for Model Compression and Acceleration on Mobile Devices [HeÂ *et al.*, ECCV 2018]](../../images/lec03/Untitled%207.png)

ì•„ë˜ì— ìë£Œì—ì„œëŠ” Channel ë³„ë¡œ í•œ Pruningì˜ ê²½ìš° ì „ì²´ ë‰´ë ¨ì„ ê°€ì§€ê³  í•œ Pruningë³´ë‹¤ ì¶”ë¡  ì‹œê°„ì„ ë” ì¤„ì¼ ìˆ˜ ìˆë‹¤ê³  ë§í•œë‹¤.

![Reference AMC: Automl for Model Compression and Acceleration on Mobile Devices [HeÂ *et al.*, ECCV 2018]](../../images/lec03/Untitled%208.png)

ìë£Œë¥¼ ë³´ë©´ **Sparsityì—ì„œëŠ” íŒ¨í„´í™” ë¼ ìˆìœ¼ë©´** **ê°€ì†í™”**ê°€ ìš©ì´í•´ **Latency, ì¶”ë¡  ì‹œê°„**ì„ ì¤„ì¼ ìˆ˜ ìˆì§€ë§Œ ê·¸ ë§Œí¼ Pruningí•˜ëŠ” ë‰´ëŸ°ì˜ ìˆ˜ê°€ ì ì–´ ê²½ëŸ‰í™” ë¹„ìœ¨ì´ ì¤„ ê²ƒìœ¼ë¡œ ë³´ì¸ë‹¤. í•˜ì§€ë§Œ **ë¹„êµì  ë¶ˆê·œì¹™í•œ ìª½ì— ì†í•˜ëŠ” Pattern-based Pruningì˜ ê²½ìš°**ê°€ **í•˜ë“œì›¨ì–´ì—ì„œ ì§€ì›**í•´ì£¼ëŠ” ê²½ìš°, **ëª¨ë¸ í¬ê¸°ì™€ Latencyë¥¼ ë‘˜ ë‹¤** ìµœì ìœ¼ë¡œ ì¡ì„ ìˆ˜ ìˆì„ ê²ƒìœ¼ë¡œ ë³´ì¸ë‹¤.

## 3. Determine the Pruning Criterion

ê·¸ë ‡ë‹¤ë©´ ì–´ë–¤ íŒŒë¼ë¯¸í„°ë¥¼ ê°€ì§€ëŠ” ë‰´ëŸ°ì„ ìš°ë¦¬ëŠ” ì˜ë¼ë‚´ì•¼ í• ê¹Œìš”? Synapseì™€ Neuronìœ¼ë¡œ ë‚˜ëˆ ì„œ ì‚´í´ë³´ì.

- Which synapses? Which neurons? **Which one is less important?**
- **How to Select Synapses and Select Neurons to Prune**

### 3.1 **Select of Synapses**

í¬ê²Œ ì„¸ ê°€ì§€ë¡œ ë¶„ë¥˜í•˜ëŠ”ë°, ê° ë‰´ëŸ°ì˜ í¬ê¸°, ê° ì±„ë„ì— ì „ì²´ ë‰´ëŸ°ì— ëŒ€í•œ í¬ê¸°, ê·¸ë¦¬ê³  í…Œì¼ëŸ¬ ê¸‰ìˆ˜ë¥¼ ì´ìš©í•˜ì—¬ gradientì™€ weightë¥¼ ëª¨ë‘ ê³ ë ¤í•œ í¬ê¸°ë¥¼ ì†Œê°œí•œë‹¤. Song han êµìˆ˜ë‹˜ì´ ë°©ë²•ë“¤ì„ ì†Œê°œí•˜ê¸°ì— ì•ì„œì„œ ìœ ìˆ˜ì˜ ê¸°ì—…ë“¤ë„ ì§€ë‚œ 5ë…„ ë™ì•ˆ ì£¼ë¡œ **Magnitude-based Pruning**ë§Œì„ ì‚¬ìš©í•´ì™”ë‹¤ê³  í•˜ëŠ”ë°, 2023ë…„ì´ ë¼ì„œ On-device AIê°€ ê°ê´‘ë°›ê¸° ì‹œì‘í•´ì„œ ì ì°¨ì ìœ¼ë¡œ ê´€ì‹¬ì„ ë°›ê¸° ì‹œì‘í•œ ê±´ê°€ ì‹¶ê¸°ë„ í•˜ë‹¤.

**3.1.1 Magnitude-based Pruning**

í¬ê¸°ë¥¼ ê¸°ì¤€ìœ¼ë¡œ í•˜ëŠ” ê²½ìš°, **â€œì–¼ë§ˆë§Œí¼ ë‰´ëŸ° ê·¸ë£¹ì—ì„œ ê³ ë ¤í•  ê²ƒì¸ê°€?â€**ì™€ â€œê·¸ë£¹**ë‚´ì—ì„œ ì–´ë–¤ ì •ê·œí™”ë¥¼ ì‚¬ìš©í•  ê²ƒì¸ê°€?**ë¥¼ ê³ ë ¤í•œë‹¤.

1. Heuristic pruning criterion, Element-wise Pruning
    
    $$
    Importance = \lvert W \lvert
    $$
    
    ![Reference. MIT-TinyML-lecture03-Pruning-1](../../images/lec03/Untitled%209.png)
    
2. Heuristic pruning criterion, Row-wise Pruning, L1-norm magnitude
    
    $$
    Importance = \sum_{i\in S}\lvert w_i \lvert, \\where\ W^{(S)}\ is\ the\ structural\ set\ S\ of\ parameters\ W
    $$
    
    ![Reference. MIT-TinyML-lecture03-Pruning-1](../../images/lec03/Untitled%2010.png)
    
3. Heuristic pruning criterion, Row-wise Pruning, L2-norm magnitude
    
    $$
    Importance = \sum_{i\in S}\lvert w_i \lvert, \\where\ W^{(S)}\ is\ the\ structural\ set\ S\ of\ parameters\ W
    $$
    
    ![Reference. MIT-TinyML-lecture03-Pruning-1](../../images/lec03/Untitled%2011.png)
    
4. Heuristic pruning criterion, $L_p$- norm
    
    $$
    \lvert\lvert W^{(S)}\lvert\lvert=\huge( \large 
      \sum_{i\in S} \lvert w_i \lvert^p 
    \huge) \large^{\frac{1}{p}} 
    $$
    

**3.1.2 Scaling-based Pruning**

ë‘ ë²ˆì§¸ë¡œ Scalingì„ í•˜ëŠ” ê²½ìš° ì±„ë„ë§ˆë‹¤ Scaling Factorë¥¼ ë‘¬ì„œ Pruningì„ í•œë‹¤. ê·¸ëŸ¼ Scaling Factorë¥¼ ì–´ë–»ê²Œ ë‘¬ì•¼ í• ê¹Œ? ê°•ì˜ì—ì„œ ì†Œê°œí•˜ëŠ” [ì´ ë…¼ë¬¸](https://arxiv.org/pdf/1708.06519.pdf)ì—ì„œëŠ” Scaling factor $\gamma$ íŒŒë¼ë¯¸í„°ë¥¼ trainable íŒŒë¼ë¯¸í„°ë¡œ ë‘ë©´ì„œ batch normalization layerì— ì‚¬ìš©í•œë‹¤.

- Scale factor is associated with each filter(i.e. output channel) in convolution layers.
- The filters or output channels with small scaling factor magnitude will be pruned
- The scaling factors can be reused from batch normalization layer
    
    $$
    z_o = \gamma\dfrac{z_i-\mu_{B}}{\sqrt{\sigma_B^2+\epsilon}}+\beta
    $$
    

![Reference. MIT-TinyML-lecture03-Pruning-1](../../images/lec03/Untitled%2012.png)

**3.1.3 Talyor Expansion Analysis on Pruning Error**

ì„¸ ë²ˆì§¸ ë°©ë²•ì€ í…Œì¼ëŸ¬ ê¸‰ìˆ˜ë¥¼ ì´ìš©í•˜ì—¬ Objective functionì„ ìµœì†Œí™” í•˜ëŠ” ì§€ì ì„ ì°¾ëŠ” ë°©ë²•ì…ë‹ˆë‹¤. Talyor Seriesì— ëŒ€í•œ [ìì„¸í•œ ë‚´ìš©](https://ooshyun.github.io/2023/07/02/Taylor-Series-Approximation-and-Error.html)ì€ ì—¬ê¸°ì„œ!

- Evaluate pruning error induced by pruning synapses.
- Minimize the objective function L(x; W)
- A Taylor series can approximate the induced error.

$$
\delta L = L(x;W)-L(x;W_p=W-\delta W) \\ = \sum_i g_i\delta w_i + \frac{1}{2} \sum_i h_{ii}\delta w_i^2 + \frac{1}{2}\sum_{i\not=j}h_{ij}\delta w_i \delta w_j + O(\lvert\lvert \delta W \lvert\lvert^3)
$$
$$
where\ g_i=\dfrac{\delta L}{\delta w_i}, h_{i, j} = \dfrac{\delta^2 L}{\delta w_i \delta w_j}
$$

1. Second-Order-based Pruning

    ![Reference. MIT-TinyML-lecture03-Pruning-1](../../images/lec03/Untitled%2013.png)
        
    ![Reference. MIT-TinyML-lecture03-Pruning-1](../../images/lec03/Untitled%2014.png)
    
    Optimal Brain Damage[LeCunÂ *et al.,*Â NeurIPS 1989] ë…¼ë¬¸ì—ì„œëŠ” ì´ ë°©ë²•ì„ ì´ìš©í•˜ê¸° ìœ„í•´ ì„¸ ê°€ì§€ë¥¼ ê°€ì •í•œë‹¤. 
    
    1. Objective function Lì´ quadratic ì´ê¸° ë•Œë¬¸ì— ë§ˆì§€ë§‰ í•­ì´ ë¬´ì‹œëœë‹¤(ì´ëŠ” Talyor Seriesì˜ Error í•­ì„ ì•Œë©´ ì´í•´ê°€ ë” ì‰½ë‹¤!)
    2. ë§Œì•½ ì‹ ê²½ë§ì´ ìˆ˜ë ´í•˜ê²Œë˜ë©´, ì²« ë²ˆì§¸í•­ë„ ë¬´ì‹œëœë‹¤.
    3. ê° íŒŒë¼ë¯¸í„°ê°€ ë…ë¦½ì ì´ë¼ë©´ Cross-termë„ ë¬´ì‹œëœë‹¤.
    
    ê·¸ëŸ¬ë©´ ì‹ì„ ì•„ë˜ì²˜ëŸ¼ ì •ë¦¬í•  ìˆ˜ ìˆëŠ”ë°, ì¤‘ìš”í•œ ë¶€ë¶„ì€ **Hessian Matrix Hì— ì‚¬ìš©í•˜ëŠ” Computationì´ ì–´ë µë‹¤ëŠ” ì !**
    
    $$
    \delta L_i = L(x;W)-L(x;W_p\lvert w_i=0)\approx \dfrac{1}{2} h_{ii}w_i^2,\ where\ h_{ii}=\dfrac{\partial^2 L}{\partial w_i \partial w_j} 
    $$
    
    $$
    importance_{w_i} = \lvert \delta L_i\lvert = \frac{1}{2}h_{ii}w_i^2
    $$
    $$
    *\ h_{ii} \text{ is non-negative}
    $$
    
2. First-Order-based Pruning 
    - ì°¸ê³ ë¡œ ì´ ë°©ë²•ì€ 2023ë…„ì—ëŠ” ì†Œê°œí•˜ì§€ ì•ŠëŠ”ë‹¤.
    
    ![Reference. MIT-TinyML-lecture03-Pruning-1 ](../../images/lec03/Untitled%2015.png)
    
    - If only first-order expansion is considered under an *i.i.d(*Independent and identically distributed**)** assumption,
    
    $$
    \delta L_i = L(x;W) - L(x; W_P\lvert w_i=0) \approx g_iw_i,\ where\ g_i=\dfrac{\partial L}{\partial w_i}
    $$
    $$
    importance_{w_i} = \lvert \delta L_i \lvert = \lvert g_i w_i \lvert \ or \ importance_{w_i} = \lvert \delta L_i \lvert^2 = (g_i w_i)^2
    $$
    
    - For coarse-grained pruning, we have,
        
        $$
        importance_{\ W^{(S)}} = \sum_{i \in S}\lvert \delta L_i \lvert^2 = \sum_{i \in S} (g_i w_i)^2,\ where \ W^{(S)}is\ the\ structural\ set\ of\ parameters 
        $$
        

### 3.2 **Select of Neurons**

ì–´ë–¤ Neuronì„ ì—†ì•¨ ì§€ë¥¼ ê³ ë ¤(**Less useful â†’ Remove)** í•œ ì´ ë°©ë²•ì€ **Neuronì˜ ê²½ìš°**ë„ ìˆì§€ë§Œ ì•„ë˜ ê·¸ë¦¼ì²˜ëŸ¼ **Channel**ë¡œ ê³ ë ¤í•  ìˆ˜ë„ ìˆë‹¤. í™•ì‹¤íˆ ì „ì— ì†Œê°œí–ˆë˜ ë°©ë²•ë“¤ë³´ë‹¤ **â€œCoarse-grained pruningâ€**ì¸ ë°©ë²•ì´ë‹¤.

![](../../images/lec03/Untitled%2016.png)

1. Percentage-of-Zero-based Pruning
    
    ì²«ë²ˆì§¸ëŠ” Channelë§ˆë‹¤ 0ì˜ ë¹„ìœ¨ì„ ë´ì„œ ë¹„ìœ¨ì´ ë†’ì€ Channel ì„ ì—†ë‚´ëŠ” ë°©ë²•ì´ë‹¤. ReLU activationì„ ì‚¬ìš©í•˜ë©´ Outputì´ 0ì´ ë‚˜ì˜¤ëŠ”ë°, ì—¬ê¸°ì„œ 0ì˜ ë¹„ìœ¨, Average Percentage of Zero activations(APoZ)ë¼ê³  ë¶€ë¥´ëŠ” ê²ƒì„ ë³´ê³  ê°€ì§€ì¹˜ê¸°í•  Channelì„ ì œê±°í•œë‹¤.
    
    - ReLU activation will generate zeros in the output activation
    - Similar to magnitude of weights, the Average Percentage of Zero activations(APoZ) can be exploited to measure the importance the neuron has
    
    ![Reference. MIT-TinyML-lecture03-Pruning-1 ](../../images/lec03/Untitled%2017.png)
    
2. First-Order-based Pruning
    - ì°¸ê³ ë¡œ ì´ ë°©ë²•ì€ 2023ë…„ì—ëŠ” ì†Œê°œí•˜ì§€ ì•ŠëŠ” ë°©ë²•ì´ë‹¤.
    - Minimize the error on loss function introduced by pruning neurons
    - Similar to previous Taylor expansion on weights, the induced error of the objective functionÂ *L*(x;Â W)Â can be approximated by a Taylor series expanded on activations.
        
        $$
        \delta L_i = L(x; W) - L(x\lvert x_i = 0; W) \approx \dfrac{\partial L}{\partial x_i}x_i
        $$
        
    - For a structural set of neuronsÂ $x^{(S)}$Â (*e.g.*, a channel plane),
        
        $$
        \lvert \delta L_{x^{(S)}} \lvert\ = \Large\lvert \small\sum_{i\in S}\dfrac{\partial L}{\partial x_i}x_i\Large\lvert
        $$
        
3. Regression-based Pruning
    
    ì´ ë°©ë²•ì€ Quantizedí•œ ë ˆì´ì–´ì˜ output $\hat Z$(construction error of the corresponding layerâ€™s outputs)ì™€ $Z$ë¥¼ Trainingì„ í†µí•´ ì°¨ì´ë¥¼ ì¤„ì´ëŠ” ë°©ë²•ì´ë‹¤. ì°¸ê³ ë¡œ ë¬¸ì œë¥¼ í‘¸ëŠ” ìì„¸í•œ ê³¼ì •ì€ 2022ë…„ ê°•ì˜ì—ë§Œ ë‚˜ì™€ ìˆë‹¤. 
    

$$
Z=XW^T=\sum_{c=0}^{c_i-1}X_cW_c^T
$$

![Reference. MIT-TinyML-lecture03-Pruning-1 ](../../images/lec03/Untitled%2018.png)

ë¬¸ì œë¥¼ ì‹ìœ¼ë¡œ ì •ì˜í•´ë³´ë©´ ì•„ë˜ì™€ ê°™ì€ë°,

- $\beta$ is the coefficient vector of length $c_i$ for channel selection.
- $\beta_c = 0$ means channel $c$ is pruned.
- $N_c$ is the number of none zero channel

ìš°ì„  ë¬¸ì œë¥¼ í‘¸ëŠ” ë‹¨ê³„ëŠ” ë‘ ë‹¨ê³„ë¡œ ë‚˜ëˆˆë‹¤. Channelì˜ Scale $\beta$ë¥¼ ìš°ì„  ê³„ì‚°í•œ í›„ì— $W$ë¥¼ Quantizedí•œ ë ˆì´ì–´ì˜ output $\hat Z$(construction error of the corresponding layerâ€™s outputs)ì™€ $Z$ì˜ ì°¨ì´ê°€ ìµœì†Œí™”ë˜ëŠ” ì§€ì ê¹Œì§€ Trainingì‹œí‚¨ë‹¤. 

Solve the problem in two folds:

- Fix **W,** solve $\beta$ for channel selection â†’ **NP(Nondeterministic polynomial)-hard**
- Fix **$\beta$**, solve W to minimize reconstruction error(**Weight Reconstruction)**

ê° ë¬¸ì œë¥¼ í‘¸ëŠ” ê³¼ì •ì„ ì¡°ê¸ˆ ë” ìì„¸íˆ ì‚´í´ë´ë³´ì. ë³¸ ë‚´ìš©ì€ 2022ë…„ ê°•ì˜ì— ìˆìœ¼ë‹ˆ ì°¸ê³ !

**NP(Nondeterministic polynomial)-hard**ëŠ” ì•„ë˜ì™€ ê°™ì´ ì‹ìœ¼ë¡œ ì •ë¦¬í•  ìˆ˜ ìˆë‹¤.

$$
\underset{\beta}{argmin} \lvert\lvert Z- \sum_{c=0}^{c_i-1} \beta_cX_cW_c^T \lvert\lvert_F^2 = \lvert\lvert \sum_{c=0}^{c_i-1}X_cW_c^T - \sum_{c=0}^{c_i-1} \beta_cX_cW_c^T \lvert\lvert_F^2 
$$
$$
= \lvert\lvert\sum_{c=0}^{c_i-1} (1-\beta_c)X_cW_c^T \lvert\lvert_F^2, \ s.t.\ \lvert\lvert\beta\lvert\lvert_0 \ \leq N_c
$$

ê°•ì˜ì—ì„œ ì†Œê°œí•˜ëŠ” ThiNetì´ë¼ëŠ” ë…¼ë¬¸ì—ì„œëŠ” greedy solutionì„ ì´ìš©í•´ì„œ ì±„ë„ í•˜ë‚˜í•˜ë‚˜ì”© Pruning í•´ë³´ë©° objective functionì˜ l2-norm ìµœì†Ÿê°’ì„ êµ¬í•œë‹¤. 

```markdown
1: S = []
2: while len(S) < N:
3:   min_norm, min_c = +inf, 0
4:   for c in range(c_i):
5:     tmpS=S+[c]
6:     Z = X[:,tmpS] * W[:,tmpS].t()
7:     norm = Z.norm(2)
8:     if norm < min_norm:
9:       min_norm, min_c = norm, c
10:   S.append(min_c)
11:   c_i.pop(min_c)
```

ì—¬ê¸°ì„œ ë”í•´ì„œ $\beta$ ë¥¼ êµ¬í•˜ëŠ” ê³¼ì •ì—ì„œ ì¼ë°˜í™”ë¥¼ ìœ„í•´ LASSO ë°©ì‹ì„ ì‚¬ìš©í•œë‹¤(LASSOì— ëŒ€í•œ ìì„¸í•œ ë‚´ìš©ì€ [ì—¬ê¸°ì„œ](https://www.notion.so/Statistics-p-value-and-L1-L2-c5cabf858b194d8b9970eb88e739888e?pvs=21)).
Relax the $l_0$ to $l_1$ regularization (LASSO):

$$
\underset{\beta}{argmin}\ \lvert\lvert Z- \sum_{c=0}^{c_i-1}\beta_cX_cW_c^T\lvert\lvert^2_F+\lambda\lvert\lvert \beta \lvert\lvert_1
$$

- $\lambda$ is a penalty coefficient. **By increasing $\lambda$, there will be more zeros in $\beta$.**
- Gradually increase $\lambda$ and solve the LASSO regression for $\beta$, until $\lvert\lvert \beta \lvert\lvert_0==N_c$ is met.
- Why $\lvert\lvert \beta \lvert\lvert_0==N_c$? 
    
    ì—¬ê¸°ì— ëŒ€í•´ì„œëŠ” ë”°ë¡œ ì–¸ê¸‰ë˜ì§€ ì•Šì•˜ì§€ë§Œ, ì˜ë¯¸ìƒ scale ì „ì²´ Nê°œ ì¤‘ì—ì„œ ìµœì ê°’ì„ ì°¾ì•„ì•¼í•œë‹¤ë©´ ì „ì²´ë¥¼ Nìœ¼ë¡œ ìœ ì§€í•˜ë©´ì„œ ìµœì ê°’ì„ ì°¾ê¸° ìœ„í•´ì„œê°€ ì•„ë‹ê¹Œ?

ë‘ ë²ˆì§¸ëŠ” êµ¬í•œ $\beta$ë¥¼ ê³ ì •í•œ ìƒíƒœë¡œ Weightë¥¼ Quantized ì „í›„ì˜ ì°¨ì´ë¥¼ ìµœì†Œí™” í•˜ê²Œ â€œWeight Reconstructionâ€ í•œë‹¤. êµ¬í•˜ëŠ” ê³¼ì •ì€ **least square approach**ë¥¼ ì´ìš©í•œ **unique closed-form solution** ì´ë¯€ë¡œ ì•„ë˜ë¥¼ ì°¸ì¡°í•˜ì.

$$
\underset{\beta}{argmin}\ \lvert\lvert Z- \sum_{c=0}^{c_i-1}\beta_cX_cW_c^T\lvert\lvert^2_F
$$

- $\beta$ is a coefficient vector from the previous step
- This is a classic **linear regression problem**, which has **a unique closed-form solution** using the **least square** approach.
    
    $$
    \underset{W}{argmin} \lvert\lvert Z-\hat{Z} \lvert\lvert^2_F = \lvert\lvert Z-UW^T \lvert\lvert_F^2
    $$
    
    
    where
    
    $$
    U= \Large[ \small\beta_0X_0\ \beta_1X_1 \ \cdots \beta_cX_c \cdots \beta_{c_i-1}X_{c_i-1} \Large]
    $$
    
    and thus,
    
    $$
    W^T = (U^TU)^{-1}U^T Z
    $$
    
    - Q. How $(U^TU)^{-1}$ exists?
        
        Least Square method, ì„ì˜ì˜ ë²¡í„° $v = (v_0, v_1, \dots, v_n)$ ê°€ ìˆì„ ë•Œ $v^Tv$ ì˜ ì—­í–‰ë ¬ì€ í•­ìƒ ìˆì„ê¹Œ? ê°€ì •ì—ì„œ â€œ**a unique closed-form solution**â€ë¼ê³  í–ˆìœ¼ë¯€ë¡œ ì´ëŠ” ì¦‰ linearly independenë¡œ ê³ ë ¤í•  ìˆê³  ì—­í–‰ë ¬ì´ ìˆë‹¤($v^Tv$  is invertible)ëŠ” ì´ì•¼ê¸°ì´ë‹¤.

## 4. Discussion

1. Pruningì„ Dropoutì´ë‘ ë¹„êµí•´ì„œ ì–´ë–¤ ì°¨ì´ì ì´ ìˆëŠ”ê°€?

    ë‘ ê°€ì§€ ë°©ë²•ì€ ë¶„ëª…íˆ Neuronê³¼ Synapseë¥¼ ì—†ëŒ„ë‹¤ëŠ” ì¸¡ë©´ì—ì„œëŠ” ë¹„ìŠ·í•˜ë‹¤. í•˜ì§€ë§Œ ë‘ ê°€ì§€ ì¸¡ë©´ì—ì„œ ì°¨ì´ì ì´ ìˆëŠ”ë°, í•œ ê°€ì§€ëŠ” ëª©ì í•˜ëŠ” ë°”ì´ê³ , ë‘ ë²ˆì§¸ëŠ” ì‹œì ì´ë‹¤. Dropoutì€ ëª©ì í•˜ëŠ” ë°”ê°€ í›ˆë ¨ì¤‘ì— overfittingì„ ë°©ì§€í•˜ê¸° ìœ„í•¨ì´ ìˆê³  Pruningì˜ ê²½ìš°ëŠ” **í›ˆë ¨ì„ ë§ˆì¹œ ëª¨ë¸**ì˜ í¬ê¸°ë¥¼ ì¤„ì´ëŠ” ê²ƒì— ìˆë‹¤. ê·¸ë¦¬ê³  ë‘ ë²ˆì§¸ ì‹œì ì˜ ê²½ìš° Dropoutì€ í›ˆë ¨ì¤‘ì— ì´ë¤„ì§€ëŠ” ë°˜ë©´ Pruningì€ í›ˆë ¨ì„ ë§ˆì¹˜ê³ , ê·¸ í¬ê¸°ë¥¼ ì¤„ì¸ í›„ì— ì„±ëŠ¥ì´ ë–¨ì–´ì§€ë©´ ê·¸ì— ë§ê²Œ Fine-tuningì„ í•œë‹¤. 
    
    ìŠ¤í„°ë””ì—ì„œëŠ” "ì™œ dropoutì„ í†µí•´ ì‚¬ì´ì¦ˆë¥¼ ì¤„ì´ì§€ ì•Šì•˜ëŠ”ê°€? ê·¸ë¦¬ê³  êµ¬ì§€ í›ˆë ¨ì„ ë§ˆì¹œ ë‹¤ìŒì— í•  í•„ìš”ê°€ ìˆë‚˜?" ë¼ê³  ì§ˆë¬¸ì´ ë‚˜ì™”ì—ˆë‹¤. ë¬¼ë¡  í›ˆë ¨ ì¤‘ì— ëª¨ë¸ì˜ ì‚¬ì´ì¦ˆë¥¼ ì‘ê²Œ ë§Œë“¤ ìˆ˜ ìˆìœ¼ë©´, ê°€ëŠ¥í•œ ê·¸ë ‡ê²Œ í•˜ë©´ ë  ê²ƒì´ë‹¤. í•˜ì§€ë§Œ, ì´ ë˜í•œ ë‘ê°€ì§€ ì¸¡ë©´ì„ ê³ ë ¤í•  í•„ìš”ê°€ ìˆë‹¤. í•˜ë‚˜ëŠ” "ê³¼ì—° ëª¨ë¸ì˜ ì‚¬ì´ì¦ˆë¥¼ í›ˆë ¨ ì¤‘ í˜¹ì€ ì „ì— ì¤„ì—¬ë‚˜ê°€ë©´ì„œ ì¶©ë¶„íˆ ì„±ëŠ¥ì„ ë‚¼ ìˆ˜ ìˆëŠ”ê°€?"ì´ê³  ë‹¤ë¥¸ í•˜ë‚˜ëŠ” Pruningì´ë‚˜ ëª¨ë¸ ê²½ëŸ‰í™”ëŠ” **ìµœì í™”ì— ì´ˆì **ì„ ë§ì¶˜ë‹¤ê³  ìƒê°í•œë‹¤. ê·¸ë ‡ê¸° ë•Œë¬¸ì— í›ˆë ¨ ì¤‘ê°„ì— Channel pruningê³¼ ê°™ì€ ê¸°ë²•ì„ ì‚¬ìš©í•  ìˆ˜ ìˆì„ ì§€ëŠ” ë¯¸ì§€ìˆ˜ì´ê³ , ì„¤ë ¹ Fine-grained Pruningê³¼ ê°™ì€ ê¸°ë²•ì„ ì‚¬ìš©í•œë‹¤ í•˜ë”ë¼ë„ ì´ëŠ” ëª¨ë¸ì˜ ì‚¬ì´ì¦ˆë§Œ ì¤„ì–´ ë“¤ ë¿, ë‚˜ë¨¸ì§€ ë©”ëª¨ë¦¬(e.g. RAM)ì´ë‚˜ Latencyê°™ì€ ì„±ëŠ¥ì€ ì¢‹ê²Œ ê°€ì ¸ê°ˆ ìˆ˜ ìˆì„ì§€ë„ ë¯¸ì§€ìˆ˜ë¼ê³  ìƒê°í•œë‹¤.

    í•„ìëŠ” ìœ„ì™€ ê°™ì€ ìµœì í™”ë¥¼ í†µí•œ ì„±ëŠ¥ ê°œì„ ì„ [ì´ ê¸€](https://ooshyun.github.io/2023/12/04/Optimization-for-tiny-engine-1.html)ì—ì„œì²˜ëŸ¼ 2022ë…„ TinyML ê°•ì˜ì—ì„œ ì œê³µí•˜ëŠ” ì‹¤ìŠµì„ í†µí•´ ê²½í—˜í–ˆì—ˆë‹¤. ì•ì„  ì˜ˆì‹œëŠ” OSë¥¼ ê°€ì§„ ë””ë°”ì´ìŠ¤ê°€ ì•„ë‹Œ Bare-metal firmwareë¡œ í™˜ê²½ì´ ì¡°ê¸ˆ íŠ¹ìˆ˜í•˜ê¸°ë„ í•˜ê³ , ì‹¤ì œë¡œ Torchë‚˜ Tensorflowliteì—ì„œ ì œê³µí•˜ëŠ” ëª¨ë¸ ê²½ëŸ‰í™”ë¥¼ ì§ì ‘ì ìœ¼ë¡œ ë¶„ì„í•´ë´ì•¼ ì‹¤ì§ˆì ì¸ ì˜ˆì‹œë¥¼ ì•Œ ìˆ˜ ìˆê² ì§€ë§Œ, í˜¹ì—¬ ì´í•´í•´ ì°¸ê³ ê°€ ë ê¹Œ ë§ë¶™ì—¬ ë†“ëŠ”ë‹¤.

## 5. Reference
- [MIT-TinyML-lecture03-Pruning-1](https://www.youtube.com/watch?v=w5WiUcDJosM&list=PL80kAHvQbh-pT4lCkDT53zT8DKmhE0idB&index=5)
- [AMC: Automl for Model Compression and Acceleration on Mobile Devices, 2018](https://arxiv.org/pdf/1802.03494.pdf)
- [Learning Efficient Convolutional Networks through Network Slimming, 2017](https://arxiv.org/pdf/1708.06519.pdf)
- [ThiNet: A Filter Level Pruning Method for Deep Neural Network Compression, 2017](https://arxiv.org/pdf/1707.06342.pdf)
- [Channel Pruning for Accelerating Very Deep Neural Networks](https://arxiv.org/pdf/1707.06168.pdf)
