---
toc: true
title: "ğŸ§‘â€ğŸ« Lecture 14"
description:  Vision Transformer for TinyML
author: "Seunghyun Oh"
date: "2024-04-26"
categories: [lecture, transformer, vision transformer]
---
<style>
p {
  line-height: 2; /* Adjust the line-height property as needed */
  text-align: justify;
  text-justify: inter-word; /* Adjust the justification mode as needed */
}

.image-container {
  text-align: center; /* Center-align the contents horizontally */
}

.centered-text {
  text-align: center; /* Center-align the text within the container */
}
</style>

# Vision Transformer (ViT) for TinyML

ì´ë²ˆ ì‹œê°„ì€ Transformer ëª¨ë¸ì—ì„œë„ Visionì— ì£¼ë¡œ ì–´ë–»ê²Œ ì‚¬ìš©í•˜ëŠ”ì§€ ì•Œì•„ë³´ë ¤ê³  í•©ë‹ˆë‹¤. ê·¸ë¦¬ê³  ì´ë¥¼ ê²½ëŸ‰í™”í•˜ê±°ë‚˜ ê°€ì†í™”í•˜ëŠ” ê¸°ë²•, ê·¸ë¦¬ê³  ì œí•œëœ ë¦¬ì†ŒìŠ¤ì—ì„œ ì–´ë–»ê²Œ ì˜ í™œìš©í•  ìˆ˜ ìˆì„ì§€ ì•Œì•„ë´…ì‹œë‹¤.

## **1. Basics of Vision Transformer (ViT)**

Vision TransformerëŠ” ë­˜ê¹Œìš”? LLMìœ¼ë¡œ ë§ì´ ì‚¬ìš©í•˜ëŠ” Language ëª¨ë¸ì˜ ê²½ìš°, ì…ë ¥ìœ¼ë¡œ í† í°ì´ ë“¤ì–´ì™€ Transformer ëª¨ë¸ êµ¬ì¡°ë¡œ Encoder, Decoder êµ¬ì¡°ì— ë”°ë¼ BERT(Encoder), GPT(Decoder) ê·¸ë¦¬ê³  BART, T5(Encoder - Decoder) êµ¬ì¡°ë¡œ ì‚¬ìš©í•˜ì£ . ê·¸ëŸ¼ Visionì—ì„œëŠ” ì´ êµ¬ì¡°ë¥¼ ì–´ë–»ê²Œ ì‚¬ìš©í• ê¹Œìš”?

![Reference. MIT-TinyML lecture14 Vision Transformer in [https://efficientml.ai](https://efficientml.ai/)](../../images/lec14/a0dce9fc-0196-4819-a392-87cd4135e492.png)

ìƒê°ë³´ë‹¤ ê°„ë‹¨í•´ìš”. ì´ë¯¸ì§€ê°€ ë§Œì•½ 96x96ì´ ìˆë‹¤ë©´ ì´ë¦„ 32x32 ì´ë¯¸ì§€ 9ê°œë¡œ ë‚˜ëˆ•ë‹ˆë‹¤. ë‚˜ëˆˆ ì´ë¯¸ì§€ë¥¼ Patchë¼ê³  ë¶€ë¥¼ê²Œìš”. ê·¸ëŸ¼ ì´ Patchë¥¼ Linear Projectionì„ í†µí•´ì„œ í† í°ì²˜ëŸ¼ 768ê°œì˜ Vision Transformer(ViT)ì˜ ì…ë ¥ìœ¼ë¡œ ë“¤ì–´ê°‘ë‹ˆë‹¤. ì‹¤ì œë¡œ ì•„ì´ë””ì–´ë¥¼ êµ¬í˜„í•  ë•ŒëŠ” 32x32 Convolution ë ˆì´ì–´ì— stride 32, padding 0, ì…ë ¥ ì±„ë„ 3, ì¶œë ¥ ì±„ë„ 768ë¡œ ì—°ì‚°í•©ë‹ˆë‹¤. ê·¸ ë‹¤ìŒì€, ì…ë ¥ì´ ë™ì¼í•´ ì¡Œìœ¼ë‹ˆ ëª¨ë¸ êµ¬ì¡°ëŠ” í”íˆ ë³´ëŠ” ì•„ë˜ ê·¸ë¦¼ì˜ êµ¬ì¡°ë¥¼ ì‚¬ìš©í•˜ì£ .

![Reference. MIT-TinyML lecture14 Vision Transformer in [https://efficientml.ai](https://efficientml.ai/)](../../images/lec14/ca5f52e2-27eb-4bea-81ef-37291e020c53.png)

ì—¬ê¸°ì„œ Patch ì˜ í¬ê¸° ë˜í•œ íŒŒë¼ë¯¸í„°ë¡œ í•  ìˆ˜ ìˆìŠµë‹ˆë‹¤. ì•ìœ¼ë¡œ ViTë¥¼ ë§í•  ë•ŒëŠ” Patchë„ ìœ ì‹¬íˆ ë³´ì…”ì•¼í•  ê²ë‹ˆë‹¤.

![Reference. MIT-TinyML lecture14 Vision Transformer in [https://efficientml.ai](https://efficientml.ai/)](../../images/lec14/ee3a9190-e81b-4b53-b1a4-147339a36c3f.png)

ê·¸ëŸ°ë° ì™œ Vision Transformerë¥¼ ì“¸ê¹Œìš”? ê¸°ì¡´ì— CNN êµ¬ì¡°ì— ResNetì´ë‚˜ MobileNetì˜ êµ¬ì¡°ë„ ì¶©ë¶„íˆ ì„±ëŠ¥ì´ ê´œì°®ì§€ ì•Šë‚˜ìš”? CNNê³¼ Transformerë¥¼ Vision taskì—ì„œ ë¹„êµí•´ë³´ë©´ í›ˆë ¨ ë°ì´í„°ìˆ˜ê°€ ì ì„ ë•ŒëŠ” í™•ì‹¤íˆ CNNì´ ê°•ì„¸ë¥¼ ë³´ì´ì£ .

![Reference. MIT-TinyML lecture14 Vision Transformer in [https://efficientml.ai](https://efficientml.ai/)](../../images/lec14/c1a83fa2-1a94-4a50-a555-336cc2c1fabc.png)

**í•˜.ì§€.ë§Œ.** ë°ì´í„°ìˆ˜ê°€ 300Mì¸ ê²½ìš°ë¥¼ ì‚´í´ë³´ì‹œì£ . **ViTëŠ” ë°ì´í„°ìˆ˜ê°€ ë§ìœ¼ë©´ ë§ì„ ìˆ˜ë¡ CNNì— ë¹„í•´ì„œ í›¨ì”¬ ê°•ì„¸ë¥¼ ë³´ì´ëŠ” ê²ƒ**ì„ í™•ì¸í•  ìˆ˜ ìˆì£ ? ì´ ë•Œë¬¸ì—, ì €í¬ëŠ” ViTì— ë§¤ë£Œë  ìˆ˜ ë°–ì— ì—†ì—ˆìŠµë‹ˆë‹¤.

![Reference. MIT-TinyML lecture14 Vision Transformer in [https://efficientml.ai](https://efficientml.ai/)](../../images/lec14/d2b025c5-739c-42b8-a4da-0f4d6a1ba893.png)

Visionì— ëŒ€í•œ Applicationìœ¼ë¡œ Medical Image Segmentation, Super Resolution, Autonomous Driving, Segmentationë¡œì¨ ì£¼ë¡œ ì‚¬ìš©í•´ìš”. 

![Reference. MIT-TinyML lecture14 Vision Transformer in [https://efficientml.ai](https://efficientml.ai/)](../../images/lec14/Lecture_14-Vision_Transformer_(5).jpg)

ë¬¸ì œëŠ” ì´ ì–´í”Œë¦¬ì¼€ì´ì…˜ë“¤ì€ ëª¨ë‘ High resolutionì— predictionì„ ìš”êµ¬í•˜ì§€ë§Œ, ViTëŠ” Input resolutionì´ ë†’ì•„ì§ˆ ë•Œë§ˆë‹¤ ì—°ì‚°ëŸ‰ì´ ì–´ë§ˆì–´ë§ˆí•´ì§‘ë‹ˆë‹¤. ì„ í˜•ì ì´ë¼ê¸° ë³´ë‹¨ ì§€ìˆ˜ì ìœ¼ë¡œ ì¦ê°€í•˜ëŠ” ê²ƒ ì²˜ëŸ¼ ë³´ì´ë„¤ìš”. ë°”ë¡œ ì´ ë¬¸ì œ ë•Œë¬¸ì—, ìš°ë¦¬ê°€ â€œEfficient and Accelerationâ€ì— ëŒ€í•´ì„œ ê³ ë¯¼í•  ìˆ˜ ë°–ì— ì—†ê²Œ ë©ë‹ˆë‹¤.

*mIoU: mean Intersection of Union, GMAC: Giga Multiply and Add Computation

![Reference. MIT-TinyML lecture14 Vision Transformer in [https://efficientml.ai](https://efficientml.ai/)](../../images/lec14/94cb6dd4-2750-415d-8e15-b8177affe2f5.png)

## **2. Efficient ViT & acceleration techniques**

### 2.1 Window attention

ì²˜ìŒìœ¼ë¡œ ì†Œê°œí•  ê¸°ìˆ ì€ **Window attention** ì…ë‹ˆë‹¤. ê¸°ì¡´ì— attentionì€ ë ˆì´ì–´ë§ˆë‹¤ patchì˜ í¬ê¸°ê°€ ë™ì¼í•˜ê²Œ ë“¤ì–´ê°€ê² ì£ . í•˜ì§€ë§Œ Window attentionì€ ë ˆì´ì–´ë§ˆë‹¤ patchì˜ í¬ê¸°ë¥¼ ë‹¤ë¥´ê²Œ í•˜ëŠ” ë°©ë²•ì…ë‹ˆë‹¤. ê·¸.ë¦¬.ê³ . ì—¬ê¸°ì„œ ì¤‘ìš”í•œ ê±´ **Window attentionì€ ê·¸ Patchì•ˆì— ë‹¤ì‹œ Patchë¥¼ í•˜ëŠ” ë°©ë²•ì…ë‹ˆë‹¤. ê·¸ë¦¬ê³  ì´ë¥¼ í†µí•´ ë³‘ë ¬ì—°ì‚°ì„ ê°€ëŠ¥í•˜ê²Œ ë§Œë“œëŠ” ê±°ì£ .**

![Reference. MIT-TinyML lecture14 Vision Transformer in [https://efficientml.ai](https://efficientml.ai/)](../../images/lec14/1244eb7d-48a3-4c07-ba3d-519815372d8c.png)

í•˜ì§€ë§Œ ì €ë ‡ê²Œ ì—°ì‚°í•˜ë©´ ë¬¸ì œì ì´ Windowê°„ ì •ë³´êµí™˜ì´ ì—†ë‹¤ëŠ” ì ì¸ë°ìš”. ì´ëŠ” ë ˆì´ì–´ë³„ë¡œ â€œShift Windowâ€ë¥¼ í†µí•´ í•´ê²°í•©ë‹ˆë‹¤.

![Reference. MIT-TinyML lecture14 Vision Transformer in [https://efficientml.ai](https://efficientml.ai/)](../../images/lec14/e3509a3f-2578-41c1-a654-1a3dd94d02d1.png)

### 2.2 Linear attention

ë‘ ë²ˆì§¸ ì†Œê°œí•  ê¸°ìˆ ì€ Linear attention ì…ë‹ˆë‹¤. ê¸°ì¡´ì— Attention ì—°ì‚°ì¤‘ì— Softmaxê°€ ìˆì—ˆì£ ? exponential ì—°ì‚°ì€ ì§ì ‘ êµ¬í˜„í•´ë³´ë©´ ì—°ì‚°ëŸ‰ì´ ìª¼ê¸ˆ í˜ë“­ë‹ˆë‹¤. ê·¸ë˜ì„œ ì´ë¥¼ Linear ë ˆì´ì–´ì¸ Reluë¡œ ëŒ€ì²´ë¥¼ í•˜ëŠ”ë°ìš”. ì—¬ê¸°ì„œ ì•Œê³ ë¦¬ì¦˜ ë³µì¡ë„ê°€ O($n^2$)ì¸ ë¶€ë¶„ê¹Œì§€ í–‰ë ¬ì˜ ê³±ì…ˆì—ì„œ ê²°í•©ë²•ì¹™ì´ ê°€ëŠ¥í•˜ë‹¤ëŠ” ë¶€ë¶„ì„ ì´ìš©í•´ O($n$)ìœ¼ë¡œ ì¤„ì—¬ë²„ë¦½ë‹ˆë‹¤.  ë§ˆì§€ë§‰ ë³µì¡ë„ê°€ ì¤„ì–´ë“œëŠ” ê²ƒì´ ì´í•´ê°€ ì•ˆê°€ì‹  ë‹¤ë©´, Scaleì´í›„ì— ë‚˜ì˜¤ëŠ” ë ˆì´ì–´ê°€ $n \times d$ ì¸ ì ê³¼ n-ì°¨ì›ê³¼ d-ì°¨ì›ì— ëŒ€í•´ì„œ ë¹„êµí•´ë³´ì‹œë©´ ë¹ ë¥´ê²Œ ë‚©ë“ì´ ê°€ì‹¤ê²ë‹ˆë‹¤!

![Reference. MIT-TinyML lecture14 Vision Transformer in [https://efficientml.ai](https://efficientml.ai/)](../../images/lec14/fab831c8-c65a-4a80-a73b-23f22f8bb524.png)

í•˜ì§€ë§Œ ì—¬ê¸°ì„œë„ ë¬¸ì œê°€ ìˆìŠµë‹ˆë‹¤. attentionì—ì„œ ì„±ëŠ¥ì„ ë³´ëŠ” ë°©ë²•ì¤‘ì— attention mapì„ í†µí•´ ë³´ë©´ Linear Attentionì´ ì‚¬ì§„ì˜ íŠ¹ì§•ì„ ì˜ ëª»ì¡ì•„ ëƒ…ë‹ˆë‹¤.

![Reference. MIT-TinyML lecture14 Vision Transformer in [https://efficientml.ai](https://efficientml.ai/)](../../images/lec14/d373e88a-17e9-4aff-8555-71b9382d01bb.png)

ë‹¹ì—°íˆ Softmax ë³´ë‹¤ distributionì´ ë‚ ì¹´ë¡­ì§€ ì•Šê¸° ë•Œë¬¸ì— íŠ¹ì§•ì ì—ì„œë„ ë‘ë“œëŸ¬ì§€ì§€ ì•ŠëŠ”ê²Œ ë¬¸ì œì£ . ì„±ëŠ¥ë„, ë‚˜ì˜¤ì§€ ì•Šì„ ê²ƒì´êµ¬ìš”. â€œMulti-scaleâ€ learningì„ í•˜ê¸°ê°€ ì–´ë ¤ìš¸ ê²ë‹ˆë‹¤.

![Reference. MIT-TinyML lecture14 Vision Transformer in [https://efficientml.ai](https://efficientml.ai/)](../../images/lec14/347fbc60-7fa7-47ff-b13f-d9b8dbd089e7.png)

í•´ê²°í•´ì•¼ê² ì£ ? ë°©ë²•ì€ ë ˆì´ì–´ë¥¼ í•˜ë‚˜ ë” ë„£ìœ¼ë©´ ë©ë‹ˆë‹¤. ì„±ëŠ¥ë„ ì˜¤íˆë ¤ ì „ë³´ë‹¤ í›¨ì”¬ ì¢‹ì•„ì¡Œë„¤ìš”.

![Reference. MIT-TinyML lecture14 Vision Transformer in [https://efficientml.ai](https://efficientml.ai/)](../../images/lec14/ea99a29b-31ee-43d2-8470-67176261ded8.png)

### 2.3 Sparse attention

ì„¸ ë²ˆì§¸ ê¸°ìˆ ì„ ì†Œê°œë“œë¦¬ê¸° ì „ì—, Vision Applicationì„ í•˜ë‹¤ ë³´ë©´ ì•„ë˜ì™€ ê°™ì€ ìƒí™©ì´ ë§ìŠµë‹ˆë‹¤. ì´ë¯¸ì§€ì˜ í•´ìƒë„ë¥¼ ì¤„ì´ê±°ë‚˜, Pruningì„ í†µí•´ ì´ë¯¸ì§€ê°€ íŠ¹ì •ë¶€ë¶„ë§Œ ë“¤ì–´ì˜¤ê²Œë˜ëŠ” ê²½ìš°ê°€ ìˆì£ .

![Reference. MIT-TinyML lecture14 Vision Transformer in [https://efficientml.ai](https://efficientml.ai/)](../../images/lec14/1676630f-64b9-4308-8891-7f7e0e55891e.png)

ê·¸ë˜ì„œ, Sparse attentionì´ë¼ëŠ” ê¸°ìˆ ì„ ì‚¬ìš©í•©ë‹ˆë‹¤. ì´ ê¸°ìˆ ì„ Pruning ê³¼ ë¹„ìŠ·í•˜ê²Œ Patchë§ˆë‹¤ Importanceë¥¼ ê³„ì‚°í•´ ì¤„ì„ ì„¸ì›ë‹ˆë‹¤. 

![Reference. MIT-TinyML lecture14 Vision Transformer in [https://efficientml.ai](https://efficientml.ai/)](../../images/lec14/Lecture_14-Vision_Transformer_(1).jpg)

ê·¸ë¦¬ê³  ì¤„ì„ ì„¸ìš´ Patchì—ì„œ Në²ˆì˜ ë°˜ë³µí•˜ëŠ” fine-tuningì„ í†µí•´ ëª¨ë¸ì„ ì¬í•™ìŠµì‹œí‚¤ì£ .

![Reference. MIT-TinyML lecture14 Vision Transformer in [https://efficientml.ai](https://efficientml.ai/)](../../images/lec14/09b90662-c653-4afb-ab8b-bf4548d89e74.png)

ì—¬ê¸°ì„œ Constraintì— ë§Œì¡±í•˜ëŠ” ì¡°í•©ì„ ì°¾ê¸° ìœ„í•´ Evolutionary Searchë¥¼ ì´ìš©í•©ë‹ˆë‹¤(Evolutionary SearchëŠ” [Lab 3](https://tinyml-kor.github.io/blog/posts/labs/lab03.html)ë¥¼ ì°¸ê³ í•´ì£¼ì„¸ìš”). 

![Reference. MIT-TinyML lecture14 Vision Transformer in [https://efficientml.ai](https://efficientml.ai/)](../../images/lec14/c88001b6-412d-4766-8c77-7ba3bf2e6d7e.png)

ì„±ëŠ¥ì´ ê¶ê¸ˆí•˜ë‹¤ë©´ [ì´ ë…¼ë¬¸](https://arxiv.org/pdf/2303.17605)ì„ ì°¸ê³ í•´ì£¼ì„¸ìš” :)

## **3. Self-supervised learning for ViT**

Efficient + Accelerationì— ëŒ€í•œ ê¸°ìˆ ì€ ì—¬ê¸°ê¹Œì§€ì…ë‹ˆë‹¤. ê·¸ëŸ¼ ë‹¤ì‹œ ì²˜ìŒìœ¼ë¡œ ëŒì•„ê°€ì„œ, í˜¹ì‹œ Vision Transformer ì²« ì„±ëŠ¥ ê·¸ë˜í”„ ê¸°ì–µí•˜ì‹œë‚˜ìš”? ë°ì´í„°ê°€ ì–´ë§ˆì–´ë§ˆí•˜ê²Œ ë§ì•„ì•¼ í–ˆë˜ ë¶€ë¶„ì´ìš”(ì•„ë˜ì— ê°€ì ¸ì™€ ë´¤ìŠµë‹ˆë‹¤). ê·¸ëŸ°ë°, ì´ë ‡ê²Œ **ë°ì´í„°ë¥¼ ë§ì´ êµ¬í•˜ëŠ” ê±´ í˜„ì‹¤ì ìœ¼ë¡œ ë§ì´ ì–´ë ¤ìš¸ ìˆ˜ ë°–ì— ì—†ìŠµë‹ˆë‹¤. ëŒ€í‘œì ì¸ ì˜ˆì‹œë¡œ ì˜ë£Œë°ì´í„°ê°€ ê·¸ë ‡ì£ .** ê·¸ëŸ¼ ì´ ë°ì´í„°ê°€ ë¶€ì¡±í•œ ê±´ ì–´ë–»ê²Œ í•´ê²°í•  ìˆ˜ ìˆì„ê¹Œìš”? 

![Reference. MIT-TinyML lecture14 Vision Transformer in [https://efficientml.ai](https://efficientml.ai/)](../../images/lec14/d2b025c5-739c-42b8-a4da-0f4d6a1ba893.png)

### 3.1 Contrastive learning

ì²«ë²ˆì§¸ ë°©ë²•ì€ Contrastive learning ì…ë‹ˆë‹¤. ì´ ë°©ë²•ì€ TinyMLì—ì„œ ë¿ ì•„ë‹ˆë¼ ë§ì´ ì“°ì´ëŠ” ë°©ë²•ì¸ë°, Positive Sampleê³¼ Negative Sampleì„ ê°€ì§€ê³  embedding vectorë¥¼ ë©€ê²Œ í•˜ëŠ” ë°©ì‹ì…ë‹ˆë‹¤. ì˜ˆë¥¼ ë“¤ì–´, ê³ ì–‘ì´ ì‚¬ì§„ì„ êµ¬ë¶„í•˜ëŠ” í…ŒìŠ¤í¬ì—ì„œ Positive Sampleì€ ê³ ì–‘ì´ ì‚¬ì§„ì´ ë  ê²ƒì´ê³ , Negative Sampleì€ ì—¬ê¸°ì„œ ê°•ì•„ì§€ ì‚¬ì§„ì´ ë  ê²ë‹ˆë‹¤.

![Reference. MIT-TinyML lecture14 Vision Transformer in [https://efficientml.ai](https://efficientml.ai/)](../../images/lec14/d5554c3e-7c99-492f-aa92-4d0c8e0ca3f3.png)

ì‹¤í—˜ê²°ê³¼ë¥¼ ë³´ë©´ Supervised ë°©ë²•ìœ¼ë¡œëŠ” CIFAR-100, Oxford Flowers-102, Oxford-IIIT Pets ë°ì´í„° ì…‹ì—ì„œëŠ” ëª¨ë¸ ì„±ëŠ¥ì´ ë‚˜ì˜¤ì§€ ì•Šì§€ë§Œ, ìœ„ ë°©ë²•ì„ ì ìš©í•œ ëª¨ë¸ì€ ì„±ëŠ¥ì´ ì–´ëŠì •ë„ ë‚˜ì˜¤ëŠ” ê²ƒì„ ë³¼ ìˆ˜ ìˆìŠµë‹ˆë‹¤. 

![Reference. An Empirical Study of Training Self-Supervised Vision Transformers [Chen et al., 2021]](../../images/lec14/78f92619-7122-4801-8b17-04e98d351ada.png)

Contrastive Learningìœ¼ë¡œ Multi-Modalì„ ì‚¬ìš©í•  ìˆ˜ë„ ìˆìŠµë‹ˆë‹¤. ì•„ë˜ ë…¼ë¬¸ì€ í…ìŠ¤íŠ¸ì™€ ì´ë¯¸ì§€ë¥¼ ëª¨ë‘ ë°›ëŠ” í˜•íƒœë¡œ ë””ìì¸ë¼ ìˆìŠµë‹ˆë‹¤.

![Reference. Learning Transferable Visual Models From Natural Language Supervision [Radford et al., 2021]](../../images/lec14/cffe9021-ac76-4827-a7ee-5405c915317c.png)

### 3.2 Masked image modeling

ë‘ë²ˆì§¸ ë°©ë²•ì€ Mask ì…ë‹ˆë‹¤. ì•„ë˜ ëª¨ë¸ êµ¬ì¡°ë¥¼ ë³´ì‹œì£ .

![Reference. MIT-TinyML lecture14 Vision Transformer in [https://efficientml.ai](https://efficientml.ai/)](../../images/lec14/9a81e7a2-dec8-43bd-a767-1ebec6f3fd76.png)

BERT ëª¨ë¸ì…ë‹ˆë‹¤. Maskë°©ë²•ì€ ì…ë ¥ í† í°ì— ë§ˆìŠ¤í¬ë¥¼ ì”Œì›Œ ì¶œë ¥ì—ì„œ ì´ë¥¼ ë§ì¶”ëŠ” í…ŒìŠ¤í¬ë¡œ ëª¨ë¸ì„ í›ˆë ¨ì‹œí‚¤ëŠ” ë°©ë²•ì…ë‹ˆë‹¤. ê·¸ëŸ¼ Visionì€ ì–´ë–»ê²Œ í• ê¹Œìš”?

![Reference. Masked Autoencoders Are Scalable Vision Learners [He et al., 2022]](../../images/lec14/2ee77c6b-afd7-4997-9617-bf66244c409b.png)

LLMê³¼ í›ˆë ¨í•˜ëŠ” ë°©ì‹ì€ Maskë¥¼ ì”Œìš°ê³  ì´ë¥¼ ì˜ˆì¸¡í•˜ë„ë¡ í•˜ëŠ” í›ˆë ¨ì¸ ê²ƒì€ ë¹„ìŠ·í•©ë‹ˆë‹¤. ë‹¤ë§Œ ViTì˜ ê²½ìš° Encoderì™€ Decoderê°€ ê°™ì´ ìˆìœ¼ë©´ì„œ Encoderë³´ë‹¤ëŠ” Decoderì˜ ëª¨ë¸í¬ê¸°ê°€ ë” ì‘ë‹¤ëŠ” ê±¸ ê°•ì¡°í•©ë‹ˆë‹¤. í¥ë¯¸ë¡œìš´ ë¶€ë¶„ì€ Masking ratioê°€ BERTì˜ ê²½ìš° 15%ì¸ ë°˜ë©´, ì•„ë˜ ì‹¤í—˜ê²°ê³¼ì—ì„œ ViTì˜ ê²½ìš°ëŠ” ë¬´ë ¤ 75%ë‚˜ ë©ë‹ˆë‹¤. ê°•ì˜ë…¸íŠ¸ì—ì„œëŠ” â€œì´ë¯¸ì§€ê°€ ì–¸ì–´ë³´ë‹¤ ë” information densityê°€ ë‚®ì•„ì„œ ê·¸ë ‡ë‹¤.â€ë¼ê³  ë§í•©ë‹ˆë‹¤.

![Reference. Masked Autoencoders Are Scalable Vision Learners [He et al., 2022]](../../images/lec14/90ae81a5-5b0a-4179-b4be-51a34dc0540e.png)

![Lecture_14-Vision Transformer (9).jpg](../../images/lec14/Lecture_14-Vision_Transformer_(9).jpg)

## 4. **Multi-modal LLM**

- [Cross attention (Flamingo)](https://arxiv.org/pdf/2204.14198)
- [Visual token (PaLM-E)](https://palm-e.github.io)

ë§ˆì§€ë§‰ìœ¼ë¡œ Multi-modal LLMì— ëŒ€í•´ì„œ ì–¸ê¸‰ì„ í•˜ëŠ”ë°, ìì„¸í•œ ë‚´ìš©ì€ ë‹¤ë£¨ì§€ ì•Šì•„ ê¶ê¸ˆí•˜ì‹  ë¶„ë“¤ì„ ìœ„í•´ ë…¼ë¬¸ì€ ë§í¬ë¡œ ë‹¬ì•„ë‘ê³  ì„¤ëª…ì€ ë„˜ì–´ê°€ë„ë¡ í•˜ê² ìŠµë‹ˆë‹¤. ì—¬ê¸°ê¹Œì§€ Vision Transformer ì˜€ìŠµë‹ˆë‹¤. LLMê³¼ ë¹„ìŠ·í•˜ë©´ì„œë„ ëª¨ë¸í¬ê¸°ê°€ ë˜‘ê°™ë‹¤ë©´ ì—°ì‚°ëŸ‰ ë†’ì€ ê²ƒê³¼ ë°ì´í„°ê°€ ë¶€ì¡±í•œ ê²ƒì„ ì–´ë–»ê²Œ í•´ê²°í•˜ëŠ”ê°€ê°€ ì¤‘ì ì¸ ê°•ì˜ì˜€ìŠµë‹ˆë‹¤. ë‹¤ìŒ ì‹œê°„ì—ëŠ” GAN, Video, and Point Cloudë¡œ ëŒì•„ì˜¤ê² ìŠµë‹ˆë‹¤ :D