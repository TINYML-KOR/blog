---
toc: true
title: "ğŸ§‘â€ğŸ« Lecture 17-18"
description:  Distributed training
author: "Gijeong Seong"
date: "2024-05-10"
categories: [lecture, distributed training]
---
<style>
p {
  line-height: 2; /* Adjust the line-height property as needed */
  text-align: justify;
  text-justify: inter-word; /* Adjust the justification mode as needed */
}

.image-container {
  text-align: center; /* Center-align the contents horizontally */
}

.centered-text {
  text-align: center; /* Center-align the text within the container */
}
</style>


ì´ë²ˆ ê°•ì˜ëŠ” ë¶„ì‚° í•™ìŠµ(distributed training)ì— ëŒ€í•œ ë‚´ìš©ì„ ë‹¤ë£¬ë‹¤.
LLMë“± ë”¥ëŸ¬ë‹ ëª¨ë¸ì´ ì ì  ë³µì¡í•´ì§€ë©´ì„œ, ëŒ€ê·œëª¨ ëª¨ë¸ì˜ í›ˆë ¨ì€ ë‹¨ì¼ GPUë¡œëŠ” ë” ì´ìƒ ì¶©ë¶„í•˜ì§€ ì•Šê³ , ì—¬ëŸ¬ê°œì˜ GPUë¥¼ ì‚¬ìš©í•´ì•¼ ê°€ëŠ¥í•˜ë‹¤.
ì´ë²ˆ ê¸€ì—ì„œëŠ” ë”¥ëŸ¬ë‹ ëª¨ë¸ì˜ í›ˆë ¨ì„ ê°€ì†í™”í•˜ê¸° ìœ„í•œ ë‹¤ì–‘í•œ ë¶„ì‚° í›ˆë ¨ ê¸°ë²•ë“¤ì„ ë‹¤ë£¨ê³ ì í•œë‹¤. 
ë°ì´í„° ë³‘ë ¬í™”, íŒŒì´í”„ë¼ì¸ ë³‘ë ¬í™”, í…ì„œ ë³‘ë ¬í™” ë“±ì˜ ê¸°ë²•ë“¤ì„ í†µí•´ íš¨ìœ¨ì ì¸ ë¶„ì‚° í›ˆë ¨ì„ ì‹¤í˜„í•˜ëŠ” ë°©ë²•ì— ëŒ€í•´ ì‚´í´ë³¼ ê²ƒì´ë‹¤.
ë˜í•œ, ë¶„ì‚° í›ˆë ¨ ë¿ë§Œ ì•„ë‹ˆë¼ GPUê°„ í†µì‹ ì˜ ë³‘ëª© í˜„ìƒì— ëŒ€í•œ ëŒ€ì²˜ë²•ìœ¼ë¡œ ê°€ì¤‘ì¹˜ ì••ì¶•(gradient compression) ë° ì§€ì—° ê°€ì¤‘ì¹˜ ì—…ë°ì´íŠ¸(delayed gradient update) ë˜í•œ ì‚´í´ë³¼ ê²ƒì´ë‹¤.

# 1. Background and motivation 

![](../../images/lec17/Pasted image 20240506122224.png)

ìœ„ í‘œëŠ” Nvidia A100 ê¸°ì¤€ìœ¼ë¡œ ëª¨ë¸ë³„ë¡œ í›ˆë ¨ì— ê±¸ë¦¬ëŠ” ì‹œê°„ì´ë‹¤.
distributed trainingì—†ì´ ë‹¨ì¼ GPUë¡œ í•™ìŠµí•œë‹¤ë©´, GPT-3ëŠ” í•™ìŠµí•˜ëŠ”ë° 355ë…„ì´ë‚˜ ê±¸ë¦°ë‹¤.

![](../../images/lec17/Pasted image 20240506122159.png)

ìœ„ ë†ë‹´ì²˜ëŸ¼, í›ˆë ¨ì— ë„ˆë¬´ ë§ì€ ì‹œê°„ì´ ê±¸ë¦°ë‹¤ë©´ ëª¨ë¸ì„ ë°œì „ì‹œí‚¤ê¸° ì–´ë ¤ìš¸ ê²ƒì´ë‹¤.
distribution trainingì„ ì‚¬ìš©í•˜ë©´, ì´ìƒì ìœ¼ë¡œëŠ” 10ì¼ì´ ê±¸ë¦¬ëŠ” ì‘ì—…ì„ 1024ê°œì˜ GPUë¡œëŠ” 14ë¶„ë§Œì— í•™ìŠµì´ ê°€ëŠ¥í•˜ë‹¤.

# 2. Parallelization methods for distributed training 
ê·¸ë ‡ë‹¤ë©´ ì–´ë–»ê²Œ ë³‘ë ¬ë¡œ í›ˆë ¨ì‹œí‚¬ ìˆ˜ ìˆì„ê¹Œ?
ì´ë²ˆ ì¥ì—ì„œëŠ” ê°„ë‹¨íˆ ì„¸ ê°€ì§€ ë°©ë²•ì„ ì†Œê°œí•˜ê³ , ê° ë°©ë²•ì— ëŒ€í•œ ìì„¸í•œ ì„¤ëª…ì€ í•˜ë‚˜ì”© ë‹¤ë£¨ë„ë¡ í•˜ê² ë‹¤.

![](../../images/lec17/Pasted image 20240506123242.png)

ì²« ë²ˆì§¸ëŠ” data parallelismì´ë‹¤.
ì´ ë°©ë²•ì€ í•™ìŠµ ë°ì´í„°ë¥¼ ì—¬ëŸ¬ê°œì˜ GPUì— ë‚˜ëˆ ì„œ í•™ìŠµí•œ ë’¤, ê³„ì‚°ëœ ê°ê°ì˜ gradientë¥¼ ë‹¤ì‹œ ì¤‘ì•™ì—ì„œ í•©ì¹˜ëŠ” ë°©ì‹ì´ë‹¤.

![](../../images/lec17/Pasted image 20240506123514.png)

ë‘ ë²ˆì§¸ëŠ” pipeline parallelismì´ë‹¤.
ì´ ë°©ë²•ì€ ëª¨ë¸ì„ layer ë³„ë¡œ ë‚˜ëˆ ì„œ í• ë‹¹í•˜ëŠ” ê²ƒì´ë‹¤. GPUë³„ë¡œ layerë¥¼ ë‚˜ëˆ  ê³„ì‚°í•˜ëŠ” ê²ƒì¸ë°, í•˜ë‚˜ì˜ GPUì— ëª¨ë¸ì˜ ëª¨ë“  ê°€ì¤‘ì¹˜ë¥¼ ë„£ê¸°ì—” ë„ˆë¬´ í´ ë•Œ ì‚¬ìš©í•œë‹¤.

![](../../images/lec17/Pasted image 20240506124054.png)

ë§ˆì§€ë§‰ìœ¼ë¡œ tensor parallelismì´ë‹¤.
ì´ëŠ” layerë³´ë‹¤ ë” ì˜ê²Œ, í…ì„œ ë‹¨ìœ„ë¡œ ìª¼ê°œëŠ” ê²ƒì´ë‹¤. 
ì§ê´€ì ìœ¼ë¡œ ë³´ë©´ ëª¨ë¸ì„ ì„¸ë¡œë¡œ ìª¼ê°œì„œ ì„œë¡œ ë‹¤ë¥¸ GPUì— í• ë‹¹í•˜ëŠ” ê²ƒì¸ë°, ìì„¸í•œ ì„¤ëª…ì€ ë’¤ì— ë‹¤ì‹œ ë‹¤ë£¨ê² ë‹¤

# 3. Data parallelism

![](../../images/lec17/Pasted image 20240506124559.png)

data parallelismì„ í•  ë–„ì—ëŠ” parameter serverì™€ worker nodesë¼ëŠ” ë‘ ê°€ì§€ ìš”ì†Œê°€ ì¡´ì¬í•œë‹¤.
worker nodesëŠ” ë¶„í• ëœ ë°ì´í„°ë¡œ í•™ìŠµì„ ì§„í–‰í•˜ê³ , parameter serverëŠ” workerë¡œë¶€í„° ì „ë‹¬ë°›ì€ gradientë¥¼ í•©ì¹œë‹¤.

![](../../images/lec17/Pasted image 20240506124712.png)

ìˆœì„œë¥¼ ì‚´í´ë³´ë©´

1. parameter serverì—ì„œ workerë¡œ ëª¨ë¸ ê°€ì¤‘ì¹˜ë¥¼ ë³µì‚¬
2. í•™ìŠµ ë°ì´í„°ì…‹ì„ workerì— ì ì ˆíˆ ë°°ë¶„
3. workerì—ì„œ gradient ê³„ì‚°
4. gradientë¥¼ parameter serverì— ì „ë‹¬
5. parameter serverì—ì„œ ìì²´ì ìœ¼ë¡œ ëª¨ë¸ì„ ì—…ë°ì´íŠ¸

# 4. Communication primitives 
ê·¸ëŸ°ë°, data parallelismì—ì„œ paramter serverì™€ workerê°„ì˜ í†µì‹ ì€ ì–´ë–»ê²Œ ì´ë£¨ì–´ì§ˆê¹Œ?

![](../../images/lec17/Pasted image 20240515195448.png)

ê°€ì¥ ë‹¨ìˆœí•œ ë°©ì‹ì€ ì†Œì¼“ í†µì‹ ê°™ì€ 1:1 ë°©ì‹ì´ë‹¤. 

![](../../images/lec17/Pasted image 20240515195549.png)

ë‹¤ë¥¸ ë°©ì‹ìœ¼ë¡œëŠ” ì¼ëŒ€ë‹¤ ë°©ì‹ì´ ìˆë‹¤.
í•œ ë…¸ë“œë¡œë¶€í„° ì •ë³´ë¥¼ ë¶„ì‚°í•˜ê³ , í•©ì¹˜ëŠ” ê²ƒì´ë‹¤.

![](../../images/lec17/Pasted image 20240515195711.png)

ì¼ëŒ€ë‹¤ ë°©ì‹ì—ì„œ ì¡°ê¸ˆ ë³€í˜•í•˜ë©´, reduce ë°©ì‹ì´ ëœë‹¤.
reduce ë°©ì‹ì€ gatherê³¼ ë¹„ìŠ·í•˜ì§€ë§Œ, ì •ë³´ë¥¼ ë‹¨ìˆœíˆ ëª¨ìœ¼ëŠ” ê²ƒì´ ì•„ë‹ˆë¼ í•©ì¹˜ê±°ë‚˜ í‰ê· ì„ ë‚´ëŠ” ê²ƒìœ¼ë¡œ(ë¹¨íŒŒë…¸ì´ˆì˜ í•©)í•˜ë‚˜ì˜ ê°’ì„ ë§Œë“¤ì–´ë‚´ê³ ,
ê·¸ëŸ° ë’¤ broadcastë¥¼ í†µí•´ ê°’ì„ ëª¨ë“  nodeë¡œ ì†¡ì‹ í•œë‹¤

![](../../images/lec17/Pasted image 20240515220645.png)
ê·¸ ë‹¤ìŒì€, ë‹¤ëŒ€ë‹¤ ë°©ì‹ì´ë‹¤.
all-reduceëŠ” ëª¨ë“  ë…¸ë“œì— ëŒ€í•´ reduceë¥¼ ì§„í–‰í•˜ëŠ” ê²ƒì´ê³ , all-gatherì„ í†µí•´ í•©ì¹˜ê±°ë‚˜ í•˜ëŠ” ê²ƒì´ ì•„ë‹ˆë¼ ëª¨ë“  ì •ë³´ë¥¼ ë‹¤ ê°–ê²Œ ëœë‹¤.

![](../../images/lec17/Pasted image 20240515220934.png)
ìš°ë¦¬ê°€ ì´ì „ ì¥ì—ì„œ ì‚´í´ë³¸ data parallelism ë°©ì‹ì˜ ë³µì¡ë„ë¥¼ êµ¬í•´ë³´ë©´, O(N)ì˜ ë³µì¡ë„ë¥¼ê°€ì§€ê³  ì´ëŠ” ê½¤ ë¶€ë‹´ë˜ëŠ” ì •ë„ì´ë‹¤.
ì´ë¥¼ í•´ê²°í•˜ê¸° ìœ„í•´ all reduceë¼ëŠ” ë°©ì‹ì„ ì‚¬ìš©í•œë‹¤.

![](../../images/lec17/Pasted image 20240515221032.png)
ê°€ì¥ ê°„ë‹¨í•œ ë°©ì‹ì€, sequentialë¡œ í•˜ë‚˜ì”© reduceí•˜ëŠ” ë°©ì‹ì´ë‹¤.
ì´ ë°©ì‹ì€ ì‹œê°„ê³¼ bandwith ì¸¡ë©´ì—ì„œ ëª¨ë‘ O(N)ì˜ ë³µì¡ë„ë¥¼ ê°€ì§„ë‹¤.

![](../../images/lec17/Pasted image 20240515221130.png)
Ring ë°©ì‹ì²˜ëŸ¼ ì˜†ì˜ ë…¸ë“œë¡œë§Œ ì „ì†¡í•˜ê²Œ ëœë‹¤ë©´, ì‹œê°„ì€ ê·¸ëŒ€ë¡œ O(N)ì´ì§€ë§Œ, bandwithëŠ” O(1)ì˜ ë³µì¡ë„ë¥¼ ê°–ê²Œ ëœë‹¤.

![](../../images/lec17/Pasted image 20240515221256.png)
ì¢…í•©í•´ë³´ë©´ ìœ„ í‘œì™€ ê°™ë‹¤.
ì–´ë–¤ ë°©ì‹ë„ O(N)ì„ ìš”êµ¬í•˜ëŠ”ë°, ì–´ë–¤ ì‹ìœ¼ë¡œ ë” ì¤„ì¼ ìˆ˜ ìˆì„ê¹Œ?

![](../../images/lec17/Pasted image 20240515221402.png)
recursive all reduce ë°©ì‹ì´ ê·¸ í•´ë²•ì´ë‹¤.
ì˜†ì— ìˆëŠ” ë…¸ë“œë¼ë¦¬ ì •ë³´ë¥¼ êµí™˜í•˜ëŠ” ë°©ì‹ìœ¼ë¡œ, log(N)ìœ¼ë¡œ í•´ê²° ê°€ëŠ¥í•˜ë‹¤.

# 5. Reducing memory in data parallelism: ZeRO-1 / 2 / 3 and FSDP 
data parallelismì—ì„œ ë©”ëª¨ë¦¬ë¥¼ ë” ì¤„ì´ëŠ” ZeRO ì‹œë¦¬ì¦ˆ ë°©ë²•ì„ ì†Œê°œí•œë‹¤.

![](../../images/lec17/Pasted image 20240515222204.png)

í•™ìŠµì„ í• ë•Œ weightì™€ gradient ì´ì™¸ì—ë„ adamë“±ì˜ optimizerë¥¼ ì‚¬ìš©í•˜ë©´ optimizer state(momentum, varianceê°’, master copy)ë¥¼ ì €ì¥í•´ì•¼ í•œë‹¤. 
ê·¸ë ‡ë‹¤ë©´ í•˜ë‚˜ì˜ íŒŒë¼ë¯¸í„°ì— 16byteê°€ í•„ìš”í•˜ê²Œ ëœë‹¤(fp16ê¸°ì¤€, weight=2, gradient=2, optim state=12(4*3))
ìœ„ ê·¸ë¦¼ì²˜ëŸ¼ ë§ì€ ë©”ëª¨ë¦¬ê°€ optimizer stateë¥¼ ì €ì¥í•˜ê¸° ìœ„í•´ ì‚¬ìš©ë˜ê²Œ ëœë‹¤.

![](../../images/lec17/Pasted image 20240515222305.png)

ê·¸ë˜ì„œ ê³ ì•ˆí•  ìˆ˜ ìˆëŠ” ë°©ë²•ì´ optimizer stateë¥¼ sharding(ë¶„ì‚°ì €ì¥)í•˜ëŠ” ë°©ì‹ì´ë‹¤.
ì´ëŸ¬ë©´ í•´ë‹¹í•˜ëŠ” êµ¬ê°„ì— ëŒ€í•œ weightë°–ì— ì—…ë°ì´íŠ¸ë¥¼ í•˜ì§€ ëª»í•˜ê²Œ ë˜ì§€ë§Œ, ë©”ëª¨ë¦¬ ì‚¬ìš©ëŸ‰ì€ ì¤„ì–´ë“¤ê²Œ ëœë‹¤.(12/N bytes for optim state)

![](../../images/lec17/Pasted image 20240515222457.png)

optimizer stateë¿ë§Œ ì•„ë‹ˆë¼, gradientë„ shardingí•  ìˆ˜ ìˆë‹¤.(2/N bytes for gradient)

![](../../images/lec17/Pasted image 20240515222601.png)

weightê¹Œì§€ë„ shardingí•  ìˆ˜ ìˆë‹¤.
í•˜ì§€ë§Œ gradientì— ë¹„í•´ ì´ê±´ ì¢€ ì–´ë ¤ìš´ë°, inferenceë¥¼ í•  ë•Œ ëª¨ë“  weightê°€ í•„ìš”í•˜ê¸° ë•Œë¬¸ì´ë‹¤.
ë”°ë¼ì„œ ì´ ë°©ì‹ì„ ì‚¬ìš©í•˜ë©´, inferenceì‹œì—ëŠ” ë‹¤ë¥¸ GPU(node)ë¡œë¶€í„° weightê°’ì„ ê°€ì ¸ì™€ì„œ ì§„í–‰í•˜ê²Œ ëœë‹¤.
(2/N bytes for weights)

# 6. Pipeline parallelism 
![](../../images/lec17/Pasted image 20240515222831.png)

data parallelismê³¼ ë‹¬ë¦¬ pipeline parallelismì€, ëª¨ë¸ì„ ë¶„í• í•˜ëŠ” ë°©ì‹ì´ë‹¤.

![](../../images/lec17/Pasted image 20240515223420.png)

ê°„ë‹¨í•˜ê²Œ êµ¬í˜„ì„ ìƒê°í•´ë³´ë©´, forward-backwardì— ëŒ€í•´ ìœ„ì™€ ê°™ì´ ì¼ìì ì¸ êµ¬ì¡°ë¡œ êµ¬í˜„í•  ìˆ˜ ìˆë‹¤.
í•˜ì§€ë§Œ, ì´ëŸ° ë°©ì‹ì´ë©´ F0ì´í›„ í•œì°¸ ë’¤ì— B0ì´ ì´ë£¨ì–´ ì§€ëŠ” ê²ƒì²˜ëŸ¼ ë¹„ê²Œë˜ëŠ” ì‹œê°„ì´ ë§ì•„ì§„ë‹¤.

![](../../images/lec17/Pasted image 20240515223531.png)

ì´ë¥¼ í•´ê²°í•˜ëŠ” ë°©ë²•ì´ Gpipeì— ì œì‹œëœë‹¤.
batchë¥¼ ì˜ê²Œ ìª¼ê°œì„œ forward-backwardê°„ì˜ ê°„ê²©ì„ ì¤„ì´ëŠ” ë°©ì‹ì´ë‹¤.
ì´ì „ì˜ naiveí•œ ë°©ë²•ì—ì„œ ì´ ë°©ë²•ìœ¼ë¡œ ë°”ê¿€ ì‹œ 2.5ë°°ì •ë„ utilizationì„ ëŠ˜ë¦´ ìˆ˜ ìˆë‹¤.(25%->57%)

# 7. Tensor parallelism
í•˜ì§€ë§Œ 57%ì˜ utilizationë„ ìƒê°í•´ë´¤ì„ ë•Œ ë„ˆë¬´ ì ê²Œ í™œìš©ë˜ëŠ” ê²ƒ ê°™ê¸°ë„ í•˜ë‹¤.
ê·¸ë ‡ë‹¤ë©´ ë” ì˜ê²Œ ìª¼ê°œëŠ” ë°©ì‹ì„ ìƒê°í•´ë³¼ ìˆ˜ ìˆì„ ê²ƒì´ë‹¤.

![](../../images/lec17/Pasted image 20240515223751.png)

ìœ„ ê·¸ë¦¼ì€ ê°ê° MLPì™€ self-attention layerì—ì„œ tensor parallelismì„ ì ìš©í–ˆì„ ë–„ì˜ êµ¬ì¡°ì´ë‹¤.
ë…¹ìƒ‰ f ë¥¼ í†µí•´ Xë¥¼ Nê°œ(ì˜ˆì‹œì—ì„œëŠ” 2ê°œ)ì˜ chunkë¡œ ìª¼ê°œê³ , ê°ê°ì˜ Xë¥¼ ì„œë¡œë‹¤ë¥¸ GPUì—ì„œ ì—°ì‚°ì„ ì§„í–‰í•œë‹¤.

# 8. Hybrid (mixed) parallelism and how to auto-parallelize 
ìœ„ì—ì„œ ìš°ë¦¬ëŠ” 3ê°€ì§€ ë°©ë²•ì˜ parallelism ë°©ë²•ì„ ì‚´í´ë³´ì•˜ë‹¤.
ì´ë²ˆ ì¥ì—ì„œëŠ” ì´ëŸ° ë°©ë²•ë“¤ì„ ì¡°í•©í•´ì„œ ì‚¬ìš©í•˜ëŠ” ë°©ë²•ì— ëŒ€í•´ ì‚´í´ë³¸ë‹¤.

![](../../images/lec17/Pasted image 20240515225649.png)

ë¨¼ì € ê° ë³‘ë ¬í™” ë°©ë²•ì„ ë‹¤ì‹œ ìš”ì•½í•´ë³´ë©´, data parallelismì€ ë°ì´í„°ë¥¼ ë‚˜ëˆ„ëŠ” ê²ƒì´ê³ , pipieline parallelismì€ modelì„ layerë‹¨ìœ„ë¡œ ë‚˜ëˆ„ëŠ” ê²ƒì´ë©° tensor parallelismì€ modelì„ tensorë‹¨ìœ„ë¡œ ë‚˜ëˆ„ëŠ” ê²ƒì´ë‹¤.

![](../../images/lec17/Pasted image 20240515225755.png)

ìœ„ ê·¸ë¦¼ì€ Data parallelism + Pipeline parallelismì„ ë™ì‹œì— ì ìš©í•œ ëª¨ìŠµì´ë‹¤.
2ê°œì˜ GPUë‹¨ìœ„ë¡œ dataë¥¼ ë‚˜ëˆ„ê³ , 2ê°œ GPUê°„ì— pipeline parallelismì„ ì ìš©í•œë‹¤.

![](../../images/lec17/Pasted image 20240515225917.png)

ìœ„ ê·¸ë¦¼ì€ pipeline parallelism + tensor parallelismì´ë‹¤.
ì´ë ‡ë“¯ 2ê°œì˜ ê¸°ë²•ë§Œì„ ì‚¬ìš©í•˜ëŠ” ê²ƒì´ ì•„ë‹ˆë¼ 3ê°œì˜ ë³‘ë ¬í™” ë°©ë²•ì„ ë™ì‹œì— ì‚¬ìš©í•  ìˆ˜ë„ ìˆë‹¤(3d parallelism)

![](../../images/lec17/Pasted image 20240515230018.png)

ê·¸ë¦¼ìœ¼ë¡œ í‘œí˜„í•˜ë©´ ìœ„ì™€ ê°™ë‹¤.

ìƒê°í•´ ë³¼ ë¬¸ì œëŠ”, ì´ë ‡ê²Œ ì—¬ëŸ¬ê°œì˜ ë³‘ë ¬í™” ë°©ì‹ì„ ì‚¬ìš©í•  ë•Œ, ì–´ë–¤ ì‹ìœ¼ë¡œ ì‚¬ìš©í•´ì•¼ ìµœì ì˜ ë°©ì‹ì´ ë˜ëŠëƒì˜ ë¬¸ì œì´ë‹¤.
ì¼ë°˜ì ìœ¼ë¡œëŠ” ëª¨ë¸ì´ GPUì—ì„œ ëŒì•„ê°€ì§€ ì•Šì„ ì •ë„ë¡œ í¬ë‹¤ë©´ pipeline, layerê°€ GPUì—ì„œ ëŒì•„ê°€ì§€ ì•Šì„ ì •ë„ë¡œ í¬ë‹¤ë©´ tensor ë°©ì‹ì„ ì‚¬ìš©í•˜ì§€ë§Œ ë‹¨ìˆœíˆ ê·¸ëŸ° ë°©ì‹ì„ ì ìš”í•˜ëŠ” ê²ƒì´ ìµœì ì˜ ë°©ë²•ì€ ì•„ë‹ˆë‹¤.

![](../../images/lec17/Pasted image 20240515230311.png)

ì´ë¥¼ NASì™€ ë¹„ìŠ·í•œ ë°©ì‹ìœ¼ë¡œ ì—°ì‚°ê°„ì˜ ê´€ê³„ë¥¼ ì„¤ì •í•˜ëŠ” inter-op passì™€ ì—°ì‚° ë‚´ë¶€ì˜ ë™ì‘ì„ ì„¤ì •í•˜ëŠ” intra-op pass ë‘ ë‹¨ê³„ë¥¼ í†µí•´ ìë™ìœ¼ë¡œ ì°¾ì•„ì£¼ëŠ” ë°©ì‹ì´ ì¡´ì¬í•œë‹¤.

# 9 Understand the bandwidth and latency bottleneck of distributed training 
![](../../images/lec17/Pasted image 20240515231159.png)
ë¶„ì‚° í•™ìŠµ ë°©ì‹ì—ëŠ” ë°˜ë“œì‹œ ë”°ë¼ì˜¤ëŠ” ë¬¸ì œì ì´, communicationì— ë”°ë¥¸ bottleneckì´ë‹¤.
ëª¨ë¸ì´ ì»¤ì§€ë©´ ë” ì—¬ëŸ¬ê³³ì— ë¶„ì‚°í•´ì•¼ í•˜ê³ , ë°ì´í„° í¬ê¸°ë„ ì»¤ì§€ë©°, ì „ì†¡ ì†ë„ë„ ê¸¸ì–´ì§„ë‹¤.
ê·¸ë ‡ê²Œ ë˜ë©´ communication latencyê°€ ê¸¸ì–´ì§ˆ ìˆ˜ ë°–ì— ì—†ë‹¤.

![](../../images/lec17/Pasted image 20240515231323.png)

ì‹¤ì œë¡œ, ë¶„ì‚° í•™ìŠµì—ì„œ GPUì˜ ê°œìˆ˜ì™€ speedëŠ” ì •í™•íˆ y=xì˜ ê·¸ë˜í”„ë¥¼ ê·¸ë¦¬ì§€ ì•ŠëŠ”ë‹¤.
ì´ëŠ” ì¤‘ê°„ì¤‘ê°„ì— bottleneckìœ¼ë¡œ ì‘ìš©í•˜ëŠ” ë¶€ë¶„ì´ ìˆìŒì„ ë³´ì—¬ì¤€ë‹¤.

![](../../images/lec17/Pasted image 20240515231413.png)

ê·¸ë¦¬ê³  ì´ latencyëŠ” nodeê°„ ê±°ë¦¬ê°€ ë©€ìˆ˜ë¡ ë‹¹ì—°íˆ í›¨ì”¬ ëŠ˜ì–´ë‚œë‹¤.

# 10. Gradient compression: overcome the bandwidth bottleneck 
ê·¸ë ‡ë‹¤ë©´ ì´ëŸ° dataì „ì†¡ì—ì„œ bottleneckì„ ì¤„ì´ë ¤ë©´ ì–´ë–»ê²Œ í•´ì•¼ í• ê¹Œ?

![](../../images/lec17/Pasted image 20240515232105.png)

ìš°ë¦¬ê°€ ì´ì „ì— í•´ì™”ë˜ pruning, quantization ë°©ì‹ì„ ë˜‘ê°™ì´ ì ìš©í•  ìˆ˜ ìˆë‹¤.
pruning/quantizationìœ¼ë¡œ ë°ì´í„° í¬ê¸°ë¥¼ ì¤„ì¸ë‹¤ë©´, ë‹¹ì—°íˆ ì „ì†¡í•  ë°ì´í„° ì–‘ë„ ì¤„ì–´ë“¤ê³  bottleneckë„ ì¤„ì–´ë“ ë‹¤.

## 10.1. Gradient Pruning: Sparse Communication, Deep Gradient Compression 
![](../../images/lec17/Pasted image 20240515232650.png)

pruning ë°©ì‹ì€ ê°„ë‹¨í•˜ê²Œ gradientì¤‘ì—ì„œ top-kê°œë§Œ ì „ì†¡í•˜ëŠ” ë°©ì‹ì´ë‹¤.
top-kì˜ ê¸°ì¤€ì€ ë‹¨ìˆœíˆ magnitudeë¥¼ ì‚¬ìš©í•œë‹¤.
ì „ì†¡ë˜ì§€ ì•Šì€ gradientë„ error feedbackì„ í†µí•´ localì— ë‚´ë¹„ë‘ì–´ ì‚¬ìš©í•œë‹¤.

![](../../images/lec17/Pasted image 20240515232822.png)

í•˜ì§€ë§Œ ì´ëŸ° ë°©ì‹ì€ ê½¤ë‚˜ ì„±ëŠ¥ ì €í•˜ë¥¼ ì¼ìœ¼í‚¨ë‹¤.
gradientë§Œ ì‚¬ìš©í•˜ë¯€ë¡œ, momentumì´ ì—†ê¸° ë–„ë¬¸ì´ë‹¤.

![](../../images/lec17/Pasted image 20240515233334.png)

ë‹¨ìˆœíˆ accumulateí•˜ë©´ ìœ„ì™€ ê°™ì´ momentumì„ ì‚¬ìš©í•˜ëŠ” ë°©ì‹ê³¼ ê½¤ ë‹¬ë¼ì§€ê²Œ ëœë‹¤.

![](../../images/lec17/Pasted image 20240515233418.png)

ë”°ë¼ì„œ gradientê°€ ì•„ë‹ˆë¼ velocityë¥¼ accumulateí•˜ëŠ” ê²ƒì´ ë” ì¢‹ë‹¤.
ë˜í•œ ì—¬ëŸ¬ê°€ì§€ warm up ë°©ì‹ì„ ì‚¬ìš©í•˜ëŠ” ê²ƒì´ ì¢‹ë‹¤.

![](../../images/lec17/Pasted image 20240515233504.png)

learning rateëŠ” ë¬¼ë¡ ì´ê³ , pruning sparsityë„ warm up ë°©ì‹ì„ ì‚¬ìš©í•˜ëŠ” ê²ƒì´ ë„ì›€ì´ ëœë‹¤.
optimizerê°€ ì ì‘í•˜ëŠ”ë° ë„ì›€ì„ ì¤€ë‹¤.

![](../../images/lec17/Pasted image 20240515233550.png)

pruning ë°©ì‹ì˜ ë¬¸ì œì ì€, ì—¬ëŸ¬ nodeë¼ë¦¬ ì •ë³´ë¥¼ êµí™˜í•˜ëŠ” all-reduce ê³¼ì •ì„ ê±°ì¹˜ë©´ì„œ pruning í•˜ëŠ” ì˜ë¯¸ê°€ ì—†ì–´ì§„ë‹¤ëŠ” ê²ƒì´ë‹¤(ë” denseí•´ì§„ë‹¤)

![](../../images/lec17/Pasted image 20240515233634.png)

ì´ëŸ° ë°©ì‹ì„ í•´ê²°í•˜ê¸° ìœ„í•´ sparseí•˜ê²Œ ë§Œë“œëŠ” ê²ƒì´ ì•„ë‹ˆë¼ low rank matrixë¥¼ ì‚¬ìš©í•˜ëŠ” ë°©ì‹ì„ ì‚¬ìš©í•˜ê¸°ë„ í•œë‹¤.

## 10.2. Gradient Quantization: 1-Bit SGD, TernGrad 
![](../../images/lec17/Pasted image 20240515233742.png)

quantization ë°©ë²•ì„ ì†Œê°œí•˜ë©´, 1bit-SGD ë°©ì‹ì´ ìˆë‹¤.
ì´ëŠ” 0ë³´ë‹¤ í¬ë©´ +, ì•„ë‹ˆë©´ -ë¡œ ë‘” ë’¤ scaling factor(u1~u4)ë¥¼ columë§ˆë‹¤ ì ìš©í•˜ëŠ” ë°©ì‹ì´ë‹¤.
 quantization errorëŠ” locallyí•˜ê²Œ ì§‘ê³„ë˜ê³ , quantizeëœ gradientë§Œ ì „ì†¡ëœë‹¤.

 ![](../../images/lec17/Pasted image 20240515234016.png)

 ë¹„ìŠ·í•œ ë°©ì‹ìœ¼ë¡œ, thresholdë¥¼ íŠ¹ì • ê°’ìœ¼ë¡œ ë‘ê³  quantizeí•˜ëŠ” ë°©ì‹ë„ ìˆë‹¤.

 ![](../../images/lec17/Pasted image 20240515234046.png)

 TerngradëŠ” í™•ë¥ ê°’ì— ë”°ë¼ 0, 1, -1 ì¤‘ í•˜ë‚˜ë¡œ ì–‘ìí™” í•˜ëŠ” ë°©ì‹ì´ë‹¤.
 
# 11. Delayed gradient update: overcome the latency bottleneck
pruningì´ë‚˜ quantize ë°©ì‹ìœ¼ë¡œ bandwithë¬¸ì œëŠ” í•´ê²°í–ˆì§€ë§Œ, latencyëŠ” ì´ëŸ° ë°©ë²•ë“¤ë¡œëŠ” í•´ê²°í•  ìˆ˜ ì—†ë‹¤.

![](../../images/lec17/Pasted image 20240515234229.png)

ê±°ë¦¬ë‚˜ ì‹ í˜¸ í˜¼ì¡ê°™ì€ ë¬¼ë¦¬ì ì¸ ì´ìœ ê°€ ìˆë‹¤.

![](../../images/lec17/Pasted image 20240515234338.png)

ê¸°ë³¸ì ì¸ ë¶„ì‚° ì²˜ë¦¬ ë°©ì‹ì—ì„œëŠ”, ê³„ì‚°->ì „ì†¡->ê³„ì‚° ë°©ì‹ìœ¼ë¡œ ì´ë£¨ì–´ì§€ëŠ”ë°, ì´ëŸ° êµ¬ì¡°ì—ì„œëŠ” ì „ì†¡ ì‹œê°„ì´ ëŠ˜ì–´ë‚˜ë©´ ê·¸ ì¦‰ì‹œ ì „ì²´ ì‹œê°„ì´ ëŠ˜ì–´ë‚œë‹¤

![](../../images/lec17/Pasted image 20240515234444.png)

ê·¸ë˜ì„œ ì „ì†¡ê³¼ ê³„ì‚°ì„ ë™ì‹œì— ì§„í–‰í•˜ë©´, ì–´ëŠì •ë„ ì‹œê°„ì„ ì ˆì•½í•  ìˆ˜ ìˆë‹¤.

![](../../images/lec17/Pasted image 20240515234559.png)

ì „ì†¡ ì‹œê°„ì´ ì–´ëŠì •ë„ ëŠ˜ì–´ë‚˜ë”ë¼ë„, trainingì„ ë°©í•´í•˜ì§€ ì•ŠëŠ”ë‹¤.

ì—¬ê¸°ê¹Œì§€ tinyML 17,18ê°•ì¸ distributed learningì— ê´€í•œ ì •ë¦¬ì˜€ë‹¤.
ì‹¤ì œë¡œ ëª¨ë¸ì´ ì ì  ì»¤ì§€ëŠ” ë§Œí¼, í˜¹ì€ edge í™˜ê²½ì—ì„œ ê°œê°œì¸ì˜ ë°ì´í„°ë¡œ í•™ìŠµì„ í•˜ëŠ” ê²½ìš°ì—ë„ ë¶„ì‚° ë°©ì‹ì´ ìœ ìš©í•˜ê²Œ ì‚¬ìš©ëœë‹¤.
í•˜ì§€ë§Œ ë¶„ì‚° ë°©ì‹ì—ì„œëŠ” ê°œì¸ì˜ ë°ì´í„°ê°€ ìœ ì¶œë  ìœ„í—˜ì´ ì¡´ì¬í•˜ê¸°ë„ í•œë‹¤.
ê·¸ëŸ° ë°©ë²•ì´ ë¬´ì—‡ì¸ì§€, ì–´ë–»ê²Œ ë§‰ì„ ìˆ˜ ìˆëŠ”ì§€ëŠ” ë’¤ì˜ ê°•ì˜ì—ì„œ ë‹¤ë£¨ë‹ˆ, ê´€ì‹¬ìˆëŠ” ë¶„ì€ ë’¤ì˜ í¬ìŠ¤íŒ…ë„ ì°¾ì•„ë³´ì‹œê¸¸ ë°”ë€ë‹¤.