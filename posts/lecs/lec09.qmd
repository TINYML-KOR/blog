---
toc: true
title: "ğŸ§‘â€ğŸ« Lecture 9"
description:  Knowledge Distillation(KD)
author: "Seunghyun Oh"
date: "2024-03-19"
categories: [lecture, knowledge distillation]
---
<style>
p {
  line-height: 2; /* Adjust the line-height property as needed */
  text-align: justify;
  text-justify: inter-word; /* Adjust the justification mode as needed */
}

.image-container {
  text-align: center; /* Center-align the contents horizontally */
}

.centered-text {
  text-align: center; /* Center-align the text within the container */
}
</style>

ì´ë²ˆ ì‹œê°„ì€ Knowledge Distillation ê¸°ë²•ì— ëŒ€í•´ì„œ ì´ì•¼ê¸° í•´ë³¼ê¹Œ í•´ìš”. ì§€ê¸ˆê¹Œì§€ **ì‘ì€ í¬ê¸°ì˜ ëª¨ë¸ì„ ë§Œë“œëŠ” ë°©ë²•ì— ëŒ€í•´ì„œ** ì•Œì•„ë´¤ì§€ë§Œ, ì—¬ì „íˆ ì‘ì€ ëª¨ë¸ì€ ì„±ëŠ¥ì ìœ¼ë¡œ ë¶€ì¡±í•œ ì ì´ ë§ì£ . ì„±ëŠ¥ì„ ê°œì„ ì‹œí‚¤ëŠ” ë‹¤ë¥¸ ë°©ë²•ì— ëŒ€í•´ì„œ ê³ ë¯¼í•˜ë‹¤ê°€ **â€œí¬ê¸°ê°€ í° ëª¨ë¸ì„ ì´ìš©í•´ë³´ì.â€** ì—ì„œ ë‚˜ì˜¨ ì•„ì´ë””ì–´ê°€ ë°”ë¡œ Knowledge Distillation ì…ë‹ˆë‹¤.

## 1. What is Knowledge Distillation?

Knowledge Distillationì€ ê°„ë‹¨í•˜ê²Œ Teach Networkë¼ê³  ë¶ˆë¦¬ëŠ” í¬ê¸°ê°€ í° ëª¨ë¸ì´ ìˆì–´ìš”. ì´ Teacher Networkê°€ ë¨¼ì € Trainingì„ í•©ë‹ˆë‹¤. ê·¸ëŸ° ë‹¤ìŒ ì˜¤ëŠ˜ì˜ ì£¼ì¸ê³µ **Student Network**ë¡œ ë¶ˆë¦¬ëŠ” í¬ê¸°ê°€ ì‘ì€ ëª¨ë¸ì´ ìˆì£ . ì´ ëª¨ë¸ì€ ë‘ ê°€ì§€ ë°©ì‹ìœ¼ë¡œ í•™ìŠµì„ í•˜ëŠ”ë°, ì²« ë²ˆì§¸ëŠ” ê¸°ì¡´ì— í•™ìŠµí•˜ë˜ ëŒ€ë¡œ Target ë°ì´í„°ë¡œë¶€í„° í•™ìŠµì´ ìˆêµ¬ìš”. ë‹¤ë¥¸ í•œ ê°€ì§€ëŠ” **Teacher Network**ë¥¼ ë”°ë¼ê°€ëŠ” í•™ìŠµì´ ìˆìŠµë‹ˆë‹¤. 

![Reference. Knowledge Distillation: A Survey [GouÂ et al., IJCV 2020]](../../images/lec09/Untitled.png)

- The goal of knowledge distillation is to align the class probability distributions from teacher and student networks.

ê·¸ëŸ¼ ê¶ê¸ˆí•œ ì ì´ **Teacher Networkì— ì–´ë–¤ ì ì„ ë°°ì›Œì•¼í• ê¹Œìš”?** ê°•ì˜ì—ì„œëŠ” ì´ 6ê°œë¡œ Output logit, Intermediate weight, Intermediate feature, Gradient, Sparsity pattern, Relational informationìœ¼ë¡œ ë‚˜ëˆ ì„œ ì„¤ëª…í•©ë‹ˆë‹¤. ì„¤ëª…í•˜ê¸°ì— ì•ì„œì„œ ê°œë…í•˜ë‚˜ ì†Œê°œí•˜ê³  ë„˜ì–´ê°ˆê»˜ìš”.

![Reference. MIT-TinyML-lecture10-Knowledge-Distillation in [https://efficientml.ai](https://efficientml.ai/)](../../images/lec09/Untitled%201.png)

ë§Œì•½ ìœ„ Teacher Networkê°€ í•™ìŠµí•œ ê²°ê³¼(T=1)ë¡œ ê³ ì–‘ì´ ì‚¬ì§„ì¼ í™•ë¥  0.982, ê°•ì•„ì§€ ì‚¬ì§„ì¼ í™•ë¥ ì´ 0.017ì´ë¼ê³  í•©ì‹œë‹¤. ê·¸ëŸ¼ Student NetworkëŠ” Output logitì„ í•™ìŠµí•œë‹¤ê³  ê°€ì •í•˜ë©´, ì´ ë‘ í™•ë¥ ì„ ë”°ë¼ ê°ˆê²ë‹ˆë‹¤. í•˜ì§€ë§Œ Student Networkì— ë”°ë¼ ì´ ìˆ˜ì¹˜ê¹Œì§€ í•™ìŠµí•˜ê¸° ì–´ë ¤ìš¸ ìˆ˜ ìˆìŠµë‹ˆë‹¤. ì´ ë•Œ â€œ**Temperature(T)**â€ì´ë¼ëŠ” ê°œë…ì„ ë“¤ì—¬ì™€ Teacher Networkì˜ Catê³¼ Dogì— ëŒ€í•œ í™•ë¥ ì„ ì¢€ ë” Smoothí•˜ê²Œ ë§Œë“¤ì£ .

$$
p(z_i,T) = \dfrac{exp(z_j/T)}{\sum_j exp(z_j/T)}
$$

ì‹ìœ¼ë¡œ ì“°ë©´ ìœ„ì™€ ê°™ì´ ë í…ë°, **ë³´í†µì€ 1ë¡œ ë‘ê³ ** í•œë‹¤ê³  ê°•ì˜ì—ì„œ ì–¸ê¸‰í•©ë‹ˆë‹¤. ì™œ ì„¤ëª…í–ˆëƒêµ¬ìš”? í˜¹ì‹œë‚˜ ê°œë…ì´ ë‚˜ì˜¤ë©´ ì´í•´í•˜ì‹œê¸° í¸í•˜ì‹œë¼êµ¬ìš”ğŸ™‚Â ê·¸ëŸ¼, **Teacher Networkì—ì„œ ì–´ë–¤ ë¶€ë¶„ì„ Student Networkì— í•™ìŠµì‹œí‚¬ì§€** ì•Œì•„ë³´ì‹œì£ .

## 2. What to match between Teacher and Student Network?

### 2.1 Output logits

ì²« ë²ˆì§¸ëŠ” Output logitì…ë‹ˆë‹¤. lossë¡œëŠ” ëŒ€í‘œì ìœ¼ë¡œ Cross entropy lossì™€ L2 lossê°€ ìˆê² ì£ .

![Reference. MIT-TinyML-lecture10-Knowledge-Distillation in [https://efficientml.ai](https://efficientml.ai/)](../../images/lec09/Untitled%202.png)

### 2.2 Intermediate weights

ë‘ ë²ˆì§¸ëŠ” Layerë§ˆë‹¤ Weightì…ë‹ˆë‹¤. í•˜ì§€ë§Œ **Student Modelì€ Weight dimesionì´ ë‹¤ë¥¼ ìˆ˜ ë°–ì— ì—†ëŠ”ë°**, ê·¸ëŸ¼ Linear Transformationì„ ì´ìš©í•´ì„œ Dimensionì„ ë§ì¶° í•™ìŠµí•˜ë©´ ë˜ê² ë„¤ìš”.

![Reference. MIT-TinyML-lecture10-Knowledge-Distillation in [https://efficientml.ai](https://efficientml.ai/)](../../images/lec09/Untitled%203.png)

![Reference. MIT-TinyML-lecture10-Knowledge-Distillation in [https://efficientml.ai](https://efficientml.ai/)](../../images/lec09/Untitled%204.png)

ìŠ¤í„°ë”” ì¤‘ì— ë‚˜ì˜¨ ì§ˆë¬¸ì´ â€œê·¸ëŸ¼ Student Networkì—ì„œ ì¶”ê°€ì ì¸ ë ˆì´ì–´ê°€ ìƒê¸°ëŠ”ë°, ì‘ê²Œ ë§Œë“œëŠ” ì˜ë¯¸ê°€ ì—†ì§€ ì•ŠëŠëƒ?â€ ì˜€ìŠµë‹ˆë‹¤. ì œ ìƒê°ì€ Weight Dimensionì„ ë§ì¶”ê¸° ìœ„í•œ Linear Transformationì„ ìœ„í•œ ë ˆì´ì–´ëŠ” ì¶”ë¡ ì‹œ ì‚¬ìš©í•˜ì§€ ì•Šì„ ìˆ˜ ìˆìœ¼ë‹ˆ, Student Networkì˜ ì„±ëŠ¥ì„ ë†’ì´ê¸° ë” ì œê²©ì•„ë‹ê¹Œìš”? ë§ˆì¹˜ ì¶”ë¡  ë•Œ í•„ìš”í•œ ë¶€í’ˆë§Œ ì¡°ë¦½í•˜ë“¯ ë§ì´ì£ .

### 2.3 Intermediate features

ì„¸ ë²ˆì§¸ëŠ” Feature ì…ë‹ˆë‹¤. ì´ì „ ê²½ìš°ê°€ Weightë¼ê³  í•˜ë©´, ì´ë²ˆì€ Layerì˜ Outputì…ë‹ˆë‹¤. Teach Networkê³¼ Student Networkì˜ Featureë¥¼ ê°™ê²Œ í•™ìŠµì‹œí‚¤ëŠ” ë°©ë²•ì€ ì—¬ëŸ¬ê°€ì§€ê°€ ìˆëŠ”ë°, ì—¬ê¸°ì„œëŠ” Cosine of angleë¡œ í•™ìŠµì„ ì‹œí‚¤ëŠ” ë°©ë²•([Like What You Like: Knowledge Distill via Neuron Selectivity Transfer [Huang and Wang, arXiv 2017]](https://arxiv.org/pdf/1707.01219.pdf)) ê³¼ Dimensionì„ ì¤„ì—¬ì„œ í•™ìŠµì„ ì‹œí‚¤ëŠ” ë°©ë²•([Paraphrasing Complex Network: Network Compression via Factor Transfer [KimÂ *et al.*, NeurIPS 2018]](https://arxiv.org/pdf/1802.04977.pdf))ì„ ì†Œê°œí•©ë‹ˆë‹¤.

![Reference. Like What You Like: Knowledge Distill via Neuron Selectivity Transfer [Huang and Wang, arXiv 2017]](../../images/lec09/Untitled%205.png)

![Reference. Paraphrasing Complex Network: Network Compression via Factor Transfer [KimÂ *et al.*, NeurIPS 2018]](../../images/lec09/Untitled%206.png)

- The paraphraser shrinks the output teacher feature map from m dimensions to m x k dimensions (called **factor** typically k=0.5) and then expands the dimensionality back to m.
- The output of paraphraser is supervised with a reconstruction loss against the original m-dimensional output.
- Student uses one layer of MLP to obtain a **factor** with the same dimensionality of m x k.
- FT minimizes the distance between teacher and student factors.

### 2.4 Gradients

ë„¤ ë²ˆì§¸ëŠ” Gradient ì…ë‹ˆë‹¤. Gradientë¥¼ ì‹œê°ì ìœ¼ë¡œ ë³¼ ìˆ˜ ìˆëŠ” ë°©ë²•ì—ëŠ” Attention Mapì´ ìˆëŠ”ë°ìš”, ì´ Attention Mapì€ ì´ë¯¸ì§€ì—ì„œ íŠ¹ì§•ì ì¸ ë¶€ë¶„ì„ ì¡ì•„ë‚¼ ìˆ˜ ìˆì£ .

- Reference: Paying More Attention to Attention: Improving the Performance of Convolutional Neural Networks via Attention Transfer [Zagoruyko and Komodakis, ICLR 2017]
- Gradients of feature maps are used to characterize attention of DNNs
- The attention of a CNN feature map $x$ is defined as $\dfrac{\partial L}{\partial x}$, where $L$ is the learning objective.
- Intuition: If $\dfrac{\partial L}{\partial x_{i,j}}$ is large, a small perturbation at $i,j$ will significantly impact the final output. As a result, the network is putting more attention on position $i, j$

![Reference. MIT-TinyML-lecture10-Knowledge-Distillation in [https://efficientml.ai](https://efficientml.ai/)](../../images/lec09/Untitled%207.png)

ì•„ë˜ ê·¸ë¦¼ì€ **â€œAttention Mapì´ ëª¨ë¸ì˜ ì„±ëŠ¥ì´ ë†’ë‹¤ë©´ ë¹„ìŠ·í•œ íŒ¨í„´ìœ¼ë¡œ ë‚˜ì˜¨ë‹¤.â€** ëŠ” ì˜ˆì‹œë¡œ ë‚˜ì˜µë‹ˆë‹¤.  Resnet34ì™€ ResNet101ì˜ Attention Mapì€ ìœ ì‚¬í•˜ê²Œ ë³´ì´ëŠ” ë°˜ë©´ NINì¸ ê²½ìš°ëŠ” ë§ì´ ë‹¤ë¥¸ ê²ƒì„ í™•ì¸ í•  ìˆ˜ ìˆìŠµë‹ˆë‹¤. 

- Performant models have similar attention maps
    
    Attention makes of performant ImageNet models (ResNets) are indeed similar to each other, but the less performant model(NIN) has quite different attention maps
    
    ![Reference. MIT-TinyML-lecture10-Knowledge-Distillation in [https://efficientml.ai](https://efficientml.ai/)](../../images/lec09/Untitled%208.png)    

### 2.5 Sparsity patterns

ë‹¤ì„¯ ë²ˆì§¸ëŠ” Sparsity Pattern ì…ë‹ˆë‹¤. Layerë§ˆë‹¤ Output Acitivationì„ ê°™ê²Œ ë§Œë“œëŠ” ë°©ë²•ì¸ë°, Intermediate Featureì™€ ìœ ì‚¬í•˜ê²Œ ë³´ì´ë„¤ìš”.

![Reference. Knowledge Transfer via Distillation of Activation Boundaries Formed by Hidden Neurons [HeoÂ *et al.*, AAAI 2019]](../../images/lec09/Untitled%209.png)

- Intuition: the teacher and student networks should have similar sparsity patterns after the ReLU activation. A neuron is activated after ReLU if its value is larger than 0, denoted by the indicator function $\rho(x) = 1 [x>0]$.

- We want to minimize $\mathscr{L}(I) = \lvert\lvert \rho(T(I))-\rho(S(I)) \lvert\lvert_1$, where $S$ and $T$ corresponds to student and teacher networks, respectively

### 2.6.1 Relational information: Different Layers

ë§ˆì§€ë§‰ìœ¼ë¡œ ëª¨ë¸ë‚´ì—ì„œ ë‚˜ì˜¤ëŠ” í…ì„œì˜ ìƒí˜¸ ì—°ê´€ì„±ì— ëŒ€í•´ì„œë„ ê°™ê²Œ í•  ìˆ˜ ìˆë‹¤ëŠ” ë°©ë²• ë‘ ê°€ì§€ê°€ ë‚˜ì˜µë‹ˆë‹¤. ì²« ë²ˆì§¸ëŠ” ê° ë ˆì´ì–´ì˜ ì…ë ¥, ì¶œë ¥ í…ì„œë¥¼ Inner product í•˜ê²Œ ë˜ë©´ í•˜ë‚˜ì˜ Matrixë¥¼ ì–»ì„ ìˆ˜ ìˆëŠ”ë°, ì´ Matrixë¥¼ ê°™ê²Œ í•™ìŠµì‹œí‚¨ë‹¤ëŠ” ì•„ì´ë””ì–´ì£ .

![Reference: A Gift from Knowledge Distillation: Fast Optimization, Network Minimization and Transfer Learning [YimÂ *et al.*, CVPR 2017]](../../images/lec09/KD-related-info.gif)


1. Use **inner product to extract relational information** (a matrix of shape $C_{in} \times C_{out}$, reduction on the spatial dimensions) for both student and teacher networks. *Note: the student and teacher networks only differ in number of layers, not number of channels

2. Then match the resulting **dot products between teacher and student networks** $(G_1^T, G_1^S)$

### 2.6.1 Relational information: Different Samples

ë‘ ë²ˆì§¸ëŠ” ì´ì „ê¹Œì§€ ì €í¬ëŠ” í•™ìŠµë°ì´í„° í•˜ë‚˜í•˜ë‚˜ë§ˆë‹¤ ë‚˜ì˜¤ëŠ” ê²°ê³¼ë¥¼ Teachì™€ Studentë¥¼ ê°™ê²Œë” í•™ìŠµì‹œì¼°ëŠ”ë°, ì´ë²ˆì—” **ì—¬ëŸ¬ í•™ìŠµë°ì´í„°ì—ì„œ ë‚˜ì˜¨ ì—¬ëŸ¬ Outputì„ í•˜ë‚˜ì˜ Matrix í˜•íƒœ**ë¡œ ë‹®ê²Œ ë§Œë“œëŠ” ë°©ë²•ì…ë‹ˆë‹¤.

- **Conventional KD** focuses on matching features / logins **for one input**. **Relation KD** looks at the relations between intermediate features **from multiple inputs.**

![Reference. Relational Knowledge Distillation [ParkÂ *et al.*, CVPR 2019]](../../images/lec09/Untitled%2010.png)

- Relation between different samples
    
    ![Reference. Relational Knowledge Distillation [ParkÂ *et al.*, CVPR 2019]](../../images/lec09/Untitled%2011.png)
        

ì§€ê¸ˆê¹Œì§€ Student Networkê°€ Teacher Networkì˜ ì–´ë–¤ Outputì„ ê°€ì§€ê³  í•™ìŠµì‹œí‚¬ì§€ì— ëŒ€í•´ì„œ ì•Œì•„ë´¤ìŠµë‹ˆë‹¤. ê·¸ëŸ°ë° ì €í¬ê°€ TinyMLì„ í•˜ëŠ” ëª©ì ì€ ì‚¬ì‹¤ **â€œë” ì‘ì€ ë„¤íŠ¸ì›Œí¬ë¥¼ ì‚¬ìš©í•´ë³´ì.â€** ì´ì§€ ì•Šì•˜ë‚˜ìš”? ì¦‰, Teacher Networkì—†ì´ Student Networkë§Œìœ¼ë¡œëŠ” í•  ìˆ˜ ìˆëŠ” ë°©ë²•ì´ ì—†ì„ê¹Œìš”? ì´ëŸ° ìƒê°ì—ì„œ ë‚˜ì˜¨ ì•„ì´ë””ì–´ê°€ Self and Online Distillation ì…ë‹ˆë‹¤.

## 3. Self and Online Distillation

- What is the disadvantage of fixed large teachers? Does it have to be the case that we need a fixed large teacher in KD?

### 3.1 Self Distillation

ì²« ë²ˆì§¸ Self Distillationì€ êµ¬ì¡°ê°€ ê°™ì€ ë„¤íŠ¸ì›Œí¬ë¥¼ ê³„ì†í•´ì„œ ë³µì‚¬í•´ ë‚˜ê°‘ë‹ˆë‹¤. ê·¸ëŸ¬ë©´ì„œ ì´ì „ì— í•™ìŠµí•œ ë„¤íŠ¸ì›Œí¬ë¡œ ë¶€í„°ë„ ë³µì‚¬ëœ ë„¤íŠ¸ì›Œí¬ê°€ í•™ìŠµí•  ìˆ˜ ìˆë„ë¡ í•˜ë©°, kê°œ ë§Œí¼ ë³µì‚¬í•˜ë©° í•™ìŠµí•œ í›„ì— ìµœì¢… Outputìœ¼ë¡œëŠ” ë³µì‚¬í•œ ë„¤íŠ¸ì›Œí¬ë“¤ì˜ Outputì„ Ensembleí•œ ê²°ê³¼ë¥¼ ì´ìš©í•˜ì£ . ì—¬ê¸°ì„œ AccuracyëŠ” kë²ˆì§¸ë¡œ ê°ˆìˆ˜ë¡ ëŠ˜ì–´ë‚˜ê² ì£ ?

![Born-Again Neural Networks [Furlanello et al., ICML 2018]](../../images/lec09/Untitled%2012.png)

- Born-Again Networks generalizes defensive distillation by **adding iterative training states** and **using both classification objective and distillation objective** in subsequent stages.

- Network architecture $T = S_1=S_2=\dots=S_k$

- Network accuracy $T < S_1 < S_2 < \dots < S_k$

- Can alteratively ensemble $T,S_1, S_2, \dots, S_k$ to get even better performance

### 3.2 Online Distillation

ë‘ ë²ˆì§¸ëŠ” Online Distillationì¸ë°, ì—¬ê¸° ì•„ì´ë””ì–´ëŠ” **â€œê°™ì€ ë„¤íŠ¸ì›Œí¬ êµ¬ì¡°ë¥¼ ì“°ì.â€** ì…ë‹ˆë‹¤. ê·¸ë¦¬ê³  Teacher networkê³¼ Student networkëŠ” ì²˜ìŒë¶€íƒ€ ê°™ì´ í•™ìŠµí•˜ëŠ”ë°, Lossì— í•œ ê°€ì§€ í•­ì´ ì¶”ê°€ë˜ì£ . ë°”ë¡œ â€œKL Divergenceâ€ ì…ë‹ˆë‹¤. 

![Reference: Deep Mutual Learning [ZhangÂ et al., CVPR 2018]](../../images/lec09/Untitled%2013.png)

- Idea: for both teach and student networks, we want to add a distillation objective that minimizes the output distribution of the other party.

- $\mathscr{L}(S) = CrossEntropy(S(I), y)+KL(S(I), T(I))$

- $\mathscr{L}(T) = CrossEntropy(T(I), y)+KL(S(I), T(I))$

- It is not necessary to retrain $T$ and $S=T$ is allowed
    
    ![Reference. Deep Mutual Learning [ZhangÂ et al., CVPR 2018]](../../images/lec09/Untitled%2014.png)

### 3.3 Combined Distillation

ë§ˆì§€ë§‰ì€ Self ì™€ Online Distillationì„ í•©ì¹œ ì—°êµ¬ë“¤ì„ ì†Œê°œí• ê²Œìš”. 

ì²« ë²ˆì§¸ëŠ” On-the-Fly Native Ensemble ì…ë‹ˆë‹¤. êµ¬ì¡°ë¥¼ ë³´ì‹œë©´ Branch ë§ˆë‹¤ ëª¨ë¸ì˜ êµ¬ì¡°ë„ ë™ì¼í•˜ê²Œ Branch 0, Branch 1, â€¦ , Branch m ìœ¼ë¡œ ë‚˜ë‰˜ëŠ” ê²Œ Self Distillationë¥¼ ë³´ëŠ” ë“¯í•˜ì£ . ê·¸ë¦¬ê³  ê° Branchë¥¼ í•™ìŠµì‹œ ë™ì‹œì— ì§„í–‰í•˜ëŠ” ê²ƒìœ¼ë¡œ ë³´ì´ë„¤ìš”. 

![Reference. Knowledge Distillation by On-the-Fly Native Ensemble [Lan et al., NeurlPS 2018]](../../images/lec09/Untitled%2015.png)

- Idea: generating **multiple** output probability distributions and **ensemble** them as the target distribution for knowledge distillation.

- Similar to DML(Deep Mutual Learning), ONE allows the teacher model to be exactly the same as the student model, and it does not require retraining the teach network first. It is also not necessary to train two models as in DML.

- Result
    
    ![Reference. Knowledge Distillation by On-the-Fly Native Ensemble [Lan et al., NeurlPS 2018]](../../images/lec09/Untitled%2016.png)
        

ë‘ ë²ˆì§¸ ì—°êµ¬ëŠ” Be Your Own Teacher ë¼ëŠ” ì—°êµ¬ì¸ë°, ì—¬ê¸°ì„œëŠ” ê° ë ˆì´ì–´ë§ˆë‹¤ ë‚˜ì˜¨ Feature mapì— ì¶”ê°€ì ì¸ ë ˆì´ì–´ë¥¼ ë¶™ì—¬ì„œ Self Distillationì˜ ë°©ë²•ì„ ì´ìš©í•©ë‹ˆë‹¤. Lossë¡œëŠ” Cross entropy(Output Logit), ì¶”ê°€ì ìœ¼ë¡œ ë¶™ì—¬ì„œ ë§Œë“  ê° ëª¨ë¸ë§ˆë‹¤ KL Divergence, ê·¸ë¦¬ê³  intermediate featureë¥¼ ì‚¬ìš©í•˜ë„¤ìš”. í¥ë¯¸ë¡œì› ë˜ ì ì€ ì²« ë²ˆì§¸, ë‘ ë²ˆì§¸ ë ˆì´ì–´ì—ì„œëŠ” ì„±ëŠ¥ì´ ê±°ì˜ ë‚˜ì˜¤ì§€ ì•Šì„ ê²ƒ ê°™ì•˜ëŠ”ë° ë‘ ë²ˆì§¸ ë ˆì´ì–´ë¶€í„°ëŠ” Ensembleê¹Œì§€ ì–´ëŠì •ë„ ì„±ëŠ¥ì´ ë‚˜ì˜¤ëŠ” ê²ƒì„ ê²°ê³¼ì—ì„œ ë³¼ ìˆ˜ ìˆì–´ìš”. 

![Reference. Be Your Own Teacher: Improve the Performance of Convolutional Neural Networks via Self Distillation [ZhangÂ *et al.*, ICCV 2019]](../../images/lec09/Untitled%2017.png)

- Use deeper layers to distill shallower layers.

- Intuition: Labels at later stages are more reliable, so the authors use them to supervise the predictions from the previous stages.

- Result
    
    ![Reference. Be Your Own Teacher: Improve the Performance of Convolutional Neural Networks via Self Distillation [ZhangÂ *et al.*, ICCV 2019]](../../images/lec09/Untitled%2018.png)    

## **4. Distillation for different tasks**

ì´ë ‡ê²Œ ì•Œì•„ë³¸ Knowledge Distillationì€ ì–´ë–¤ Applicationì— ì‚¬ìš©ë  ìˆ˜ ìˆì„ê¹Œìš”? ê°•ì˜ëŠ” Object Detection, Semantic Segmentation, GAN, Transformer ëª¨ë¸ë¡œ ë‚˜ëˆ ì„œ ì´ì•¼ê¸°í•©ë‹ˆë‹¤. ê° ë¶€ë¶„ë§ˆë‹¤ ì–´ë–¤ ë¬¸ì œë¥¼ í•´ê²°í•˜ëŠ”ì§€ í˜¹ì€ ì–´ë–¤ ì•„ì´ë””ì–´ë¥¼ ì‚¬ìš©í–ˆëŠ”ì§€ë§Œ ì§šê³  ë„˜ì–´ê°€ë³¼ê²Œìš”(ìì„¸í•œ ë‚´ìš©ì€ ë…¼ë¬¸ì„ ì°¸ì¡°!).

### 4.1 Object Detection

Object Detectionì€ ì„¸ ê°€ì§€ë¡œ í•´ê²°í•´ì•¼í•  ë¬¸ì œê°€ ëŠ˜ì–´ë‚¬ìŠµë‹ˆë‹¤. í•˜ë‚˜ëŠ” Classification, ê·¸ë™ì•ˆ í•´ì™”ë˜ ë¶€ë¶„ì´êµ¬ìš”, ë‹¤ë¥¸ ë‘ ê°œëŠ” Backgroundì™€ Foregroundì„ êµ¬ë¶„í•˜ëŠ” ê²ƒê³¼ Bounding block ë¬¸ì œ ì…ë‹ˆë‹¤.

![Reference. Object Detection: Learning Efficient Object Detection Models with Knowledge Distillation [ChenÂ *et al.*, NeurIPS 2017]](../../images/lec09/object-detection-KD.gif)

[ì´ ì—°êµ¬](https://proceedings.neurips.cc/paper_files/paper/2017/file/e1e32e235eee1f970470a3a6658dfdd5-Paper.pdf)ëŠ” Classifcationê³¼ Background, Foreground ë¬¸ì œë¥¼ ìœ„í•´ ì„¸ ê°€ì§€ Lossë¥¼ ì‚¬ìš©í•©ë‹ˆë‹¤. í•˜ë‚˜ëŠ” Feature, ê·¸ë¦¬ê³  Output Logitì—ì„œ Background, Foregroundë¥¼ ê°ê° ë‹¤ë¥¸ Weightë¥¼ ì¤€ Cross Entropy, ë§ˆì§€ë§‰ì€ Bounded í•œ Regression Loss ì…ë‹ˆë‹¤. 

![Reference. Localization: Localization Distillation for Dense Object Detection [ZhengÂ *et al.*, CVPR 2022]](../../images/lec09/Untitled%2019.png)

ê·¸ëŸ¼ Bounding block ì€ ì–´ë–»ê²Œ í•´ê²°í• ê¹Œìš”?  [ì´ ë…¼ë¬¸](https://arxiv.org/pdf/2102.12252.pdf)ì—ì„œëŠ” Xì¶•ê³¼ Yì¶•ìœ¼ë¡œ 6ê°œë¡œ ë‚˜ëˆ ì§„ êµ¬ì—­ì—ì„œ ë‘ ì ìœ¼ë¡œ bounding blockì„ ì¡ìŠµë‹ˆë‹¤.  ì´ë ‡ê²Œ ì¡ì€ Bounding blockì˜ ë¶„í¬ë¥¼ Student Networkê°€ í•™ìŠµí•˜ëŠ” ê²ë‹ˆë‹¤.

![Reference. Localization: Localization Distillation for Dense Object Detection [ZhengÂ *et al.*, CVPR 2022]](../../images/lec09/Lecture_09-Knowledge-Distillation_(4).jpg)

### 4.2 Semantic Segmentation

ë‘ ë²ˆì§¸ Taskì¸ Semantic Segmentationì—ì„œëŠ” Featureì™€ Output Logitì—ì„œ **Pixel ë‹¨ìœ„ë¡œ Loss**ë¥¼ êµ¬í•œ ë‹¤ëŠ” ì , ê·¸ë¦¬ê³  **Discriminator ëª¨ë¸**ì„ ê°€ì§€ê³  í•™ìŠµì„ ì‹œí‚¨ë‹¤ëŠ” ì ì´ ë”í•´ì¡ŒìŠµë‹ˆë‹¤.

![Reference. Semantic Segmentation: Structured Knowledge Distillation for Semantic Segmentation [LiuÂ *et al.*, CVPR 2019]](../../images/lec09/Untitled%2020.png)

### 4.3 GAN

ì„¸ë²ˆì§¸ TaskëŠ” GAN ì…ë‹ˆë‹¤. ë§¤ Task ë§ˆë‹¤ feature mapì„ KD-lossë¡œ ê°€ì ¸ê°€ê³  ê¸°ì¡´ì— Output Logitì€ ë™ì¼í•˜ê²Œ ê°€ì ¸ê°€ë„¤ìš”. ì¶”ê°€ë¡œ í•´ë‹¹ ì—°êµ¬ì—ì„œëŠ” ê° ë ˆì´ì–´ë§ˆë‹¤ ì±„ë„ ìˆ˜ ì¤‘ ì„±ëŠ¥ì´ ê°€ì¥ ì¢‹ì€ ì¼€ì´ìŠ¤ì— í•œí•´ Fine-Tuningì„ ì§„í–‰í•œë‹¤ëŠ” ì ì´ ìˆìŠµë‹ˆë‹¤.

![Reference. GAN: GAN Compression: Efficient Architectures for Interactive Conditional GANs [LiÂ *et al.*, CVPR 2020]](../../images/lec09/GAN-KD.gif)

### 4.4 Transformer

ë§ˆì§€ë§‰ì€ Transformer ëª¨ë¸ì—ì„œ Knowledge Distillation ì…ë‹ˆë‹¤. TransformerëŠ” Feature Map, Attention Mapì„ ì•ˆ ë³¼ ìˆ˜ê°€ ì—†ëŠ”ë°ìš”, ì•„ë˜ ê·¸ë¦¼ì—ì„œ ë³´ë©´ attention transferë¥¼ í•˜ê³  í•˜ì§€ ì•Šì€ ê²½ìš°í•˜ê³  í™•ì‹¤íˆ Teacherì™€ Attention mapê°€ ë¹„êµê°€ ë˜ë„¤ìš”.

![Refernece. NLP: MobileBERT: a Compact Task-Agnostic BERT for Resource-Limited Devices [SunÂ *et al.*, ACL 2020]](../../images/lec09/Transformer-KD.gif)

## 5. Network Augmentation, **a training technique for tiny machine learning models.**

ì§€ê¸ˆê¹Œì§€ Taskì— ëŒ€í•´ì„œ ì‚´í´ë´¤ëŠ”ë°ìš”, ê·¸ëŸ¼ Tiny Modelë„ overfitting ë¬¸ì œê°€ ìˆì§€ ì•Šì„ê¹Œìš”? ê·¸ë ˆì„œ overfittingì„ í•´ê²°í•˜ëŠ” ë°©ë²•ì—ëŠ” Data Augmentationì´ ìˆìŠµë‹ˆë‹¤. ì•„ë˜ ê·¸ë¦¼ ì²˜ëŸ¼ Cutoff, Mixup, AutoAugment, Dropoutê³¼ ê°™ì€ ë°©ë²•ë“¤ì´ ìˆìŠµë‹ˆë‹¤.  

![Reference. Data Augmentation(AutoAugment: Learning Augmentation Policies from Data [CubukÂ *et al.*, CVPR 2019]) ](../../images/lec09/Untitled%2021.png)

![Reference. Dropout(DropBlock: A regularization method for convolutional networks [GhiasiÂ *et al.*, NeurIPS 2018])](../../images/lec09/Untitled%2022.png)

í•˜ì§€ë§Œ Data Augmentationì„ ì ìš©í•œ Tiny Modelì˜ ì„±ëŠ¥ì„ ë³´ì‹œë©´ ì ìš©í•˜ëŠ” ë°©ë²•ë§ˆë‹¤ ë–¨ì–´ì§€ëŠ” ê²ƒì„ í™•ì¸í•  ìˆ˜ ìˆìŠµë‹ˆë‹¤. ê·¸ë˜ì„œ ì—¬ê¸°ì„œ ì œì•ˆí•œ ì•„ì´ë””ì–´ê°€ â€œ**Network Augmentation**â€ ì…ë‹ˆë‹¤.

![Reference. MIT-TinyML-lecture10-Knowledge-Distillation in [https://efficientml.ai](https://efficientml.ai/)](../../images/lec09/Untitled%2023.png)

- Tiny Neural Network lacks capacity! â†’ NetAug

### 5.2 Network Augmentation

Network Augmentationì€ ê¸°ì¡´ì— ë””ìì¸í•œ ëª¨ë¸ì„ ê°€ì§€ê³  í•™ìŠµì„ ì‹œí‚¨ í›„, ì› ëª¨ë¸ê³¼ **ê° ë ˆì´ì–´ë§ˆë‹¤ íŒŒë¼ë¯¸í„°ë¥¼ ë³€ê²½í•œ ëª¨ë¸ì„ í•¨ê»˜ ì¬í•™ìŠµì„ ì‹œí‚¤ëŠ” ë°©ë²•**ì…ë‹ˆë‹¤. íŒŒë¼ë¯¸í„°ë¥¼ ë³€ê²½í•˜ëŠ” ëª¨ë¸ê°™ì€ ê²½ìš° [ì´ì „ì‹œê°„ ì‹¤ìŠµ](https://tinyml-kor.github.io/blog/posts/labs/lab03.html)ì— ìˆìœ¼ë‹ˆ ê¶ê¸ˆí•˜ì‹œë©´ ì°¸ê³ í•´ì£¼ì„¸ìš”. ì‹¤í—˜ê²°ê³¼ëŠ” 1.3 ~ 1.8 % Tiny ëª¨ë¸ì´ ì„±ëŠ¥ ê°œì„ ì´ ì´ë¤„ì§„ ê²ƒì„ ë³¼ ìˆ˜ ìˆì–´ìš”. ì—¬ê¸°ì„œ ì› ëª¨ë¸(ResNet50)ì´ Evaluationì—ì„œëŠ” ì´ë¯¸ ê°€ì§„ ë°ì´í„°ë¡œ ëª¨ë¸ì„ ì¶©ë¶„íˆ í›ˆë ¨ì‹œì¼°ê¸° ë•Œë¬¸ì— ë”ì´ìƒ ëŠ˜ì–´ë‚˜ì§€ ì•ŠëŠ” ê²ƒë„ í™•ì¸í•  ìˆ˜ ìˆê² ë„¤ìš”.

- Training Process
    
    ![Reference. Network Augmentation for Tiny Deep Learning [CaiÂ *et al.*, ICLR 2022]](../../images/lec09/Untitled%2024.png)
    
    $$
    \mathscr{L}_{aug} = \mathscr{L}(W_{base}) + \alpha \mathscr{L}([W_{base}, W_{aug}])
    $$
    
    - $\mathscr{L}_{aug}$ = base supervision + $\alpha \cdot$auxiliary supervision

- Learning Curve
    
    ![Reference. Network Augmentation for Tiny Deep Learning [CaiÂ *et al.*, ICLR 2022]](../../images/lec09/Untitled%2025.png)
        
- Result
    
    ![Reference. Network Augmentation for Tiny Deep Learning [CaiÂ *et al.*, ICLR 2022]](../../images/lec09/Untitled%2026.png)
    
- Result for Transfer Learning
    
    ![Reference. Network Augmentation for Tiny Deep Learning [CaiÂ *et al.*, ICLR 2022]](../../images/lec09/Untitled%2027.png)
    
ì§€ê¸ˆê¹Œì§€ Knowledge Distilationì˜ ê¸°ë²•ë“¤ ê·¸ë¦¬ê³  ì´ë¥¼ ì´ìš©í•œ Appllcationì— ëŒ€í•´ì„œ ë‹¤ë¤„ë´¤ìŠµë‹ˆë‹¤. ë‹¤ìŒ ì‹œê°„ì—ëŠ” TinyEngineì„ ìœ„í•œ ìµœì í™” ê¸°ë²•ìœ¼ë¡œ ë‹¤ì‹œ ì°¾ì•„ì˜¬ê²Œìš” ğŸ™‚